SyntAct : A Synthesized Database of Basic Emotions .
Speech emotion recognition is in the focus of research since several decades and has many applications .
One problem is sparse data for supervised learning .
One way to tackle this problem is the synthesis of data with emotion - simulating speech synthesis approaches .
We present a synthesized database of five basic emotions and neutral expression based on rule - based manipulation for a diphone synthesizer which we release to the public .
The database has been validated in several machine learning experiments as a training set to detect emotional expression from natural speech data .
The scripts to generate such a database have been made open source and could be used to aid speech emotion recognition for a low resourced language , as MBROLA supports 35 languages .
Introduction .
The recognition of affect in speech is quite an old research field that gains momentum with the spread of vocal interfaces as numerous applications appear .
Examples are natural human machine interaction , gaming and the security domain .
The overwhelming majority of approaches to machine learning is still based on supervised learning , which is even stronger for emotional arousal as there is no clear definition , compared to other speech features like speaker age or textual contents .
Obviously , labeling emotional data manually is costly and methods to multiply existent data are needed .
With the dawn of modern deep learning based speech synthesizers , emotional expression is usually learned with the data , see Section 2 for some references .
About a decade ago , we have chosen a different approach by simulating emotional arousal with manually adapted prosody rules .
In ( Schuller and Burkhardt , 2010;Schuller et al . , 2012 ) , we already tried successfully to add the synthesized samples to the training of emotional recognition models .
We now re - synthesized five emotional categories plus neutral versions and published them to the research community .
We call the database " SyntAct " because it displays basic emotions with always the same prosodic expression , like a bad actor would do .
This paper describes the process of generation of the samples in Section 3 , the format of the database in Section 4 , and an evaluation experiment in Section 5 .
Contributions of this paper are : • We introduce a new dataset of simulated emotional expression that is available to the public .
• The simulation is based on rules that can target specific emotions .
• We release the scripts that generate the database as well so researchers can extend the data as needed .
• We evaluate the database with respect to its usefulness to train a machine learning model to detect prototypical emotional categories in natural data .
Related Work .
The idea to synthesize training data is not a new one .
Based on an existing training database , with deep learning techniques like variational autoencoders ( VAEs ) ( Baird et al . , 2019;Deng et al . , 2014;Deng et al . , 2013 ) or generative adversarial networks ( GANs ) ( Latif et al . , 2020;Eskimez et al . , 2020 ) , the generation of new training data has been realised .
In some of these approaches only non - audible acoustic parameters have been generated , in other ones , actual speech samples ( though not necessarily with semantic content ) .
An alternative approach is to generate new training data from scratch with traditional speech synthesis approaches .
In ( Baird et al . , 2018 ) , we investigated the likability and human likeness of synthesized speech in general and found that many systems are acceptable .
The approaches to synthesize speech can be categorized like this : • articulatory synthesis • formant synthesis • diphone synthesis • non - uniform unit selection synthesis • HMM based synthesis • deep learning based synthesis in the order of historic importance .
Basically , there has been a trade - off between flexibility and naturalness for the algorithms .
While formant synthesis for example is quite flexible with respect to signal manipulation , for the non - uniform unit - selection approach , all envisaged emotional expression must already be contained in the training database .
The highest quality of speech synthesizes approaches ( with respect to out - of - domain naturalness ) are these days based on artificial neural networks ( ANN ) under the label deep learning ( DL ) ( Zhou et al . , 2020 ) .
Although these DL based systems deliver very natural speech , it is difficult to determine which emotional expression will be simulated , as they usually are generated by manipulation of the latent space inside the network .
Preparing the Speech Samples .
The synthesised database consists of samples that were generated with the rule - based emotion simulation software " Emofilt " ( Burkhardt , 2005 ) .
It utilises the diphone speech synthesiser MBROLA ( Dutoit et al . , 1996 ) to generate a wave form from a phonetic description ( SAMPA symbols with duration values and fundamental frequency contours ) and the MARY textto - speech ( TTS ) system ( Schröder and Trouvain , 2003 ) to generate the neutral phonetic description from text .
Emofilt acts as a filter in between to ' emotionalise ' the neutral phonetic description ; the rules are based on a literature research described in ( Burkhardt , 2000 ) .
All six available German MBROLA voices -de2 , de4 , and de7 as female , and de1 , de3 , and de6 as male -were used .
Text Material .
With respect to emotional speech , usually three kinds of texts are distinguished : • Emotional texts , where the emotion is also expressed in the words .
This happens often in real world conversations .
• Mundane texts , where strong emotions seem inappropriate .
• Emotional arbitrary texts , that might indicate an emotional event but it 's not clear which emotion .
For the experiment at hand , the intention is to soften the problem of limited prosodic variability with a large number of different texts .
Therefore we utilized a German news corpus from the University of Leipzig 1 ( Goldhahn et al . , 2012 ) .
No special preprocessing was applied .
The following lists three sentences ( in the original language : German ) .
1 https://wortschatz.uni-leipzig.de ; we used the list " deu news 1995 10K-sentences.txt " .
Rule - Based Emotion Simulation .
Emofilt differentiates four different kinds of acoustic features : • Intonation : Here , we model intonation contours that can be specified for the whole phrase or for specific kinds of stressed syllables , with a special treatment of the last one .
• Duration : General duration as well as duration on syllable ( differentiated for stress type ) and phoneme level can be specified .
• Voice quality : Although voice quality is inherently fixed for diphone synthesis , some databases have voice quality variants ( Schröder and Grice , 2003 ) .
In addition , a simulation of jitter is achieved by shifting alternating F0 values .
• Articulation : Because with diphone synthesis a manipulation of format tracks is not directly possible , we achieve a simulation of articulatory effort by substituting tense with lax vowels in stressed syllables and vice versa , following an idea by ( Cahn , 2000 ) .
The exact values that we decided upon to generate the samples per emotion are detailed in the following subsections .
The scripts to generate such a database have been made open source and could be used to aid speech emotion recognition for a low resourced language , as MBROLA supports 35 languages 2 .
It must be noted though , that many MBROLA languages miss implementations of a natural language processing ( NLP ) component which means that " emotionally neutral " input samples would have to be specified in the native phonetic MBROLA format .
Also , for most languages , only two or even only one voice has been made publicly available .
Simulation of Sadness .
We applied the following configuration to simulate sadness : < p i t c h > < v a r i a b i l i t y r a t e = " 80 " / > < f 0 R a n g e r a t e = " 80 " / > < c o n t o u r F o c u s s t r e s s r a t e = " 30 " t y p e = " s t r a i g h t " / > < l a s t S y l C o n t o u r r a t e = " 10 " t y p e = " r i s e " / > < / p i t c h > < p h o n a t i o n > < j i t t e r r a t e = " 10 " / > < v o c a l E f f o r t e f f o r t = " s o f t " / > < / p h o n a t i o n > < d u r a t i o n > < s p e e c h R a t e r a t e = " 140 " / > < / d u r a t i o n > < a r t i c u l a t i o n > < v o w e l T a r g e t t a r g e t = " u n d e r s h o o t " / > < / a r t i c u l a t i o n > This means that the variability of F0 in general has been reduced to 80 % , the F0 contour of the stressed syllables is now straight and the last syllable rises by 10 % .
The F0 range has also been reduced by 20 % .
With respect to phoneme duration , the speech rate ( syllable per second ) has been made slower by 40 % .
The voice quality has been set to soft vocal effort , meaning that the respective samples from voices " de6 " and " de7 " were used .
Simulation of Happiness .
We decided on the following configuration to simulate happiness : < p i t c h > < f0Mean r a t e = " 120 " / > < f 0 R a n g e r a t e = " 130 " / > < c o n t o u r F o c u s s t r e s s r a t e = " 40 " t y p e = " r i s e " / > < l e v e l F o c u s s t r e s s r a t e = " 110 " / > < / p i t c h > < d u r a t i o n > < d u r V L F r i c r a t e = " 150 " / > < s p e e c h R a t e r a t e = " 70 " / > < durVowel r a t e = " 130 " / > < / d u r a t i o n > < p h o n a t i o n > < v o c a l E f f o r t e f f o r t = " l o u d " / > < / p h o n a t i o n > We enlarge the F0 range by 30 % and raise the whole contour by 20 % .
The stressed syllables are raised by additional 10 % .
The stressed syllables gets an upward pitch direction by 10 % .
The speech rate gets faster by 30 % in general , but voiceless fricatives and vowels get an extra speed accelerator by 50 and 30 % respectively .
The vocal effort gets stronger .
Simulation of Anger .
We applied the following configuration to simulate anger : < p i t c h > < f 0 R a n g e r a t e = " 140 " / > < l e v e l F o c u s S t r e s s r a t e = " 130 " / > < v a r i a b i l i t y r a t e = " 130 " / > < c o n t o u r F o c u s s t r e s s r a t e = " 10 " t y p e = " f a l l " / > < l e v e l F o c u s s t r e s s r a t e = " 130 " / > < / p i t c h > < d u r a t i o n > < durVowel r a t e = " 70 " / > < s p e e c h R a t e r a t e = " 70 " / > < d u r a t i o n F o c u s s t r e s s e d S y l s r a t e = " 130 " / > < / d u r a t i o n > < p h o n a t i o n > < v o c a l E f f o r t e f f o r t = " l o u d " / > < j i t t e r r a t e = " 2 " / > < / p h o n a t i o n > < a r t i c u l a t i o n > < v o w e l T a r g e t t a r g e t = " o v e r s h o o t " / > < / a r t i c u l a t i o n > To simulate anger the F0 range is compressed by 20 % , the contour of the stressed syllables gets a downwards direction and they are raised by 30 % .
The speech is made faster by 30 % for all non - stressed syllables whereas the stressed syllables are made longer by 30 % .
In addition we apply jitter simulation and the " loud " phonation type .
Simulation of Fear .
Additionally , we simulated two emotional states that were discussed in ( Burkhardt , 2000 ) , though we did not test them against real databases within the work reported here .
< p i t c h > < p h r a s e C o n t o u r r a t e = " 10 " t y p e = " r i s e " / > < c o n t o u r F o c u s s t r e s s r a t e = " 10 " t y p e = " s t r a i g h t " / > < l a s t S y l C o n t o u r r a t e = " 10 " t y p e = " r i s e " / > < f0Mean r a t e = " 200 " / > < / p i t c h > < d u r a t i o n > < s p e e c h R a t e r a t e = " 70 " / > < d u r a t i o n F o c u s s t r e s s e d S y l s r a t e = " 80"/ > < d u r P a u s e r a t e = " 200 " / > < / d u r a t i o n > < p h o n a t i o n > < j i t t e r r a t e = " 5 " / > < v o c a l E f f o r t e f f o r t = " l o u d " / > < / p h o n a t i o n > < a r t i c u l a t i o n > < v o w e l T a r g e t t a r g e t = " u n d e r s h o o t " / > < / a r t i c u l a t i o n > Fear is characterized by a rising phrase pitch contour , straight stressed syllables and an additional rise at the end .
The speech rate is faster , especially for the stressed syllables and the duration of pauses longer .
The articulation vowel target is undershot , meaning that stressed vowels get replaced by unstressed ones .
Simulation of Boredom .
The second additional emotion we simulate is boredom .
< p i t c h > < f0Mean r a t e = " 120 " / > < p h r a s e C o n t o u r r a t e = " 40 " t y p e = " f a l l " / > < / p i t c h > .
Description of the Database .
The database is downloadable 3 as a zip file .
The format of the data is in the audformat style being described in the next section .
The audformat Package .
audformat 4 defines an open format for storing media data , such as audio or video , together with corresponding annotations .
The format was designed to be universal enough to be applicable to as many use cases as possible , yet simple enough to be understood easily by a human and parsed efficiently by a machine .
A database in audformat consists of a header , which stores information about the database ( source , author , language , etc . ) , the type of media ( sampling rate , bit depth , etc . ) , the raters ( age , mother tongue , etc . ) , the schemes ( numerical , categories , text , etc . ) , and the splits ( train , test , etc . ) .
It also keeps reference of all tables that belong to the database , which hold the actual annotations and are stored in separate files in text and/or binary format .
A corresponding Python implementation 5 provides tools to access the data , create statistics , merge annotations , and search / filter information .
Specifics of the Database .
We generated in total 6000 samples with different textual content , for all six German voices ( de1 , de2 , de3 , de4 , de6 , de7 ) and each emotion .
There are two reasons why not all combinations could be synthesized : • For some phrases and emotions , the modifications led to the total elimination of phonemes and the result did not adhere to the phonotactics of the voice .
• In some cases , the MARY software , being used to generate the " neutral " phoneme version , ignored the phonotactics of the MBROLA voices .
Figure 2 shows a t - SNE plot ( van der Maaten and Hinton , 2008 ) for the eGeMAPS ( Eyben et al . , 2015 ) features , colored by intended emotion .
As can be seen by the colored clustered , the emotions can be separated based on acoustic features .
Evaluation .
With respect to evaluation two approaches make sense : • Evaluate the validity of the emotional expression by a human perception experiment .
neutral happiness sadness anger mean .6 .35 .5 .55 .5 Table 1 : Results ( total accuracy over all labels ) per emotion in the perception experiment .
• Evaluate the usefulness with respect to machine learning as a training set .
Perception Experiment .
We conducted a perception experiment after we defined the modification rules as described in the literature ( Burkhardt , 2000 , chapter 5 , pages 97 - 105 ) .
It was a forced choice listening experiment with 20 participants who listened to the stimuli in random order .
The results are displayed in Table 1 .
All emotions were recognized well above chance , the rather low value for happiness is mainly due to the fact that it was often confused with anger .
This confusion is often seen in the literature ( Yildirim et al . , 2004 ) and caused by a similar level of arousal .
Interestingly , the mean accuracy for all emotions in this perception experiment for the rule - based simulation was highest compared to four others that used prosody copy from actors of the Berlin emotional database ( Burkhardt et al . , 2005 ) .
Evaluation Databases .
We investigate the usefulness of the data as a training set for machine classifiers by setting up a series of experiments with databases displaying acted basic emotions .
Although these emotion expressions appear rarely in the real world , its detection still might be of practical value , as for example in Gaming scenarios or to teach children in the autism spectrum ( Burkhardt et al . , 2019 ) .
Figure 3 : Overview of databases with respect to basic emotion portrays We look at the following six databases from different countries : • ' emodb ' ( Germany ): The Berlin Emotional Speech Database ( emodb ) 6 ( Burkhardt et al . , 2005 ) is a well known studio recorded dataset .
• ' emovo ' ( Italy ): Italian Emotional Speech EMOVO 7 ( Costantini et al . , 2014 ) is a database consisting of the voices of six actors ( three female , three male ) who utter 14 Italian sentences simulating seven emotional states : anger , disgust , fear , joy , neutral , sadness , and surprise .
• ' ravdess ' ( USA ): The Ryerson Audio - Visual Database of Emotional Speech and Song ( ravdess ) 8 ( Livingstone and Russo , 2018 ) contains recordings of 24 professional actors ( 12 female , 12 male ) , vocalising two English statements in a neutral North American accent .
We excluded the songs .
• ' polish ' ( Poland ): The Database of Polish Emotional Speech ( Powroźnik , 2017 ) consists of speech from eight actors ( four female , four male ) .
Each speaker utters five different sentences with six types of emotional state : anger , boredom , fear , joy , neutral , and sadness .
• ' des ' ( Denmark ): The Danish Emotional Speech ( des ) ( Engberg et al . , 1997 ) database comprises acted emotions of four professional actors -two males and two females -for five emotional states : anger , happiness , neutral , sadness , and surprise .
• ' busim ' ( Turkey ): For the Turkish Emotional Database ( busim ) ( Kaya et al . , 2014 ) , eleven amateur actors ( eight female , three male ) provided eleven Turkish sentences with emotionally neutral content .
An overview of the databases is provided in Table 5.2 . We tested these databases with a subset of the synthesized data being used solely as training .
Therefore , all database emotion designations were mapped to the four target emotions of SyntAct , or removed if not part of the four target emotions .
The resulting distributions per emotion category can be seen in Figure 3 .
For the four target emotions ( angry , happy , neutral , and sad ) , out of the 1000 samples per speaker we selected randomly 30 samples per speaker and emotion , getting 720 samples with distinct texts .
We realize that emotional expression is culture and language specific ( Neumann and Vu , 2018;Feraru et al . , 2015;Burkhardt et al . , 2006;Scherer et al . , 1999 ) an emotion recognition classifier at least for European languages .
For the experiments we employed the Nkululeko framework 9 ( Burkhardt et al . , 2022 ) with an XGBoost classifier 10 ( Chen and Guestrin , 2016 ) with the default meta parameters ( eta = 0.3 , max depth = 6 , subsample = 1 ) .
This classifier is basically a very sophisticated algorithm based on classification trees and has been working quite well in many of our experiments ( Burkhardt et al . , 2021 ) .
As acoustic features , we used the eGeMAPS set ( Eyben et al . , 2015 ) , an expert set of 88 acoustic features for the openSMILE feature extractor ( Eyben et al . , 2010 ) that were optimised to work well to explain speaker characteristics and in particular emotions .
These features are being used in numerous articles in the literature as baseline features ( e.
g. , ( Ringeval et al . , 2018;Schuller et al . , 2016 ) ) as they work reasonably well with many tasks and are easy to handle for most classifiers based on their small number .
Results .
In Figure 4 and possible to detect emotional arousal with the database for several languages , but considering the results for the specific emotions in the figure , it is striking that the results depend extremely on which emotion is classified in which database .
For example , while the simulation of sadness does work quite well fort the Italian database ( emovo ) , this is not at all true for the Polish , the ravdess ( American English ) , and especially the busim ( Turkish ) databases .
On average , happiness simulation result in a much better model than sadness .
The German database shows the best performance and it is probably not by chance that the database is also German .
Although we did not use linguistic features , the expression of emotions is influenced by culture ( Scherer et al . , 1999;Burkhardt et al . , 2006;Barrett et al . , 2019 ) ..
As an example , we present in Figure 5 the confusion matrix for the Emodb database as a test set .
As can be seen , the classification mainly worked , especially well for happiness , which is in general the best working simulation based on Figure 4 .
" Angry " was often confused with happy , which is a quite typical confusion based on a similar level of arousal , but also with sadness , which we ca n't explain really .
" Neutral " was sometimes confused with sadness , and " sadness " consequently with " neutral " , probably based on the common low level of arousal .
Figure 5 : Example confusion matrix for Emodb database ( German ) as a test set .
Ethical Considerations and Broader Impact .
With respect to ethical considerations , generally it must be stated that the processing of emotional states is of great severity ( Batliner et al . , 2022 ) .
It should be made transparent to users of such technology that the attribution of emotional states based on human signals by machines based on statistics and pattern recognition is simply a substitute technology , as the true emotional state can never be inferred by others .
It is a large part of human - human communication and also humanmachine communication benefits , but severe decisions , that affect users well - being , should definitely not be based on this .
Nonetheless , we do believe that the interpretation of the emotional channel is important for natural humanmachine speech communication and hope , as stated above , that especially lower resourced languages might benefit from the idea to train emotion aware systems with simulated prosodic variation , which comes cheap and does not require much data .
Conclusion and Outlook .
We described a database of prototypical emotional expression in German that has been synthesized with rulebased speaking style modifications and made accessible to the public .
The application of this data to generate a training set for natural emotional expression has been investigated with six international databases .
With respect to the 35 languages the MBROLA supports , we plan to extend the database in the future .
Also the number of emotion portrayals may be extended .
A very interesting approach would be the simulation of emotional dimensions like pleasure , arousal , and dominance because on the one hand , many natural databases have been annotated with these dimensions ( Lotfian and Busso , 2017 ) , and on the other hand , the dimensions might be mapped flexible to specific categories , like for example " interest " .
At the time of writing , a first implementation of rule - based independent arousal and valence simulation has already been implemented and awaits evaluation experiments .
Acknowledgements .
This research has been partly funded by the European EASIER ( Intelligent Automatic Sign Language Translation ) project ( Grant Agreement number : 101016982 ) .
