Point Process Modelling of Rumour Dynamics in Social Media .
Rumours on social media exhibit complex temporal patterns .
This paper develops a model of rumour prevalence using a point process , namely a log - Gaussian Cox process , to infer an underlying continuous temporal probabilistic model of post frequencies .
To generalize over different rumours , we present a multi - task learning method parametrized by the text in posts which allows data statistics to be shared between groups of similar rumours .
Our experiments demonstrate that our model outperforms several strong baseline methods for rumour frequency prediction evaluated on tweets from the 2014 Ferguson riots .
Introduction .
The ability to model rumour dynamics helps with identifying those , which , if not debunked early , will likely spread very fast .
One such example is the false rumour of rioters breaking into McDonald 's during the 2011 England riots .
An effective early warning system of this kind is of interest to government bodies and news outlets , who struggle with monitoring and verifying social media posts during emergencies and social unrests .
Another application of modelling rumour dynamics could be to predict the prevalence of a rumour throughout its lifespan , based on occasional spot checks by journalists .
The challenge comes from the observation that different rumours exhibit different trajectories .
Figure 1 shows two example rumours from our dataset ( see Section 3 ): online discussion of rumour # 10 quickly drops away , whereas rumour # 37 takes a lot longer to die out .
Two characteristics can help determine if a rumour will continue to be discussed .
One is the dynamics of post occurrences , e.g. if the frequency profile decays quickly , chances are it would not attract further attention .
A second factor is text from the posts themselves , where phrases such as not true , unconfirmed , or debunk help users judge veracity and thus limit rumour spread ( Zhao et al . , 2015 ) .
This paper considers the problem of modelling temporal frequency profiles of rumours by taking into account both the temporal and textual information .
Since posts occur at continuous timestamps , and their density is typically a smooth function of time , we base our model on point processes , which have been shown to model well such data in epidemiology and conflict mapping ( Brix and Diggle , 2001;Zammit - Mangion et al . , 2012 ) .
This framework models count data in a continuous time through the underlying intensity of a Poisson distribution .
The posterior distribution can then be used for several inference problems , e.g. to query the expected count of posts , or to find the probability of a count of posts occurring during an arbitrary time interval .
We model frequency profiles using a log - Gaussian Cox process ( Møller and Syversveen , 1998 ) , a point process where the log - intensity of the Poisson distribution is modelled via a Gaussian Process ( GP ) .
GP is a nonparametric model which allows for powerful modelling of the underlying intensity function .
Modelling the frequency profile of a rumour based on posts is extremely challenging , since many rumours consist of only a small number of posts and exhibit complex patterns .
To overcome this difficulty we propose a multi - task learning approach , where patterns are correlated across multiple rumours .
In this way statistics over a larger training set are shared , enabling more reliable predictions for distant time periods , in which no posts from the target rumour have been observed .
We demonstrate how text from observed posts can be used to weight influence across rumours .
prediction of rumour popularity .
This paper makes the following contributions : 1 .
Introduces the problem of modelling rumour frequency profiles , and presents a method based on a log - Gaussian Cox process ; 2 .
Incorporates multi - task learning to generalize across disparate rumours ; and 3 .
Demonstrates how incorporating text into multi - task learning improves results .
Related Work .
There have been several descriptive studies of rumours in social media , e.g. Procter et al .
( 2013 ) analyzed rumours in tweets about the 2011 London riots and showed that they follow similar lifecycles .
Friggeri et al .
( 2014 ) showed how Facebook constitutes a rich source of rumours and conversation threads on the topic .
However , none of these studies tried to model rumour dynamics .
The problem of modelling the temporal nature of social media explicitly has received little attention .
The work most closely related modelled hash tag frequency time - series in Twitter using GP ( Preotiuc - Pietro and Cohn , 2013 ) .
It made several simplifications , including discretising time and treating the problem of modelling counts as regression , which are both inappropriate .
In contrast we take a more principled approach , using a point process .
We use the proposed GP - based method as a baseline to demonstrate the benefit of using our approaches .
The log - Gaussian Cox process has been applied for disease and conflict mapping , e.g. Zammit - Mangion et al .
( 2012 ) developed a spatio - temporal model of conflict events in Afghanistan .
In contrast here we deal with temporal text data , and model several correlated outputs rather than their single output .
Related also is the extensive work done in spatio - temporal modelling of meme spread .
One example is application of Hawkes processes ( Yang and Zha , 2013 ) , a probabilistic framework for modelling self - excitatory phenomena .
However , these models were mainly used for network modelling rather than revealing complex temporal patterns , which may emerge only implicitly , and are more limited in the kinds of temporal patterns that may be represented .
Data & Problem .
In this section we describe the data and we formalize the problem of modelling rumour popularity .
Data We use the Ferguson rumour data set ( Zubiaga et al . , 2015 ) , consisting of tweets collected in August and September 2014 during the Ferguson unrest .
It contains both source tweets and the conversational threads around these ( where available ) .
All source tweets are categorized as rumour vs non - rumour , other tweets from the same thread are assigned automatically as belonging to the same event as the source tweet .
Since some rumours have few posts , we consider only those with at least 15 posts in the first hour as rumours of particular interest .
This results in 114 rumours consisting of a total of 4098 tweets .
Problem Definition Let us consider a time interval .
[ 0 , l ] of length l=2 hours , a set of n rumours R = { E i } n i=1 , where rumour E i consists of a set of m i posts E i = { p i j } m i j=1 .
Posts are tuples p i j = ( x i j , t i j ) , where x i j is text ( in our case a bag of words text representation ) and t i j is a timestamp describing post p i j , measured in time elapsed since the first post on rumour E i .
Posts occur at different timestamps , yielding varying density of posts over time , which we are interested in estimating .
To evaluate the predicted density for a given rumour E i we leave out posts from a set of intervals T te = { [ s i k , e i k ] } K i k=1 ( where s i k and e i k are respectively start and end points of interval k for rumour i ) and estimate performance at predicting counts in them by the trained model .
The problem is considered in supervised settings , where posts on this rumour outside of these intervals form the training set E O i = { p i j : t i j ∈ K i k=1 [ s i k , e i k ] } .
Let the number of elements in E O i be m O i .
We also consider a domain adaptation setting , where additionally posts from other rumours are observed R O i = R\E i .
Two instantiations of this problem formulation are considered .
The first is interpolation , where the test intervals are not ordered in any particular way .
This corresponds to a situation , e.g. , when a journalist analyses a rumour during short spot checks , but wants to know the prevalence of the rumour at other times , thus limiting the need for constant attention .
The second formulation is that of extrapolation , where all observed posts occur before the test intervals .
This corresponds to a scenario where the user seeks to predict the future profile of the rumour , e.g. , to identify rumours that will attract further attention or wither away .
Although our focus here is on rumours , our model is more widely applicable .
For example , one could use it to predict whether an advertisement campaign would be successful or how a political campaign would proceed .
Model .
We consider a log - Gaussian Cox process ( LGCP ) ( Møller and Syversveen , 1998 ) , a generalization of inhomogeneous Poisson process .
In LGCP the intensity function is assumed to be a stochastic process which varies over time .
In fact , the intensity function λ(t ) is modelled using a latent function f ( t ) sampled from a Gaussian process ( Rasmussen and Williams , 2005 ) , such that λ(t ) = exp ( f ( t ) ) ( exponent ensures positivity ) .
This provides a non - parametric approach to model the intensity function .
The intensity function can be automatically learned from the data set and its complexity depends on the data points .
We model the occurrence of posts in a rumour E i to follow log - Gaussian Cox process ( LGCP ) with intensity λ i ( t ) , where λ i ( t ) = exp(f i ( t ) ) .
We associate a distinct intensity function with each rumour as they have varying temporal profiles .
LGCP models the likelihood that a single tweet occurs at time t in the interval [ s , t ] for a rumour E i given the latent function f i ( t ) as p(y = 1|f i ) = exp(f i ( t ) ) exp(− t s exp(f i ( u))du ) .
Then , the likelihood of posts E O i in time interval T given a latent function f i can be obtained as p(E O i |f i ) = exp   − T −Tte exp ( f i ( u ) ) du + m O i j=1 f i ( t i j )   ( 1 ) The likelihood of posts in the rumour data is obtained by taking the product of the likelihoods over individual rumours .
The likelihood ( 1 ) is commonly approximated by considering subregions of T and assuming constant intensities in sub - regions of T ( Møller and Syversveen , 1998;Vanhatalo et al . , 2013 ) to overcome computational difficulties arising due to integration .
Following this , we approximate the likelihood as p(E O i |f i ) = S s=1 Poisson(y s | l s exp f i ( ṫs ) ) .
Here , time is divided into S intervals indexed by s , ṫs is the centre of the s th interval , l s is the length of the s th interval and y s is number of tweets posted during this interval .
The latent function f is modelled via a Gaussian process ( GP ) ( Rasmussen and Williams , 2005 ): f ( t ) ∼ GP(m(t ) , k(t , t ) ) , where m is the mean function ( equal 0 ) and k is the kernel specifying how outputs covary as a function of the inputs .
We use a Radial Basis Function ( RBF ) kernel , k(t , t ) = a exp(−(t − t ) 2 /l ) , where lengthscale l controls the extent to which nearby points influence one another and a controls the scale of the function .
The distribution of the posterior p(f i ( t)|E O i ) at an arbitrary timestamp t is calculated based on the specified prior and the Poisson likelihood .
It is intractable and approximation techniques are required .
There exist various methods to deal with calculating the posterior ; here we use the Laplace approximation , where the posterior is approximated by a Gaussian distribution based on the first 2 moments .
For more details about the model and inference we refer the reader to ( Rasmussen and Williams , 2005 ) .
The predictive distribution over time t * is obtained using the approximated posterior .
This predictive distribution is then used to obtain the intensity function value at the point t * : λ i ( t * |E O i ) = exp ( f i ( t ) ) p f i ( t)|E O i df i .
The predictive distribution over counts at a particular time interval of length w with a mid - point t * for rumour E i is Poisson distributed with rate wλ i ( t * |E O i ) .
Multi - task learning and incorporating text In order to exploit similarities across rumours we propose a multi - task approach where each rumour represents a task .
We consider two approaches .
First , we employ a multiple output GP based on the Intrinsic Coregionalization Model ( ICM ) ( Álvarez et al . , 2012 ) .
It is a method which has been successfully applied to a range of NLP tasks ( Beck et al . , 2014;Cohn and Specia , 2013 ) .
ICM parametrizes the kernel by a matrix representing similarities between pairs of tasks .
We expect it to find correlations between rumours exhibiting similar temporal patterns .
The kernel takes the form k ICM ( ( t , i ) , ( t , i ) ) = k time ( t , t ) B i , i , where B is a square coregionalization matrix ( rank 1 , B = κI + vv T ) , i and i denote the tasks of the two inputs , k time is a kernel for comparing inputs t and t ( here RBF ) and κ is a vector of values modulating the extent of each task independence .
In a second approach , we parametrize the intertask similarity measures by incorporating text of the posts .
The full multi - task kernel takes form k TXT ( ( t , i ) , ( t , i ) ) = k time ( t , t ) × k text p i j ∈E O i x i j , p i j ∈E O i x i j .
We compare text vectors using cosine similarity , k text ( x , y ) = b + c x T y x y , where the hyperparameters b > 0 and c > 0 modulate between text similarity and a global constant similarity .
We also consider combining both multi - task kernels , yielding k ICM+TXT = k ICM + k TXT .
Optimization All hyperparameters are optimized by maximizing the marginal likelihood of the data L(E O i |θ ) , where θ = ( a , l , κ , v , b , c ) or a subset thereof , depending on the choice of kernel .
Experimental Setup .
Evaluation metric We use mean squared error ( MSE ) to measure the difference between true counts and predicted counts in the test intervals .
Since probabilistic models ( GP , LGCP ) return distributions over possible outputs , we also evaluate them via the log - likelihood ( LL ) of the true counts under the returned distributions ( respectively Gaussian and Poisson distribution ) .
Baselines We use the following baselines .
The first is the Homogenous Poisson Process ( HPP ) trained on the training set of the rumour .
We select its intensity λ using maximum likelihood estimate , which equals to the mean frequency of posts in the training intervals .
The second baseline is Gaussian Process ( GP ) used for predicting hashtag frequencies in Twitter by Preotiuc - Pietro and Cohn ( 2013 ) .
Authors considered various kernels in their experiments , most notably periodic kernels .
In our case it is not apparent that rumours exhibit periodic characteristics , as can be seen in Figure 1 .
We restrict our focus to RBF kernel and leave inspection of other kernels such as periodic ones for both GP and LGCP models for future .
The third baseline is to always predict 0 posts in all intervals .
The fourth baseline is tailored for the interpolation setting , and uses simple interpolation by averaging over the frequencies of the closest left and right intervals , or the frequency of the closest interval for test intervals on a boundary .
Data preprocessing .
In our experiments , we consider the first two hours of each rumour lifespan , which we split into 20 evenly spaced intervals .
This way , our dataset consists in total of 2280 intervals .
We iterate over rumours using a form of folded cross - validation , where in each iteration we exclude some ( but not all ) time intervals for a single target rumour .
The excluded time intervals form the test set : either by selecting half at random ( interpolation ) ; or by taking only the second half for testing ( extrapolation ) .
To ameliorate the problems of data sparsity , we replace words with their Brown cluster ids , using 1000 clusters acquired on a large scale Twitter corpus ( Owoputi et al . , 2013 ) .
The mean function for the underlying GP in LGCP methods is assumed to be 0 , which results in intensity function to be around 1 in the absence of nearby observations .
This prevents our method from predicting 0 counts in these regions .
We add 1 to the counts in the intervals to deal with this problem as a preprocessing step .
The original counts can be obtained by decrementing 1 from the predicted counts .
Instead , one could use a GP with a non - zero mean function and learn the mean function , a more elegant way of approaching this problem , which we leave for future work .
Experiments .
The left columns of proaches .
This is due to GP modelling a distribution with continuous support , which is inappropriate for modelling discrete counts .
Changing the model from a GP to a better fitting to the modelling temporal count data LGCP gives a big improvement , even when a point estimate of the prediction is considered ( MSE ) .
The 0 baseline is very strong , since many rumours have comparatively little discussion in the second hour of their lifespan relative to the first hour .
Incorporating information about other rumours helps outperform this method .
ICM , TXT and ICM+TXT multitask learning approaches achieve the best scores and significantly outperform all baselines .
TXT turns out to be a good approach to multi - task learning and outperforms ICM .
In Figure 1a we show an example rumour frequency profile for the extrapolation setting .
TXT makes a lower error than LGCP and LGCPICM , both of which underestimate the counts in the second hour .
Next , we move to the interpolation setting .
Unsurprisingly , Interpolate is the strongest baseline , and outperforms the raw LGCP method .
Again , HPP and GP are outperformed by LGCP in terms of both MSE and LL .
Considering the output distributions ( LL ) the difference in performance between the Poisson Process based approaches and GP is especially big , demonstrating how well the principled models handle uncertainty in the predictive distributions .
As for the multi - task methods , we notice that text is particularly useful , with TXT achieving the highest MSE score out of all considered models .
ICM turns out to be not very helpful in this setting .
For example , ICM ( just as LGCP ) does not learn there should be a peak at the beginning of a rumour frequency profile depicted in Figure 1b .
TXT manages to make a significantly smaller error by predicting a large posting frequency there .
We also found , that for a few rumours ICM made a big error by predicting a high frequency at the start of a rumour lifespan when there was no such peak .
We hypothesize ICM performs poorly because it is hard to learn correct correlations between frequency profiles when training intervals do not form continuous segments of significant sizes .
ICM manages to learn correlations more properly in extrapolation setting , where the first hour is fully observed .
Conclusions .
This paper introduced the problem of modelling frequency profiles of rumours in social media .
We demonstrated that joint modelling of collective data over multiple rumours using multi - task learning resulted in more accurate models that are able to recognise and predict commonly occurring temporal patterns .
We showed how text data from social media posts added important information about similarities between different rumours .
Our method is generalizable to problems other than modelling rumour popularity , such as predicting success of advertisement campaigns .
Acknowledgments .
We would like to thank Srijith P.
K.
for helpful comments .
This work was funded by the PHEME FP7 project ( grant No . 611233 ) and partially supported by the Australian Research Council .
