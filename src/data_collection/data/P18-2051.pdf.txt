Multi - representation Ensembles and Delayed SGD Updates Improve Syntax - based NMT .
We explore strategies for incorporating target syntax into Neural Machine Translation .
We specifically focus on syntax in ensembles containing multiple sentence representations .
We formulate beam search over such ensembles using WFSTs , and describe a delayed SGD update training procedure that is especially effective for long representations like linearized syntax .
Our approach gives state - of - the - art performance on a difficult Japanese - English task .
Introduction .
Ensembles of multiple NMT models consistently and significantly improve over single models ( Garmash and Monz , 2016 ) .
Previous work has observed that NMT models trained to generate target syntax can exhibit improved sentence structure ( Aharoni and Goldberg , 2017;Eriguchi et al . , 2017 ) relative to those trained on plain - text , while plain - text models produce shorter sequences and so may encode lexical information more easily ( Nadejde et al . , 2017 ) .
We hypothesize that an NMT ensemble would be strengthened if its component models were complementary in this way .
However , ensembling often requires component models to make predictions relating to the same output sequence position at each time step .
Models producing different sentence representations are necessarily synchronized to enable this .
We propose an approach to decoding ensembles of models generating different representations , focusing on models generating syntax .
As part of our investigation we suggest strategies for practical NMT with very long target sequences .
These long sequences may arise through the use of linearized constituency trees and can be much longer than their plain byte pair encoded ( BPE ) equivalent representations ( Table 1 ) .
Long sequences make training more difficult ( Bahdanau et al . , 2015 ) , which we address with an adjusted training procedure for the Transformer architecture ( Vaswani et al . , 2017 ) , using delayed SGD updates which accumulate gradients over multiple batches .
We also suggest a syntax representation which results in much shorter sequences .
Related Work .
Nadejde et al .
( 2017 ) perform NMT with syntax annotation in the form of Combinatory Categorial Grammar ( CCG ) supertags .
Aharoni and Goldberg ( 2017 ) translate from source BPE into target linearized parse trees , but omit POS tags to reduce sequence length .
They demonstrate improved target language reordering when producing syntax .
Eriguchi et al .
( 2017 ) combine recurrent neural network grammar ( RNNG ) models ( Dyer et al . , 2016 ) with attention - based models to produce well - formed dependency trees .
Wu et al .
( 2017 ) similarly produce both words and arcstandard algorithm actions ( Nivre , 2004 ) .
Previous approaches to ensembling diverse models focus on model inputs .
Hokamp ( 2017 ) shows improvements in the quality estimation task using ensembles of NMT models with multiple input representations which share an output representation .
Garmash and Monz ( 2016 ) show translation improvements with multi - source - language NMT ensembles .
Ensembles of Syntax Models .
We wish to ensemble using models which generate linearized constituency trees but these representations can be very long and difficult to model .
We therefore propose a derivation - based representation which is much more compact than a linearized parse tree ( examples in Table 1 ) .
Our linearized derivation representation ( ( 4 ) in Table 1 ) consists of the derivation 's right - hand side tokens with an end - of - rule marker , < /R > , marking the last non - terminal in each rule .
The original tree can be directly reproduced from the sequence , so that structure information is maintained .
We map words to subwords as described in Section 3 .
Delayed SGD Update Training for Long Sequences .
We suggest a training strategy for the Transformer model ( Vaswani et al . , 2017 ) which gives improved performance for long sequences , like syntax representations , without requiring additional GPU memory .
The Ten - sor2Tensor framework ( Vaswani et al . , 2018 ) defines batch size as the number of tokens per batch , so batches will contain fewer sequences if their average length increases .
During NMT training , by default , the gradients used to update model parameters are calculated over individual batches .
A possible consequence is that batches containing fewer sequences per update may have ' noisier ' estimated gradients than batches with more sequences .
Previous research has used very large batches to improve training convergence while requiring fewer model updates ( Smith et al . , 2017;Neishi et al . , 2017 ) .
However , with such large batches the model size may exceed available GPU memory .
Training on multiple GPUs is one way to increase the amount of data used to estimate gradients , but it requires significant resources .
Our strategy avoids this problem by using delayed SGD updates .
We accumulate gradients over a fixed number of batches before using the accumulated gradients to update the model 1 .
This lets us effectively use very large batch sizes without requiring multiple GPUs . Ensembling Representations .
Table 1 shows several different representations of the same hypothesis .
To formulate an ensembling decoder over pairs of these representations , we assume we have a transducer T that maps from one representation to the other representation .
The complexity of the transduction depends on the representations .
Mapping from word to BPE representations is straightforward , and mapping from ( linearized ) syntax to plain - text simply deletes non - terminals .
Let P be the paths in T leading from the start state to any final state .
A path ( Sennrich et al . , 2016 ) .
In practice , beam decoding is performed in the external representation , i.e. over projections of paths in P 2 .
Let h = h 1 .
h j be a partial hypothesis in the output representation .
The set of partial paths yielding h are : M ( h ) = ( 2 ) { ( x , y)|xyz ∈ P , o(x ) = h < j , o(xy ) = h } 2 See the tokenization wrappers in https:// github.com/ucam-smt/sgnmt Here z is the path suffix .
The ensembled score of h is then : P ( h j |h < j ) = P o ( h j |h < j ) × ( 3 ) max ( x , y)∈M ( h ) P i ( i(y)|i(x ) ) The max performed for each partial hypothesis h is itself approximated by a beam search .
This leads to an outer beam search over external representations with inner beam searches for the best matching internal representations .
As search proceeds , each model score is updated separately with its appropriate representation .
Symbols in the internal representation are consumed as needed to stay synchronized with the external representation , as illustrated in Figure 1 ; epsilons are consumed with a probability of 1 .
Experiments .
We first explore the effect of our delayed SGD update training scheme on single models , contrasting updates every batch with accumulated updates every 8 batches .
To compare target representations we train Transformer models with target representations ( 1 ) , ( 2 ) , ( 4 ) and ( 5 ) shown in Table 1 , using delayed SGD updates every 8 batches .
We decode with individual models and two - model ensembles , comparing results for single - representation and multi - representation ensembles .
Each multirepresentation ensemble consists of the plain BPE model and one other individual model .
All Transformer architectures are Ten - sor2Tensor 's base Transformer model ( Vaswani et al . , 2018 ) with a batch size of 4096 .
In all cases we decode using SGNMT ( Stahlberg et al . , 2017 ) with beam size 4 , using the average of the final 20 checkpoints .
For comparison with earlier target syntax work , we also train two RNN attention - based seq2seq models ( Bahdanau et al . , 2015 ) with normal SGD to produce plain BPE sequences and linearized derivations .
For these models we use embedding size 400 , a single BiLSTM layer of size 750 , and batch size 80 .
We report all experiments for Japanese - English , using the first 1 M training sentences of the Japanese - English ASPEC data ( Nakazawa et al . , 2016 ) .
All models use plain BPE Japanese source sentences .
English constituency trees are obtained using CKYlark ( Oda et al . , 2015 ) , with words replaced by BPE subwords .
We train separate Japanese ( lowercased ) and English ( cased ) BPE vocabularies on the plain - text , with 30 K merges each .
Non - terminals are included as separate tokens .
The linearized derivation uses additional tokens for non - terminals with < /R > .
Results and Discussion .
Our first results in Table 3 show that large batch training can significantly improve the performance of single Transformers , particularly when trained to produce longer sequences .
Accumulating the gradient over 8 batches of size 4096 gives a 3 BLEU improvement for the linear derivation model .
It has been suggested that decaying the learning rate can have a similar effect to large batch training ( Smith et al . , 2017 ) , but reducing the initial learning rate by a factor of 8 alone did not give the same improvements .
Our plain BPE baseline ( Table 4 ) outperforms the current best system on WAT Ja - En , an 8 - model ensemble ( Morishita et al . , 2017 ) .
Our syntax models achieve similar results despite producing much longer sequences .
Ensembles of two identical models trained with different seeds only slightly improve over the single model ( Table 5 ) .
However , an ensemble of models producing plain BPE and linearized derivations improves by 0.5 BLEU over the plain BPE baseline .
By ensembling syntax and plain - text we hope to benefit from their complementary strengths .
To highlight these , we examine hypotheses generated by the plain BPE and linearized derivation models .
We find that the syntax model is often more grammatical , even when the plain BPE model may share more vocabulary with the reference ( Table 2 ) .
In ensembling plain - text with a syntax external representation we observed that in a small proportion of cases non - terminals were over - generated , due to the mismatch in target sequence lengths .
Our solution was to penalise scores of non - terminals under the syntax model by a constant factor .
It is also possible to constrain decoding of linearized trees and derivations to wellformed outputs .
However , we found that this gives little improvement in BLEU over unconstrained decoding although it remains an interesting line of research .
Conclusions .
We report strong performance with individual models that meets or improves over the recent best WAT Ja - En ensemble results .
We train these models using a delayed SGD update training procedure that is especially effective for the long representations that arise from including target language syntactic information in the output .
We further improve on the individual results via a decoding strategy allowing ensembling of models producing different output representations , such as subword units and syntax .
We propose these techniques as practical approaches to including target syntax in NMT .
Acknowledgments .
This work was supported by EPSRC grant EP / L027623/1 .
