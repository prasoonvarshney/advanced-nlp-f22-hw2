-DOCSTART- -X- O
Conclusion -X- _ O
and -X- _ O
Future -X- _ O
Work -X- _ O
. -X- _ O

We -X- _ O
addressed -X- _ O
the -X- _ O
problem -X- _ O
of -X- _ O
utilizing -X- _ O
GNNs -X- _ B-MethodName
to -X- _ O
perform -X- _ O
relational -X- _ B-TaskName
reasoning -X- _ I-TaskName
with -X- _ I-TaskName
natural -X- _ I-TaskName
languages -X- _ I-TaskName
. -X- _ O

Our -X- _ O
proposed -X- _ O
model -X- _ O
, -X- _ O
GP -X- _ B-MethodName
- -X- _ I-MethodName
GNN -X- _ I-MethodName
, -X- _ O
solves -X- _ O
the -X- _ O
relational -X- _ B-TaskName
message -X- _ I-TaskName
- -X- _ I-TaskName
passing -X- _ I-TaskName
task -X- _ O
by -X- _ O
encoding -X- _ O
natural -X- _ O
language -X- _ O
as -X- _ O
parameters -X- _ O
and -X- _ O
performing -X- _ O
propagation -X- _ O
from -X- _ O
layer -X- _ O
to -X- _ O
layer -X- _ O
. -X- _ O

Our -X- _ O
model -X- _ O
can -X- _ O
also -X- _ O
be -X- _ O
considered -X- _ O
as -X- _ O
a -X- _ O
more -X- _ O
generic -X- _ O
framework -X- _ O
for -X- _ O
graph -X- _ B-TaskName
generation -X- _ I-TaskName
problem -X- _ I-TaskName
with -X- _ I-TaskName
unstructured -X- _ I-TaskName
input -X- _ I-TaskName
other -X- _ O
than -X- _ O
text -X- _ O
, -X- _ O
e.g. -X- _ O
image -X- _ O
, -X- _ O
video -X- _ O
, -X- _ O
audio -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
work -X- _ O
, -X- _ O
we -X- _ O
demonstrate -X- _ O
its -X- _ O
effectiveness -X- _ O
in -X- _ O
predicting -X- _ B-TaskName
the -X- _ I-TaskName
relationship -X- _ I-TaskName
between -X- _ I-TaskName
entities -X- _ I-TaskName
in -X- _ I-TaskName
natural -X- _ I-TaskName
language -X- _ I-TaskName
and -X- _ O
bag -X- _ B-TaskName
- -X- _ I-TaskName
level -X- _ I-TaskName
and -X- _ O
show -X- _ O
that -X- _ O
by -X- _ O
considering -X- _ O
more -X- _ O
hops -X- _ O
in -X- _ O
reasoning -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
relation -X- _ B-TaskName
extraction -X- _ I-TaskName
could -X- _ O
be -X- _ O
significantly -X- _ O
improved -X- _ O
. -X- _ O

Consequently -X- _ O
, -X- _ O
Context -X- _ B-MethodName
- -X- _ I-MethodName
Aware -X- _ I-MethodName
RE -X- _ I-MethodName
makes -X- _ O
a -X- _ O
mistake -X- _ O
by -X- _ O
predicting -X- _ O
( -X- _ O
Kentucky -X- _ O
, -X- _ O
share -X- _ O
boarder -X- _ O
with -X- _ O
, -X- _ O
Ohio -X- _ O
) -X- _ O
. -X- _ O

Ground -X- _ O
truth -X- _ O
graphs -X- _ O
are -X- _ O
the -X- _ O
subgraph -X- _ O
in -X- _ O
Wikidata -X- _ B-DatasetName
knowledge -X- _ O
graph -X- _ O
induced -X- _ O
by -X- _ O
the -X- _ O
sets -X- _ O
of -X- _ O
entities -X- _ O
in -X- _ O
the -X- _ O
sentences -X- _ O
. -X- _ O

Table -X- _ O
4 -X- _ O
: -X- _ O
Sample -X- _ O
predictions -X- _ O
from -X- _ O
the -X- _ O
baseline -X- _ O
models -X- _ O
and -X- _ O
our -X- _ O
GP -X- _ B-MethodName
- -X- _ I-MethodName
GNN -X- _ I-MethodName
model -X- _ O
. -X- _ O

We -X- _ O
also -X- _ O
find -X- _ O
that -X- _ O
Context -X- _ B-MethodName
- -X- _ I-MethodName
Aware -X- _ I-MethodName
RE -X- _ I-MethodName
tends -X- _ O
to -X- _ O
predict -X- _ O
relations -X- _ O
with -X- _ O
similar -X- _ O
topics -X- _ O
. -X- _ O

Note -X- _ O
that -X- _ O
( -X- _ O
BankUnited -X- _ O
Center -X- _ O
, -X- _ O
located -X- _ O
in -X- _ O
, -X- _ O
English -X- _ O
) -X- _ O
is -X- _ O
even -X- _ O
not -X- _ O
in -X- _ O
Wikidata -X- _ B-DatasetName
, -X- _ O
but -X- _ O
our -X- _ O
model -X- _ O
could -X- _ O
identify -X- _ O
this -X- _ O
fact -X- _ O
through -X- _ O
reasoning -X- _ O
. -X- _ O

In -X- _ O
the -X- _ O
first -X- _ O
case -X- _ O
, -X- _ O
GP -X- _ B-MethodName
- -X- _ I-MethodName
GNN -X- _ I-MethodName
implicitly -X- _ O
learns -X- _ O
a -X- _ O
logic -X- _ O
rule -X- _ O
∃y -X- _ O
, -X- _ O
x -X- _ O
( -X- _ O
BankUnited -X- _ O
Center -X- _ O
, -X- _ O
located -X- _ O
in -X- _ O
, -X- _ O
English -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
results -X- _ O
show -X- _ O
that -X- _ O
GP -X- _ B-MethodName
- -X- _ I-MethodName
GNN -X- _ I-MethodName
has -X- _ O
the -X- _ O
ability -X- _ O
to -X- _ O
infer -X- _ O
the -X- _ O
relationship -X- _ O
between -X- _ O
two -X- _ O
entities -X- _ O
with -X- _ O
reasoning -X- _ O
. -X- _ O

4 -X- _ O
shows -X- _ O
qualitative -X- _ O
results -X- _ O
that -X- _ O
compare -X- _ O
our -X- _ O
GP -X- _ B-MethodName
- -X- _ I-MethodName
GNN -X- _ I-MethodName
model -X- _ O
and -X- _ O
the -X- _ O
baseline -X- _ O
models -X- _ O
. -X- _ O

It -X- _ O
is -X- _ O
probably -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
reason -X- _ O
that -X- _ O
bag -X- _ B-TaskName
- -X- _ I-TaskName
level -X- _ I-TaskName
relation -X- _ I-TaskName
extraction -X- _ I-TaskName
is -X- _ O
much -X- _ O
easier -X- _ O
. -X- _ O

We -X- _ O
could -X- _ O
also -X- _ O
see -X- _ O
that -X- _ O
on -X- _ O
the -X- _ O
human -X- _ O
annotated -X- _ O
test -X- _ O
set -X- _ O
3layer -X- _ B-HyperparameterValue
version -X- _ O
to -X- _ O
have -X- _ O
a -X- _ O
greater -X- _ O
improvement -X- _ O
over -X- _ O
2 -X- _ B-HyperparameterValue
- -X- _ O
layer -X- _ B-HyperparameterName
version -X- _ O
as -X- _ O
compared -X- _ O
with -X- _ O
2 -X- _ B-HyperparameterValue
- -X- _ O
layer -X- _ B-HyperparameterName
version -X- _ O
over -X- _ O
1 -X- _ B-HyperparameterValue
- -X- _ O
layer -X- _ B-HyperparameterName
version -X- _ O
. -X- _ O

3 -X- _ O
that -X- _ O
as -X- _ O
the -X- _ O
number -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
layers -X- _ I-HyperparameterName
grows -X- _ O
, -X- _ O
the -X- _ O
curves -X- _ O
get -X- _ O
higher -X- _ O
and -X- _ O
higher -X- _ O
precision -X- _ O
, -X- _ O
indicating -X- _ O
considering -X- _ O
more -X- _ O
hops -X- _ O
in -X- _ O
reasoning -X- _ O
leads -X- _ O
to -X- _ O
better -X- _ O
performance -X- _ O
. -X- _ O

From -X- _ O
Table -X- _ O
2 -X- _ O
and -X- _ O
Table -X- _ O
3 -X- _ O
, -X- _ O
we -X- _ O
could -X- _ O
see -X- _ O
that -X- _ O
on -X- _ O
all -X- _ O
three -X- _ O
datasets -X- _ O
, -X- _ O
3 -X- _ B-HyperparameterValue
- -X- _ O
layer -X- _ B-HyperparameterName
version -X- _ O
achieves -X- _ O
the -X- _ O
best -X- _ O
. -X- _ O

To -X- _ O
demonstrate -X- _ O
the -X- _ O
effects -X- _ O
of -X- _ O
the -X- _ O
number -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
layers -X- _ I-HyperparameterName
, -X- _ O
we -X- _ O
also -X- _ O
compare -X- _ O
our -X- _ O
models -X- _ O
with -X- _ O
different -X- _ O
numbers -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
lay -X- _ I-HyperparameterName
- -X- _ I-HyperparameterName
ers -X- _ I-HyperparameterName
. -X- _ O

A -X- _ O
K -X- _ B-HyperparameterName
- -X- _ O
layer -X- _ O
version -X- _ O
has -X- _ O
the -X- _ O
ability -X- _ O
to -X- _ O
infer -X- _ O
K -X- _ B-HyperparameterName
- -X- _ O
hop -X- _ O
relations -X- _ O
. -X- _ O

The -X- _ O
number -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
layers -X- _ I-HyperparameterName
represents -X- _ O
the -X- _ O
reasoning -X- _ O
ability -X- _ O
of -X- _ O
our -X- _ O
models -X- _ O
. -X- _ O

The -X- _ O
Effectiveness -X- _ O
of -X- _ O
the -X- _ O
Number -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
Layers -X- _ I-HyperparameterName
. -X- _ O

( -X- _ O
2015 -X- _ O
) -X- _ O
formalize -X- _ O
the -X- _ O
bag -X- _ B-TaskName
- -X- _ I-TaskName
level -X- _ I-TaskName
relation -X- _ I-TaskName
extraction -X- _ I-TaskName
as -X- _ O
multi -X- _ O
- -X- _ O
instance -X- _ O
learning -X- _ O
. -X- _ O

To -X- _ O
evaluate -X- _ O
our -X- _ O
models -X- _ O
and -X- _ O
baseline -X- _ O
models -X- _ O
in -X- _ O
bag -X- _ B-TaskName
- -X- _ I-TaskName
level -X- _ I-TaskName
, -X- _ O
we -X- _ O
utilize -X- _ O
a -X- _ O
bag -X- _ O
of -X- _ O
sentences -X- _ O
with -X- _ O
a -X- _ O
given -X- _ O
entity -X- _ O
pair -X- _ O
to -X- _ O
score -X- _ O
the -X- _ O
relations -X- _ O
between -X- _ O
them -X- _ O
. -X- _ O

So -X- _ O
far -X- _ O
, -X- _ O
we -X- _ O
have -X- _ O
only -X- _ O
talked -X- _ O
about -X- _ O
the -X- _ O
way -X- _ O
to -X- _ O
implement -X- _ O
sentence -X- _ B-TaskName
- -X- _ I-TaskName
level -X- _ I-TaskName
relation -X- _ I-TaskName
extraction -X- _ I-TaskName
. -X- _ O

We -X- _ O
have -X- _ O
also -X- _ O
tried -X- _ O
two -X- _ O
forms -X- _ O
of -X- _ O
adjacent -X- _ B-HyperparameterName
matrices -X- _ I-HyperparameterName
: -X- _ O
tied -X- _ B-HyperparameterValue
- -X- _ I-HyperparameterValue
weights -X- _ I-HyperparameterValue
( -X- _ O
set -X- _ O
A -X- _ O
( -X- _ O
n -X- _ O
) -X- _ O
= -X- _ O
A -X- _ O
( -X- _ O
n+1 -X- _ O
) -X- _ O
) -X- _ O
and -X- _ O
untied -X- _ B-HyperparameterValue
- -X- _ I-HyperparameterValue
weights -X- _ I-HyperparameterValue
. -X- _ O

We -X- _ O
select -X- _ O
non -X- _ B-HyperparameterName
- -X- _ I-HyperparameterName
linear -X- _ I-HyperparameterName
activation -X- _ I-HyperparameterName
functions -X- _ I-HyperparameterName
between -X- _ O
relu -X- _ B-HyperparameterValue
and -X- _ O
tanh -X- _ B-HyperparameterValue
, -X- _ O
and -X- _ O
select -X- _ O
d -X- _ B-HyperparameterName
n -X- _ I-HyperparameterName
among -X- _ O
{ -X- _ O
2 -X- _ B-HyperparameterValue
, -X- _ O
4 -X- _ B-HyperparameterValue
, -X- _ O
8 -X- _ B-HyperparameterValue
, -X- _ O
12 -X- _ B-HyperparameterValue
, -X- _ O
16 -X- _ B-HyperparameterValue
} -X- _ O
9 -X- _ O
. -X- _ O

GP -X- _ B-MethodName
- -X- _ I-MethodName
GNN -X- _ I-MethodName
with -X- _ O
K -X- _ B-HyperparameterName
= -X- _ O
2 -X- _ B-HyperparameterValue
or -X- _ O
K -X- _ B-HyperparameterName
= -X- _ O
3 -X- _ B-HyperparameterValue
layers -X- _ B-HyperparameterName
. -X- _ O

Bidirectional -X- _ B-MethodName
LSTM -X- _ I-MethodName
( -X- _ O
Schuster -X- _ O
and -X- _ O
Paliwal -X- _ O
, -X- _ O
1997 -X- _ O
) -X- _ O
could -X- _ O
be -X- _ O
seen -X- _ O
as -X- _ O
an -X- _ O
1 -X- _ B-HyperparameterValue
- -X- _ O
layer -X- _ B-HyperparameterName
variant -X- _ O
of -X- _ O
our -X- _ O
model -X- _ O
. -X- _ O

LSTM -X- _ B-MethodName
or -X- _ O
GP -X- _ B-MethodName
- -X- _ I-MethodName
GNN -X- _ I-MethodName
with -X- _ O
K -X- _ B-HyperparameterName
= -X- _ O
1 -X- _ B-HyperparameterValue
layer -X- _ B-HyperparameterName
. -X- _ O

For -X- _ O
CNN -X- _ B-MethodName
and -X- _ O
following -X- _ O
PCNN -X- _ B-MethodName
, -X- _ O
the -X- _ O
entity -X- _ O
markers -X- _ O
are -X- _ O
the -X- _ O
same -X- _ O
as -X- _ O
originally -X- _ O
proposed -X- _ O
in -X- _ O
Zeng -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

PCNN -X- _ B-MethodName
, -X- _ O
proposed -X- _ O
by -X- _ O
Zeng -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

( -X- _ O
2014 -X- _ O
) -X- _ O
, -X- _ O
our -X- _ O
implementation -X- _ O
, -X- _ O
follows -X- _ O
Nguyen -X- _ O
and -X- _ O
Grishman -X- _ O
( -X- _ O
2015 -X- _ O
) -X- _ O
, -X- _ O
concatenates -X- _ O
features -X- _ O
extracted -X- _ O
by -X- _ O
three -X- _ O
different -X- _ O
window -X- _ B-HyperparameterName
sizes -X- _ I-HyperparameterName
: -X- _ O
3 -X- _ B-HyperparameterValue
, -X- _ O
5 -X- _ B-HyperparameterValue
, -X- _ O
7 -X- _ B-HyperparameterValue
. -X- _ O

Different -X- _ O
from -X- _ O
the -X- _ O
original -X- _ O
version -X- _ O
of -X- _ O
CNN -X- _ B-MethodName
proposed -X- _ O
in -X- _ O
Zeng -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

Multi -X- _ B-MethodName
- -X- _ I-MethodName
Window -X- _ I-MethodName
CNN -X- _ I-MethodName
. -X- _ O

It -X- _ O
was -X- _ O
the -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
models -X- _ O
on -X- _ O
Wikipedia -X- _ B-DatasetName
dataset -X- _ I-DatasetName
. -X- _ O

Context -X- _ B-MethodName
- -X- _ I-MethodName
Aware -X- _ I-MethodName
RE -X- _ I-MethodName
, -X- _ O
proposed -X- _ O
by -X- _ O
Sorokin -X- _ O
and -X- _ O
Gurevych -X- _ O
( -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
further -X- _ O
split -X- _ O
a -X- _ O
dense -X- _ O
test -X- _ O
set -X- _ O
from -X- _ O
the -X- _ O
distantly -X- _ B-DatasetName
labeled -X- _ I-DatasetName
test -X- _ O
set -X- _ O
. -X- _ O

Dense -X- _ O
distantly -X- _ B-DatasetName
labeled -X- _ I-DatasetName
test -X- _ O
set -X- _ O
. -X- _ O

Human -X- _ B-DatasetName
annotated -X- _ I-DatasetName
test -X- _ O
set -X- _ O
Based -X- _ O
on -X- _ O
the -X- _ O
test -X- _ O
set -X- _ O
provided -X- _ O
by -X- _ O
( -X- _ O
Sorokin -X- _ O
and -X- _ O
Gurevych -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
, -X- _ O
5 -X- _ O
annotators -X- _ O
6 -X- _ O
are -X- _ O
asked -X- _ O
to -X- _ O
label -X- _ O
the -X- _ O
dataset -X- _ O
. -X- _ O

There -X- _ O
is -X- _ O
a -X- _ O
small -X- _ O
difference -X- _ O
between -X- _ O
our -X- _ O
task -X- _ O
and -X- _ O
theirs -X- _ O
: -X- _ O
our -X- _ O
task -X- _ O
is -X- _ O
to -X- _ O
extract -X- _ B-TaskName
the -X- _ I-TaskName
relationship -X- _ I-TaskName
between -X- _ I-TaskName
every -X- _ I-TaskName
pair -X- _ I-TaskName
of -X- _ I-TaskName
entities -X- _ I-TaskName
in -X- _ I-TaskName
the -X- _ I-TaskName
sentence -X- _ I-TaskName
, -X- _ O
whereas -X- _ O
their -X- _ O
task -X- _ O
is -X- _ O
to -X- _ O
extract -X- _ O
the -X- _ O
relationship -X- _ O
between -X- _ O
the -X- _ O
given -X- _ O
entity -X- _ O
pair -X- _ O
and -X- _ O
the -X- _ O
context -X- _ O
entity -X- _ O
pairs -X- _ O
. -X- _ O

Distantly -X- _ O
labeled -X- _ O
set -X- _ O
Sorokin -X- _ O
and -X- _ O
Gurevych -X- _ O
( -X- _ O
2017 -X- _ O
) -X- _ O
have -X- _ O
proposed -X- _ O
a -X- _ O
dataset -X- _ O
with -X- _ O
Wikipedia -X- _ B-DatasetName
corpora -X- _ I-DatasetName
. -X- _ O

In -X- _ O
both -X- _ O
part -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
and -X- _ O
part -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
do -X- _ O
three -X- _ O
subparts -X- _ O
of -X- _ O
experiments -X- _ O
: -X- _ O
( -X- _ O
i -X- _ O
) -X- _ O
we -X- _ O
will -X- _ O
first -X- _ O
show -X- _ O
that -X- _ O
our -X- _ O
models -X- _ O
could -X- _ O
improve -X- _ O
instance -X- _ B-TaskName
- -X- _ I-TaskName
level -X- _ I-TaskName
relation -X- _ I-TaskName
extraction -X- _ I-TaskName
on -X- _ O
a -X- _ O
human -X- _ B-DatasetName
annotated -X- _ I-DatasetName
test -X- _ O
set -X- _ O
, -X- _ O
and -X- _ O
( -X- _ O
ii -X- _ O
) -X- _ O
then -X- _ O
we -X- _ O
will -X- _ O
show -X- _ O
that -X- _ O
our -X- _ O
models -X- _ O
could -X- _ O
also -X- _ O
help -X- _ O
enhance -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
bag -X- _ B-TaskName
- -X- _ I-TaskName
level -X- _ I-TaskName
relation -X- _ I-TaskName
extraction -X- _ I-TaskName
on -X- _ O
a -X- _ O
distantly -X- _ B-DatasetName
labeled -X- _ I-DatasetName
test -X- _ O
set -X- _ O
4 -X- _ O
, -X- _ O
and -X- _ O
( -X- _ O
iii -X- _ O
) -X- _ O
we -X- _ O
also -X- _ O
split -X- _ O
a -X- _ O
subset -X- _ O
of -X- _ O
distantly -X- _ B-DatasetName
labeled -X- _ I-DatasetName
test -X- _ O
set -X- _ O
, -X- _ O
where -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
entities -X- _ O
and -X- _ O
edges -X- _ O
is -X- _ O
large -X- _ O
. -X- _ O

Our -X- _ O
experiments -X- _ O
mainly -X- _ O
aim -X- _ O
at -X- _ O
: -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
showing -X- _ O
that -X- _ O
our -X- _ O
best -X- _ O
models -X- _ O
could -X- _ O
improve -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
relation -X- _ B-TaskName
extraction -X- _ I-TaskName
under -X- _ O
a -X- _ O
variety -X- _ O
of -X- _ O
settings -X- _ O
; -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
illustrating -X- _ O
that -X- _ O
how -X- _ O
the -X- _ O
number -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
layers -X- _ I-HyperparameterName
affect -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
our -X- _ O
model -X- _ O
; -X- _ O
and -X- _ O
( -X- _ O
3 -X- _ O
) -X- _ O
performing -X- _ O
a -X- _ O
qualitative -X- _ O
investigation -X- _ O
to -X- _ O
highlight -X- _ O
the -X- _ O
difference -X- _ O
between -X- _ O
our -X- _ O
models -X- _ O
and -X- _ O
baseline -X- _ O
models -X- _ O
. -X- _ O

We -X- _ O
treat -X- _ O
K -X- _ B-HyperparameterName
as -X- _ O
a -X- _ O
hyperparameter -X- _ O
, -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
which -X- _ O
will -X- _ O
be -X- _ O
discussed -X- _ O
in -X- _ O
detail -X- _ O
( -X- _ O
Sect -X- _ O
. -X- _ O

In -X- _ O
general -X- _ O
graphs -X- _ O
, -X- _ O
the -X- _ O
number -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
layers -X- _ I-HyperparameterName
K -X- _ O
is -X- _ O
chosen -X- _ O
to -X- _ O
be -X- _ O
of -X- _ O
the -X- _ O
order -X- _ O
of -X- _ O
the -X- _ O
graph -X- _ O
diameter -X- _ O
so -X- _ O
that -X- _ O
all -X- _ O
nodes -X- _ O
obtain -X- _ O
information -X- _ O
from -X- _ O
the -X- _ O
entire -X- _ O
graph -X- _ O
. -X- _ O

Number -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
Layers -X- _ I-HyperparameterName
. -X- _ O

In -X- _ O
our -X- _ O
experiments -X- _ O
, -X- _ O
we -X- _ O
generalize -X- _ O
the -X- _ O
idea -X- _ O
of -X- _ O
Gated -X- _ B-MethodName
Graph -X- _ I-MethodName
Neural -X- _ I-MethodName
Networks -X- _ I-MethodName
( -X- _ O
Li -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
by -X- _ O
setting -X- _ O
a -X- _ O
subject -X- _ B-HyperparameterName
= -X- _ O
[ -X- _ B-HyperparameterValue
1 -X- _ I-HyperparameterValue
; -X- _ I-HyperparameterValue
0 -X- _ I-HyperparameterValue
] -X- _ I-HyperparameterValue
and -X- _ O
a -X- _ O
object -X- _ B-HyperparameterName
= -X- _ O
[ -X- _ B-HyperparameterValue
0 -X- _ I-HyperparameterValue
; -X- _ I-HyperparameterValue
1 -X- _ I-HyperparameterValue
] -X- _ I-HyperparameterValue
3 -X- _ O
. -X- _ O

To -X- _ O
encode -X- _ O
the -X- _ O
context -X- _ O
of -X- _ O
entity -X- _ O
pairs -X- _ O
( -X- _ O
or -X- _ O
edges -X- _ O
in -X- _ O
the -X- _ O
graph -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
first -X- _ O
concatenate -X- _ O
the -X- _ O
position -X- _ O
embeddings -X- _ O
with -X- _ O
word -X- _ O
embeddings -X- _ O
in -X- _ O
the -X- _ O
sentence -X- _ O
: -X- _ O
E(x -X- _ O
i -X- _ O
, -X- _ O
j -X- _ O
t -X- _ O
) -X- _ O
= -X- _ O
[ -X- _ O
x -X- _ O
t -X- _ O
; -X- _ O
p -X- _ O
i -X- _ O
, -X- _ O
j -X- _ O
t -X- _ O
] -X- _ O
, -X- _ O
( -X- _ O
4 -X- _ O
) -X- _ O
where -X- _ O
x -X- _ O
t -X- _ O
denotes -X- _ O
the -X- _ O
word -X- _ O
embedding -X- _ O
of -X- _ O
word -X- _ O
x -X- _ O
t -X- _ O
and -X- _ O
p -X- _ O
i -X- _ O
, -X- _ O
j -X- _ O
t -X- _ O
denotes -X- _ O
the -X- _ O
position -X- _ O
embedding -X- _ O
of -X- _ O
word -X- _ O
position -X- _ O
t -X- _ O
relative -X- _ O
to -X- _ O
the -X- _ O
entity -X- _ O
pair -X- _ O
's -X- _ O
position -X- _ O
i -X- _ O
, -X- _ O
j -X- _ O
( -X- _ O
Details -X- _ O
of -X- _ O
these -X- _ O
two -X- _ O
embeddings -X- _ O
are -X- _ O
introduced -X- _ O
in -X- _ O
the -X- _ O
next -X- _ O
two -X- _ O
paragraphs -X- _ O
. -X- _ O
) -X- _ O
After -X- _ O
that -X- _ O
, -X- _ O
we -X- _ O
feed -X- _ O
the -X- _ O
representations -X- _ O
of -X- _ O
entity -X- _ O
pairs -X- _ O
into -X- _ O
encoder -X- _ O
f -X- _ O
( -X- _ O
• -X- _ O
) -X- _ O
which -X- _ O
contains -X- _ O
a -X- _ O
bi -X- _ B-MethodName
- -X- _ I-MethodName
directional -X- _ I-MethodName
LSTM -X- _ I-MethodName
and -X- _ O
a -X- _ O
multilayer -X- _ B-MethodName
perceptron -X- _ I-MethodName
: -X- _ O
A -X- _ O
( -X- _ O
n -X- _ O
) -X- _ O
i -X- _ O
, -X- _ O
j -X- _ O
= -X- _ O
[ -X- _ O
MLPn(BiLSTMn((E(x -X- _ O
i -X- _ O
, -X- _ O
j -X- _ O
0 -X- _ O
) -X- _ O
, -X- _ O
E(x -X- _ O
i -X- _ O
, -X- _ O
j -X- _ O
1 -X- _ O
) -X- _ O
, -X- _ O
• -X- _ O
• -X- _ O
• -X- _ O
, -X- _ O
E(x -X- _ O
i -X- _ O
, -X- _ O
j -X- _ O
l−1 -X- _ O
) -X- _ O
) -X- _ O
] -X- _ O
, -X- _ O
( -X- _ O
5 -X- _ O
) -X- _ O
where -X- _ O
n -X- _ O
denotes -X- _ O
the -X- _ O
index -X- _ O
of -X- _ O
layer -X- _ O
1 -X- _ O
, -X- _ O
[ -X- _ O
• -X- _ O
] -X- _ O
means -X- _ O
reshaping -X- _ O
a -X- _ O
vector -X- _ O
as -X- _ O
a -X- _ O
matrix -X- _ O
, -X- _ O
BiLSTM -X- _ B-MethodName
encodes -X- _ O
a -X- _ O
sequence -X- _ O
by -X- _ O
concatenating -X- _ O
tail -X- _ O
hidden -X- _ O
states -X- _ O
of -X- _ O
the -X- _ O
forward -X- _ O
LSTM -X- _ B-MethodName
and -X- _ O
head -X- _ O
hidden -X- _ O
states -X- _ O
of -X- _ O
the -X- _ O
backward -X- _ O
LSTM -X- _ B-MethodName
together -X- _ O
and -X- _ O
MLP -X- _ B-MethodName
denotes -X- _ O
a -X- _ O
multilayer -X- _ B-MethodName
perceptron -X- _ I-MethodName
with -X- _ O
non -X- _ O
- -X- _ O
linear -X- _ O
activation -X- _ O
σ -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
section -X- _ O
, -X- _ O
we -X- _ O
will -X- _ O
introduce -X- _ O
how -X- _ O
to -X- _ O
apply -X- _ O
GP -X- _ B-MethodName
- -X- _ I-MethodName
GNNs -X- _ I-MethodName
to -X- _ O
relation -X- _ B-TaskName
extraction -X- _ I-TaskName
. -X- _ O

, -X- _ O
v -X- _ O
|Vs| -X- _ O
} -X- _ O
, -X- _ O
where -X- _ O
each -X- _ O
v -X- _ O
i -X- _ O
consists -X- _ O
of -X- _ O
one -X- _ O
or -X- _ O
a -X- _ O
sequence -X- _ O
of -X- _ O
tokens -X- _ O
, -X- _ O
relation -X- _ B-TaskName
extraction -X- _ I-TaskName
from -X- _ O
text -X- _ O
is -X- _ O
to -X- _ O
identify -X- _ O
the -X- _ O
pairwise -X- _ O
relationship -X- _ O
r -X- _ O
v -X- _ O
i -X- _ O
, -X- _ O
v -X- _ O
j -X- _ O
∈ -X- _ O
R -X- _ O
between -X- _ O
each -X- _ O
entity -X- _ O
pair -X- _ O
( -X- _ O
v -X- _ O
i -X- _ O
, -X- _ O
v -X- _ O
j -X- _ O
) -X- _ O
. -X- _ O

Relation -X- _ B-TaskName
Extraction -X- _ I-TaskName
with -X- _ O
GP -X- _ B-MethodName
- -X- _ I-MethodName
GNNs -X- _ I-MethodName
. -X- _ O
Relation -X- _ B-TaskName
extraction -X- _ I-TaskName
from -X- _ I-TaskName
text -X- _ I-TaskName
is -X- _ O
a -X- _ O
classic -X- _ O
natural -X- _ B-TaskName
language -X- _ I-TaskName
relational -X- _ I-TaskName
reasoning -X- _ I-TaskName
task -X- _ O
. -X- _ O

The -X- _ O
parameters -X- _ O
in -X- _ O
GP -X- _ B-MethodName
- -X- _ I-MethodName
GNNs -X- _ I-MethodName
are -X- _ O
trained -X- _ O
by -X- _ O
gradient -X- _ O
descent -X- _ O
methods -X- _ O
. -X- _ O

Therefore -X- _ O
, -X- _ O
the -X- _ O
loss -X- _ O
of -X- _ O
GP -X- _ B-MethodName
- -X- _ I-MethodName
GNNs -X- _ I-MethodName
could -X- _ O
be -X- _ O
calculated -X- _ O
as -X- _ O
L -X- _ O
= -X- _ O
g(h -X- _ O
0 -X- _ O
0:|V|−1 -X- _ O
, -X- _ O
h -X- _ O
1 -X- _ O
0:|V|−1 -X- _ O
, -X- _ O
. -X- _ O

The -X- _ O
encoding -X- _ O
module -X- _ O
converts -X- _ O
sequences -X- _ O
into -X- _ O
transition -X- _ O
matrices -X- _ O
corresponding -X- _ O
to -X- _ O
edges -X- _ O
, -X- _ O
i.e. -X- _ O
the -X- _ O
parameters -X- _ O
of -X- _ O
the -X- _ O
propagation -X- _ O
module -X- _ O
, -X- _ O
by -X- _ O
A -X- _ O
( -X- _ O
n -X- _ O
) -X- _ O
i -X- _ O
, -X- _ O
j -X- _ O
= -X- _ O
f -X- _ O
( -X- _ O
E(x -X- _ O
i -X- _ O
, -X- _ O
j -X- _ O
0 -X- _ O
) -X- _ O
, -X- _ O
E(x -X- _ O
i -X- _ O
, -X- _ O
j -X- _ O
1 -X- _ O
) -X- _ O
, -X- _ O
• -X- _ O
• -X- _ O
• -X- _ O
, -X- _ O
E(x -X- _ O
i -X- _ O
, -X- _ O
j -X- _ O
l−1 -X- _ O
) -X- _ O
; -X- _ O
θ -X- _ O
n -X- _ O
e -X- _ O
) -X- _ O
, -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
where -X- _ O
f -X- _ O
( -X- _ O
• -X- _ O
) -X- _ O
could -X- _ O
be -X- _ O
any -X- _ O
model -X- _ O
that -X- _ O
could -X- _ O
encode -X- _ O
sequential -X- _ O
data -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
LSTMs -X- _ B-MethodName
, -X- _ O
GRUs -X- _ B-MethodName
, -X- _ O
CNNs -X- _ B-MethodName
, -X- _ O
E(• -X- _ O
) -X- _ O
indicates -X- _ O
an -X- _ O
embedding -X- _ O
function -X- _ O
, -X- _ O
and -X- _ O
θ -X- _ O
n -X- _ O
e -X- _ O
denotes -X- _ O
the -X- _ O
parameters -X- _ O
of -X- _ O
the -X- _ O
encoding -X- _ O
module -X- _ O
of -X- _ O
n -X- _ O
- -X- _ O
th -X- _ O
layer -X- _ O
. -X- _ O

After -X- _ O
that -X- _ O
, -X- _ O
GP -X- _ B-MethodName
- -X- _ I-MethodName
GNNs -X- _ I-MethodName
employ -X- _ O
three -X- _ O
modules -X- _ O
including -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
encoding -X- _ O
module -X- _ O
, -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
propagation -X- _ O
module -X- _ O
and -X- _ O
( -X- _ O
3 -X- _ O
) -X- _ O
classification -X- _ O
module -X- _ O
to -X- _ O
process -X- _ O
relational -X- _ O
reasoning -X- _ O
, -X- _ O
as -X- _ O
shown -X- _ O
in -X- _ O
Fig -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
section -X- _ O
, -X- _ O
we -X- _ O
will -X- _ O
introduce -X- _ O
the -X- _ O
general -X- _ O
framework -X- _ O
of -X- _ O
GP -X- _ B-MethodName
- -X- _ I-MethodName
GNNs -X- _ I-MethodName
. -X- _ O
GP -X- _ B-MethodName
- -X- _ I-MethodName
GNNs -X- _ I-MethodName
first -X- _ O
build -X- _ O
a -X- _ O
fully -X- _ O
- -X- _ O
connected -X- _ O
graph -X- _ O
G -X- _ O
= -X- _ O
( -X- _ O
V -X- _ O
, -X- _ O
E -X- _ O
) -X- _ O
, -X- _ O
where -X- _ O
V -X- _ O
is -X- _ O
the -X- _ O
set -X- _ O
of -X- _ O
entities -X- _ O
, -X- _ O
and -X- _ O
each -X- _ O
edge -X- _ O
( -X- _ O
v -X- _ O
i -X- _ O
, -X- _ O
v -X- _ O
j -X- _ O
) -X- _ O
∈ -X- _ O
E -X- _ O
, -X- _ O
v -X- _ O
i -X- _ O
, -X- _ O
v -X- _ O
j -X- _ O
∈ -X- _ O
V -X- _ O
corresponds -X- _ O
to -X- _ O
a -X- _ O
sequence -X- _ O
s -X- _ O
= -X- _ O
x -X- _ O
i -X- _ O
, -X- _ O
j -X- _ O
0 -X- _ O
, -X- _ O
x -X- _ O
i -X- _ O
, -X- _ O
j -X- _ O
1 -X- _ O
, -X- _ O
. -X- _ O

Generated -X- _ O
Parameters -X- _ O
( -X- _ O
GP -X- _ B-MethodName
- -X- _ I-MethodName
GNNs -X- _ I-MethodName
) -X- _ O
We -X- _ O
first -X- _ O
define -X- _ O
the -X- _ O
task -X- _ O
of -X- _ O
natural -X- _ O
language -X- _ O
relational -X- _ O
reasoning -X- _ O
. -X- _ O

Graph -X- _ B-MethodName
Neural -X- _ I-MethodName
Network -X- _ I-MethodName
with -X- _ O
. -X- _ O

( -X- _ O
2018 -X- _ O
) -X- _ O
proposed -X- _ O
a -X- _ O
walk -X- _ O
- -X- _ O
based -X- _ O
model -X- _ O
to -X- _ O
do -X- _ O
relation -X- _ B-TaskName
extraction -X- _ I-TaskName
. -X- _ O

Miwa -X- _ O
and -X- _ O
Bansal -X- _ O
( -X- _ O
2016 -X- _ O
) -X- _ O
show -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
LSTMs -X- _ B-MethodName
( -X- _ O
Hochreiter -X- _ O
and -X- _ O
Schmidhuber -X- _ O
, -X- _ O
1997 -X- _ O
) -X- _ O
in -X- _ O
relation -X- _ B-TaskName
extraction -X- _ I-TaskName
. -X- _ O

( -X- _ O
2017 -X- _ O
) -X- _ O
show -X- _ O
that -X- _ O
the -X- _ O
relation -X- _ O
path -X- _ O
has -X- _ O
an -X- _ O
important -X- _ O
role -X- _ O
in -X- _ O
relation -X- _ B-TaskName
extraction -X- _ I-TaskName
. -X- _ O

( -X- _ O
2017 -X- _ O
) -X- _ O
predict -X- _ O
n -X- _ O
- -X- _ O
ary -X- _ O
relations -X- _ O
of -X- _ O
entities -X- _ O
in -X- _ O
different -X- _ O
sentences -X- _ O
with -X- _ O
Graph -X- _ B-MethodName
LSTMs -X- _ I-MethodName
. -X- _ O
Le -X- _ O
and -X- _ O
Titov -X- _ O
( -X- _ O
2018 -X- _ O
) -X- _ O
treat -X- _ O
relations -X- _ O
as -X- _ O
latent -X- _ O
variables -X- _ O
which -X- _ O
are -X- _ O
capable -X- _ O
of -X- _ O
inducing -X- _ O
the -X- _ O
relations -X- _ O
without -X- _ O
any -X- _ O
supervision -X- _ O
signals -X- _ O
. -X- _ O

( -X- _ O
2016 -X- _ O
) -X- _ O
study -X- _ O
an -X- _ O
attention -X- _ O
mechanism -X- _ O
for -X- _ O
relation -X- _ B-TaskName
extraction -X- _ I-TaskName
tasks -X- _ O
. -X- _ O

Nguyen -X- _ O
and -X- _ O
Grishman -X- _ O
( -X- _ O
2015 -X- _ O
) -X- _ O
propose -X- _ O
a -X- _ O
multi -X- _ O
- -X- _ O
window -X- _ O
version -X- _ O
of -X- _ O
CNN -X- _ B-MethodName
for -X- _ O
relation -X- _ B-TaskName
extraction -X- _ I-TaskName
. -X- _ O

( -X- _ O
2015 -X- _ O
) -X- _ O
further -X- _ O
extends -X- _ O
it -X- _ O
with -X- _ O
piece -X- _ B-MethodName
- -X- _ I-MethodName
wise -X- _ I-MethodName
maxpooling -X- _ I-MethodName
. -X- _ O

( -X- _ O
2014 -X- _ O
) -X- _ O
is -X- _ O
one -X- _ O
of -X- _ O
the -X- _ O
earliest -X- _ O
works -X- _ O
that -X- _ O
applies -X- _ O
a -X- _ O
simple -X- _ O
CNN -X- _ B-MethodName
to -X- _ O
this -X- _ O
task -X- _ O
, -X- _ O
and -X- _ O
Zeng -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
focus -X- _ O
on -X- _ O
the -X- _ O
relational -X- _ B-TaskName
reasoning -X- _ I-TaskName
in -X- _ O
the -X- _ O
natural -X- _ O
language -X- _ O
domain -X- _ O
. -X- _ O

Relational -X- _ B-TaskName
Reasoning -X- _ I-TaskName
. -X- _ O

Relational -X- _ B-TaskName
reasoning -X- _ I-TaskName
has -X- _ O
been -X- _ O
explored -X- _ O
in -X- _ O
various -X- _ O
fields -X- _ O
. -X- _ O

In -X- _ O
sharp -X- _ O
contrast -X- _ O
, -X- _ O
this -X- _ O
paper -X- _ O
focuses -X- _ O
on -X- _ O
extracting -X- _ B-TaskName
relations -X- _ I-TaskName
from -X- _ I-TaskName
real -X- _ I-TaskName
- -X- _ I-TaskName
world -X- _ I-TaskName
relation -X- _ I-TaskName
datasets -X- _ O
. -X- _ O

Although -X- _ O
they -X- _ O
also -X- _ O
consider -X- _ O
applying -X- _ O
GNNs -X- _ B-MethodName
to -X- _ O
natural -X- _ O
language -X- _ O
processing -X- _ O
tasks -X- _ O
, -X- _ O
they -X- _ O
still -X- _ O
perform -X- _ O
message -X- _ O
- -X- _ O
passing -X- _ O
on -X- _ O
predefined -X- _ O
graphs -X- _ O
. -X- _ O

( -X- _ O
2018 -X- _ O
) -X- _ O
apply -X- _ O
GNNs -X- _ B-MethodName
to -X- _ O
multi -X- _ O
- -X- _ O
hop -X- _ O
question -X- _ O
answering -X- _ O
by -X- _ O
encoding -X- _ O
co -X- _ O
- -X- _ O
occurence -X- _ O
and -X- _ O
coreference -X- _ O
relationships -X- _ O
. -X- _ O

( -X- _ O
2018 -X- _ O
) -X- _ O
apply -X- _ O
GNNs -X- _ B-MethodName
to -X- _ O
relation -X- _ O
extraction -X- _ O
by -X- _ O
encoding -X- _ O
dependency -X- _ O
trees -X- _ O
, -X- _ O
and -X- _ O
De -X- _ O
Cao -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

( -X- _ O
2017 -X- _ O
) -X- _ O
apply -X- _ O
GNNs -X- _ B-MethodName
to -X- _ O
knowledge -X- _ O
base -X- _ O
completion -X- _ O
tasks -X- _ O
. -X- _ O

For -X- _ O
example -X- _ O
, -X- _ O
Marcheggiani -X- _ O
and -X- _ O
Titov -X- _ O
( -X- _ O
2017 -X- _ O
) -X- _ O
propose -X- _ O
to -X- _ O
apply -X- _ O
GNNs -X- _ B-MethodName
to -X- _ O
semantic -X- _ O
role -X- _ O
labeling -X- _ O
and -X- _ O
Schlichtkrull -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

There -X- _ O
are -X- _ O
relatively -X- _ O
fewer -X- _ O
papers -X- _ O
discussing -X- _ O
how -X- _ O
to -X- _ O
adapt -X- _ O
GNNs -X- _ B-MethodName
to -X- _ O
natural -X- _ O
language -X- _ O
tasks -X- _ O
. -X- _ O

Garcia -X- _ O
and -X- _ O
Bruna -X- _ O
( -X- _ O
2018 -X- _ O
) -X- _ O
shows -X- _ O
how -X- _ O
to -X- _ O
use -X- _ O
GNNs -X- _ B-MethodName
to -X- _ O
learn -X- _ O
classifiers -X- _ O
on -X- _ O
image -X- _ O
datasets -X- _ O
in -X- _ O
a -X- _ O
few -X- _ O
- -X- _ O
shot -X- _ O
manner -X- _ O
. -X- _ O

( -X- _ O
2017 -X- _ O
) -X- _ O
propose -X- _ O
to -X- _ O
apply -X- _ O
GNNs -X- _ B-MethodName
to -X- _ O
molecular -X- _ O
property -X- _ O
prediction -X- _ O
tasks -X- _ O
. -X- _ O

( -X- _ O
2016 -X- _ O
) -X- _ O
replace -X- _ O
the -X- _ O
Almeida -X- _ O
- -X- _ O
Pineda -X- _ O
algorithm -X- _ O
with -X- _ O
the -X- _ O
more -X- _ O
generic -X- _ O
backpropagation -X- _ O
and -X- _ O
demonstrate -X- _ O
its -X- _ O
effectiveness -X- _ O
empirically -X- _ O
. -X- _ O

GNNs -X- _ B-MethodName
were -X- _ O
first -X- _ O
proposed -X- _ O
in -X- _ O
( -X- _ O
Scarselli -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2009 -X- _ O
) -X- _ O
and -X- _ O
are -X- _ O
trained -X- _ O
via -X- _ O
the -X- _ O
Almeida -X- _ O
- -X- _ O
Pineda -X- _ O
algorithm -X- _ O
( -X- _ O
Almeida -X- _ O
, -X- _ O
1987 -X- _ O
) -X- _ O
. -X- _ O

Graph -X- _ B-MethodName
Neural -X- _ I-MethodName
Networks -X- _ I-MethodName
( -X- _ O
GNNs -X- _ B-MethodName
) -X- _ O
. -X- _ O

( -X- _ O
2 -X- _ O
) -X- _ O
We -X- _ O
verify -X- _ O
our -X- _ O
GP -X- _ B-MethodName
- -X- _ I-MethodName
GNNs -X- _ I-MethodName
on -X- _ O
the -X- _ O
task -X- _ O
of -X- _ O
relation -X- _ B-TaskName
extraction -X- _ I-TaskName
from -X- _ I-TaskName
text -X- _ I-TaskName
, -X- _ O
which -X- _ O
demonstrates -X- _ O
its -X- _ O
ability -X- _ O
on -X- _ O
multi -X- _ O
- -X- _ O
hop -X- _ O
relational -X- _ O
reasoning -X- _ O
as -X- _ O
compared -X- _ O
to -X- _ O
those -X- _ O
models -X- _ O
which -X- _ O
extract -X- _ O
relationships -X- _ O
separately -X- _ O
. -X- _ O

Our -X- _ O
main -X- _ O
contributions -X- _ O
are -X- _ O
in -X- _ O
two -X- _ O
- -X- _ O
fold -X- _ O
: -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
We -X- _ O
extend -X- _ O
a -X- _ O
novel -X- _ O
graph -X- _ B-MethodName
neural -X- _ I-MethodName
network -X- _ I-MethodName
model -X- _ I-MethodName
with -X- _ I-MethodName
generated -X- _ I-MethodName
parameters -X- _ I-MethodName
, -X- _ O
to -X- _ O
enable -X- _ O
relational -X- _ O
message -X- _ O
- -X- _ O
passing -X- _ O
with -X- _ O
rich -X- _ O
text -X- _ O
information -X- _ O
, -X- _ O
which -X- _ O
could -X- _ O
be -X- _ O
applied -X- _ O
to -X- _ O
process -X- _ B-TaskName
relational -X- _ I-TaskName
reasoning -X- _ I-TaskName
on -X- _ O
unstructured -X- _ O
inputs -X- _ O
such -X- _ O
as -X- _ O
natural -X- _ O
language -X- _ O
. -X- _ O

We -X- _ O
carry -X- _ O
out -X- _ O
experiments -X- _ O
on -X- _ O
Wikipedia -X- _ B-DatasetName
corpus -X- _ I-DatasetName
aligned -X- _ I-DatasetName
with -X- _ I-DatasetName
Wikidata -X- _ I-DatasetName
knowledge -X- _ I-DatasetName
base -X- _ I-DatasetName
( -X- _ O
Vrandečić -X- _ O
and -X- _ O
Krötzsch -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
and -X- _ O
build -X- _ O
a -X- _ O
human -X- _ O
annotated -X- _ O
test -X- _ O
set -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
two -X- _ O
distantly -X- _ O
labeled -X- _ O
test -X- _ O
sets -X- _ O
with -X- _ O
different -X- _ O
levels -X- _ O
of -X- _ O
denseness -X- _ O
. -X- _ O
Experiment -X- _ O
results -X- _ O
show -X- _ O
that -X- _ O
our -X- _ O
model -X- _ O
outperforms -X- _ O
other -X- _ O
models -X- _ O
on -X- _ O
relation -X- _ B-TaskName
extraction -X- _ I-TaskName
task -X- _ I-TaskName
by -X- _ O
considering -X- _ O
multi -X- _ B-MethodName
- -X- _ I-MethodName
hop -X- _ I-MethodName
relational -X- _ I-MethodName
reasoning -X- _ I-MethodName
. -X- _ O

relation -X- _ B-TaskName
extraction -X- _ I-TaskName
from -X- _ O
text -X- _ O
. -X- _ O

Given -X- _ O
a -X- _ O
sentence -X- _ O
with -X- _ O
several -X- _ O
entities -X- _ O
marked -X- _ O
, -X- _ O
we -X- _ O
model -X- _ O
the -X- _ O
interaction -X- _ O
between -X- _ O
these -X- _ O
entities -X- _ O
by -X- _ O
generating -X- _ O
the -X- _ O
weights -X- _ O
of -X- _ O
graph -X- _ B-MethodName
neural -X- _ I-MethodName
networks -X- _ I-MethodName
. -X- _ O

Language -X- _ O
Spoken -X- _ O
Language -X- _ O
Cast -X- _ O
member -X- _ O
Figure -X- _ O
1 -X- _ O
: -X- _ O
An -X- _ O
example -X- _ O
of -X- _ O
relation -X- _ B-TaskName
extraction -X- _ I-TaskName
from -X- _ O
plain -X- _ O
text -X- _ O
. -X- _ O

In -X- _ O
the -X- _ O
experiments -X- _ O
, -X- _ O
we -X- _ O
apply -X- _ O
GP -X- _ B-MethodName
- -X- _ I-MethodName
GNNs -X- _ I-MethodName
to -X- _ O
a -X- _ O
classic -X- _ O
natural -X- _ O
language -X- _ O
relational -X- _ O
reasoning -X- _ O
task -X- _ O
: -X- _ O
Léon -X- _ O
: -X- _ O
The -X- _ O
Professional -X- _ O
is -X- _ O
a -X- _ O
1996 -X- _ O
English -X- _ O
- -X- _ O
language -X- _ O
French -X- _ O
thriller -X- _ O
film -X- _ O
directed -X- _ O
by -X- _ O
Luc -X- _ O
Besson -X- _ O
. -X- _ O

As -X- _ O
compared -X- _ O
to -X- _ O
traditional -X- _ O
GNNs -X- _ B-MethodName
, -X- _ O
GP -X- _ B-MethodName
- -X- _ I-MethodName
GNNs -X- _ I-MethodName
could -X- _ O
learn -X- _ O
edge -X- _ O
parameters -X- _ O
from -X- _ O
natural -X- _ O
languages -X- _ O
, -X- _ O
extending -X- _ O
it -X- _ O
from -X- _ O
performing -X- _ O
inference -X- _ O
on -X- _ O
only -X- _ O
non -X- _ O
- -X- _ O
relational -X- _ O
graphs -X- _ O
or -X- _ O
graphs -X- _ O
with -X- _ O
a -X- _ O
limited -X- _ O
number -X- _ O
of -X- _ O
edge -X- _ O
types -X- _ O
to -X- _ O
unstructured -X- _ O
inputs -X- _ O
such -X- _ O
as -X- _ O
texts -X- _ O
. -X- _ O

After -X- _ O
that -X- _ O
, -X- _ O
it -X- _ O
employs -X- _ O
three -X- _ O
modules -X- _ O
to -X- _ O
process -X- _ O
relational -X- _ B-TaskName
reasoning -X- _ I-TaskName
: -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
an -X- _ O
encoding -X- _ O
module -X- _ O
which -X- _ O
enables -X- _ O
edges -X- _ O
to -X- _ O
encode -X- _ O
rich -X- _ O
information -X- _ O
from -X- _ O
natural -X- _ O
languages -X- _ O
, -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
a -X- _ O
propagation -X- _ O
module -X- _ O
which -X- _ O
propagates -X- _ O
relational -X- _ O
information -X- _ O
among -X- _ O
various -X- _ O
nodes -X- _ O
, -X- _ O
and -X- _ O
( -X- _ O
3 -X- _ O
) -X- _ O
a -X- _ O
classification -X- _ O
module -X- _ O
which -X- _ O
makes -X- _ O
predictions -X- _ O
with -X- _ O
node -X- _ O
representations -X- _ O
. -X- _ O

GP -X- _ B-MethodName
- -X- _ I-MethodName
GNNs -X- _ I-MethodName
first -X- _ O
constructs -X- _ O
a -X- _ O
fullyconnected -X- _ O
graph -X- _ O
with -X- _ O
the -X- _ O
entities -X- _ O
in -X- _ O
the -X- _ O
sequence -X- _ O
of -X- _ O
text -X- _ O
. -X- _ O

To -X- _ O
address -X- _ O
this -X- _ O
issue -X- _ O
, -X- _ O
in -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
graph -X- _ B-MethodName
neural -X- _ I-MethodName
networks -X- _ I-MethodName
with -X- _ I-MethodName
generated -X- _ I-MethodName
parameters -X- _ I-MethodName
( -X- _ O
GP -X- _ B-MethodName
- -X- _ I-MethodName
GNNs -X- _ I-MethodName
) -X- _ O
, -X- _ O
to -X- _ O
adapt -X- _ O
graph -X- _ B-MethodName
neural -X- _ I-MethodName
networks -X- _ I-MethodName
to -X- _ O
solve -X- _ O
the -X- _ O
natural -X- _ B-TaskName
language -X- _ I-TaskName
relational -X- _ I-TaskName
reasoning -X- _ I-TaskName
task -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
most -X- _ O
existing -X- _ O
GNNs -X- _ B-MethodName
can -X- _ O
only -X- _ O
process -X- _ O
multi -X- _ O
- -X- _ O
hop -X- _ O
relational -X- _ O
reasoning -X- _ O
on -X- _ O
pre -X- _ O
- -X- _ O
defined -X- _ O
graphs -X- _ O
and -X- _ O
can -X- _ O
not -X- _ O
be -X- _ O
directly -X- _ O
applied -X- _ O
in -X- _ O
natural -X- _ O
language -X- _ O
relational -X- _ O
reasoning -X- _ O
. -X- _ O

1 -X- _ O
, -X- _ O
existing -X- _ O
relation -X- _ B-TaskName
extraction -X- _ I-TaskName
models -X- _ O
could -X- _ O
easily -X- _ O
extract -X- _ O
the -X- _ O
facts -X- _ O
that -X- _ O
Luc -X- _ O
Besson -X- _ O
directed -X- _ O
a -X- _ O
film -X- _ O
Léon -X- _ O
: -X- _ O
The -X- _ O
Professional -X- _ O
and -X- _ O
that -X- _ O
the -X- _ O
film -X- _ O
is -X- _ O
in -X- _ O
English -X- _ O
, -X- _ O
but -X- _ O
fail -X- _ O
to -X- _ O
infer -X- _ O
the -X- _ O
relationship -X- _ O
between -X- _ O
Luc -X- _ O
Besson -X- _ O
and -X- _ O
English -X- _ O
without -X- _ O
multi -X- _ O
- -X- _ O
hop -X- _ O
relational -X- _ O
reasoning -X- _ O
. -X- _ O

These -X- _ O
works -X- _ O
have -X- _ O
demonstrated -X- _ O
GNNs -X- _ B-MethodName
' -X- _ O
strong -X- _ O
power -X- _ O
to -X- _ O
process -X- _ O
relational -X- _ O
reasoning -X- _ O
on -X- _ O
graphs -X- _ O
. -X- _ O

In -X- _ O
recent -X- _ O
years -X- _ O
, -X- _ O
graph -X- _ B-MethodName
neural -X- _ I-MethodName
networks -X- _ I-MethodName
( -X- _ O
GNNs -X- _ B-MethodName
) -X- _ O
have -X- _ O
been -X- _ O
applied -X- _ O
to -X- _ O
various -X- _ O
fields -X- _ O
of -X- _ O
machine -X- _ O
learning -X- _ O
, -X- _ O
including -X- _ O
node -X- _ O
classification -X- _ O
( -X- _ O
Kipf -X- _ O
and -X- _ O
Welling -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
, -X- _ O
relation -X- _ O
classification -X- _ O
( -X- _ O
Schlichtkrull -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
, -X- _ O
molecular -X- _ O
property -X- _ O
prediction -X- _ O
( -X- _ O
Gilmer -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
, -X- _ O
few -X- _ O
- -X- _ O
shot -X- _ O
learning -X- _ O
( -X- _ O
Garcia -X- _ O
and -X- _ O
Bruna -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
achieved -X- _ O
promising -X- _ O
results -X- _ O
on -X- _ O
these -X- _ O
tasks -X- _ O
. -X- _ O

We -X- _ O
verify -X- _ O
GP -X- _ B-MethodName
- -X- _ I-MethodName
GNNs -X- _ I-MethodName
in -X- _ O
relation -X- _ B-TaskName
extraction -X- _ I-TaskName
from -X- _ O
text -X- _ O
, -X- _ O
both -X- _ O
on -X- _ O
bag -X- _ O
- -X- _ O
and -X- _ O
instancesettings -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
a -X- _ O
novel -X- _ O
graph -X- _ B-MethodName
neural -X- _ I-MethodName
network -X- _ I-MethodName
with -X- _ I-MethodName
generated -X- _ I-MethodName
parameters -X- _ I-MethodName
( -X- _ O
GP -X- _ B-MethodName
- -X- _ I-MethodName
GNNs -X- _ I-MethodName
) -X- _ O
. -X- _ O

Graph -X- _ B-MethodName
Neural -X- _ I-MethodName
Networks -X- _ I-MethodName
with -X- _ I-MethodName
Generated -X- _ I-MethodName
Parameters -X- _ I-MethodName
for -X- _ O
Relation -X- _ B-TaskName
Extraction -X- _ I-TaskName
. -X- _ O

However -X- _ O
, -X- _ O
in -X- _ O
our -X- _ O
model -X- _ O
, -X- _ O
since -X- _ O
Ohio -X- _ O
and -X- _ O
Johnson -X- _ O
County -X- _ O
have -X- _ O
no -X- _ O
relationship -X- _ O
, -X- _ O
this -X- _ O
wrong -X- _ O
relation -X- _ O
is -X- _ O
not -X- _ O
predicted -X- _ O
. -X- _ O

As -X- _ O
we -X- _ O
have -X- _ O
discussed -X- _ O
before -X- _ O
, -X- _ O
this -X- _ O
is -X- _ O
due -X- _ O
to -X- _ O
its -X- _ O
mechanism -X- _ O
to -X- _ O
model -X- _ O
cooccurrence -X- _ O
of -X- _ O
multiple -X- _ O
relations -X- _ O
. -X- _ O

ritory -X- _ O
issues -X- _ O
. -X- _ O

Although -X- _ O
" -X- _ O
No -X- _ O
Relation -X- _ O
" -X- _ O
is -X- _ O
also -X- _ O
be -X- _ O
seen -X- _ O
as -X- _ O
a -X- _ O
type -X- _ O
of -X- _ O
relation -X- _ O
, -X- _ O
we -X- _ O
only -X- _ O
show -X- _ O
other -X- _ O
relation -X- _ O
types -X- _ O
in -X- _ O
the -X- _ O
graphs -X- _ O
. -X- _ O

The -X- _ O
models -X- _ O
take -X- _ O
sentences -X- _ O
and -X- _ O
entity -X- _ O
markers -X- _ O
as -X- _ O
input -X- _ O
and -X- _ O
produce -X- _ O
a -X- _ O
graph -X- _ O
containing -X- _ O
entities -X- _ O
( -X- _ O
colored -X- _ O
and -X- _ O
bold -X- _ O
) -X- _ O
and -X- _ O
relations -X- _ O
between -X- _ O
them -X- _ O
. -X- _ O

Hao -X- _ O
Zhu -X- _ O
is -X- _ O
supported -X- _ O
by -X- _ O
Tsinghua -X- _ O
Initiative -X- _ O
Research -X- _ O
Program -X- _ O
. -X- _ O

This -X- _ O
work -X- _ O
10 -X- _ O
http://thunlp.org -X- _ O
is -X- _ O
jointly -X- _ O
supported -X- _ O
by -X- _ O
the -X- _ O
NSFC -X- _ O
project -X- _ O
under -X- _ O
the -X- _ O
grant -X- _ O
No -X- _ O
. -X- _ O
61661146007 -X- _ O
and -X- _ O
the -X- _ O
NExT++ -X- _ O
project -X- _ O
, -X- _ O
the -X- _ O
National -X- _ O
Research -X- _ O
Foundation -X- _ O
, -X- _ O
Prime -X- _ O
Ministers -X- _ O
Office -X- _ O
, -X- _ O
Singapore -X- _ O
under -X- _ O
its -X- _ O
IRC@Singapore -X- _ O
Funding -X- _ O
Initiative -X- _ O
. -X- _ O

The -X- _ O
authors -X- _ O
thank -X- _ O
the -X- _ O
members -X- _ O
of -X- _ O
Tsinghua -X- _ O
NLP -X- _ O
lab -X- _ O
10 -X- _ O
for -X- _ O
their -X- _ O
thoughtful -X- _ O
suggestions -X- _ O
. -X- _ O

Acknowledgement -X- _ O
. -X- _ O

For -X- _ O
example -X- _ O
, -X- _ O
in -X- _ O
the -X- _ O
third -X- _ O
case -X- _ O
, -X- _ O
share -X- _ O
border -X- _ O
with -X- _ O
and -X- _ O
located -X- _ O
in -X- _ O
are -X- _ O
both -X- _ O
relations -X- _ O
about -X- _ O
ter- -X- _ O
. -X- _ O

Tab -X- _ O
. -X- _ O

Qualitative -X- _ O
Results -X- _ O
: -X- _ O
Case -X- _ O
Study -X- _ O
. -X- _ O

We -X- _ O
leave -X- _ O
these -X- _ O
explorations -X- _ O
for -X- _ O
future -X- _ O
work -X- _ O
. -X- _ O

In -X- _ O
real -X- _ O
applications -X- _ O
, -X- _ O
different -X- _ O
variants -X- _ O
could -X- _ O
be -X- _ O
selected -X- _ O
for -X- _ O
different -X- _ O
kind -X- _ O
of -X- _ O
sentences -X- _ O
or -X- _ O
we -X- _ O
can -X- _ O
also -X- _ O
ensemble -X- _ O
the -X- _ O
prediction -X- _ O
from -X- _ O
different -X- _ O
models -X- _ O
. -X- _ O

This -X- _ O
observation -X- _ O
reveals -X- _ O
that -X- _ O
the -X- _ O
reasoning -X- _ O
mechanism -X- _ O
could -X- _ O
help -X- _ O
us -X- _ O
identify -X- _ O
relations -X- _ O
especially -X- _ O
on -X- _ O
sentences -X- _ O
where -X- _ O
there -X- _ O
are -X- _ O
more -X- _ O
entities -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
the -X- _ O
improvement -X- _ O
of -X- _ O
the -X- _ O
third -X- _ O
layer -X- _ O
is -X- _ O
much -X- _ O
smaller -X- _ O
on -X- _ O
the -X- _ O
overall -X- _ O
distantly -X- _ O
supervised -X- _ O
test -X- _ O
set -X- _ O
than -X- _ O
the -X- _ O
one -X- _ O
on -X- _ O
the -X- _ O
dense -X- _ O
subset -X- _ O
. -X- _ O

We -X- _ O
could -X- _ O
also -X- _ O
see -X- _ O
from -X- _ O
Fig -X- _ O
. -X- _ O

Here -X- _ O
, -X- _ O
we -X- _ O
fol- -X- _ O
P@5 -X- _ O
% -X- _ O
P@10 -X- _ O
% -X- _ O
P@15 -X- _ O
% -X- _ O
P@20 -X- _ O
% -X- _ O
P@5 -X- _ O
% -X- _ O
P@10 -X- _ O
% -X- _ O
P@15 -X- _ O
% -X- _ O
P@20 -X- _ O
% -X- _ O
. -X- _ O

Zeng -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

Evaluation -X- _ O
Details -X- _ O
. -X- _ O

Table -X- _ O
1 -X- _ O
shows -X- _ O
our -X- _ O
best -X- _ O
hyper -X- _ O
- -X- _ O
parameter -X- _ O
settings -X- _ O
, -X- _ O
which -X- _ O
are -X- _ O
used -X- _ O
in -X- _ O
all -X- _ O
of -X- _ O
our -X- _ O
experiments -X- _ O
. -X- _ O

We -X- _ O
select -X- _ O
the -X- _ O
best -X- _ O
parameters -X- _ O
for -X- _ O
the -X- _ O
validation -X- _ O
set -X- _ O
. -X- _ O

Hyper -X- _ O
- -X- _ O
parameters -X- _ O
. -X- _ O

These -X- _ O
models -X- _ O
are -X- _ O
capable -X- _ O
of -X- _ O
performing -X- _ O
2 -X- _ O
- -X- _ O
hop -X- _ O
reasoning -X- _ O
and -X- _ O
3 -X- _ O
- -X- _ O
hop -X- _ O
reasoning -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O

( -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
) -X- _ O
. -X- _ O

( -X- _ O
2014Zeng -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

This -X- _ O
model -X- _ O
divides -X- _ O
the -X- _ O
whole -X- _ O
sentence -X- _ O
into -X- _ O
three -X- _ O
pieces -X- _ O
and -X- _ O
applies -X- _ O
max -X- _ O
- -X- _ O
pooling -X- _ O
after -X- _ O
convolution -X- _ O
layer -X- _ O
piece -X- _ O
- -X- _ O
wisely -X- _ O
. -X- _ O

( -X- _ O
2015 -X- _ O
) -X- _ O
. -X- _ O

( -X- _ O
2014 -X- _ O
) -X- _ O
utilize -X- _ O
convolutional -X- _ O
neural -X- _ O
networks -X- _ O
to -X- _ O
classify -X- _ O
relations -X- _ O
. -X- _ O

Zeng -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

This -X- _ O
baseline -X- _ O
is -X- _ O
implemented -X- _ O
by -X- _ O
ourselves -X- _ O
based -X- _ O
on -X- _ O
authors -X- _ O
' -X- _ O
public -X- _ O
repo -X- _ O
8 -X- _ O
. -X- _ O

This -X- _ O
model -X- _ O
utilizes -X- _ O
attention -X- _ O
mechanism -X- _ O
to -X- _ O
encode -X- _ O
the -X- _ O
context -X- _ O
relations -X- _ O
for -X- _ O
predicting -X- _ O
target -X- _ O
relations -X- _ O
. -X- _ O

We -X- _ O
select -X- _ O
the -X- _ O
following -X- _ O
models -X- _ O
for -X- _ O
comparison -X- _ O
, -X- _ O
the -X- _ O
first -X- _ O
four -X- _ O
of -X- _ O
which -X- _ O
are -X- _ O
our -X- _ O
baseline -X- _ O
models -X- _ O
. -X- _ O

Models -X- _ O
for -X- _ O
Comparison -X- _ O
. -X- _ O

There -X- _ O
are -X- _ O
1,350 -X- _ O
sentences -X- _ O
and -X- _ O
more -X- _ O
than -X- _ O
17,915 -X- _ O
triples -X- _ O
and -X- _ O
7,906 -X- _ O
relational -X- _ O
facts -X- _ O
in -X- _ O
this -X- _ O
test -X- _ O
set -X- _ O
. -X- _ O

This -X- _ O
test -X- _ O
set -X- _ O
could -X- _ O
be -X- _ O
used -X- _ O
to -X- _ O
test -X- _ O
our -X- _ O
methods -X- _ O
' -X- _ O
performance -X- _ O
on -X- _ O
sentences -X- _ O
with -X- _ O
the -X- _ O
complex -X- _ O
interaction -X- _ O
between -X- _ O
entities -X- _ O
. -X- _ O

Our -X- _ O
criteria -X- _ O
are -X- _ O
: -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
entities -X- _ O
should -X- _ O
be -X- _ O
strictly -X- _ O
larger -X- _ O
than -X- _ O
2 -X- _ O
; -X- _ O
and -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
there -X- _ O
must -X- _ O
be -X- _ O
at -X- _ O
least -X- _ O
one -X- _ O
circle -X- _ O
( -X- _ O
with -X- _ O
at -X- _ O
least -X- _ O
three -X- _ O
entities -X- _ O
) -X- _ O
in -X- _ O
the -X- _ O
ground -X- _ O
- -X- _ O
truth -X- _ O
label -X- _ O
of -X- _ O
the -X- _ O
sentence -X- _ O
7 -X- _ O
. -X- _ O

There -X- _ O
are -X- _ O
350 -X- _ O
sentences -X- _ O
and -X- _ O
1,230 -X- _ O
triples -X- _ O
in -X- _ O
this -X- _ O
test -X- _ O
set -X- _ O
. -X- _ O

Only -X- _ O
the -X- _ O
instances -X- _ O
accepted -X- _ O
by -X- _ O
all -X- _ O
5 -X- _ O
annotators -X- _ O
are -X- _ O
incorporated -X- _ O
into -X- _ O
the -X- _ O
human -X- _ O
annotated -X- _ O
test -X- _ O
set -X- _ O
. -X- _ O

They -X- _ O
are -X- _ O
asked -X- _ O
to -X- _ O
decide -X- _ O
whether -X- _ O
or -X- _ O
not -X- _ O
the -X- _ O
distant -X- _ O
supervision -X- _ O
is -X- _ O
right -X- _ O
for -X- _ O
every -X- _ O
pair -X- _ O
of -X- _ O
entities -X- _ O
. -X- _ O

5 -X- _ O
We -X- _ O
use -X- _ O
the -X- _ O
same -X- _ O
training -X- _ O
set -X- _ O
for -X- _ O
all -X- _ O
of -X- _ O
the -X- _ O
experiments -X- _ O
. -X- _ O

Therefore -X- _ O
, -X- _ O
we -X- _ O
need -X- _ O
to -X- _ O
modify -X- _ O
their -X- _ O
dataset -X- _ O
: -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
We -X- _ O
added -X- _ O
reversed -X- _ O
edges -X- _ O
if -X- _ O
they -X- _ O
are -X- _ O
missing -X- _ O
from -X- _ O
a -X- _ O
given -X- _ O
triple -X- _ O
, -X- _ O
e.g. -X- _ O
if -X- _ O
triple -X- _ O
( -X- _ O
Earth -X- _ O
, -X- _ O
part -X- _ O
of -X- _ O
, -X- _ O
Solar -X- _ O
System -X- _ O
) -X- _ O
exists -X- _ O
in -X- _ O
the -X- _ O
sentence -X- _ O
, -X- _ O
we -X- _ O
add -X- _ O
a -X- _ O
reversed -X- _ O
label -X- _ O
, -X- _ O
( -X- _ O
Solar -X- _ O
System -X- _ O
, -X- _ O
has -X- _ O
a -X- _ O
member -X- _ O
, -X- _ O
Earth -X- _ O
) -X- _ O
, -X- _ O
to -X- _ O
it -X- _ O
; -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
For -X- _ O
all -X- _ O
of -X- _ O
the -X- _ O
entity -X- _ O
pairs -X- _ O
with -X- _ O
no -X- _ O
relations -X- _ O
, -X- _ O
we -X- _ O
added -X- _ O
" -X- _ O
NA -X- _ O
" -X- _ O
labels -X- _ O
to -X- _ O
them -X- _ O
. -X- _ O

Datasets -X- _ O
. -X- _ O

Experiment -X- _ O
Settings -X- _ O
. -X- _ O

Experiments -X- _ O
. -X- _ O

To -X- _ O
make -X- _ O
it -X- _ O
more -X- _ O
efficient -X- _ O
, -X- _ O
we -X- _ O
avoid -X- _ O
using -X- _ O
loop -X- _ O
- -X- _ O
based -X- _ O
, -X- _ O
scalar -X- _ O
- -X- _ O
oriented -X- _ O
code -X- _ O
by -X- _ O
matrix -X- _ O
and -X- _ O
vector -X- _ O
operations -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
PyTorch -X- _ O
( -X- _ O
Paszke -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
to -X- _ O
implement -X- _ O
our -X- _ O
models -X- _ O
. -X- _ O

In -X- _ O
practice -X- _ O
, -X- _ O
we -X- _ O
stack -X- _ O
the -X- _ O
embeddings -X- _ O
for -X- _ O
every -X- _ O
target -X- _ O
entity -X- _ O
pairs -X- _ O
together -X- _ O
to -X- _ O
infer -X- _ O
the -X- _ O
underlying -X- _ O
relationship -X- _ O
between -X- _ O
each -X- _ O
pair -X- _ O
of -X- _ O
entities -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
cross -X- _ O
entropy -X- _ O
here -X- _ O
as -X- _ O
the -X- _ O
classification -X- _ O
loss -X- _ O
L -X- _ O
= -X- _ O
s∈S -X- _ O
i -X- _ O
= -X- _ O
j -X- _ O
log -X- _ O
P(rv -X- _ O
i -X- _ O
, -X- _ O
v -X- _ O
j -X- _ O
|i -X- _ O
, -X- _ O
j -X- _ O
, -X- _ O
s),(8 -X- _ O
) -X- _ O
where -X- _ O
r -X- _ O
v -X- _ O
i -X- _ O
, -X- _ O
v -X- _ O
j -X- _ O
denotes -X- _ O
the -X- _ O
relation -X- _ O
label -X- _ O
for -X- _ O
entity -X- _ O
pair -X- _ O
( -X- _ O
v -X- _ O
i -X- _ O
, -X- _ O
v -X- _ O
j -X- _ O
) -X- _ O
and -X- _ O
S -X- _ O
denotes -X- _ O
the -X- _ O
whole -X- _ O
corpus -X- _ O
. -X- _ O

This -X- _ O
could -X- _ O
be -X- _ O
used -X- _ O
for -X- _ O
classification -X- _ O
: -X- _ O
P(rv -X- _ O
i -X- _ O
, -X- _ O
v -X- _ O
j -X- _ O
|h -X- _ O
, -X- _ O
t -X- _ O
, -X- _ O
s -X- _ O
) -X- _ O
= -X- _ O
softmax(MLP(rv -X- _ O
i -X- _ O
, -X- _ O
v -X- _ O
j -X- _ O
) -X- _ O
) -X- _ O
, -X- _ O
( -X- _ O
7 -X- _ O
) -X- _ O
where -X- _ O
r -X- _ O
v -X- _ O
i -X- _ O
, -X- _ O
v -X- _ O
j -X- _ O
∈ -X- _ O
R -X- _ O
, -X- _ O
and -X- _ O
MLP -X- _ O
denotes -X- _ O
a -X- _ O
multi -X- _ O
- -X- _ O
layer -X- _ O
perceptron -X- _ O
module -X- _ O
. -X- _ O

; -X- _ O
[ -X- _ O
h -X- _ O
( -X- _ O
K -X- _ O
) -X- _ O
v -X- _ O
i -X- _ O
h -X- _ O
( -X- _ O
K -X- _ O
) -X- _ O
v -X- _ O
j -X- _ O
] -X- _ O
] -X- _ O
, -X- _ O
( -X- _ O
6 -X- _ O
) -X- _ O
where -X- _ O
represents -X- _ O
element -X- _ O
- -X- _ O
wise -X- _ O
multiplication -X- _ O
. -X- _ O

The -X- _ O
output -X- _ O
module -X- _ O
takes -X- _ O
the -X- _ O
embeddings -X- _ O
of -X- _ O
the -X- _ O
target -X- _ O
entity -X- _ O
pair -X- _ O
( -X- _ O
v -X- _ O
i -X- _ O
, -X- _ O
v -X- _ O
j -X- _ O
) -X- _ O
as -X- _ O
input -X- _ O
, -X- _ O
which -X- _ O
are -X- _ O
first -X- _ O
converted -X- _ O
by -X- _ O
: -X- _ O
rv -X- _ O
i -X- _ O
, -X- _ O
v -X- _ O
j -X- _ O
= -X- _ O
[ -X- _ O
[ -X- _ O
h -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
v -X- _ O
i -X- _ O
h -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
v -X- _ O
j -X- _ O
] -X- _ O
; -X- _ O
[ -X- _ O
h -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
v -X- _ O
i -X- _ O
h -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
v -X- _ O
j -X- _ O
] -X- _ O
; -X- _ O
. -X- _ O

Classification -X- _ O
Module -X- _ O
. -X- _ O

5.4 -X- _ O
) -X- _ O
. -X- _ O

In -X- _ O
our -X- _ O
context -X- _ O
, -X- _ O
however -X- _ O
, -X- _ O
since -X- _ O
the -X- _ O
graph -X- _ O
is -X- _ O
densely -X- _ O
connected -X- _ O
, -X- _ O
the -X- _ O
depth -X- _ O
is -X- _ O
interpreted -X- _ O
simply -X- _ O
as -X- _ O
giving -X- _ O
the -X- _ O
model -X- _ O
more -X- _ O
expressive -X- _ O
power -X- _ O
. -X- _ O

Annotators -X- _ O
a -X- _ O
subject -X- _ O
and -X- _ O
a -X- _ O
object -X- _ O
could -X- _ O
also -X- _ O
carry -X- _ O
the -X- _ O
prior -X- _ O
knowledge -X- _ O
about -X- _ O
subject -X- _ O
entity -X- _ O
and -X- _ O
object -X- _ O
entity -X- _ O
. -X- _ O

We -X- _ O
set -X- _ O
special -X- _ O
values -X- _ O
for -X- _ O
the -X- _ O
head -X- _ O
and -X- _ O
tail -X- _ O
entity -X- _ O
's -X- _ O
initial -X- _ O
embeddings -X- _ O
as -X- _ O
a -X- _ O
kind -X- _ O
of -X- _ O
" -X- _ O
flag -X- _ O
" -X- _ O
messages -X- _ O
which -X- _ O
we -X- _ O
expect -X- _ O
to -X- _ O
be -X- _ O
passed -X- _ O
through -X- _ O
propagation -X- _ O
. -X- _ O

The -X- _ O
Initial -X- _ O
Embeddings -X- _ O
of -X- _ O
Nodes -X- _ O
Suppose -X- _ O
we -X- _ O
are -X- _ O
focusing -X- _ O
on -X- _ O
extracting -X- _ O
the -X- _ O
relationship -X- _ O
between -X- _ O
entity -X- _ O
v -X- _ O
i -X- _ O
and -X- _ O
entity -X- _ O
v -X- _ O
j -X- _ O
, -X- _ O
the -X- _ O
initial -X- _ O
embeddings -X- _ O
of -X- _ O
them -X- _ O
are -X- _ O
annotated -X- _ O
as -X- _ O
h -X- _ O
( -X- _ O
0 -X- _ O
) -X- _ O
v -X- _ O
i -X- _ O
= -X- _ O
a -X- _ O
subject -X- _ O
, -X- _ O
and -X- _ O
h -X- _ O
( -X- _ O
0 -X- _ O
) -X- _ O
v -X- _ O
j -X- _ O
= -X- _ O
a -X- _ O
object -X- _ O
, -X- _ O
while -X- _ O
the -X- _ O
initial -X- _ O
embeddings -X- _ O
of -X- _ O
other -X- _ O
entities -X- _ O
are -X- _ O
set -X- _ O
to -X- _ O
all -X- _ O
zeros -X- _ O
. -X- _ O

Next -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
Eq -X- _ O
. -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
to -X- _ O
propagate -X- _ O
information -X- _ O
among -X- _ O
nodes -X- _ O
where -X- _ O
the -X- _ O
initial -X- _ O
embeddings -X- _ O
of -X- _ O
nodes -X- _ O
and -X- _ O
number -X- _ O
of -X- _ O
layers -X- _ O
are -X- _ O
further -X- _ O
specified -X- _ O
as -X- _ O
follows -X- _ O
. -X- _ O

Propagation -X- _ O
Module -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
notation -X- _ O
p -X- _ O
i -X- _ O
, -X- _ O
j -X- _ O
t -X- _ O
to -X- _ O
represent -X- _ O
the -X- _ O
position -X- _ O
embedding -X- _ O
for -X- _ O
x -X- _ O
t -X- _ O
corresponding -X- _ O
to -X- _ O
entity -X- _ O
pair -X- _ O
( -X- _ O
v -X- _ O
i -X- _ O
, -X- _ O
v -X- _ O
j -X- _ O
) -X- _ O
. -X- _ O

Each -X- _ O
position -X- _ O
marker -X- _ O
is -X- _ O
also -X- _ O
mapped -X- _ O
to -X- _ O
a -X- _ O
d -X- _ O
p -X- _ O
-dimensional -X- _ O
vector -X- _ O
by -X- _ O
a -X- _ O
position -X- _ O
embedding -X- _ O
matrix -X- _ O
P -X- _ O
∈ -X- _ O
R -X- _ O
3×dp -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
work -X- _ O
, -X- _ O
we -X- _ O
consider -X- _ O
a -X- _ O
simple -X- _ O
entity -X- _ O
marking -X- _ O
scheme -X- _ O
2 -X- _ O
: -X- _ O
we -X- _ O
mark -X- _ O
each -X- _ O
token -X- _ O
in -X- _ O
the -X- _ O
sentence -X- _ O
as -X- _ O
either -X- _ O
belonging -X- _ O
to -X- _ O
the -X- _ O
first -X- _ O
entity -X- _ O
v -X- _ O
i -X- _ O
, -X- _ O
the -X- _ O
second -X- _ O
entity -X- _ O
v -X- _ O
j -X- _ O
or -X- _ O
to -X- _ O
neither -X- _ O
of -X- _ O
those -X- _ O
. -X- _ O

Position -X- _ O
Embedding -X- _ O
. -X- _ O

Throughout -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
stick -X- _ O
to -X- _ O
50 -X- _ O
- -X- _ O
dimensional -X- _ O
GloVe -X- _ O
embeddings -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
on -X- _ O
a -X- _ O
6 -X- _ O
- -X- _ O
billion -X- _ O
- -X- _ O
word -X- _ O
corpus -X- _ O
( -X- _ O
Pennington -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
. -X- _ O

, -X- _ O
x -X- _ O
l−1 -X- _ O
} -X- _ O
to -X- _ O
a -X- _ O
kdimensional -X- _ O
embedding -X- _ O
vector -X- _ O
x -X- _ O
t -X- _ O
using -X- _ O
a -X- _ O
word -X- _ O
embedding -X- _ O
matrix -X- _ O
W -X- _ O
e -X- _ O
∈ -X- _ O
R -X- _ O
|V -X- _ O
|×dw -X- _ O
, -X- _ O
where -X- _ O
|V -X- _ O
| -X- _ O
is -X- _ O
the -X- _ O
size -X- _ O
of -X- _ O
the -X- _ O
vocabulary -X- _ O
. -X- _ O

We -X- _ O
first -X- _ O
map -X- _ O
each -X- _ O
token -X- _ O
x -X- _ O
t -X- _ O
of -X- _ O
sentence -X- _ O
{ -X- _ O
x -X- _ O
0 -X- _ O
, -X- _ O
x -X- _ O
1 -X- _ O
, -X- _ O
. -X- _ O

Word -X- _ O
Representations -X- _ O
. -X- _ O

Encoding -X- _ O
Module -X- _ O
. -X- _ O

Module -X- _ O
Classification -X- _ O
Module -X- _ O
h -X- _ O
( -X- _ O
n -X- _ O
) -X- _ O
1 -X- _ O
h -X- _ O
( -X- _ O
n -X- _ O
) -X- _ O
2 -X- _ O
h -X- _ O
( -X- _ O
n -X- _ O
) -X- _ O
3 -X- _ O
A -X- _ O
( -X- _ O
n -X- _ O
) -X- _ O
1,2 -X- _ O
A -X- _ O
( -X- _ O
n -X- _ O
) -X- _ O
2,3 -X- _ O
A -X- _ O
( -X- _ O
n -X- _ O
) -X- _ O
3,1 -X- _ O
x -X- _ O
1,2 -X- _ O
3 -X- _ O
x -X- _ O
1,2 -X- _ O
4 -X- _ O
x -X- _ O
1,2 -X- _ O
2 -X- _ O
x -X- _ O
1,2 -X- _ O
1 -X- _ O
x -X- _ O
1,2 -X- _ O
0 -X- _ O
Figure -X- _ O
2 -X- _ O
: -X- _ O
Overall -X- _ O
architecture -X- _ O
: -X- _ O
an -X- _ O
encoding -X- _ O
module -X- _ O
takes -X- _ O
a -X- _ O
sequence -X- _ O
of -X- _ O
vector -X- _ O
representations -X- _ O
as -X- _ O
inputs -X- _ O
, -X- _ O
and -X- _ O
output -X- _ O
a -X- _ O
transition -X- _ O
matrix -X- _ O
as -X- _ O
output -X- _ O
; -X- _ O
a -X- _ O
propagation -X- _ O
module -X- _ O
propagates -X- _ O
the -X- _ O
hidden -X- _ O
states -X- _ O
from -X- _ O
nodes -X- _ O
to -X- _ O
its -X- _ O
neighbours -X- _ O
with -X- _ O
the -X- _ O
generated -X- _ O
transition -X- _ O
matrix -X- _ O
; -X- _ O
a -X- _ O
classification -X- _ O
module -X- _ O
provides -X- _ O
task -X- _ O
- -X- _ O
related -X- _ O
predictions -X- _ O
according -X- _ O
to -X- _ O
nodes -X- _ O
representations -X- _ O
. -X- _ O

Propagation -X- _ O
. -X- _ O

Encoding -X- _ O
Module -X- _ O
. -X- _ O

, -X- _ O
x -X- _ O
l−1 -X- _ O
) -X- _ O
, -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
relations -X- _ O
R -X- _ O
and -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
entities -X- _ O
in -X- _ O
this -X- _ O
sentence -X- _ O
V -X- _ O
s -X- _ O
= -X- _ O
{ -X- _ O
v -X- _ O
1 -X- _ O
, -X- _ O
v -X- _ O
2 -X- _ O
, -X- _ O
. -X- _ O

Given -X- _ O
a -X- _ O
sentence -X- _ O
s -X- _ O
= -X- _ O
( -X- _ O
x -X- _ O
0 -X- _ O
, -X- _ O
x -X- _ O
1 -X- _ O
, -X- _ O
. -X- _ O

, -X- _ O
h -X- _ O
K -X- _ O
0:|V|−1 -X- _ O
, -X- _ O
Y -X- _ O
; -X- _ O
θ -X- _ O
c -X- _ O
) -X- _ O
, -X- _ O
( -X- _ O
3 -X- _ O
) -X- _ O
where -X- _ O
θ -X- _ O
c -X- _ O
denotes -X- _ O
the -X- _ O
parameters -X- _ O
of -X- _ O
the -X- _ O
classification -X- _ O
module -X- _ O
, -X- _ O
K -X- _ O
is -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
layers -X- _ O
in -X- _ O
propagation -X- _ O
module -X- _ O
and -X- _ O
Y -X- _ O
denotes -X- _ O
the -X- _ O
ground -X- _ O
truth -X- _ O
label -X- _ O
. -X- _ O

Generally -X- _ O
, -X- _ O
the -X- _ O
classification -X- _ O
module -X- _ O
takes -X- _ O
node -X- _ O
representations -X- _ O
as -X- _ O
inputs -X- _ O
and -X- _ O
outputs -X- _ O
predictions -X- _ O
. -X- _ O

Classification -X- _ O
Module -X- _ O
. -X- _ O

Given -X- _ O
representations -X- _ O
of -X- _ O
layer -X- _ O
n -X- _ O
, -X- _ O
the -X- _ O
representations -X- _ O
of -X- _ O
layer -X- _ O
n -X- _ O
+ -X- _ O
1 -X- _ O
are -X- _ O
calculated -X- _ O
by -X- _ O
h -X- _ O
( -X- _ O
n+1 -X- _ O
) -X- _ O
i -X- _ O
= -X- _ O
v -X- _ O
j -X- _ O
∈N -X- _ O
( -X- _ O
v -X- _ O
i -X- _ O
) -X- _ O
σ(A -X- _ O
( -X- _ O
n -X- _ O
) -X- _ O
i -X- _ O
, -X- _ O
j -X- _ O
h -X- _ O
( -X- _ O
n -X- _ O
) -X- _ O
j -X- _ O
) -X- _ O
, -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
where -X- _ O
N -X- _ O
( -X- _ O
v -X- _ O
i -X- _ O
) -X- _ O
denotes -X- _ O
the -X- _ O
neighbours -X- _ O
of -X- _ O
node -X- _ O
v -X- _ O
i -X- _ O
in -X- _ O
graph -X- _ O
G -X- _ O
and -X- _ O
σ(• -X- _ O
) -X- _ O
denotes -X- _ O
a -X- _ O
non -X- _ O
- -X- _ O
linear -X- _ O
activation -X- _ O
function -X- _ O
. -X- _ O

The -X- _ O
initial -X- _ O
embeddings -X- _ O
of -X- _ O
nodes -X- _ O
, -X- _ O
i.e. -X- _ O
the -X- _ O
representations -X- _ O
of -X- _ O
layer -X- _ O
0 -X- _ O
, -X- _ O
are -X- _ O
task -X- _ O
- -X- _ O
related -X- _ O
, -X- _ O
which -X- _ O
could -X- _ O
be -X- _ O
embeddings -X- _ O
that -X- _ O
encode -X- _ O
features -X- _ O
of -X- _ O
nodes -X- _ O
or -X- _ O
just -X- _ O
one -X- _ O
- -X- _ O
hot -X- _ O
embeddings -X- _ O
. -X- _ O

The -X- _ O
propagation -X- _ O
module -X- _ O
learns -X- _ O
representations -X- _ O
for -X- _ O
nodes -X- _ O
layer -X- _ O
by -X- _ O
layer -X- _ O
. -X- _ O

Propagation -X- _ O
Module -X- _ O
. -X- _ O

Encoding -X- _ O
Module -X- _ O
. -X- _ O

2 -X- _ O
. -X- _ O

, -X- _ O
x -X- _ O
i -X- _ O
, -X- _ O
j -X- _ O
l−1 -X- _ O
extracted -X- _ O
from -X- _ O
the -X- _ O
text -X- _ O
. -X- _ O

Given -X- _ O
a -X- _ O
sequence -X- _ O
of -X- _ O
text -X- _ O
with -X- _ O
m -X- _ O
entities -X- _ O
, -X- _ O
it -X- _ O
aims -X- _ O
to -X- _ O
reason -X- _ O
on -X- _ O
both -X- _ O
the -X- _ O
text -X- _ O
and -X- _ O
entities -X- _ O
and -X- _ O
make -X- _ O
a -X- _ O
prediction -X- _ O
of -X- _ O
the -X- _ O
labels -X- _ O
of -X- _ O
the -X- _ O
entities -X- _ O
or -X- _ O
entity -X- _ O
pairs -X- _ O
. -X- _ O

The -X- _ O
drawback -X- _ O
of -X- _ O
existing -X- _ O
approaches -X- _ O
is -X- _ O
that -X- _ O
they -X- _ O
could -X- _ O
not -X- _ O
make -X- _ O
full -X- _ O
use -X- _ O
of -X- _ O
the -X- _ O
multihop -X- _ O
inference -X- _ O
patterns -X- _ O
among -X- _ O
multiple -X- _ O
entity -X- _ O
pairs -X- _ O
and -X- _ O
their -X- _ O
relations -X- _ O
within -X- _ O
the -X- _ O
sentence -X- _ O
. -X- _ O

The -X- _ O
most -X- _ O
related -X- _ O
work -X- _ O
is -X- _ O
Sorokin -X- _ O
and -X- _ O
Gurevych -X- _ O
( -X- _ O
2017 -X- _ O
) -X- _ O
, -X- _ O
where -X- _ O
the -X- _ O
proposed -X- _ O
model -X- _ O
incorporates -X- _ O
contextual -X- _ O
relations -X- _ O
with -X- _ O
an -X- _ O
attention -X- _ O
mechanism -X- _ O
when -X- _ O
predicting -X- _ O
the -X- _ O
relation -X- _ O
of -X- _ O
a -X- _ O
target -X- _ O
entity -X- _ O
pair -X- _ O
. -X- _ O

Christopoulou -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

Zeng -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

Peng -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

Lin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

For -X- _ O
example -X- _ O
, -X- _ O
Zeng -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

Existing -X- _ O
works -X- _ O
( -X- _ O
Zeng -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2014(Zeng -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
, -X- _ O
2015;;Lin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
have -X- _ O
demonstrated -X- _ O
that -X- _ O
neural -X- _ O
networks -X- _ O
are -X- _ O
capa -X- _ O
- -X- _ O
ble -X- _ O
of -X- _ O
capturing -X- _ O
the -X- _ O
pair -X- _ O
- -X- _ O
wise -X- _ O
relationship -X- _ O
between -X- _ O
entities -X- _ O
in -X- _ O
certain -X- _ O
situations -X- _ O
. -X- _ O

( -X- _ O
2018 -X- _ O
) -X- _ O
model -X- _ O
the -X- _ O
interaction -X- _ O
of -X- _ O
physical -X- _ O
objects -X- _ O
. -X- _ O

( -X- _ O
2017 -X- _ O
) -X- _ O
build -X- _ O
up -X- _ O
a -X- _ O
scene -X- _ O
graph -X- _ O
according -X- _ O
to -X- _ O
an -X- _ O
image -X- _ O
, -X- _ O
and -X- _ O
Kipf -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

( -X- _ O
2017 -X- _ O
) -X- _ O
propose -X- _ O
a -X- _ O
simple -X- _ O
neural -X- _ O
network -X- _ O
to -X- _ O
reason -X- _ O
the -X- _ O
relationship -X- _ O
of -X- _ O
objects -X- _ O
in -X- _ O
a -X- _ O
picture -X- _ O
, -X- _ O
Xu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

For -X- _ O
example -X- _ O
, -X- _ O
Santoro -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

Johnson -X- _ O
( -X- _ O
2017 -X- _ O
) -X- _ O
introduces -X- _ O
a -X- _ O
novel -X- _ O
neural -X- _ O
architecture -X- _ O
to -X- _ O
generate -X- _ O
a -X- _ O
graph -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
textual -X- _ O
input -X- _ O
and -X- _ O
dynamically -X- _ O
update -X- _ O
the -X- _ O
relationship -X- _ O
during -X- _ O
the -X- _ O
learning -X- _ O
process -X- _ O
. -X- _ O

Zhang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

( -X- _ O
2017 -X- _ O
) -X- _ O
apply -X- _ O
message -X- _ O
- -X- _ O
passing -X- _ O
on -X- _ O
a -X- _ O
graph -X- _ O
constructed -X- _ O
by -X- _ O
coreference -X- _ O
links -X- _ O
to -X- _ O
answer -X- _ O
relational -X- _ O
questions -X- _ O
. -X- _ O

Dhingra -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

( -X- _ O
2017 -X- _ O
) -X- _ O
study -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
message -X- _ O
- -X- _ O
passing -X- _ O
in -X- _ O
quantum -X- _ O
chemistry -X- _ O
. -X- _ O

Gilmer -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

Gilmer -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

Later -X- _ O
the -X- _ O
authors -X- _ O
in -X- _ O
Li -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

Related -X- _ O
Work -X- _ O
. -X- _ O

Moreover -X- _ O
, -X- _ O
we -X- _ O
also -X- _ O
present -X- _ O
three -X- _ O
datasets -X- _ O
, -X- _ O
which -X- _ O
could -X- _ O
help -X- _ O
future -X- _ O
researchers -X- _ O
compare -X- _ O
their -X- _ O
models -X- _ O
in -X- _ O
different -X- _ O
settings -X- _ O
. -X- _ O

We -X- _ O
also -X- _ O
perform -X- _ O
a -X- _ O
qualitative -X- _ O
analysis -X- _ O
which -X- _ O
shows -X- _ O
that -X- _ O
our -X- _ O
model -X- _ O
could -X- _ O
discover -X- _ O
more -X- _ O
relations -X- _ O
by -X- _ O
reasoning -X- _ O
more -X- _ O
robustly -X- _ O
as -X- _ O
compared -X- _ O
to -X- _ O
baseline -X- _ O
models -X- _ O
. -X- _ O

Modeling -X- _ O
the -X- _ O
relationship -X- _ O
between -X- _ O
" -X- _ O
Léon -X- _ O
" -X- _ O
and -X- _ O
" -X- _ O
English -X- _ O
" -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
" -X- _ O
Luc -X- _ O
Besson -X- _ O
" -X- _ O
helps -X- _ O
discover -X- _ O
the -X- _ O
relationship -X- _ O
between -X- _ O
" -X- _ O
Luc -X- _ O
Besson -X- _ O
" -X- _ O
and -X- _ O
" -X- _ O
English -X- _ O
" -X- _ O
. -X- _ O

English -X- _ O
Luc -X- _ O
Besson -X- _ O
. -X- _ O

Léon -X- _ O
. -X- _ O

Enabling -X- _ O
multi -X- _ O
- -X- _ O
hop -X- _ O
relational -X- _ O
reasoning -X- _ O
in -X- _ O
natural -X- _ O
languages -X- _ O
remains -X- _ O
an -X- _ O
open -X- _ O
problem -X- _ O
. -X- _ O

By -X- _ O
considering -X- _ O
the -X- _ O
reasoning -X- _ O
patterns -X- _ O
, -X- _ O
one -X- _ O
can -X- _ O
discover -X- _ O
that -X- _ O
Luc -X- _ O
Besson -X- _ O
could -X- _ O
speak -X- _ O
English -X- _ O
following -X- _ O
a -X- _ O
reasoning -X- _ O
logic -X- _ O
that -X- _ O
Luc -X- _ O
Besson -X- _ O
directed -X- _ O
Léon -X- _ O
: -X- _ O
The -X- _ O
Professional -X- _ O
and -X- _ O
this -X- _ O
film -X- _ O
is -X- _ O
in -X- _ O
English -X- _ O
indicates -X- _ O
Luc -X- _ O
Besson -X- _ O
could -X- _ O
speak -X- _ O
English -X- _ O
. -X- _ O

Consider -X- _ O
the -X- _ O
example -X- _ O
shown -X- _ O
in -X- _ O
Fig -X- _ O
. -X- _ O

Besides -X- _ O
graphs -X- _ O
, -X- _ O
relational -X- _ O
reasoning -X- _ O
is -X- _ O
also -X- _ O
of -X- _ O
great -X- _ O
importance -X- _ O
in -X- _ O
many -X- _ O
natural -X- _ O
language -X- _ O
processing -X- _ O
tasks -X- _ O
such -X- _ O
as -X- _ O
question -X- _ O
answering -X- _ O
, -X- _ O
relation -X- _ O
extraction -X- _ O
, -X- _ O
summarization -X- _ O
, -X- _ O
etc -X- _ O
. -X- _ O

Relational -X- _ O
reasoning -X- _ O
aims -X- _ O
to -X- _ O
abstractly -X- _ O
reason -X- _ O
about -X- _ O
entities -X- _ O
/ -X- _ O
objects -X- _ O
and -X- _ O
their -X- _ O
relations -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
an -X- _ O
important -X- _ O
part -X- _ O
of -X- _ O
human -X- _ O
intelligence -X- _ O
. -X- _ O

Introduction -X- _ O
. -X- _ O

Codes -X- _ O
and -X- _ O
data -X- _ O
are -X- _ O
released -X- _ O
at -X- _ O
https -X- _ O
: -X- _ O
//github.com -X- _ O
/ -X- _ O
thunlp -X- _ O
/ -X- _ O
gp -X- _ O
- -X- _ O
gnn -X- _ O
. -X- _ O

We -X- _ O
also -X- _ O
perform -X- _ O
a -X- _ O
qualitative -X- _ O
analysis -X- _ O
to -X- _ O
demonstrate -X- _ O
that -X- _ O
our -X- _ O
model -X- _ O
could -X- _ O
discover -X- _ O
more -X- _ O
accurate -X- _ O
relations -X- _ O
by -X- _ O
multi -X- _ O
- -X- _ O
hop -X- _ O
relational -X- _ O
reasoning -X- _ O
. -X- _ O

Experimental -X- _ O
results -X- _ O
on -X- _ O
a -X- _ O
humanannotated -X- _ O
dataset -X- _ O
and -X- _ O
two -X- _ O
distantly -X- _ O
supervised -X- _ O
datasets -X- _ O
show -X- _ O
that -X- _ O
multi -X- _ O
- -X- _ O
hop -X- _ O
reasoning -X- _ O
mechanism -X- _ O
yields -X- _ O
significant -X- _ O
improvements -X- _ O
. -X- _ O

The -X- _ O
parameters -X- _ O
in -X- _ O
the -X- _ O
propagation -X- _ O
module -X- _ O
, -X- _ O
i.e. -X- _ O
the -X- _ O
transition -X- _ O
matrices -X- _ O
used -X- _ O
in -X- _ O
message -X- _ O
passing -X- _ O
procedure -X- _ O
, -X- _ O
are -X- _ O
produced -X- _ O
by -X- _ O
a -X- _ O
generator -X- _ O
taking -X- _ O
natural -X- _ O
language -X- _ O
sentences -X- _ O
as -X- _ O
inputs -X- _ O
. -X- _ O

