-DOCSTART- -X- O
While -X- _ O
we -X- _ O
focus -X- _ O
on -X- _ O
datetime -X- _ O
information -X- _ O
, -X- _ O
we -X- _ O
demonstrate -X- _ O
that -X- _ O
our -X- _ O
approach -X- _ O
can -X- _ O
be -X- _ O
applied -X- _ O
to -X- _ O
any -X- _ O
type -X- _ O
of -X- _ O
non -X- _ O
- -X- _ O
linguistic -X- _ O
context -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
geolocation -X- _ O
and -X- _ O
dialogue -X- _ O
prompts -X- _ O
. -X- _ O

Moreover -X- _ O
, -X- _ O
the -X- _ O
attention -X- _ O
mechanism -X- _ O
we -X- _ O
propose -X- _ O
can -X- _ O
improve -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
contextual -X- _ O
LM -X- _ O
models -X- _ O
by -X- _ O
over -X- _ O
2.8 -X- _ B-MetricValue
% -X- _ I-MetricValue
relative -X- _ O
in -X- _ O
terms -X- _ O
of -X- _ O
perplexity -X- _ B-MetricName
. -X- _ O

We -X- _ O
find -X- _ O
that -X- _ O
incorporating -X- _ O
datetime -X- _ O
context -X- _ O
into -X- _ O
a -X- _ O
LM -X- _ O
can -X- _ O
yield -X- _ O
a -X- _ O
relative -X- _ O
reduction -X- _ O
in -X- _ O
perplexity -X- _ B-MetricName
of -X- _ O
9.0 -X- _ B-MetricValue
% -X- _ I-MetricValue
over -X- _ O
a -X- _ O
model -X- _ O
that -X- _ O
does -X- _ O
not -X- _ O
incorporate -X- _ O
context -X- _ O
. -X- _ O

The -X- _ O
proposed -X- _ O
model -X- _ O
dynamically -X- _ O
builds -X- _ O
up -X- _ O
a -X- _ O
representation -X- _ O
of -X- _ O
contextual -X- _ O
information -X- _ O
that -X- _ O
can -X- _ O
be -X- _ O
ingested -X- _ O
into -X- _ O
a -X- _ O
RNN -X- _ O
- -X- _ O
LM -X- _ O
via -X- _ O
a -X- _ O
concatenation -X- _ B-MethodName
- -X- _ I-MethodName
based -X- _ I-MethodName
or -X- _ I-MethodName
factorization -X- _ I-MethodName
- -X- _ I-MethodName
based -X- _ I-MethodName
approach -X- _ I-MethodName
. -X- _ O

In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
introduce -X- _ O
an -X- _ O
attention -X- _ B-MethodName
- -X- _ I-MethodName
based -X- _ I-MethodName
mechanism -X- _ I-MethodName
to -X- _ O
condition -X- _ O
neural -X- _ O
LMs -X- _ O
for -X- _ O
ASR -X- _ B-TaskName
on -X- _ O
non -X- _ O
- -X- _ O
linguistic -X- _ O
contextual -X- _ O
information -X- _ O
. -X- _ O

Conclusion -X- _ O
. -X- _ O

While -X- _ O
attention -X- _ O
- -X- _ O
based -X- _ O
models -X- _ O
have -X- _ O
been -X- _ O
used -X- _ O
to -X- _ O
condition -X- _ O
neural -X- _ O
models -X- _ O
on -X- _ O
particular -X- _ O
aspects -X- _ O
or -X- _ O
traits -X- _ O
( -X- _ O
Zheng -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019;Tang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
focus -X- _ O
on -X- _ O
contextual -X- _ O
information -X- _ O
that -X- _ O
benefits -X- _ O
ASR -X- _ O
systems -X- _ O
. -X- _ O

( -X- _ O
2015 -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
attention -X- _ B-MethodName
mechanism -X- _ I-MethodName
we -X- _ O
propose -X- _ O
builds -X- _ O
on -X- _ O
the -X- _ O
global -X- _ O
attention -X- _ O
model -X- _ O
proposed -X- _ O
by -X- _ O
Luong -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

Our -X- _ O
contribution -X- _ O
lies -X- _ O
first -X- _ O
in -X- _ O
the -X- _ O
application -X- _ O
of -X- _ O
these -X- _ O
models -X- _ O
to -X- _ O
ASR -X- _ B-TaskName
, -X- _ O
and -X- _ O
secondly -X- _ O
their -X- _ O
extension -X- _ O
with -X- _ O
an -X- _ O
attention -X- _ B-MethodName
mechanism -X- _ I-MethodName
. -X- _ O

Methods -X- _ O
that -X- _ O
apply -X- _ O
low -X- _ O
- -X- _ O
rank -X- _ O
matrix -X- _ O
factorization -X- _ O
to -X- _ O
RNNs -X- _ O
are -X- _ O
somewhat -X- _ O
newer -X- _ O
, -X- _ O
and -X- _ O
were -X- _ O
first -X- _ O
explored -X- _ O
by -X- _ O
Kuchaiev -X- _ O
and -X- _ O
Ginsburg -X- _ O
( -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
concatenation -X- _ B-MethodName
- -X- _ I-MethodName
based -X- _ I-MethodName
ap -X- _ I-MethodName
- -X- _ I-MethodName
proach -X- _ I-MethodName
has -X- _ O
been -X- _ O
adopted -X- _ O
as -X- _ O
a -X- _ O
common -X- _ O
method -X- _ O
for -X- _ O
incorporating -X- _ O
non -X- _ O
- -X- _ O
linguistic -X- _ O
context -X- _ O
into -X- _ O
a -X- _ O
neural -X- _ O
LM -X- _ O
( -X- _ O
Yogatama -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017;Wen -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2013;Ma -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018;Ghosh -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
. -X- _ O

Outside -X- _ O
of -X- _ O
ASR -X- _ B-TaskName
, -X- _ O
our -X- _ O
work -X- _ O
directly -X- _ O
builds -X- _ O
upon -X- _ O
the -X- _ O
concatenation -X- _ O
- -X- _ O
based -X- _ O
( -X- _ O
Mikolov -X- _ O
and -X- _ O
Zweig -X- _ O
, -X- _ O
2012 -X- _ O
) -X- _ O
and -X- _ O
factorization -X- _ O
- -X- _ O
based -X- _ O
( -X- _ O
Jaech -X- _ O
and -X- _ O
Ostendorf -X- _ O
, -X- _ O
2018a -X- _ O
) -X- _ O
approaches -X- _ O
to -X- _ O
condition -X- _ O
RNN -X- _ B-MethodName
- -X- _ I-MethodName
LMs -X- _ I-MethodName
on -X- _ O
sentence -X- _ O
context -X- _ O
. -X- _ O

Related -X- _ O
Work -X- _ O
. -X- _ O

Another -X- _ O
related -X- _ O
line -X- _ O
of -X- _ O
research -X- _ O
has -X- _ O
explored -X- _ O
learning -X- _ O
utterance -X- _ O
embeddings -X- _ O
for -X- _ O
dialogue -X- _ O
systems -X- _ O
using -X- _ O
Gaussian -X- _ O
mixture -X- _ O
models -X- _ O
that -X- _ O
are -X- _ O
enhanced -X- _ O
with -X- _ O
utterance -X- _ O
- -X- _ O
level -X- _ O
context -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
intent -X- _ O
( -X- _ O
Yan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O

Overall -X- _ O
, -X- _ O
the -X- _ O
behavior -X- _ O
of -X- _ O
the -X- _ O
attention -X- _ B-MethodName
mechanism -X- _ I-MethodName
is -X- _ O
consistent -X- _ O
with -X- _ O
our -X- _ O
initial -X- _ O
hypothesis -X- _ O
that -X- _ O
certain -X- _ O
types -X- _ O
of -X- _ O
datetime -X- _ O
information -X- _ O
can -X- _ O
benefit -X- _ O
a -X- _ O
contextual -X- _ O
LM -X- _ O
model -X- _ O
more -X- _ O
than -X- _ O
others -X- _ O
over -X- _ O
the -X- _ O
course -X- _ O
of -X- _ O
an -X- _ O
utterance -X- _ O
. -X- _ O

This -X- _ O
shift -X- _ O
might -X- _ O
indicate -X- _ O
that -X- _ O
the -X- _ O
model -X- _ O
has -X- _ O
learned -X- _ O
to -X- _ O
condition -X- _ O
the -X- _ O
type -X- _ O
of -X- _ O
music -X- _ O
users -X- _ O
listen -X- _ O
to -X- _ O
to -X- _ O
the -X- _ O
hour -X- _ O
of -X- _ O
the -X- _ O
day -X- _ O
. -X- _ O

Finally -X- _ O
when -X- _ O
the -X- _ O
word -X- _ O
" -X- _ O
songs -X- _ O
" -X- _ O
is -X- _ O
ingested -X- _ O
, -X- _ O
the -X- _ O
model -X- _ O
substantially -X- _ O
reduces -X- _ O
the -X- _ O
weight -X- _ O
placed -X- _ O
on -X- _ O
month -X- _ O
information -X- _ O
and -X- _ O
in -X- _ O
turn -X- _ O
increases -X- _ O
the -X- _ O
weight -X- _ O
on -X- _ O
hour -X- _ O
information -X- _ O
. -X- _ O

Once -X- _ O
the -X- _ O
model -X- _ O
observes -X- _ O
the -X- _ O
word -X- _ O
" -X- _ O
christmas -X- _ O
" -X- _ O
, -X- _ O
it -X- _ O
places -X- _ O
all -X- _ O
of -X- _ O
the -X- _ O
attention -X- _ O
on -X- _ O
the -X- _ O
month -X- _ O
information -X- _ O
, -X- _ O
indicating -X- _ O
the -X- _ O
model -X- _ O
has -X- _ O
successfully -X- _ O
learned -X- _ O
that -X- _ O
" -X- _ O
christmas -X- _ O
" -X- _ O
is -X- _ O
a -X- _ O
word -X- _ O
strongly -X- _ O
associated -X- _ O
with -X- _ O
a -X- _ O
particular -X- _ O
month -X- _ O
( -X- _ O
i.e. -X- _ O
, -X- _ O
December -X- _ O
) -X- _ O
. -X- _ O

This -X- _ O
would -X- _ O
suggest -X- _ O
that -X- _ O
conditioning -X- _ O
on -X- _ O
the -X- _ O
fact -X- _ O
that -X- _ O
the -X- _ O
utterance -X- _ O
was -X- _ O
spoken -X- _ O
in -X- _ O
December -X- _ O
can -X- _ O
help -X- _ O
the -X- _ O
model -X- _ O
predict -X- _ O
the -X- _ O
type -X- _ O
of -X- _ O
media -X- _ O
to -X- _ O
play -X- _ O
. -X- _ O

However -X- _ O
as -X- _ O
the -X- _ O
model -X- _ O
processes -X- _ O
the -X- _ O
subsequent -X- _ O
words -X- _ O
" -X- _ O
play -X- _ O
me -X- _ O
best -X- _ O
" -X- _ O
, -X- _ O
the -X- _ O
attention -X- _ O
begins -X- _ O
to -X- _ O
shift -X- _ O
towards -X- _ O
using -X- _ O
more -X- _ O
of -X- _ O
the -X- _ O
month -X- _ O
information -X- _ O
( -X- _ O
i.e. -X- _ O
, -X- _ O
that -X- _ O
this -X- _ O
utterance -X- _ O
was -X- _ O
spoken -X- _ O
in -X- _ O
December -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
away -X- _ O
from -X- _ O
hour -X- _ O
and -X- _ O
day -X- _ O
information -X- _ O
. -X- _ O

When -X- _ O
the -X- _ O
model -X- _ O
processes -X- _ O
the -X- _ O
start -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
sentence -X- _ O
token -X- _ O
, -X- _ O
the -X- _ O
attention -X- _ O
mechanism -X- _ O
weights -X- _ O
each -X- _ O
of -X- _ O
the -X- _ O
datetime -X- _ O
information -X- _ O
roughly -X- _ O
equally -X- _ O
. -X- _ O

Figure -X- _ O
4 -X- _ O
shows -X- _ O
this -X- _ O
analysis -X- _ O
. -X- _ O

For -X- _ O
a -X- _ O
given -X- _ O
utterance -X- _ O
like -X- _ O
" -X- _ O
play -X- _ O
me -X- _ O
best -X- _ O
christmas -X- _ O
songs -X- _ O
" -X- _ O
spoken -X- _ O
in -X- _ O
December -X- _ O
, -X- _ O
we -X- _ O
highlight -X- _ O
the -X- _ O
changing -X- _ O
weight -X- _ O
placed -X- _ O
on -X- _ O
each -X- _ O
of -X- _ O
the -X- _ O
datetime -X- _ O
information -X- _ O
. -X- _ O

To -X- _ O
do -X- _ O
so -X- _ O
, -X- _ O
we -X- _ O
visualize -X- _ O
the -X- _ O
weights -X- _ O
of -X- _ O
the -X- _ O
attention -X- _ O
mechanism -X- _ O
as -X- _ O
an -X- _ O
utterance -X- _ O
is -X- _ O
processed -X- _ O
by -X- _ O
the -X- _ O
model -X- _ O
. -X- _ O

We -X- _ O
next -X- _ O
seek -X- _ O
to -X- _ O
understand -X- _ O
how -X- _ O
the -X- _ O
attention -X- _ O
mechanism -X- _ O
constructs -X- _ O
a -X- _ O
dynamic -X- _ O
representations -X- _ O
of -X- _ O
datetime -X- _ O
context -X- _ O
. -X- _ O

Attention -X- _ O
Weights -X- _ O
. -X- _ O

This -X- _ O
analysis -X- _ O
further -X- _ O
corroborates -X- _ O
that -X- _ O
the -X- _ O
trained -X- _ O
contextual -X- _ O
LMs -X- _ O
successfully -X- _ O
condition -X- _ O
their -X- _ O
predictions -X- _ O
on -X- _ O
datetime -X- _ O
information -X- _ O
. -X- _ O

The -X- _ O
horizontal -X- _ O
blue -X- _ O
dashed -X- _ O
line -X- _ O
indicates -X- _ O
the -X- _ O
conditional -X- _ O
probability -X- _ O
of -X- _ O
the -X- _ O
word -X- _ O
' -X- _ O
snooze -X- _ O
' -X- _ O
following -X- _ O
the -X- _ O
start -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
sentence -X- _ O
token -X- _ O
when -X- _ O
evaluated -X- _ O
with -X- _ O
a -X- _ O
LM -X- _ O
that -X- _ O
does -X- _ O
not -X- _ O
ingest -X- _ O
datetime -X- _ O
information -X- _ O
. -X- _ O

As -X- _ O
we -X- _ O
would -X- _ O
expect -X- _ O
, -X- _ O
the -X- _ O
probability -X- _ O
of -X- _ O
this -X- _ O
" -X- _ O
snooze -X- _ O
" -X- _ O
is -X- _ O
highest -X- _ O
in -X- _ O
the -X- _ O
morning -X- _ O
( -X- _ O
between -X- _ O
5 -X- _ O
and -X- _ O
6 -X- _ O
am -X- _ O
) -X- _ O
, -X- _ O
as -X- _ O
users -X- _ O
are -X- _ O
waking -X- _ O
up -X- _ O
and -X- _ O
snoozing -X- _ O
their -X- _ O
alarms -X- _ O
. -X- _ O

In -X- _ O
Figure -X- _ O
3 -X- _ O
, -X- _ O
we -X- _ O
evaluate -X- _ O
the -X- _ O
conditional -X- _ O
probability -X- _ O
of -X- _ O
the -X- _ O
word -X- _ O
" -X- _ O
snooze -X- _ O
" -X- _ O
in -X- _ O
an -X- _ O
utterance -X- _ O
following -X- _ O
the -X- _ O
start -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
sentence -X- _ O
token -X- _ O
, -X- _ O
as -X- _ O
we -X- _ O
vary -X- _ O
the -X- _ O
hour -X- _ O
of -X- _ O
day -X- _ O
information -X- _ O
associated -X- _ O
with -X- _ O
this -X- _ O
utterance -X- _ O
. -X- _ O

For -X- _ O
a -X- _ O
given -X- _ O
utterance -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
evaluate -X- _ O
the -X- _ O
probability -X- _ O
of -X- _ O
the -X- _ O
words -X- _ O
in -X- _ O
the -X- _ O
utterance -X- _ O
as -X- _ O
we -X- _ O
vary -X- _ O
the -X- _ O
datetime -X- _ O
information -X- _ O
associated -X- _ O
with -X- _ O
the -X- _ O
utterance -X- _ O
. -X- _ O

In -X- _ O
addition -X- _ O
to -X- _ O
these -X- _ O
results -X- _ O
, -X- _ O
we -X- _ O
visualize -X- _ O
how -X- _ O
the -X- _ O
contextual -X- _ O
LMs -X- _ O
leverage -X- _ O
datetime -X- _ O
contexts -X- _ O
. -X- _ O

Visual -X- _ O
Analysis -X- _ O
. -X- _ O

Method -X- _ O
. -X- _ O

Recall -X- _ O
that -X- _ O
when -X- _ O
trained -X- _ O
on -X- _ O
correct -X- _ O
datetime -X- _ O
information -X- _ O
this -X- _ O
was -X- _ O
our -X- _ O
best -X- _ O
- -X- _ O
performing -X- _ O
model -X- _ O
overall -X- _ O
in -X- _ O
terms -X- _ O
of -X- _ O
both -X- _ O
perplexity -X- _ B-MetricName
and -X- _ O
WER -X- _ B-MetricName
, -X- _ O
indicating -X- _ O
that -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
this -X- _ O
model -X- _ O
can -X- _ O
be -X- _ O
attributed -X- _ O
in -X- _ O
part -X- _ O
to -X- _ O
its -X- _ O
use -X- _ O
of -X- _ O
contextual -X- _ O
information -X- _ O
. -X- _ O

We -X- _ O
observed -X- _ O
the -X- _ O
overall -X- _ O
largest -X- _ O
relative -X- _ O
degradation -X- _ O
in -X- _ O
perplexity -X- _ B-MetricName
, -X- _ O
when -X- _ O
using -X- _ O
the -X- _ O
concatenationbased -X- _ O
model -X- _ O
. -X- _ O

In -X- _ O
general -X- _ O
, -X- _ O
if -X- _ O
a -X- _ O
model -X- _ O
uses -X- _ O
datetime -X- _ O
information -X- _ O
as -X- _ O
an -X- _ O
additional -X- _ O
signal -X- _ O
, -X- _ O
we -X- _ O
would -X- _ O
expect -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
decrease -X- _ O
when -X- _ O
the -X- _ O
datetime -X- _ O
context -X- _ O
is -X- _ O
shuffled -X- _ O
. -X- _ O

In -X- _ O
Table -X- _ O
4 -X- _ O
, -X- _ O
we -X- _ O
report -X- _ O
the -X- _ O
relative -X- _ O
degradation -X- _ O
( -X- _ O
i.e. -X- _ O
, -X- _ O
a -X- _ O
negative -X- _ O
reduction -X- _ O
) -X- _ O
in -X- _ O
perplexity -X- _ O
resulting -X- _ O
from -X- _ O
evaluating -X- _ O
these -X- _ O
models -X- _ O
on -X- _ O
the -X- _ O
shuffled -X- _ O
datetime -X- _ O
contexts -X- _ O
. -X- _ O

For -X- _ O
each -X- _ O
of -X- _ O
our -X- _ O
best -X- _ O
- -X- _ O
performing -X- _ O
models -X- _ O
in -X- _ O
a -X- _ O
given -X- _ O
category -X- _ O
of -X- _ O
method -X- _ O
( -X- _ O
prepend -X- _ B-MethodName
, -X- _ O
concat -X- _ B-MethodName
, -X- _ O
or -X- _ O
factor -X- _ B-MethodName
) -X- _ O
, -X- _ O
we -X- _ O
retrained -X- _ O
and -X- _ O
evaluated -X- _ O
those -X- _ O
models -X- _ O
on -X- _ O
the -X- _ O
dataset -X- _ O
containing -X- _ O
shuffled -X- _ O
datetime -X- _ O
information -X- _ O
. -X- _ O

terance -X- _ O
in -X- _ O
our -X- _ O
training -X- _ O
and -X- _ O
test -X- _ O
sets -X- _ O
. -X- _ O

To -X- _ O
answer -X- _ O
this -X- _ O
question -X- _ O
, -X- _ O
we -X- _ O
randomly -X- _ O
shuffled -X- _ O
the -X- _ O
datetime -X- _ O
information -X- _ O
associated -X- _ O
with -X- _ O
each -X- _ O
ut-2 -X- _ O
We -X- _ O
use -X- _ O
a -X- _ O
two -X- _ O
integer -X- _ O
precision -X- _ O
geo -X- _ O
- -X- _ O
hash -X- _ O
. -X- _ O

The -X- _ O
first -X- _ O
question -X- _ O
we -X- _ O
hope -X- _ O
to -X- _ O
answer -X- _ O
is -X- _ O
: -X- _ O
to -X- _ O
what -X- _ O
extent -X- _ O
can -X- _ O
the -X- _ O
relative -X- _ O
improvements -X- _ O
in -X- _ O
perplexity -X- _ B-MetricName
and -X- _ O
WER -X- _ B-MetricName
in -X- _ O
the -X- _ O
models -X- _ O
that -X- _ O
incorporate -X- _ O
datetime -X- _ O
context -X- _ O
be -X- _ O
explained -X- _ O
by -X- _ O
the -X- _ O
additional -X- _ O
signal -X- _ O
from -X- _ O
the -X- _ O
context -X- _ O
versus -X- _ O
the -X- _ O
additional -X- _ O
parameters -X- _ O
that -X- _ O
these -X- _ O
models -X- _ O
contain -X- _ O
? -X- _ O

Datetime -X- _ O
Context -X- _ O
Signal -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
section -X- _ O
, -X- _ O
we -X- _ O
focus -X- _ O
once -X- _ O
again -X- _ O
on -X- _ O
datetime -X- _ O
information -X- _ O
to -X- _ O
better -X- _ O
understand -X- _ O
how -X- _ O
contextual -X- _ O
LMs -X- _ O
use -X- _ O
datetime -X- _ O
signal -X- _ O
. -X- _ O

Analysis -X- _ O
. -X- _ O

In -X- _ O
general -X- _ O
, -X- _ O
we -X- _ O
find -X- _ O
that -X- _ O
conditioning -X- _ O
neural -X- _ O
LMs -X- _ O
on -X- _ O
each -X- _ O
of -X- _ O
the -X- _ O
different -X- _ O
types -X- _ O
of -X- _ O
context -X- _ O
reduces -X- _ O
perplexity -X- _ B-MetricName
and -X- _ O
WER -X- _ B-MetricName
. -X- _ O

Table -X- _ O
3 -X- _ O
summarizes -X- _ O
the -X- _ O
results -X- _ O
. -X- _ O

We -X- _ O
evaluate -X- _ O
these -X- _ O
models -X- _ O
on -X- _ O
a -X- _ O
test -X- _ O
set -X- _ O
of -X- _ O
de -X- _ O
- -X- _ O
identified -X- _ O
utterances -X- _ O
representative -X- _ O
of -X- _ O
user -X- _ O
interactions -X- _ O
with -X- _ O
Alexa -X- _ O
. -X- _ O

Dialogue -X- _ O
prompts -X- _ O
indicate -X- _ O
whether -X- _ O
a -X- _ O
transcribed -X- _ O
utterance -X- _ O
was -X- _ O
an -X- _ O
initial -X- _ O
query -X- _ O
to -X- _ O
the -X- _ O
dialog -X- _ O
system -X- _ O
or -X- _ O
if -X- _ O
it -X- _ O
was -X- _ O
a -X- _ O
follow -X- _ O
- -X- _ O
up -X- _ O
turn -X- _ O
. -X- _ O

Relative -X- _ O
PPL -X- _ B-MetricName
Reduction -X- _ I-MetricName
( -X- _ O
% -X- _ O
) -X- _ O
WERR -X- _ B-MetricName
( -X- _ O
% -X- _ O
) -X- _ O
Full -X- _ O
Tail -X- _ O
Full -X- _ O
Tail -X- _ O
Default -X- _ O
0.0 -X- _ O
0.0 -X- _ O
0.0 -X- _ O
0.0 -X- _ O
Datetime -X- _ O
11.4 -X- _ O
11.6 -X- _ O
1.6 -X- _ O
1.7 -X- _ O
Geo -X- _ O
- -X- _ O
hash -X- _ O
12.4 -X- _ O
12.5 -X- _ O
0.5 -X- _ O
1.0 -X- _ O
Dialogue -X- _ O
Prompt -X- _ O
13.9 -X- _ O
14.1 -X- _ O
0.3 -X- _ O
0.6 -X- _ O
We -X- _ O
train -X- _ O
the -X- _ O
LMs -X- _ O
on -X- _ O
a -X- _ O
subset -X- _ O
of -X- _ O
the -X- _ O
utterances -X- _ O
of -X- _ O
the -X- _ O
initial -X- _ O
dataset -X- _ O
which -X- _ O
also -X- _ O
contain -X- _ O
utterancelevel -X- _ O
geo -X- _ O
- -X- _ O
hash -X- _ O
information -X- _ O
and -X- _ O
dialogue -X- _ O
prompt -X- _ O
information -X- _ O
. -X- _ O

Context -X- _ O
Type -X- _ O
. -X- _ O

To -X- _ O
illustrate -X- _ O
this -X- _ O
point -X- _ O
we -X- _ O
train -X- _ O
two -X- _ O
neural -X- _ O
LMs -X- _ O
using -X- _ O
two -X- _ O
other -X- _ O
types -X- _ O
of -X- _ O
context -X- _ O
: -X- _ O
geolocation -X- _ O
information -X- _ O
and -X- _ O
dialogue -X- _ O
prompts -X- _ O
. -X- _ O

We -X- _ O
underscore -X- _ O
, -X- _ O
however -X- _ O
, -X- _ O
that -X- _ O
the -X- _ O
contextual -X- _ O
mechanism -X- _ O
we -X- _ O
introduce -X- _ O
can -X- _ O
be -X- _ O
applied -X- _ O
to -X- _ O
any -X- _ O
type -X- _ O
of -X- _ O
contextual -X- _ O
information -X- _ O
that -X- _ O
can -X- _ O
be -X- _ O
represented -X- _ O
as -X- _ O
embeddings -X- _ O
. -X- _ O

So -X- _ O
far -X- _ O
, -X- _ O
our -X- _ O
experiments -X- _ O
have -X- _ O
focused -X- _ O
exclusively -X- _ O
on -X- _ O
conditioning -X- _ O
neural -X- _ O
LMs -X- _ O
on -X- _ O
datetime -X- _ O
context -X- _ O
. -X- _ O

Other -X- _ O
Non -X- _ O
- -X- _ O
Linguistic -X- _ O
Context -X- _ O
. -X- _ O

Method -X- _ O
. -X- _ O

As -X- _ O
in -X- _ O
Table -X- _ O
1 -X- _ O
, -X- _ O
the -X- _ O
concatenationbased -X- _ O
model -X- _ O
with -X- _ O
attention -X- _ O
mechanism -X- _ O
achieved -X- _ O
the -X- _ O
largest -X- _ O
reductions -X- _ O
in -X- _ O
WER -X- _ O
. -X- _ O

We -X- _ O
evaluated -X- _ O
the -X- _ O
relative -X- _ O
WER -X- _ B-MetricName
reduction(WERR -X- _ I-MetricName
) -X- _ O
on -X- _ O
a -X- _ O
large -X- _ O
test -X- _ O
set -X- _ O
of -X- _ O
de -X- _ O
- -X- _ O
identified -X- _ O
, -X- _ O
transcribed -X- _ O
utterances -X- _ O
representative -X- _ O
of -X- _ O
general -X- _ O
user -X- _ O
interactions -X- _ O
with -X- _ O
Alexa -X- _ O
, -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
on -X- _ O
the -X- _ O
tail -X- _ O
of -X- _ O
this -X- _ O
dataset -X- _ O
. -X- _ O

Table -X- _ O
2 -X- _ O
summarizes -X- _ O
the -X- _ O
results -X- _ O
. -X- _ O

As -X- _ O
the -X- _ O
LM -X- _ O
component -X- _ O
of -X- _ O
this -X- _ O
system -X- _ O
, -X- _ O
we -X- _ O
used -X- _ O
the -X- _ O
bestperforming -X- _ O
models -X- _ O
within -X- _ O
each -X- _ O
category -X- _ O
of -X- _ O
method -X- _ O
that -X- _ O
we -X- _ O
report -X- _ O
in -X- _ O
Table -X- _ O
1 -X- _ O
. -X- _ O

In -X- _ O
addition -X- _ O
to -X- _ O
evaluating -X- _ O
the -X- _ O
models -X- _ O
on -X- _ O
relative -X- _ O
reductions -X- _ O
in -X- _ O
perplexity -X- _ O
, -X- _ O
we -X- _ O
also -X- _ O
validated -X- _ O
the -X- _ O
downstream -X- _ O
performance -X- _ O
of -X- _ O
a -X- _ O
hybrid -X- _ B-MethodName
CTC -X- _ I-MethodName
- -X- _ I-MethodName
HMM -X- _ I-MethodName
( -X- _ O
Graves -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2006 -X- _ O
) -X- _ O
ASR -X- _ O
system -X- _ O
that -X- _ O
incorporated -X- _ O
contextual -X- _ O
information -X- _ O
in -X- _ O
its -X- _ O
LM -X- _ O
component -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
instance -X- _ O
, -X- _ O
perplexity -X- _ B-MetricName
was -X- _ O
reduced -X- _ O
by -X- _ O
2.8 -X- _ B-MetricValue
% -X- _ I-MetricValue
on -X- _ O
the -X- _ O
tail -X- _ O
of -X- _ O
our -X- _ O
evaluation -X- _ O
set -X- _ O
, -X- _ O
and -X- _ O
by -X- _ O
2.1 -X- _ B-MetricValue
% -X- _ I-MetricValue
on -X- _ O
the -X- _ O
full -X- _ O
dataset -X- _ O
. -X- _ O

The -X- _ O
use -X- _ O
of -X- _ O
attention -X- _ O
led -X- _ O
to -X- _ O
the -X- _ O
largest -X- _ O
relative -X- _ O
improvement -X- _ O
in -X- _ O
the -X- _ O
factorization -X- _ O
- -X- _ O
based -X- _ O
approach -X- _ O
when -X- _ O
using -X- _ O
learned -X- _ O
context -X- _ O
embeddings -X- _ O
. -X- _ O

In -X- _ O
nearly -X- _ O
every -X- _ O
experiment -X- _ O
we -X- _ O
ran -X- _ O
, -X- _ O
we -X- _ O
found -X- _ O
that -X- _ O
our -X- _ O
attention -X- _ B-MethodName
mechanism -X- _ I-MethodName
further -X- _ O
reduced -X- _ O
perplexity -X- _ O
. -X- _ O

Again -X- _ O
, -X- _ O
we -X- _ O
found -X- _ O
that -X- _ O
on -X- _ O
the -X- _ O
full -X- _ O
dataset -X- _ O
the -X- _ O
improvement -X- _ O
in -X- _ O
perplexity -X- _ B-MetricName
by -X- _ O
using -X- _ O
our -X- _ O
attention -X- _ O
mechanism -X- _ O
was -X- _ O
statistically -X- _ O
significant -X- _ O
. -X- _ O

In -X- _ O
the -X- _ O
case -X- _ O
of -X- _ O
the -X- _ O
factorization -X- _ O
- -X- _ O
based -X- _ O
approach -X- _ O
, -X- _ O
we -X- _ O
achieved -X- _ O
the -X- _ O
lowest -X- _ O
perplexity -X- _ O
when -X- _ O
the -X- _ O
attention -X- _ B-MethodName
mechanism -X- _ I-MethodName
used -X- _ O
the -X- _ O
hidden -X- _ O
state -X- _ O
of -X- _ O
the -X- _ O
RNN -X- _ O
model -X- _ O
as -X- _ O
the -X- _ O
query -X- _ O
vector -X- _ O
and -X- _ O
datetime -X- _ O
information -X- _ O
was -X- _ O
represented -X- _ O
as -X- _ O
a -X- _ O
feature -X- _ O
- -X- _ O
engineered -X- _ O
vector -X- _ O
. -X- _ O

Figure -X- _ O
2 -X- _ O
visualizes -X- _ O
the -X- _ O
intervals -X- _ O
. -X- _ O

Confidence -X- _ O
intervals -X- _ O
were -X- _ O
calculated -X- _ O
by -X- _ O
running -X- _ O
the -X- _ O
training -X- _ O
algorithm -X- _ O
10 -X- _ O
times -X- _ O
for -X- _ O
each -X- _ O
model -X- _ O
type -X- _ O
. -X- _ O

We -X- _ O
corroborated -X- _ O
these -X- _ O
results -X- _ O
by -X- _ O
computing -X- _ O
95 -X- _ O
% -X- _ O
confidence -X- _ O
intervals -X- _ O
for -X- _ O
the -X- _ O
best -X- _ O
- -X- _ O
performing -X- _ O
concatenation -X- _ O
- -X- _ O
based -X- _ O
models -X- _ O
with -X- _ O
and -X- _ O
without -X- _ O
attention -X- _ O
. -X- _ O

We -X- _ O
obtained -X- _ O
the -X- _ O
best -X- _ O
results -X- _ O
when -X- _ O
representing -X- _ O
datetime -X- _ O
information -X- _ O
as -X- _ O
learned -X- _ O
embeddings -X- _ O
and -X- _ O
using -X- _ O
the -X- _ O
input -X- _ O
embedding -X- _ O
at -X- _ O
a -X- _ O
given -X- _ O
time -X- _ O
- -X- _ O
step -X- _ O
as -X- _ O
the -X- _ O
query -X- _ O
vector -X- _ O
. -X- _ O

For -X- _ O
the -X- _ O
concatenation -X- _ O
- -X- _ O
based -X- _ O
model -X- _ O
, -X- _ O
we -X- _ O
found -X- _ O
that -X- _ O
adding -X- _ O
our -X- _ O
attention -X- _ O
mechanism -X- _ O
led -X- _ O
to -X- _ O
further -X- _ O
reductions -X- _ O
in -X- _ O
perplexity -X- _ B-MetricName
, -X- _ O
regardless -X- _ O
of -X- _ O
the -X- _ O
type -X- _ O
of -X- _ O
query -X- _ O
vector -X- _ O
or -X- _ O
context -X- _ O
representation -X- _ O
used -X- _ O
. -X- _ O

We -X- _ O
also -X- _ O
differentiate -X- _ O
between -X- _ O
the -X- _ O
two -X- _ O
variants -X- _ O
of -X- _ O
encoding -X- _ O
the -X- _ O
query -X- _ O
vector -X- _ O
used -X- _ O
by -X- _ O
the -X- _ O
attention -X- _ B-MethodName
mechanism -X- _ I-MethodName
: -X- _ O
either -X- _ O
by -X- _ O
using -X- _ O
the -X- _ O
hidden -X- _ O
state -X- _ O
vector -X- _ O
, -X- _ O
or -X- _ O
the -X- _ O
input -X- _ O
embedding -X- _ O
at -X- _ O
a -X- _ O
given -X- _ O
time -X- _ O
- -X- _ O
step -X- _ O
. -X- _ O

In -X- _ O
reporting -X- _ O
our -X- _ O
results -X- _ O
, -X- _ O
we -X- _ O
distinguish -X- _ O
between -X- _ O
the -X- _ O
two -X- _ O
forms -X- _ O
of -X- _ O
representing -X- _ O
contextual -X- _ O
information -X- _ O
: -X- _ O
either -X- _ O
as -X- _ O
learned -X- _ O
embeddings -X- _ O
or -X- _ O
as -X- _ O
a -X- _ O
featureengineered -X- _ O
representation -X- _ O
. -X- _ O

We -X- _ O
additionally -X- _ O
trained -X- _ O
a -X- _ O
simple -X- _ O
baseline -X- _ O
, -X- _ O
Prepend -X- _ B-MethodName
, -X- _ O
which -X- _ O
was -X- _ O
comprised -X- _ O
of -X- _ O
a -X- _ O
standard -X- _ O
LSTM -X- _ O
model -X- _ O
that -X- _ O
treated -X- _ O
datetime -X- _ O
context -X- _ O
as -X- _ O
input -X- _ O
tokens -X- _ O
that -X- _ O
were -X- _ O
prepended -X- _ O
to -X- _ O
the -X- _ O
input -X- _ O
texts -X- _ O
. -X- _ O

In -X- _ O
Table -X- _ O
1 -X- _ O
, -X- _ O
we -X- _ O
report -X- _ O
the -X- _ O
relative -X- _ O
decrease -X- _ O
in -X- _ O
perplexity -X- _ O
of -X- _ O
models -X- _ O
that -X- _ O
leveraged -X- _ O
datetime -X- _ O
context -X- _ O
compared -X- _ O
to -X- _ O
a -X- _ O
baseline -X- _ O
LSTM -X- _ B-MethodName
model -X- _ O
that -X- _ O
did -X- _ O
not -X- _ O
use -X- _ O
any -X- _ O
contextual -X- _ O
information -X- _ O
. -X- _ O

In -X- _ O
practice -X- _ O
, -X- _ O
these -X- _ O
two -X- _ O
statistics -X- _ O
have -X- _ O
been -X- _ O
shown -X- _ O
to -X- _ O
be -X- _ O
correlated -X- _ O
by -X- _ O
a -X- _ O
power -X- _ O
law -X- _ O
relationship -X- _ O
( -X- _ O
Klakow -X- _ O
and -X- _ O
Peters -X- _ O
, -X- _ O
2002 -X- _ O
) -X- _ O
. -X- _ O

Word -X- _ O
error -X- _ B-MetricName
rate -X- _ I-MetricName
, -X- _ O
on -X- _ O
the -X- _ O
other -X- _ O
hand -X- _ O
, -X- _ O
measures -X- _ O
the -X- _ O
Levenshtein -X- _ O
( -X- _ O
minimum -X- _ O
edit -X- _ O
) -X- _ O
distance -X- _ O
between -X- _ O
a -X- _ O
recognized -X- _ O
word -X- _ O
sequence -X- _ O
and -X- _ O
a -X- _ O
reference -X- _ O
word -X- _ O
sequence -X- _ O
. -X- _ O

Perplexity -X- _ B-MetricName
is -X- _ O
a -X- _ O
common -X- _ O
statistic -X- _ O
widely -X- _ O
used -X- _ O
in -X- _ O
language -X- _ O
modeling -X- _ O
and -X- _ O
speech -X- _ O
recognition -X- _ O
to -X- _ O
measure -X- _ O
how -X- _ O
well -X- _ O
a -X- _ O
language -X- _ O
model -X- _ O
predicts -X- _ O
a -X- _ O
sample -X- _ O
of -X- _ O
text -X- _ O
( -X- _ O
Jelinek -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
1977 -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
used -X- _ O
two -X- _ O
metrics -X- _ O
for -X- _ O
our -X- _ O
evaluations -X- _ O
: -X- _ O
perplexity -X- _ B-MetricName
and -X- _ O
word -X- _ O
error -X- _ B-MetricName
rate -X- _ I-MetricName
. -X- _ O

We -X- _ O
also -X- _ O
defined -X- _ O
the -X- _ O
head -X- _ O
and -X- _ O
tail -X- _ O
subsets -X- _ O
of -X- _ O
our -X- _ O
development -X- _ O
set -X- _ O
, -X- _ O
representing -X- _ O
, -X- _ O
respectively -X- _ O
, -X- _ O
the -X- _ O
top -X- _ O
5 -X- _ O
% -X- _ O
most -X- _ O
frequently -X- _ O
occurring -X- _ O
utterances -X- _ O
, -X- _ O
and -X- _ O
utterances -X- _ O
occurring -X- _ O
only -X- _ O
once -X- _ O
. -X- _ O

The -X- _ O
utterances -X- _ O
in -X- _ O
our -X- _ O
training -X- _ O
and -X- _ O
evaluation -X- _ O
set -X- _ O
were -X- _ O
collected -X- _ O
in -X- _ O
the -X- _ O
same -X- _ O
time -X- _ O
- -X- _ O
range -X- _ O
. -X- _ O

We -X- _ O
evaluated -X- _ O
our -X- _ O
models -X- _ O
on -X- _ O
a -X- _ O
heldout -X- _ O
set -X- _ O
of -X- _ O
utterances -X- _ O
that -X- _ O
were -X- _ O
randomly -X- _ O
sampled -X- _ O
from -X- _ O
the -X- _ O
full -X- _ O
dataset -X- _ O
. -X- _ O

Datetime -X- _ O
. -X- _ O

6 -X- _ O
Results -X- _ O
. -X- _ O

We -X- _ O
initialized -X- _ O
random -X- _ O
weights -X- _ O
using -X- _ O
Xavier -X- _ B-HyperparameterValue
- -X- _ I-HyperparameterValue
He -X- _ I-HyperparameterValue
weight -X- _ B-HyperparameterName
initialization -X- _ I-HyperparameterName
( -X- _ O
He -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
. -X- _ O

Other -X- _ O
hyperparameters -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
the -X- _ O
initial -X- _ B-HyperparameterName
learning -X- _ I-HyperparameterName
rate -X- _ I-HyperparameterName
, -X- _ O
were -X- _ O
selected -X- _ O
via -X- _ O
random -X- _ O
search -X- _ O
. -X- _ O

In -X- _ O
practice -X- _ O
, -X- _ O
we -X- _ O
found -X- _ O
that -X- _ O
the -X- _ O
larger -X- _ O
the -X- _ O
rank -X- _ B-HyperparameterName
size -X- _ O
the -X- _ O
less -X- _ O
stable -X- _ O
the -X- _ O
training -X- _ O
procedure -X- _ O
became -X- _ O
. -X- _ O

We -X- _ O
set -X- _ O
the -X- _ O
rank -X- _ B-HyperparameterName
of -X- _ O
the -X- _ O
basis -X- _ O
tensors -X- _ O
in -X- _ O
the -X- _ O
factorization -X- _ O
- -X- _ O
approach -X- _ O
to -X- _ O
5 -X- _ B-HyperparameterValue
, -X- _ O
after -X- _ O
experimenting -X- _ O
with -X- _ O
rank -X- _ B-HyperparameterName
sizes -X- _ O
2 -X- _ B-HyperparameterValue
, -X- _ O
3 -X- _ B-HyperparameterValue
, -X- _ O
10 -X- _ B-HyperparameterValue
, -X- _ O
15 -X- _ B-HyperparameterValue
, -X- _ O
20 -X- _ B-HyperparameterValue
. -X- _ O

We -X- _ O
initially -X- _ O
experimented -X- _ O
with -X- _ O
smaller -X- _ O
and -X- _ O
larger -X- _ O
embedding -X- _ B-HyperparameterName
sizes -X- _ I-HyperparameterName
( -X- _ O
50 -X- _ B-HyperparameterValue
, -X- _ O
100 -X- _ B-HyperparameterValue
, -X- _ O
1024 -X- _ B-HyperparameterValue
) -X- _ O
, -X- _ O
but -X- _ O
found -X- _ O
that -X- _ O
512 -X- _ B-HyperparameterValue
generally -X- _ O
provided -X- _ O
a -X- _ O
good -X- _ O
tradeoff -X- _ O
between -X- _ O
model -X- _ O
performance -X- _ O
and -X- _ O
compute -X- _ O
resources -X- _ O
required -X- _ O
to -X- _ O
train -X- _ O
a -X- _ O
model -X- _ O
. -X- _ O

We -X- _ O
used -X- _ O
a -X- _ O
fixed -X- _ O
dimensionality -X- _ B-HyperparameterName
of -X- _ O
512 -X- _ B-HyperparameterValue
for -X- _ O
word -X- _ O
, -X- _ O
context -X- _ O
and -X- _ O
hidden -X- _ O
state -X- _ O
embeddings -X- _ O
. -X- _ O

Implementation -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
and -X- _ O
training -X- _ O
procedure -X- _ O
was -X- _ O
written -X- _ O
in -X- _ O
PyTorch -X- _ O
and -X- _ O
native -X- _ O
PyTorch -X- _ O
libraries -X- _ O
. -X- _ O

The -X- _ O
training -X- _ O
of -X- _ O
each -X- _ O
model -X- _ O
was -X- _ O
conducted -X- _ O
on -X- _ O
a -X- _ O
single -X- _ O
V100 -X- _ O
GPU -X- _ O
, -X- _ O
with -X- _ O
16 -X- _ O
GB -X- _ O
of -X- _ O
memory -X- _ O
on -X- _ O
a -X- _ O
Linux -X- _ O
cluster -X- _ O
, -X- _ O
and -X- _ O
took -X- _ O
roughly -X- _ O
6 -X- _ O
hours -X- _ O
to -X- _ O
train -X- _ O
. -X- _ O

400,000 -X- _ B-HyperparameterName
batch -X- _ B-HyperparameterValue
update -X- _ I-HyperparameterValue
steps -X- _ I-HyperparameterValue
, -X- _ O
using -X- _ O
a -X- _ O
batch -X- _ B-HyperparameterName
- -X- _ I-HyperparameterName
size -X- _ I-HyperparameterName
of -X- _ O
256 -X- _ B-HyperparameterValue
. -X- _ O

Models -X- _ O
were -X- _ O
trained -X- _ O
using -X- _ O
the -X- _ O
Adam -X- _ B-HyperparameterValue
optimizer -X- _ B-HyperparameterName
with -X- _ O
an -X- _ O
initial -X- _ B-HyperparameterName
learning -X- _ I-HyperparameterName
rate -X- _ I-HyperparameterName
of -X- _ O
0.001 -X- _ B-HyperparameterValue
, -X- _ O
and -X- _ O
a -X- _ O
standard -X- _ O
cross -X- _ O
entropy -X- _ O
loss -X- _ O
function -X- _ O
. -X- _ O

Both -X- _ O
the -X- _ O
concatenationbased -X- _ O
and -X- _ O
factorization -X- _ O
- -X- _ O
based -X- _ O
methods -X- _ O
can -X- _ O
be -X- _ O
easily -X- _ O
adapted -X- _ O
to -X- _ O
use -X- _ O
an -X- _ O
LSTM -X- _ O
cell -X- _ O
. -X- _ O

We -X- _ O
used -X- _ O
a -X- _ O
1 -X- _ B-HyperparameterValue
- -X- _ O
recurrent -X- _ B-HyperparameterName
- -X- _ I-HyperparameterName
layer -X- _ I-HyperparameterName
LSTM -X- _ O
model -X- _ O
( -X- _ O
Hochreiter -X- _ O
and -X- _ O
Schmidhuber -X- _ O
, -X- _ O
1997 -X- _ O
) -X- _ O
as -X- _ O
the -X- _ O
base -X- _ O
model -X- _ O
in -X- _ O
all -X- _ O
of -X- _ O
our -X- _ O
experiments -X- _ O
. -X- _ O

Experimental -X- _ O
Setup -X- _ O
. -X- _ O

For -X- _ O
an -X- _ O
utterance -X- _ O
like -X- _ O
" -X- _ O
turn -X- _ O
alarm -X- _ O
off -X- _ O
" -X- _ O
, -X- _ O
we -X- _ O
showcase -X- _ O
how -X- _ O
the -X- _ O
model -X- _ O
builds -X- _ O
a -X- _ O
dynamic -X- _ O
representation -X- _ O
of -X- _ O
the -X- _ O
datetime -X- _ O
context -X- _ O
, -X- _ O
at -X- _ O
a -X- _ O
given -X- _ O
time -X- _ O
- -X- _ O
step -X- _ O
t -X- _ O
( -X- _ O
t -X- _ O
= -X- _ O
2 -X- _ O
in -X- _ O
the -X- _ O
figure -X- _ O
) -X- _ O
. -X- _ O

In -X- _ O
Figure -X- _ O
1 -X- _ O
we -X- _ O
illustrate -X- _ O
how -X- _ O
the -X- _ O
attention -X- _ O
mechanism -X- _ O
augments -X- _ O
the -X- _ O
concatenation -X- _ O
- -X- _ O
based -X- _ O
approach -X- _ O
. -X- _ O

m -X- _ O
t -X- _ O
= -X- _ O
|M -X- _ O
| -X- _ O
i=1 -X- _ O
α -X- _ O
i -X- _ O
, -X- _ O
t -X- _ O
m -X- _ O
i -X- _ O
We -X- _ O
can -X- _ O
now -X- _ O
use -X- _ O
this -X- _ O
constructed -X- _ O
context -X- _ O
, -X- _ O
as -X- _ O
the -X- _ O
context -X- _ O
input -X- _ O
to -X- _ O
either -X- _ O
the -X- _ O
concatenation -X- _ O
- -X- _ O
based -X- _ O
or -X- _ O
the -X- _ O
factorization -X- _ O
- -X- _ O
based -X- _ O
approach -X- _ O
. -X- _ O

We -X- _ O
then -X- _ O
define -X- _ O
the -X- _ O
alignment -X- _ O
score -X- _ O
as -X- _ O
α -X- _ O
i -X- _ O
, -X- _ O
t -X- _ O
= -X- _ O
sof -X- _ O
tmax(score(m -X- _ O
i -X- _ O
, -X- _ O
q -X- _ O
t -X- _ O
) -X- _ O
) -X- _ O
The -X- _ O
alignment -X- _ O
scores -X- _ O
are -X- _ O
finally -X- _ O
used -X- _ O
to -X- _ O
build -X- _ O
up -X- _ O
a -X- _ O
dynamic -X- _ O
representation -X- _ O
of -X- _ O
the -X- _ O
context -X- _ O
, -X- _ O
m -X- _ O
t -X- _ O
, -X- _ O
for -X- _ O
a -X- _ O
given -X- _ O
time -X- _ O
- -X- _ O
step -X- _ O
. -X- _ O

For -X- _ O
a -X- _ O
given -X- _ O
m -X- _ O
i -X- _ O
∈ -X- _ O
M -X- _ O
and -X- _ O
q -X- _ O
t -X- _ O
, -X- _ O
we -X- _ O
calculate -X- _ O
a -X- _ O
score -X- _ O
as -X- _ O
score(m -X- _ O
i -X- _ O
, -X- _ O
q -X- _ O
t -X- _ O
) -X- _ O
= -X- _ O
m -X- _ O
T -X- _ O
i -X- _ O
W -X- _ O
a -X- _ O
q -X- _ O
t -X- _ O
. -X- _ O

The -X- _ O
size -X- _ O
of -X- _ O
W -X- _ O
a -X- _ O
is -X- _ O
R -X- _ O
f -X- _ O
×e -X- _ O
if -X- _ O
q -X- _ O
t -X- _ O
= -X- _ O
x -X- _ O
t -X- _ O
, -X- _ O
or -X- _ O
R -X- _ O
f -X- _ O
×d -X- _ O
if -X- _ O
q -X- _ O
t -X- _ O
= -X- _ O
h -X- _ O
t -X- _ O
. -X- _ O

To -X- _ O
compute -X- _ O
this -X- _ O
score -X- _ O
, -X- _ O
we -X- _ O
learn -X- _ O
a -X- _ O
weight -X- _ O
matrix -X- _ O
W -X- _ O
a -X- _ O
. -X- _ O

Regardless -X- _ O
of -X- _ O
the -X- _ O
choice -X- _ O
of -X- _ O
q -X- _ O
t -X- _ O
, -X- _ O
the -X- _ O
attention -X- _ O
mechanism -X- _ O
first -X- _ O
computes -X- _ O
a -X- _ O
score -X- _ O
for -X- _ O
each -X- _ O
context -X- _ O
embedding -X- _ O
m -X- _ O
i -X- _ O
∈ -X- _ O
M -X- _ O
for -X- _ O
a -X- _ O
given -X- _ O
q -X- _ O
t -X- _ O
. -X- _ O

This -X- _ O
can -X- _ O
not -X- _ O
be -X- _ O
done -X- _ O
when -X- _ O
q -X- _ O
t -X- _ O
= -X- _ O
h -X- _ O
t -X- _ O
, -X- _ O
as -X- _ O
the -X- _ O
attention -X- _ O
mechanism -X- _ O
can -X- _ O
only -X- _ O
be -X- _ O
computed -X- _ O
sequentially -X- _ O
for -X- _ O
each -X- _ O
hidden -X- _ O
state -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
. -X- _ O

Let -X- _ O
q -X- _ O
t -X- _ O
= -X- _ O
h -X- _ O
t -X- _ O
, -X- _ O
where -X- _ O
h -X- _ O
t -X- _ O
is -X- _ O
the -X- _ O
hidden -X- _ O
state -X- _ O
of -X- _ O
the -X- _ O
RNN -X- _ O
model -X- _ O
at -X- _ O
time -X- _ O
- -X- _ O
step -X- _ O
t. -X- _ O

2 -X- _ O
. -X- _ O

Let -X- _ O
q -X- _ O
t -X- _ O
= -X- _ O
x -X- _ O
t -X- _ O
, -X- _ O
where -X- _ O
x -X- _ O
t -X- _ O
is -X- _ O
the -X- _ O
embedding -X- _ O
for -X- _ O
the -X- _ O
input -X- _ O
word -X- _ O
at -X- _ O
time -X- _ O
- -X- _ O
step -X- _ O
t. -X- _ O

We -X- _ O
propose -X- _ O
two -X- _ O
methods -X- _ O
for -X- _ O
defining -X- _ O
this -X- _ O
query -X- _ O
vector -X- _ O
1 -X- _ O
. -X- _ O

In -X- _ O
addition -X- _ O
to -X- _ O
the -X- _ O
set -X- _ O
M -X- _ O
, -X- _ O
the -X- _ O
attention -X- _ O
mechanism -X- _ O
takes -X- _ O
in -X- _ O
a -X- _ O
query -X- _ O
vector -X- _ O
, -X- _ O
q -X- _ O
t -X- _ O
, -X- _ O
for -X- _ O
each -X- _ O
time -X- _ O
- -X- _ O
step -X- _ O
t. -X- _ O

We -X- _ O
also -X- _ O
experiment -X- _ O
with -X- _ O
adding -X- _ O
a -X- _ O
similar -X- _ O
vector -X- _ O
of -X- _ O
all -X- _ O
0s -X- _ O
in -X- _ O
the -X- _ O
case -X- _ O
where -X- _ O
context -X- _ O
embeddings -X- _ O
are -X- _ O
learned -X- _ O
, -X- _ O
but -X- _ O
find -X- _ O
no -X- _ O
improvement -X- _ O
. -X- _ O

Thus -X- _ O
, -X- _ O
the -X- _ O
attention -X- _ B-MethodName
mechanism -X- _ I-MethodName
can -X- _ O
act -X- _ O
as -X- _ O
a -X- _ O
learnable -X- _ O
gate -X- _ O
to -X- _ O
limit -X- _ O
the -X- _ O
non -X- _ O
- -X- _ O
linguistic -X- _ O
context -X- _ O
passed -X- _ O
into -X- _ O
the -X- _ O
model -X- _ O
. -X- _ O

We -X- _ O
do -X- _ O
so -X- _ O
because -X- _ O
our -X- _ O
attention -X- _ O
mechanism -X- _ O
builds -X- _ O
a -X- _ O
dynamic -X- _ O
representation -X- _ O
of -X- _ O
the -X- _ O
context -X- _ O
by -X- _ O
interpolating -X- _ O
over -X- _ O
multiple -X- _ O
context -X- _ O
embeddings -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
in -X- _ O
the -X- _ O
case -X- _ O
where -X- _ O
datetime -X- _ O
information -X- _ O
is -X- _ O
represented -X- _ O
as -X- _ O
a -X- _ O
feature -X- _ O
- -X- _ O
engineered -X- _ O
vector -X- _ O
, -X- _ O
we -X- _ O
augment -X- _ O
M -X- _ O
to -X- _ O
include -X- _ O
an -X- _ O
8 -X- _ O
- -X- _ O
dimensional -X- _ O
vector -X- _ O
of -X- _ O
all -X- _ O
0s -X- _ O
: -X- _ O
M -X- _ O
= -X- _ O
{ -X- _ O
m -X- _ O
, -X- _ O
0 -X- _ O
} -X- _ O
. -X- _ O

We -X- _ O
assume -X- _ O
as -X- _ O
input -X- _ O
to -X- _ O
the -X- _ O
attention -X- _ O
mechanism -X- _ O
the -X- _ O
same -X- _ O
set -X- _ O
M -X- _ O
of -X- _ O
context -X- _ O
representations -X- _ O
. -X- _ O

Using -X- _ O
an -X- _ O
attention -X- _ O
mechanism -X- _ O
enables -X- _ O
us -X- _ O
to -X- _ O
dynamically -X- _ O
weight -X- _ O
the -X- _ O
importance -X- _ O
that -X- _ O
the -X- _ O
model -X- _ O
places -X- _ O
on -X- _ O
particular -X- _ O
datetime -X- _ O
context -X- _ O
as -X- _ O
the -X- _ O
model -X- _ O
processes -X- _ O
an -X- _ O
utterance -X- _ O
. -X- _ O

By -X- _ O
the -X- _ O
time -X- _ O
the -X- _ O
model -X- _ O
has -X- _ O
observed -X- _ O
the -X- _ O
words -X- _ O
" -X- _ O
what -X- _ O
temperature -X- _ O
will -X- _ O
" -X- _ O
, -X- _ O
we -X- _ O
would -X- _ O
expect -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
condition -X- _ O
the -X- _ O
predictions -X- _ O
of -X- _ O
the -X- _ O
remaining -X- _ O
words -X- _ O
primarily -X- _ O
on -X- _ O
the -X- _ O
hour -X- _ O
and -X- _ O
day -X- _ O
information -X- _ O
. -X- _ O

For -X- _ O
instance -X- _ O
, -X- _ O
assume -X- _ O
a -X- _ O
LM -X- _ O
is -X- _ O
given -X- _ O
the -X- _ O
phrase -X- _ O
" -X- _ O
what -X- _ O
temperature -X- _ O
will -X- _ O
it -X- _ O
be -X- _ O
on -X- _ O
friday -X- _ O
" -X- _ O
. -X- _ O

We -X- _ O
hypothesize -X- _ O
that -X- _ O
at -X- _ O
certain -X- _ O
time -X- _ O
- -X- _ O
steps -X- _ O
within -X- _ O
an -X- _ O
utterance -X- _ O
, -X- _ O
attending -X- _ O
to -X- _ O
particular -X- _ O
datetime -X- _ O
information -X- _ O
will -X- _ O
facilitate -X- _ O
the -X- _ O
model -X- _ O
's -X- _ O
predictions -X- _ O
more -X- _ O
than -X- _ O
other -X- _ O
information -X- _ O
. -X- _ O

We -X- _ O
apply -X- _ O
this -X- _ O
mechanism -X- _ O
to -X- _ O
the -X- _ O
context -X- _ O
embeddings -X- _ O
at -X- _ O
each -X- _ O
time -X- _ O
- -X- _ O
step -X- _ O
of -X- _ O
the -X- _ O
RNN -X- _ O
model -X- _ O
, -X- _ O
in -X- _ O
order -X- _ O
to -X- _ O
adapt -X- _ O
the -X- _ O
context -X- _ O
representation -X- _ O
dynamically -X- _ O
. -X- _ O

We -X- _ O
propose -X- _ O
an -X- _ O
attention -X- _ B-MethodName
mechanism -X- _ I-MethodName
that -X- _ O
augments -X- _ O
both -X- _ O
the -X- _ O
concatenation -X- _ O
- -X- _ O
based -X- _ O
and -X- _ O
factorizationbased -X- _ O
approaches -X- _ O
. -X- _ O

Attention -X- _ O
Mechanism -X- _ O
. -X- _ O

A -X- _ O
prediction -X- _ O
, -X- _ O
ŷt -X- _ O
, -X- _ O
is -X- _ O
generated -X- _ O
in -X- _ O
the -X- _ O
same -X- _ O
manner -X- _ O
as -X- _ O
in -X- _ O
the -X- _ O
concatenation -X- _ O
- -X- _ O
based -X- _ O
model -X- _ O
. -X- _ O

The -X- _ O
resulting -X- _ O
matrices -X- _ O
W -X- _ O
x -X- _ O
, -X- _ O
W -X- _ O
h -X- _ O
are -X- _ O
now -X- _ O
used -X- _ O
as -X- _ O
the -X- _ O
weights -X- _ O
in -X- _ O
the -X- _ O
RNN -X- _ O
cell -X- _ O
. -X- _ O

We -X- _ O
can -X- _ O
now -X- _ O
use -X- _ O
the -X- _ O
contextual -X- _ O
representation -X- _ O
to -X- _ O
interpolate -X- _ O
the -X- _ O
two -X- _ O
sets -X- _ O
of -X- _ O
basis -X- _ O
tensors -X- _ O
to -X- _ O
produce -X- _ O
two -X- _ O
new -X- _ O
weight -X- _ O
matrices -X- _ O
, -X- _ O
W -X- _ O
x -X- _ O
and -X- _ O
W -X- _ O
h -X- _ O
, -X- _ O
where -X- _ O
W -X- _ O
x -X- _ O
= -X- _ O
W -X- _ O
x -X- _ O
+ -X- _ O
( -X- _ O
W -X- _ O
( -X- _ O
L -X- _ O
) -X- _ O
x -X- _ O
T -X- _ O
m -X- _ O
) -X- _ O
T -X- _ O
( -X- _ O
W -X- _ O
( -X- _ O
R -X- _ O
) -X- _ O
x -X- _ O
T -X- _ O
m -X- _ O
) -X- _ O
W -X- _ O
h -X- _ O
= -X- _ O
W -X- _ O
h -X- _ O
+ -X- _ O
( -X- _ O
W -X- _ O
( -X- _ O
L -X- _ O
) -X- _ O
h -X- _ O
T -X- _ O
m -X- _ O
) -X- _ O
T -X- _ O
( -X- _ O
W -X- _ O
( -X- _ O
R -X- _ O
) -X- _ O
h -X- _ O
T -X- _ O
m -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
right -X- _ O
adaptation -X- _ O
tensors -X- _ O
, -X- _ O
W -X- _ O
( -X- _ O
R -X- _ O
) -X- _ O
x -X- _ O
, -X- _ O
W -X- _ O
( -X- _ O
R -X- _ O
) -X- _ O
h -X- _ O
are -X- _ O
both -X- _ O
of -X- _ O
dimensionality -X- _ O
R -X- _ O
r×d×f -X- _ O
. -X- _ O

The -X- _ O
left -X- _ O
adaptation -X- _ O
tensors -X- _ O
, -X- _ O
W -X- _ O
( -X- _ O
L -X- _ O
) -X- _ O
x -X- _ O
, -X- _ O
W -X- _ O
( -X- _ O
L -X- _ O
) -X- _ O
h -X- _ O
, -X- _ O
are -X- _ O
of -X- _ O
dimensionality -X- _ O
R -X- _ O
f -X- _ O
×e×r -X- _ O
, -X- _ O
and -X- _ O
R -X- _ O
f -X- _ O
×d×r -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O

These -X- _ O
basis -X- _ O
tensors -X- _ O
are -X- _ O
of -X- _ O
fixed -X- _ O
rank -X- _ B-HyperparameterName
r -X- _ B-HyperparameterName
, -X- _ O
where -X- _ O
r -X- _ B-HyperparameterName
is -X- _ O
a -X- _ O
tuned -X- _ O
hyperparameter -X- _ O
. -X- _ O

The -X- _ O
adaption -X- _ O
process -X- _ O
involves -X- _ O
learning -X- _ O
basis -X- _ O
tensors -X- _ O
W -X- _ O
( -X- _ O
L -X- _ O
) -X- _ O
x -X- _ O
, -X- _ O
W -X- _ O
( -X- _ O
R -X- _ O
) -X- _ O
x -X- _ O
and -X- _ O
W -X- _ O
( -X- _ O
L -X- _ O
) -X- _ O
h -X- _ O
, -X- _ O
W -X- _ O
( -X- _ O
R -X- _ O
) -X- _ O
h -X- _ O
. -X- _ O

Compared -X- _ O
to -X- _ O
the -X- _ O
concatenation -X- _ O
- -X- _ O
based -X- _ O
architecture -X- _ O
, -X- _ O
this -X- _ O
approach -X- _ O
adapts -X- _ O
a -X- _ O
larger -X- _ O
fraction -X- _ O
of -X- _ O
the -X- _ O
RNN -X- _ O
model -X- _ O
's -X- _ O
parameters -X- _ O
. -X- _ O

Unlike -X- _ O
the -X- _ O
concatenation -X- _ O
- -X- _ O
based -X- _ O
approach -X- _ O
, -X- _ O
which -X- _ O
directly -X- _ O
inserts -X- _ O
contextual -X- _ O
information -X- _ O
into -X- _ O
the -X- _ O
RNN -X- _ O
cell -X- _ O
, -X- _ O
the -X- _ O
factorization -X- _ O
- -X- _ O
based -X- _ O
method -X- _ O
adapts -X- _ O
the -X- _ O
weight -X- _ O
matrices -X- _ O
W -X- _ O
x -X- _ O
, -X- _ O
W -X- _ O
h -X- _ O
of -X- _ O
the -X- _ O
RNN -X- _ O
model -X- _ O
. -X- _ O

Factorization -X- _ O
- -X- _ O
based -X- _ O
LM -X- _ O
Adaptation -X- _ O
. -X- _ O

To -X- _ O
generate -X- _ O
a -X- _ O
prediction -X- _ O
, -X- _ O
ŷt -X- _ O
for -X- _ O
a -X- _ O
word -X- _ O
at -X- _ O
time -X- _ O
- -X- _ O
step -X- _ O
t -X- _ O
, -X- _ O
h -X- _ O
t -X- _ O
is -X- _ O
passed -X- _ O
through -X- _ O
a -X- _ O
projection -X- _ O
layer -X- _ O
, -X- _ O
W -X- _ O
v -X- _ O
∈ -X- _ O
R -X- _ O
d×|V -X- _ O
| -X- _ O
to -X- _ O
match -X- _ O
the -X- _ O
dimension -X- _ O
of -X- _ O
the -X- _ O
vocabulary -X- _ O
size -X- _ O
|V -X- _ O
| -X- _ O
, -X- _ O
before -X- _ O
applying -X- _ O
a -X- _ O
softmax -X- _ O
layer -X- _ O
ŷt -X- _ O
= -X- _ O
sof -X- _ O
tmax(W -X- _ O
v -X- _ O
h -X- _ O
t -X- _ O
) -X- _ O
. -X- _ O

Notice -X- _ O
that -X- _ O
the -X- _ O
expression -X- _ O
above -X- _ O
can -X- _ O
be -X- _ O
equivalently -X- _ O
calculated -X- _ O
by -X- _ O
concatenating -X- _ O
the -X- _ O
matrices -X- _ O
W -X- _ O
m -X- _ O
and -X- _ O
W -X- _ O
x -X- _ O
, -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
the -X- _ O
vectors -X- _ O
m -X- _ O
and -X- _ O
x -X- _ O
t -X- _ O
h -X- _ O
t -X- _ O
= -X- _ O
σ([W -X- _ O
x -X- _ O
; -X- _ O
W -X- _ O
m -X- _ O
] -X- _ O
[ -X- _ O
x -X- _ O
t -X- _ O
; -X- _ O
m -X- _ O
] -X- _ O
+ -X- _ O
W -X- _ O
h -X- _ O
h -X- _ O
t−1 -X- _ O
+ -X- _ O
b -X- _ O
) -X- _ O
. -X- _ O

In -X- _ O
the -X- _ O
concatenation -X- _ O
- -X- _ O
based -X- _ O
approach -X- _ O
, -X- _ O
this -X- _ O
hidden -X- _ O
- -X- _ O
state -X- _ O
is -X- _ O
adapted -X- _ O
in -X- _ O
the -X- _ O
following -X- _ O
manner -X- _ O
h -X- _ O
t -X- _ O
= -X- _ O
σ(W -X- _ O
m -X- _ O
m -X- _ O
+ -X- _ O
W -X- _ O
x -X- _ O
x -X- _ O
t -X- _ O
+ -X- _ O
W -X- _ O
h -X- _ O
h -X- _ O
t−1 -X- _ O
+ -X- _ O
b -X- _ O
) -X- _ O
. -X- _ O

A -X- _ O
standard -X- _ O
RNN -X- _ O
model -X- _ O
without -X- _ O
contextual -X- _ O
information -X- _ O
keeps -X- _ O
track -X- _ O
of -X- _ O
a -X- _ O
hidden -X- _ O
- -X- _ O
state -X- _ O
at -X- _ O
time -X- _ O
- -X- _ O
step -X- _ O
t -X- _ O
, -X- _ O
h -X- _ O
t -X- _ O
, -X- _ O
that -X- _ O
is -X- _ O
calculated -X- _ O
as -X- _ O
h -X- _ O
t -X- _ O
= -X- _ O
σ(W -X- _ O
x -X- _ O
x -X- _ O
t -X- _ O
+ -X- _ O
W -X- _ O
h -X- _ O
h -X- _ O
t−1 -X- _ O
+ -X- _ O
b -X- _ O
) -X- _ O
, -X- _ O
where -X- _ O
x -X- _ O
t -X- _ O
represents -X- _ O
the -X- _ O
word -X- _ O
embedding -X- _ O
at -X- _ O
timestep -X- _ O
t -X- _ O
, -X- _ O
b -X- _ O
is -X- _ O
a -X- _ O
bias -X- _ O
vector -X- _ O
, -X- _ O
W -X- _ O
x -X- _ O
∈ -X- _ O
R -X- _ O
e×d -X- _ O
, -X- _ O
and -X- _ O
W -X- _ O
h -X- _ O
∈ -X- _ O
R -X- _ O
d×d -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
case -X- _ O
, -X- _ O
f -X- _ B-HyperparameterName
is -X- _ O
four -X- _ O
- -X- _ O
times -X- _ O
the -X- _ O
size -X- _ O
of -X- _ O
each -X- _ O
individual -X- _ O
context -X- _ O
embedding -X- _ O
. -X- _ O

When -X- _ O
representing -X- _ O
contextual -X- _ O
information -X- _ O
as -X- _ O
learned -X- _ O
embeddings -X- _ O
, -X- _ O
recall -X- _ O
that -X- _ O
we -X- _ O
first -X- _ O
concatenate -X- _ O
the -X- _ O
embeddings -X- _ O
together -X- _ O
before -X- _ O
passing -X- _ O
these -X- _ O
into -X- _ O
the -X- _ O
model -X- _ O
. -X- _ O

In -X- _ O
practice -X- _ O
, -X- _ O
f -X- _ B-HyperparameterName
is -X- _ O
either -X- _ O
a -X- _ O
hyperparameter -X- _ O
when -X- _ O
datetime -X- _ O
context -X- _ O
is -X- _ O
represented -X- _ O
as -X- _ O
learned -X- _ O
embeddings -X- _ O
, -X- _ O
or -X- _ O
f -X- _ B-HyperparameterName
= -X- _ O
8 -X- _ B-HyperparameterValue
when -X- _ O
this -X- _ O
context -X- _ O
is -X- _ O
represented -X- _ O
as -X- _ O
a -X- _ O
feature -X- _ O
- -X- _ O
engineered -X- _ O
vector -X- _ O
. -X- _ O

The -X- _ O
concatenation -X- _ O
- -X- _ O
based -X- _ O
approach -X- _ O
learns -X- _ O
a -X- _ O
weight -X- _ O
matrix -X- _ O
W -X- _ O
m -X- _ O
of -X- _ O
dimensionality -X- _ O
R -X- _ O
f -X- _ O
×d -X- _ O
, -X- _ O
where -X- _ O
f -X- _ B-HyperparameterName
represents -X- _ O
the -X- _ O
size -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
the -X- _ I-HyperparameterName
context -X- _ I-HyperparameterName
representation -X- _ I-HyperparameterName
and -X- _ O
d -X- _ B-HyperparameterName
represents -X- _ O
the -X- _ O
hidden -X- _ B-HyperparameterName
- -X- _ I-HyperparameterName
dimensionality -X- _ I-HyperparameterName
of -X- _ O
the -X- _ O
RNN -X- _ O
model -X- _ O
. -X- _ O

Concatenation -X- _ O
- -X- _ O
based -X- _ O
LM -X- _ O
Adaptation -X- _ O
. -X- _ O

The -X- _ O
methods -X- _ O
we -X- _ O
discuss -X- _ O
, -X- _ O
however -X- _ O
, -X- _ O
can -X- _ O
be -X- _ O
applied -X- _ O
to -X- _ O
each -X- _ O
layer -X- _ O
of -X- _ O
a -X- _ O
multi -X- _ O
- -X- _ O
layer -X- _ O
RNN -X- _ O
model -X- _ O
. -X- _ O

The -X- _ O
notation -X- _ O
we -X- _ O
use -X- _ O
to -X- _ O
describe -X- _ O
architectures -X- _ O
assumes -X- _ O
a -X- _ O
1 -X- _ B-HyperparameterValue
- -X- _ O
layer -X- _ B-HyperparameterName
RNN -X- _ O
model -X- _ O
. -X- _ O

We -X- _ O
then -X- _ O
introduce -X- _ O
our -X- _ O
attentionmechanism -X- _ B-MethodName
that -X- _ O
can -X- _ O
be -X- _ O
used -X- _ O
to -X- _ O
augment -X- _ O
both -X- _ O
of -X- _ O
these -X- _ O
approaches -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
section -X- _ O
, -X- _ O
we -X- _ O
first -X- _ O
describe -X- _ O
the -X- _ O
architecture -X- _ O
of -X- _ O
the -X- _ O
concatenation -X- _ O
- -X- _ O
based -X- _ O
and -X- _ O
factorization -X- _ O
- -X- _ O
based -X- _ O
approaches -X- _ O
. -X- _ O

Model -X- _ O
. -X- _ O

A -X- _ O
set -X- _ O
, -X- _ O
M -X- _ O
, -X- _ O
containing -X- _ O
a -X- _ O
single -X- _ O
embedding -X- _ O
M -X- _ O
= -X- _ O
{ -X- _ O
m -X- _ O
} -X- _ O
, -X- _ O
where -X- _ O
m -X- _ O
represents -X- _ O
an -X- _ O
8dimensional -X- _ O
feature -X- _ O
- -X- _ O
engineered -X- _ O
contextual -X- _ O
datetime -X- _ O
representation -X- _ O
, -X- _ O
as -X- _ O
described -X- _ O
in -X- _ O
the -X- _ O
previous -X- _ O
section -X- _ O
. -X- _ O

2 -X- _ O
. -X- _ O

When -X- _ O
using -X- _ O
the -X- _ O
concatenationbased -X- _ O
or -X- _ O
factorization -X- _ O
- -X- _ O
based -X- _ O
approaches -X- _ O
without -X- _ O
attention -X- _ O
, -X- _ O
we -X- _ O
first -X- _ O
concatenate -X- _ O
the -X- _ O
embeddings -X- _ O
together -X- _ O
, -X- _ O
m -X- _ O
= -X- _ O
[ -X- _ O
m -X- _ O
1 -X- _ O
; -X- _ O
m -X- _ O
2 -X- _ O
; -X- _ O
m -X- _ O
3 -X- _ O
; -X- _ O
m -X- _ O
4 -X- _ O
] -X- _ O
, -X- _ O
and -X- _ O
use -X- _ O
the -X- _ O
resulting -X- _ O
vector -X- _ O
as -X- _ O
input -X- _ O
to -X- _ O
the -X- _ O
model -X- _ O
. -X- _ O

A -X- _ O
set -X- _ O
, -X- _ O
M -X- _ O
, -X- _ O
of -X- _ O
four -X- _ O
learned -X- _ O
context -X- _ O
embeddings -X- _ O
M -X- _ O
= -X- _ O
{ -X- _ O
m -X- _ O
1 -X- _ O
, -X- _ O
m -X- _ O
2 -X- _ O
, -X- _ O
m -X- _ O
3 -X- _ O
, -X- _ O
m -X- _ O
4 -X- _ O
} -X- _ O
, -X- _ O
where -X- _ O
m -X- _ O
1 -X- _ O
is -X- _ O
an -X- _ O
encoding -X- _ O
of -X- _ O
the -X- _ O
month -X- _ O
information -X- _ O
, -X- _ O
m -X- _ O
2 -X- _ O
is -X- _ O
an -X- _ O
encoding -X- _ O
of -X- _ O
the -X- _ O
week -X- _ O
information -X- _ O
, -X- _ O
m -X- _ O
3 -X- _ O
is -X- _ O
an -X- _ O
encoding -X- _ O
of -X- _ O
the -X- _ O
day -X- _ O
of -X- _ O
the -X- _ O
week -X- _ O
information -X- _ O
, -X- _ O
and -X- _ O
m -X- _ O
4 -X- _ O
is -X- _ O
an -X- _ O
encoding -X- _ O
of -X- _ O
the -X- _ O
hour -X- _ O
of -X- _ O
the -X- _ O
day -X- _ O
information -X- _ O
. -X- _ O

We -X- _ O
additionally -X- _ O
represent -X- _ O
the -X- _ O
contextual -X- _ O
information -X- _ O
as -X- _ O
either -X- _ O
: -X- _ O
1 -X- _ O
. -X- _ O

, -X- _ O
n -X- _ O
} -X- _ O
, -X- _ O
where -X- _ O
n -X- _ O
is -X- _ O
the -X- _ O
length -X- _ O
of -X- _ O
the -X- _ O
input -X- _ O
sequence -X- _ O
and -X- _ O
e -X- _ B-HyperparameterName
is -X- _ O
the -X- _ O
dimensionality -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
the -X- _ I-HyperparameterName
word -X- _ I-HyperparameterName
embeddings -X- _ I-HyperparameterName
. -X- _ O

, -X- _ O
n -X- _ O
} -X- _ O
, -X- _ O
that -X- _ O
are -X- _ O
converted -X- _ O
by -X- _ O
an -X- _ O
embedding -X- _ O
layer -X- _ O
into -X- _ O
embeddings -X- _ O
x -X- _ O
i -X- _ O
∈ -X- _ O
R -X- _ O
e -X- _ O
for -X- _ O
i -X- _ O
∈ -X- _ O
{ -X- _ O
1 -X- _ O
, -X- _ O
. -X- _ O

We -X- _ O
assume -X- _ O
as -X- _ O
input -X- _ O
to -X- _ O
a -X- _ O
model -X- _ O
a -X- _ O
sequence -X- _ O
of -X- _ O
either -X- _ O
word -X- _ O
or -X- _ O
subword -X- _ O
tokens -X- _ O
, -X- _ O
w -X- _ O
i -X- _ O
for -X- _ O
i -X- _ O
∈ -X- _ O
{ -X- _ O
1 -X- _ O
, -X- _ O
. -X- _ O

Input -X- _ O
Representation -X- _ O
. -X- _ O

Feature -X- _ O
- -X- _ O
engineered -X- _ O
representation -X- _ O
: -X- _ O
Additionally -X- _ O
, -X- _ O
we -X- _ O
consider -X- _ O
transforming -X- _ O
the -X- _ O
datetime -X- _ O
information -X- _ O
into -X- _ O
a -X- _ O
single -X- _ O
8 -X- _ O
- -X- _ O
dimensional -X- _ O
feature -X- _ O
- -X- _ O
engineered -X- _ O
vector -X- _ O
, -X- _ O
where -X- _ O
the -X- _ O
dimensions -X- _ O
of -X- _ O
the -X- _ O
vector -X- _ O
are -X- _ O
defined -X- _ O
as -X- _ O
 -X- _ O
 -X- _ O
 -X- _ O
 -X- _ O
 -X- _ O
 -X- _ O
 -X- _ O
 -X- _ O
 -X- _ O
 -X- _ O
 -X- _ O
sin -X- _ O
( -X- _ O
2π•hour -X- _ O
24 -X- _ O
) -X- _ O
cos -X- _ O
( -X- _ O
2π•hour -X- _ O
24 -X- _ O
) -X- _ O
sin -X- _ O
( -X- _ O
2π•day -X- _ O
7 -X- _ O
) -X- _ O
cos -X- _ O
( -X- _ O
2π•day -X- _ O
7 -X- _ O
) -X- _ O
sin -X- _ O
( -X- _ O
2π•week -X- _ O
53 -X- _ O
) -X- _ O
cos -X- _ O
( -X- _ O
2π•week -X- _ O
) -X- _ O
sin -X- _ O
( -X- _ O
2π•month -X- _ O
12 -X- _ O
) -X- _ O
cos -X- _ O
( -X- _ O
2π•month -X- _ O
12 -X- _ O
) -X- _ O
 -X- _ O
 -X- _ O
 -X- _ O
 -X- _ O
 -X- _ O
 -X- _ O
 -X- _ O
 -X- _ O
 -X- _ O
 -X- _ O
 -X- _ O
Since -X- _ O
the -X- _ O
datetime -X- _ O
context -X- _ O
is -X- _ O
continuous -X- _ O
and -X- _ O
cyclical -X- _ O
, -X- _ O
this -X- _ O
approach -X- _ O
explicitly -X- _ O
encodes -X- _ O
tem -X- _ O
- -X- _ O
poral -X- _ O
proximity -X- _ O
in -X- _ O
the -X- _ O
date -X- _ O
and -X- _ O
time -X- _ O
information -X- _ O
. -X- _ O

2 -X- _ O
. -X- _ O

We -X- _ O
experiment -X- _ O
with -X- _ O
different -X- _ O
ways -X- _ O
of -X- _ O
parsing -X- _ O
the -X- _ O
information -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
encoding -X- _ O
weekday -X- _ O
versus -X- _ O
weekend -X- _ O
, -X- _ O
or -X- _ O
morning -X- _ O
versus -X- _ O
evening -X- _ O
, -X- _ O
but -X- _ O
find -X- _ O
this -X- _ O
information -X- _ O
is -X- _ O
largely -X- _ O
entailed -X- _ O
within -X- _ O
our -X- _ O
method -X- _ O
for -X- _ O
processing -X- _ O
datetime -X- _ O
information -X- _ O
. -X- _ O

These -X- _ O
embeddings -X- _ O
are -X- _ O
initialized -X- _ O
as -X- _ O
random -X- _ O
vectors -X- _ O
, -X- _ O
and -X- _ O
trained -X- _ O
along -X- _ O
with -X- _ O
the -X- _ O
rest -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
. -X- _ O

These -X- _ O
tokens -X- _ O
are -X- _ O
subsequently -X- _ O
used -X- _ O
as -X- _ O
input -X- _ O
to -X- _ O
the -X- _ O
model -X- _ O
, -X- _ O
where -X- _ O
they -X- _ O
are -X- _ O
passed -X- _ O
through -X- _ O
an -X- _ O
embedding -X- _ O
layer -X- _ O
to -X- _ O
generate -X- _ O
context -X- _ O
embeddings -X- _ O
. -X- _ O

In -X- _ O
the -X- _ O
example -X- _ O
above -X- _ O
, -X- _ O
we -X- _ O
would -X- _ O
transform -X- _ O
the -X- _ O
datetime -X- _ O
information -X- _ O
into -X- _ O
tokens -X- _ O
representing -X- _ O
: -X- _ O
month-12 -X- _ O
, -X- _ O
week-52 -X- _ O
, -X- _ O
wednesday -X- _ O
, -X- _ O
7 -X- _ O
am -X- _ O
. -X- _ O

Learned -X- _ O
embeddings -X- _ O
: -X- _ O
We -X- _ O
first -X- _ O
consider -X- _ O
creating -X- _ O
tokens -X- _ O
for -X- _ O
the -X- _ O
month -X- _ O
number -X- _ O
, -X- _ O
week -X- _ O
number -X- _ O
, -X- _ O
day -X- _ O
of -X- _ O
the -X- _ O
week -X- _ O
and -X- _ O
hour -X- _ O
that -X- _ O
an -X- _ O
utterance -X- _ O
was -X- _ O
spoken -X- _ O
. -X- _ O

In -X- _ O
order -X- _ O
to -X- _ O
condition -X- _ O
a -X- _ O
LM -X- _ O
on -X- _ O
this -X- _ O
datetime -X- _ O
information -X- _ O
, -X- _ O
we -X- _ O
consider -X- _ O
two -X- _ O
methods -X- _ O
for -X- _ O
transforming -X- _ O
the -X- _ O
contextual -X- _ O
information -X- _ O
into -X- _ O
a -X- _ O
continuous -X- _ O
vector -X- _ O
representation -X- _ O
: -X- _ O
1 -X- _ O
. -X- _ O

In -X- _ O
the -X- _ O
example -X- _ O
above -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
infer -X- _ O
that -X- _ O
the -X- _ O
utterance -X- _ O
" -X- _ O
play -X- _ O
christmas -X- _ O
music -X- _ O
" -X- _ O
was -X- _ O
spoken -X- _ O
on -X- _ O
December -X- _ O
23 -X- _ O
, -X- _ O
2020 -X- _ O
at -X- _ O
7 -X- _ O
in -X- _ O
the -X- _ O
morning -X- _ O
local -X- _ O
time -X- _ O
. -X- _ O

A -X- _ O
typical -X- _ O
utterance -X- _ O
in -X- _ O
our -X- _ O
dataset -X- _ O
might -X- _ O
look -X- _ O
like -X- _ O
this -X- _ O
: -X- _ O
2020 -X- _ O
- -X- _ O
12 -X- _ O
- -X- _ O
23 -X- _ O
07:00 -X- _ O
play -X- _ O
christmas -X- _ O
music -X- _ O
. -X- _ O

Context -X- _ O
Representation -X- _ O
. -X- _ O

We -X- _ O
randomly -X- _ O
split -X- _ O
our -X- _ O
dataset -X- _ O
into -X- _ O
a -X- _ O
training -X- _ O
set -X- _ O
, -X- _ O
development -X- _ O
set -X- _ O
and -X- _ O
test -X- _ O
set -X- _ O
, -X- _ O
using -X- _ O
a -X- _ O
partition -X- _ B-HyperparameterName
ratio -X- _ I-HyperparameterName
of -X- _ O
90/5/5 -X- _ B-HyperparameterValue
and -X- _ O
we -X- _ O
ensure -X- _ O
that -X- _ O
each -X- _ O
partition -X- _ O
contains -X- _ O
more -X- _ O
than -X- _ O
500 -X- _ O
hours -X- _ O
worth -X- _ O
of -X- _ O
data -X- _ O
. -X- _ O

Any -X- _ O
information -X- _ O
about -X- _ O
the -X- _ O
device -X- _ O
or -X- _ O
the -X- _ O
speaker -X- _ O
from -X- _ O
which -X- _ O
an -X- _ O
utterance -X- _ O
originates -X- _ O
has -X- _ O
been -X- _ O
removed -X- _ O
. -X- _ O

The -X- _ O
datetime -X- _ O
information -X- _ O
is -X- _ O
reported -X- _ O
according -X- _ O
to -X- _ O
the -X- _ O
local -X- _ O
time -X- _ O
zone -X- _ O
of -X- _ O
each -X- _ O
given -X- _ O
user -X- _ O
. -X- _ O

Each -X- _ O
utterance -X- _ O
also -X- _ O
contains -X- _ O
associated -X- _ O
information -X- _ O
about -X- _ O
the -X- _ O
year -X- _ O
, -X- _ O
month -X- _ O
, -X- _ O
day -X- _ O
, -X- _ O
and -X- _ O
hour -X- _ O
that -X- _ O
the -X- _ O
utterance -X- _ O
was -X- _ O
spoken -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
a -X- _ O
corpus -X- _ O
of -X- _ O
over -X- _ O
5,000 -X- _ O
hours -X- _ O
of -X- _ O
deidentified -X- _ O
, -X- _ O
transcribed -X- _ O
English -X- _ O
utterances -X- _ O
, -X- _ O
collected -X- _ O
over -X- _ O
several -X- _ O
years -X- _ O
. -X- _ O

Data -X- _ O
. -X- _ O

Moreover -X- _ O
, -X- _ O
our -X- _ O
attention -X- _ O
mechanism -X- _ O
can -X- _ O
improve -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
methods -X- _ O
for -X- _ O
conditional -X- _ O
LMs -X- _ O
by -X- _ O
over -X- _ O
2.8 -X- _ B-MetricValue
% -X- _ I-MetricValue
relative -X- _ O
in -X- _ O
terms -X- _ O
of -X- _ O
perplexity -X- _ B-MetricName
. -X- _ O

Compared -X- _ O
to -X- _ O
a -X- _ O
standard -X- _ O
model -X- _ O
that -X- _ O
does -X- _ O
not -X- _ O
include -X- _ O
contextual -X- _ O
information -X- _ O
, -X- _ O
using -X- _ O
our -X- _ O
method -X- _ O
to -X- _ O
contextualize -X- _ O
a -X- _ O
neural -X- _ O
LM -X- _ O
on -X- _ O
datetime -X- _ O
information -X- _ O
achieves -X- _ O
a -X- _ O
relative -X- _ O
reduction -X- _ O
in -X- _ O
perplexity -X- _ B-MetricName
of -X- _ O
7.0 -X- _ B-MetricValue
% -X- _ I-MetricValue
, -X- _ O
and -X- _ O
a -X- _ O
relative -X- _ O
reduction -X- _ O
in -X- _ O
perplexity -X- _ B-MetricName
of -X- _ O
9.0 -X- _ B-MetricValue
% -X- _ I-MetricValue
when -X- _ O
evaluated -X- _ O
on -X- _ O
the -X- _ O
tail -X- _ O
of -X- _ O
this -X- _ O
dataset -X- _ O
. -X- _ O

We -X- _ O
evaluate -X- _ O
our -X- _ O
method -X- _ O
on -X- _ O
a -X- _ O
large -X- _ O
de -X- _ O
- -X- _ O
identified -X- _ O
dataset -X- _ O
of -X- _ O
transcribed -X- _ O
utterances -X- _ O
. -X- _ O

To -X- _ O
underscore -X- _ O
this -X- _ O
point -X- _ O
we -X- _ O
also -X- _ O
provide -X- _ O
results -X- _ O
for -X- _ O
conditioning -X- _ O
LMs -X- _ O
on -X- _ O
geolocation -X- _ O
information -X- _ O
and -X- _ O
dialogue -X- _ O
prompts -X- _ O
that -X- _ O
are -X- _ O
commonly -X- _ O
available -X- _ O
in -X- _ O
ASR -X- _ O
systems -X- _ O
. -X- _ O

Our -X- _ O
approach -X- _ O
, -X- _ O
however -X- _ O
, -X- _ O
can -X- _ O
generalized -X- _ O
to -X- _ O
any -X- _ O
type -X- _ O
of -X- _ O
context -X- _ O
. -X- _ O

We -X- _ O
concentrate -X- _ O
on -X- _ O
datetime -X- _ O
information -X- _ O
because -X- _ O
of -X- _ O
its -X- _ O
widespread -X- _ O
availability -X- _ O
in -X- _ O
many -X- _ O
ASR -X- _ O
systems -X- _ O
. -X- _ O

Our -X- _ O
experiments -X- _ O
focus -X- _ O
primarily -X- _ O
on -X- _ O
conditioning -X- _ O
neural -X- _ O
LMs -X- _ O
on -X- _ O
datetime -X- _ O
context -X- _ O
. -X- _ O

The -X- _ O
resulting -X- _ O
embedding -X- _ O
can -X- _ O
be -X- _ O
used -X- _ O
as -X- _ O
an -X- _ O
additional -X- _ O
input -X- _ O
to -X- _ O
either -X- _ O
the -X- _ O
concatenation -X- _ O
- -X- _ O
based -X- _ O
or -X- _ O
factorization -X- _ O
- -X- _ O
based -X- _ O
model -X- _ O
. -X- _ O

The -X- _ O
attention -X- _ B-MethodName
mechanism -X- _ I-MethodName
that -X- _ O
we -X- _ O
propose -X- _ O
builds -X- _ O
up -X- _ O
a -X- _ O
dynamic -X- _ O
context -X- _ O
representation -X- _ O
over -X- _ O
the -X- _ O
course -X- _ O
of -X- _ O
processing -X- _ O
an -X- _ O
utterance -X- _ O
. -X- _ O

We -X- _ O
introduce -X- _ O
an -X- _ O
attention -X- _ O
mechanism -X- _ O
that -X- _ O
augments -X- _ O
both -X- _ O
the -X- _ O
concatenation -X- _ O
- -X- _ O
based -X- _ O
and -X- _ O
factorization -X- _ O
- -X- _ O
based -X- _ O
approaches -X- _ O
to -X- _ O
condition -X- _ O
a -X- _ O
neural -X- _ O
LM -X- _ O
on -X- _ O
context -X- _ O
. -X- _ O

This -X- _ O
factorization -X- _ O
- -X- _ O
based -X- _ O
approach -X- _ O
has -X- _ O
proven -X- _ O
effective -X- _ O
in -X- _ O
generating -X- _ O
automatic -X- _ O
completions -X- _ O
of -X- _ O
sentences -X- _ O
that -X- _ O
are -X- _ O
personalized -X- _ O
for -X- _ O
particular -X- _ O
users -X- _ O
( -X- _ O
Jaech -X- _ O
and -X- _ O
Ostendorf -X- _ O
, -X- _ O
2018b -X- _ O
) -X- _ O
. -X- _ O

Factorizing -X- _ O
the -X- _ O
weight -X- _ O
- -X- _ O
matrix -X- _ O
enables -X- _ O
a -X- _ O
larger -X- _ O
fraction -X- _ O
of -X- _ O
a -X- _ O
model -X- _ O
's -X- _ O
parameters -X- _ O
to -X- _ O
adjust -X- _ O
to -X- _ O
a -X- _ O
given -X- _ O
contextual -X- _ O
signal -X- _ O
. -X- _ O

The -X- _ O
authors -X- _ O
propose -X- _ O
decomposing -X- _ O
the -X- _ O
weight -X- _ O
- -X- _ O
matrix -X- _ O
into -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
left -X- _ O
and -X- _ O
right -X- _ O
basis -X- _ O
tensors -X- _ O
which -X- _ O
are -X- _ O
then -X- _ O
multiplied -X- _ O
by -X- _ O
a -X- _ O
learned -X- _ O
context -X- _ O
embedding -X- _ O
to -X- _ O
produce -X- _ O
a -X- _ O
new -X- _ O
weight -X- _ O
- -X- _ O
matrix -X- _ O
. -X- _ O

In -X- _ O
contrast -X- _ O
to -X- _ O
these -X- _ O
methods -X- _ O
, -X- _ O
Jaech -X- _ O
and -X- _ O
Ostendorf -X- _ O
( -X- _ O
2018a -X- _ O
) -X- _ O
adapt -X- _ O
the -X- _ O
weight -X- _ O
- -X- _ O
matrix -X- _ O
used -X- _ O
in -X- _ O
an -X- _ O
RNN -X- _ O
model -X- _ O
to -X- _ O
a -X- _ O
given -X- _ O
contextual -X- _ O
input -X- _ O
. -X- _ O

( -X- _ O
2019 -X- _ O
) -X- _ O
use -X- _ O
an -X- _ O
attention -X- _ B-MethodName
mechanism -X- _ I-MethodName
over -X- _ O
learned -X- _ O
personality -X- _ O
trait -X- _ O
embeddings -X- _ O
, -X- _ O
in -X- _ O
order -X- _ O
to -X- _ O
generate -X- _ O
personalized -X- _ O
dialogue -X- _ O
responses -X- _ O
. -X- _ O

The -X- _ O
aforementioned -X- _ O
approaches -X- _ O
learn -X- _ O
a -X- _ O
representation -X- _ O
of -X- _ O
the -X- _ O
context -X- _ O
that -X- _ O
is -X- _ O
directly -X- _ O
used -X- _ O
as -X- _ O
input -X- _ O
to -X- _ O
a -X- _ O
neural -X- _ O
LM -X- _ O
. -X- _ O

Similarly -X- _ O
, -X- _ O
Zheng -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

( -X- _ O
2016 -X- _ O
) -X- _ O
use -X- _ O
an -X- _ O
attention -X- _ O
module -X- _ O
that -X- _ O
attends -X- _ O
to -X- _ O
wordlocation -X- _ O
information -X- _ O
to -X- _ O
predict -X- _ O
the -X- _ O
polarity -X- _ O
of -X- _ O
a -X- _ O
sentence -X- _ O
. -X- _ O

Tang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

Recently -X- _ O
, -X- _ O
attention -X- _ O
mechanisms -X- _ O
, -X- _ O
initially -X- _ O
developed -X- _ O
for -X- _ O
machine -X- _ O
translation -X- _ O
( -X- _ O
Bahdanau -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2014;Luong -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
, -X- _ O
have -X- _ O
been -X- _ O
used -X- _ O
by -X- _ O
neural -X- _ O
LMs -X- _ O
to -X- _ O
adaptively -X- _ O
condition -X- _ O
their -X- _ O
predictions -X- _ O
on -X- _ O
certain -X- _ O
non -X- _ O
- -X- _ O
linguistic -X- _ O
contexts -X- _ O
. -X- _ O

This -X- _ O
concatenation -X- _ O
- -X- _ O
based -X- _ O
approach -X- _ O
has -X- _ O
been -X- _ O
used -X- _ O
in -X- _ O
a -X- _ O
variety -X- _ O
of -X- _ O
domains -X- _ O
including -X- _ O
text -X- _ O
classification -X- _ O
( -X- _ O
Yogatama -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
, -X- _ O
personalized -X- _ O
conversational -X- _ O
agents -X- _ O
( -X- _ O
Wen -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2013 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
voice -X- _ O
search -X- _ O
queries -X- _ O
( -X- _ O
Ma -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
most -X- _ O
common -X- _ O
method -X- _ O
for -X- _ O
incorporating -X- _ O
nonlinguistic -X- _ O
information -X- _ O
into -X- _ O
a -X- _ O
RNN -X- _ O
- -X- _ O
LM -X- _ O
is -X- _ O
to -X- _ O
learn -X- _ O
a -X- _ O
representation -X- _ O
of -X- _ O
the -X- _ O
context -X- _ O
that -X- _ O
is -X- _ O
concatenated -X- _ O
with -X- _ O
word -X- _ O
embeddings -X- _ O
as -X- _ O
input -X- _ O
to -X- _ O
the -X- _ O
model -X- _ O
. -X- _ O

While -X- _ O
, -X- _ O
outside -X- _ O
of -X- _ O
ASR -X- _ O
, -X- _ O
transformer -X- _ O
- -X- _ O
based -X- _ O
( -X- _ O
Vaswani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
language -X- _ O
models -X- _ O
have -X- _ O
largely -X- _ O
replaced -X- _ O
RNN -X- _ B-MethodName
- -X- _ I-MethodName
LMs -X- _ I-MethodName
, -X- _ O
RNNs -X- _ O
remain -X- _ O
dominant -X- _ O
in -X- _ O
ASR -X- _ O
architectures -X- _ O
such -X- _ O
as -X- _ O
connectionist -X- _ O
temporal -X- _ O
classification -X- _ O
( -X- _ O
Graves -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2006 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
RNN -X- _ B-MethodName
- -X- _ I-MethodName
T -X- _ I-MethodName
( -X- _ O
Graves -X- _ O
, -X- _ O
2012;He -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
focus -X- _ O
on -X- _ O
adapting -X- _ O
recurrent -X- _ B-MethodName
neural -X- _ I-MethodName
network -X- _ I-MethodName
language -X- _ I-MethodName
models -X- _ I-MethodName
( -X- _ O
RNN -X- _ B-MethodName
- -X- _ I-MethodName
LMs -X- _ I-MethodName
) -X- _ O
to -X- _ O
use -X- _ O
both -X- _ O
text -X- _ O
and -X- _ O
non -X- _ O
- -X- _ O
linguistic -X- _ O
contextual -X- _ O
data -X- _ O
for -X- _ O
speech -X- _ O
recognition -X- _ O
in -X- _ O
general -X- _ O
. -X- _ O

These -X- _ O
past -X- _ O
efforts -X- _ O
, -X- _ O
however -X- _ O
, -X- _ O
have -X- _ O
largely -X- _ O
focused -X- _ O
on -X- _ O
improving -X- _ O
a -X- _ O
particular -X- _ O
skill -X- _ O
of -X- _ O
an -X- _ O
ASR -X- _ O
system -X- _ O
, -X- _ O
and -X- _ O
not -X- _ O
the -X- _ O
system -X- _ O
's -X- _ O
speech -X- _ O
recognition -X- _ O
in -X- _ O
general -X- _ O
. -X- _ O

To -X- _ O
- -X- _ O
date -X- _ O
, -X- _ O
some -X- _ O
voice -X- _ O
assistants -X- _ O
have -X- _ O
leveraged -X- _ O
coarse -X- _ O
geographic -X- _ O
information -X- _ O
for -X- _ O
improving -X- _ O
location -X- _ O
search -X- _ O
queries -X- _ O
( -X- _ O
Bocchieri -X- _ O
and -X- _ O
Caseiro -X- _ O
, -X- _ O
2010;Lloyd -X- _ O
and -X- _ O
Kristjansson -X- _ O
, -X- _ O
2012 -X- _ O
) -X- _ O
. -X- _ O

As -X- _ O
an -X- _ O
example -X- _ O
, -X- _ O
knowing -X- _ O
that -X- _ O
an -X- _ O
utterance -X- _ O
was -X- _ O
spoken -X- _ O
on -X- _ O
December -X- _ O
25th -X- _ O
, -X- _ O
a -X- _ O
LM -X- _ O
should -X- _ O
learn -X- _ O
that -X- _ O
the -X- _ O
word -X- _ O
" -X- _ O
christmas -X- _ O
" -X- _ O
rather -X- _ O
than -X- _ O
" -X- _ O
easter -X- _ O
" -X- _ O
is -X- _ O
more -X- _ O
likely -X- _ O
to -X- _ O
follow -X- _ O
the -X- _ O
phrase -X- _ O
" -X- _ O
lookup -X- _ O
cookie -X- _ O
recipes -X- _ O
for -X- _ O
" -X- _ O
. -X- _ O

We -X- _ O
hypothesize -X- _ O
that -X- _ O
these -X- _ O
additional -X- _ O
data -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
the -X- _ O
time -X- _ O
at -X- _ O
which -X- _ O
an -X- _ O
utterance -X- _ O
was -X- _ O
spoken -X- _ O
, -X- _ O
provide -X- _ O
a -X- _ O
useful -X- _ O
input -X- _ O
signal -X- _ O
for -X- _ O
a -X- _ O
LM -X- _ O
. -X- _ O

They -X- _ O
often -X- _ O
collect -X- _ O
utterances -X- _ O
spoken -X- _ O
by -X- _ O
users -X- _ O
, -X- _ O
along -X- _ O
with -X- _ O
associated -X- _ O
de -X- _ O
- -X- _ O
identified -X- _ O
contextual -X- _ O
information -X- _ O
. -X- _ O

Voice -X- _ O
assistants -X- _ O
have -X- _ O
become -X- _ O
ubiquitous -X- _ O
and -X- _ O
crucially -X- _ O
rely -X- _ O
on -X- _ O
ASR -X- _ O
systems -X- _ O
to -X- _ O
convert -X- _ O
user -X- _ O
inputs -X- _ O
to -X- _ O
text -X- _ O
. -X- _ O

The -X- _ O
LM -X- _ O
component -X- _ O
is -X- _ O
trained -X- _ O
separately -X- _ O
, -X- _ O
typically -X- _ O
on -X- _ O
large -X- _ O
amounts -X- _ O
of -X- _ O
transcribed -X- _ O
utterances -X- _ O
that -X- _ O
have -X- _ O
been -X- _ O
collected -X- _ O
by -X- _ O
an -X- _ O
existing -X- _ O
speech -X- _ O
recognition -X- _ O
system -X- _ O
. -X- _ O

Conventional -X- _ O
automatic -X- _ O
speech -X- _ O
recognition -X- _ O
( -X- _ O
ASR -X- _ O
) -X- _ O
systems -X- _ O
include -X- _ O
a -X- _ O
language -X- _ O
model -X- _ O
( -X- _ O
LM -X- _ O
) -X- _ O
and -X- _ O
an -X- _ O
acoustic -X- _ O
model -X- _ O
( -X- _ O
AM -X- _ O
) -X- _ O
. -X- _ O

Introduction -X- _ O
. -X- _ O

When -X- _ O
evaluated -X- _ O
on -X- _ O
utterances -X- _ O
extracted -X- _ O
from -X- _ O
the -X- _ O
long -X- _ O
tail -X- _ O
of -X- _ O
the -X- _ O
dataset -X- _ O
, -X- _ O
our -X- _ O
method -X- _ O
improves -X- _ O
perplexity -X- _ B-MetricName
by -X- _ O
9.0 -X- _ B-MetricValue
% -X- _ I-MetricValue
relative -X- _ O
over -X- _ O
a -X- _ O
standard -X- _ O
LM -X- _ O
and -X- _ O
by -X- _ O
over -X- _ O
2.8 -X- _ B-MetricValue
% -X- _ I-MetricValue
relative -X- _ O
when -X- _ O
compared -X- _ O
to -X- _ O
a -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
theart -X- _ O
model -X- _ O
for -X- _ O
contextual -X- _ O
LM -X- _ O
. -X- _ O

When -X- _ O
applied -X- _ O
to -X- _ O
a -X- _ O
large -X- _ O
de -X- _ O
- -X- _ O
identified -X- _ O
dataset -X- _ O
of -X- _ O
utterances -X- _ O
collected -X- _ O
by -X- _ O
a -X- _ O
popular -X- _ O
voice -X- _ O
assistant -X- _ O
platform -X- _ O
, -X- _ O
our -X- _ O
method -X- _ O
reduces -X- _ O
perplexity -X- _ O
by -X- _ O
7.0 -X- _ B-MetricValue
% -X- _ I-MetricValue
relative -X- _ O
over -X- _ O
a -X- _ O
standard -X- _ O
LM -X- _ O
that -X- _ O
does -X- _ O
not -X- _ O
incorporate -X- _ O
contextual -X- _ O
information -X- _ O
. -X- _ O

We -X- _ O
introduce -X- _ O
an -X- _ O
attention -X- _ B-MethodName
mechanism -X- _ I-MethodName
for -X- _ O
training -X- _ O
neural -X- _ O
speech -X- _ O
recognition -X- _ O
language -X- _ O
models -X- _ O
on -X- _ O
both -X- _ O
text -X- _ O
and -X- _ O
nonlinguistic -X- _ O
contextual -X- _ O
data -X- _ O
1 -X- _ O
. -X- _ O

For -X- _ O
some -X- _ O
domains -X- _ O
like -X- _ O
voice -X- _ O
assistants -X- _ O
, -X- _ O
however -X- _ O
, -X- _ O
additional -X- _ O
context -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
time -X- _ O
at -X- _ O
which -X- _ O
an -X- _ O
utterance -X- _ O
was -X- _ O
spoken -X- _ O
, -X- _ O
provides -X- _ O
a -X- _ O
rich -X- _ O
input -X- _ O
signal -X- _ O
. -X- _ O

Language -X- _ O
modeling -X- _ O
( -X- _ O
LM -X- _ O
) -X- _ O
for -X- _ O
automatic -X- _ B-TaskName
speech -X- _ I-TaskName
recognition -X- _ I-TaskName
( -X- _ O
ASR -X- _ B-TaskName
) -X- _ O
does -X- _ O
not -X- _ O
usually -X- _ O
incorporate -X- _ O
utterance -X- _ O
level -X- _ O
contextual -X- _ O
information -X- _ O
. -X- _ O

Attention -X- _ B-MethodName
- -X- _ I-MethodName
based -X- _ I-MethodName
Contextual -X- _ I-MethodName
Language -X- _ I-MethodName
Model -X- _ I-MethodName
Adaptation -X- _ I-MethodName
for -X- _ O
Speech -X- _ B-TaskName
Recognition -X- _ I-TaskName
. -X- _ O

As -X- _ O
we -X- _ O
move -X- _ O
away -X- _ O
from -X- _ O
the -X- _ O
morning -X- _ O
hours -X- _ O
, -X- _ O
the -X- _ O
conditional -X- _ O
probability -X- _ O
of -X- _ O
the -X- _ O
word -X- _ O
" -X- _ O
snooze -X- _ O
" -X- _ O
decreases -X- _ O
substantially -X- _ O
, -X- _ O
reaching -X- _ O
a -X- _ O
low -X- _ O
- -X- _ O
point -X- _ O
by -X- _ O
the -X- _ O
afternoon -X- _ O
and -X- _ O
evening -X- _ O
. -X- _ O

We -X- _ O
ingest -X- _ O
both -X- _ O
types -X- _ O
of -X- _ O
contexts -X- _ O
via -X- _ O
the -X- _ O
concatenationbased -X- _ O
approach -X- _ O
, -X- _ O
using -X- _ O
word -X- _ O
- -X- _ O
embeddings -X- _ O
as -X- _ O
the -X- _ O
attention -X- _ O
queries -X- _ O
. -X- _ O

We -X- _ O
learn -X- _ O
embeddings -X- _ O
to -X- _ O
represent -X- _ O
both -X- _ O
the -X- _ O
geohash -X- _ O
and -X- _ O
the -X- _ O
dialogue -X- _ O
prompt -X- _ O
information -X- _ O
. -X- _ O

The -X- _ O
geo -X- _ O
- -X- _ O
hash -X- _ O
information -X- _ O
2 -X- _ O
associated -X- _ O
with -X- _ O
each -X- _ O
utterance -X- _ O
encodes -X- _ O
a -X- _ O
very -X- _ O
rough -X- _ O
estimate -X- _ O
of -X- _ O
the -X- _ O
geolocation -X- _ O
of -X- _ O
a -X- _ O
user -X- _ O
's -X- _ O
device -X- _ O
. -X- _ O

Importantly -X- _ O
, -X- _ O
when -X- _ O
q -X- _ O
t -X- _ O
is -X- _ O
chosen -X- _ O
such -X- _ O
that -X- _ O
q -X- _ O
t -X- _ O
= -X- _ O
x -X- _ O
t -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
parallelize -X- _ O
the -X- _ O
computation -X- _ O
of -X- _ O
the -X- _ O
attention -X- _ O
mechanism -X- _ O
for -X- _ O
all -X- _ O
time -X- _ O
- -X- _ O
steps -X- _ O
before -X- _ O
running -X- _ O
a -X- _ O
forward -X- _ O
pass -X- _ O
through -X- _ O
the -X- _ O
model -X- _ O
. -X- _ O

