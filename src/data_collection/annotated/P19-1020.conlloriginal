-DOCSTART- -X- O
We -X- _ O
also -X- _ O
thank -X- _ O
Takeru -X- _ O
Miyato -X- _ O
, -X- _ O
who -X- _ O
gave -X- _ O
us -X- _ O
valuable -X- _ O
comments -X- _ O
about -X- _ O
AdvT -X- _ B-MethodName
/ -X- _ O
VAT -X- _ B-MethodName
. -X- _ O

We -X- _ O
believe -X- _ O
that -X- _ O
adversarial -X- _ B-MethodName
regularization -X- _ I-MethodName
can -X- _ O
be -X- _ O
one -X- _ O
of -X- _ O
the -X- _ O
common -X- _ O
and -X- _ O
fundamental -X- _ O
technologies -X- _ O
to -X- _ O
further -X- _ O
improve -X- _ O
the -X- _ O
translation -X- _ O
quality -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
model -X- _ B-MethodName
ensemble -X- _ I-MethodName
, -X- _ O
byte -X- _ B-MethodName
- -X- _ I-MethodName
pair -X- _ I-MethodName
encoding -X- _ I-MethodName
, -X- _ O
and -X- _ O
back -X- _ B-MethodName
- -X- _ I-MethodName
translation -X- _ I-MethodName
. -X- _ O

Additionally -X- _ O
, -X- _ O
we -X- _ O
confirmed -X- _ O
that -X- _ O
adversarial -X- _ B-MethodName
regularization -X- _ I-MethodName
techniques -X- _ O
effectively -X- _ O
worked -X- _ O
even -X- _ O
if -X- _ O
we -X- _ O
performed -X- _ O
them -X- _ O
with -X- _ O
the -X- _ O
training -X- _ O
data -X- _ O
increased -X- _ O
by -X- _ O
a -X- _ O
back -X- _ B-MethodName
- -X- _ I-MethodName
translation -X- _ I-MethodName
method -X- _ O
. -X- _ O

Our -X- _ O
experimental -X- _ O
results -X- _ O
demonstrated -X- _ O
that -X- _ O
applying -X- _ O
VAT -X- _ B-MethodName
to -X- _ O
both -X- _ O
encoder -X- _ O
and -X- _ O
decoder -X- _ O
embeddings -X- _ O
consistently -X- _ O
outperformed -X- _ O
other -X- _ O
configurations -X- _ O
. -X- _ O

This -X- _ O
paper -X- _ O
discussed -X- _ O
the -X- _ O
practical -X- _ O
usage -X- _ O
and -X- _ O
benefit -X- _ O
of -X- _ O
adversarial -X- _ B-MethodName
regularization -X- _ I-MethodName
based -X- _ O
on -X- _ O
adversarial -X- _ B-MethodName
perturbation -X- _ I-MethodName
in -X- _ O
the -X- _ O
current -X- _ O
NMT -X- _ B-TaskName
models -X- _ O
. -X- _ O

We -X- _ O
observe -X- _ O
that -X- _ O
Transformer+VAT -X- _ B-MethodName
with -X- _ O
using -X- _ O
training -X- _ O
data -X- _ O
increased -X- _ O
by -X- _ O
the -X- _ O
backtranslation -X- _ B-MethodName
method -X- _ O
seems -X- _ O
to -X- _ O
generate -X- _ O
higher -X- _ O
qual -X- _ O
- -X- _ O
ity -X- _ O
translations -X- _ O
compared -X- _ O
with -X- _ O
those -X- _ O
of -X- _ O
the -X- _ O
baseline -X- _ B-MethodName
Transformer -X- _ I-MethodName
. -X- _ O

In -X- _ O
addition -X- _ O
, -X- _ O
the -X- _ O
rows -X- _ O
+ -X- _ O
VAT+AdvT -X- _ O
show -X- _ O
the -X- _ O
performance -X- _ O
obtained -X- _ O
by -X- _ O
applying -X- _ O
both -X- _ O
AdvT -X- _ B-MethodName
and -X- _ O
VAT -X- _ B-MethodName
simultaneously -X- _ O
. -X- _ O

Therefore -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
expect -X- _ O
that -X- _ O
VAT -X- _ B-MethodName
can -X- _ O
improve -X- _ O
the -X- _ O
translation -X- _ O
performance -X- _ O
on -X- _ O
other -X- _ O
datasets -X- _ O
and -X- _ O
settings -X- _ O
with -X- _ O
relatively -X- _ O
highconfidence -X- _ O
. -X- _ O

We -X- _ O
report -X- _ O
that -X- _ O
VAT -X- _ B-MethodName
did -X- _ O
not -X- _ O
require -X- _ O
us -X- _ O
to -X- _ O
perform -X- _ O
additional -X- _ O
heavy -X- _ O
hyper -X- _ O
- -X- _ O
parameter -X- _ O
search -X- _ O
( -X- _ O
excluding -X- _ O
the -X- _ O
hyper -X- _ O
- -X- _ O
parameter -X- _ O
search -X- _ O
in -X- _ O
base -X- _ O
models -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
observe -X- _ O
that -X- _ O
Transformer+VAT -X- _ B-MethodName
consistently -X- _ O
outperformed -X- _ O
the -X- _ O
baseline -X- _ B-MethodName
Transformer -X- _ I-MethodName
results -X- _ O
in -X- _ O
both -X- _ O
standard -X- _ B-MethodName
( -X- _ O
a -X- _ O
) -X- _ O
and -X- _ O
back -X- _ B-MethodName
- -X- _ I-MethodName
translation -X- _ I-MethodName
( -X- _ O
b -X- _ O
) -X- _ O
settings -X- _ O
. -X- _ O

Furthermore -X- _ O
, -X- _ O
the -X- _ O
row -X- _ O
( -X- _ O
b -X- _ O
) -X- _ O
shows -X- _ O
the -X- _ O
results -X- _ O
obtained -X- _ O
when -X- _ O
we -X- _ O
incorporated -X- _ O
pseudo -X- _ O
- -X- _ O
parallel -X- _ O
corpora -X- _ O
generated -X- _ O
using -X- _ O
the -X- _ O
back -X- _ O
- -X- _ O
translation -X- _ O
method -X- _ O
( -X- _ O
Sennrich -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016a -X- _ O
) -X- _ O
generating -X- _ O
the -X- _ O
pseudo -X- _ O
- -X- _ O
parallel -X- _ O
corpora -X- _ O
, -X- _ O
we -X- _ O
used -X- _ O
the -X- _ O
WMT14 -X- _ B-DatasetName
news -X- _ I-DatasetName
translation -X- _ I-DatasetName
corpus -X- _ I-DatasetName
. -X- _ O

Results -X- _ O
on -X- _ O
four -X- _ O
language -X- _ O
pairs -X- _ O
Table -X- _ O
3 -X- _ O
shows -X- _ O
the -X- _ O
BLEU -X- _ B-MetricName
scores -X- _ O
of -X- _ O
averaged -X- _ O
over -X- _ O
five -X- _ O
models -X- _ O
on -X- _ O
four -X- _ O
different -X- _ O
language -X- _ O
pairs -X- _ O
( -X- _ O
directions -X- _ O
) -X- _ O
, -X- _ O
namely -X- _ O
German!English -X- _ O
, -X- _ O
French!English -X- _ O
, -X- _ O
English!German -X- _ O
, -X- _ O
and -X- _ O
English!French -X- _ O
. -X- _ O

They -X- _ O
referred -X- _ O
to -X- _ O
this -X- _ O
phenomenon -X- _ O
of -X- _ O
AdvT -X- _ B-MethodName
as -X- _ O
label -X- _ O
leaking -X- _ O
. -X- _ O

( -X- _ O
2017 -X- _ O
) -X- _ O
, -X- _ O
AdvT -X- _ B-MethodName
generates -X- _ O
the -X- _ O
adversarial -X- _ O
examples -X- _ O
from -X- _ O
correct -X- _ O
examples -X- _ O
, -X- _ O
and -X- _ O
thus -X- _ O
, -X- _ O
the -X- _ O
models -X- _ O
trained -X- _ O
by -X- _ O
AdvT -X- _ B-MethodName
tend -X- _ O
to -X- _ O
overfit -X- _ O
to -X- _ O
training -X- _ O
data -X- _ O
rather -X- _ O
than -X- _ O
those -X- _ O
trained -X- _ O
by -X- _ O
VAT -X- _ B-MethodName
. -X- _ O

Furthermore -X- _ O
, -X- _ O
the -X- _ O
results -X- _ O
of -X- _ O
VAT -X- _ B-MethodName
was -X- _ O
consistently -X- _ O
better -X- _ O
than -X- _ O
those -X- _ O
of -X- _ O
AdvT. -X- _ B-MethodName

Moreover -X- _ O
, -X- _ O
we -X- _ O
achieved -X- _ O
better -X- _ O
performance -X- _ O
when -X- _ O
we -X- _ O
added -X- _ O
perturbation -X- _ O
to -X- _ O
the -X- _ O
encoder -X- _ B-HyperparameterValue
- -X- _ I-HyperparameterValue
side -X- _ I-HyperparameterValue
( -X- _ O
encemb -X- _ B-HyperparameterValue
) -X- _ O
rather -X- _ O
than -X- _ O
the -X- _ O
decoder -X- _ B-HyperparameterValue
- -X- _ I-HyperparameterValue
side -X- _ I-HyperparameterValue
( -X- _ O
dec -X- _ B-HyperparameterValue
- -X- _ I-HyperparameterValue
emb -X- _ I-HyperparameterValue
) -X- _ O
. -X- _ O

Investigation -X- _ O
of -X- _ O
effective -X- _ O
configuration -X- _ O
Table -X- _ O
2 -X- _ O
shows -X- _ O
the -X- _ O
experimental -X- _ O
results -X- _ O
with -X- _ O
configurations -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
perturbation -X- _ I-HyperparameterName
positions -X- _ I-HyperparameterName
( -X- _ O
enc -X- _ B-HyperparameterValue
- -X- _ I-HyperparameterValue
emb -X- _ I-HyperparameterValue
, -X- _ O
decemb -X- _ B-HyperparameterValue
, -X- _ O
or -X- _ O
enc -X- _ B-HyperparameterValue
- -X- _ I-HyperparameterValue
dec -X- _ I-HyperparameterValue
- -X- _ I-HyperparameterValue
emb -X- _ I-HyperparameterValue
) -X- _ O
and -X- _ O
adversarial -X- _ B-MethodName
regularization -X- _ I-MethodName
techniques -X- _ O
( -X- _ O
AdvT -X- _ B-MethodName
or -X- _ O
VAT -X- _ B-MethodName
) -X- _ O
. -X- _ O

Firstly -X- _ O
, -X- _ O
in -X- _ O
terms -X- _ O
of -X- _ O
the -X- _ O
effective -X- _ B-HyperparameterName
perturbation -X- _ I-HyperparameterName
position -X- _ I-HyperparameterName
, -X- _ O
enc -X- _ B-HyperparameterValue
- -X- _ I-HyperparameterValue
dec -X- _ I-HyperparameterValue
- -X- _ I-HyperparameterValue
emb -X- _ I-HyperparameterValue
configurations -X- _ O
, -X- _ O
which -X- _ O
add -X- _ O
perturbations -X- _ O
to -X- _ O
both -X- _ O
encoder -X- _ O
and -X- _ O
decoder -X- _ O
embeddings -X- _ O
, -X- _ O
consistently -X- _ O
outperformed -X- _ O
other -X- _ O
configurations -X- _ O
, -X- _ O
which -X- _ O
used -X- _ O
either -X- _ O
encoder -X- _ O
or -X- _ O
decoder -X- _ O
only -X- _ O
. -X- _ O

Note -X- _ O
that -X- _ O
all -X- _ O
reported -X- _ O
BLEU -X- _ B-MetricName
scores -X- _ O
are -X- _ O
averaged -X- _ O
over -X- _ O
five -X- _ O
models -X- _ O
. -X- _ O

As -X- _ O
evaluation -X- _ O
metrics -X- _ O
, -X- _ O
we -X- _ O
used -X- _ O
BLEU -X- _ B-MetricName
scores -X- _ O
( -X- _ O
Papineni -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2002 -X- _ O
) -X- _ O
6 -X- _ O
. -X- _ O

We -X- _ O
set -X- _ O
= -X- _ O
1 -X- _ B-HyperparameterValue
and -X- _ O
‚úè -X- _ B-HyperparameterName
= -X- _ O
1 -X- _ B-HyperparameterValue
for -X- _ O
all -X- _ O
AdvT -X- _ B-MethodName
and -X- _ O
VAT -X- _ B-MethodName
experiments -X- _ O
. -X- _ O

Hereafter -X- _ O
, -X- _ O
we -X- _ O
refer -X- _ O
to -X- _ O
the -X- _ O
model -X- _ O
trained -X- _ O
with -X- _ O
the -X- _ O
adversarial -X- _ B-MethodName
regularization -X- _ I-MethodName
( -X- _ O
` -X- _ O
in -X- _ O
Eq -X- _ O
. -X- _ O
7 -X- _ O
) -X- _ O
as -X- _ O
AdvT -X- _ B-MethodName
, -X- _ O
and -X- _ O
similarly -X- _ O
, -X- _ O
with -X- _ O
the -X- _ O
virtual -X- _ B-MethodName
adversarial -X- _ I-MethodName
training -X- _ I-MethodName
( -X- _ O
` -X- _ O
K -X- _ O
L -X- _ O
in -X- _ O
Eq -X- _ O
. -X- _ O
11 -X- _ O
) -X- _ O
as -X- _ O
VAT -X- _ B-MethodName
. -X- _ O

( -X- _ O
2015 -X- _ O
) -X- _ O
and -X- _ O
self -X- _ B-MethodName
- -X- _ I-MethodName
attentionbased -X- _ I-MethodName
encoder -X- _ I-MethodName
- -X- _ I-MethodName
decoder -X- _ I-MethodName
, -X- _ O
the -X- _ O
so -X- _ O
- -X- _ O
called -X- _ O
Transformer -X- _ B-MethodName
( -X- _ O
Vaswani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
selected -X- _ O
two -X- _ O
widely -X- _ O
used -X- _ O
model -X- _ O
architectures -X- _ O
, -X- _ O
namely -X- _ O
, -X- _ O
LSTM -X- _ B-MethodName
- -X- _ I-MethodName
based -X- _ I-MethodName
encoder -X- _ I-MethodName
- -X- _ I-MethodName
decoder -X- _ I-MethodName
2 -X- _ O
https://github.com/moses-smt/ -X- _ O
mosesdecoder -X- _ O
/ -X- _ O
blob -X- _ O
/ -X- _ O
master -X- _ O
/ -X- _ O
scripts/ -X- _ O
tokenizer -X- _ O
/ -X- _ O
tokenizer.perl -X- _ O
3 -X- _ O
https://github.com/moses-smt/ -X- _ O
mosesdecoder -X- _ O
/ -X- _ O
blob -X- _ O
/ -X- _ O
master -X- _ O
/ -X- _ O
scripts/ -X- _ O
recaser -X- _ O
/ -X- _ O
truecase.perl -X- _ O
4 -X- _ O
https://github.com/rsennrich/ -X- _ O
subword -X- _ O
- -X- _ O
nmt -X- _ O
used -X- _ O
in -X- _ O
Luong -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

For -X- _ O
preprocessing -X- _ O
of -X- _ O
our -X- _ O
experimental -X- _ O
datasets -X- _ O
, -X- _ O
we -X- _ O
used -X- _ O
the -X- _ O
Moses -X- _ B-MethodName
tokenizer -X- _ I-MethodName
2 -X- _ O
and -X- _ O
the -X- _ O
truecaser -X- _ B-MethodName
3 -X- _ O
. -X- _ O

We -X- _ O
used -X- _ O
the -X- _ O
IWSLT -X- _ B-DatasetName
2016 -X- _ I-DatasetName
training -X- _ O
set -X- _ O
for -X- _ O
training -X- _ O
models -X- _ O
, -X- _ O
2012 -X- _ B-DatasetName
test -X- _ O
set -X- _ O
( -X- _ O
test2012 -X- _ O
) -X- _ O
as -X- _ O
the -X- _ O
development -X- _ O
set -X- _ O
, -X- _ O
and -X- _ O
2013 -X- _ B-DatasetName
and -X- _ O
2014 -X- _ B-DatasetName
test -X- _ O
sets -X- _ O
( -X- _ O
test2013 -X- _ O
and -X- _ O
test2014 -X- _ O
) -X- _ O
as -X- _ O
our -X- _ O
test -X- _ O
sets -X- _ O
. -X- _ O

We -X- _ O
conducted -X- _ O
experiments -X- _ O
on -X- _ O
the -X- _ O
IWSLT -X- _ B-DatasetName
evaluation -X- _ I-DatasetName
campaign -X- _ I-DatasetName
dataset -X- _ O
( -X- _ O
Cettolo -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2012 -X- _ O
) -X- _ O
. -X- _ O

Finally -X- _ O
, -X- _ O
we -X- _ O
have -X- _ O
three -X- _ O
options -X- _ O
for -X- _ O
applying -X- _ O
the -X- _ O
perturbation -X- _ O
into -X- _ O
typical -X- _ O
NMT -X- _ B-TaskName
models -X- _ O
, -X- _ O
namely -X- _ O
, -X- _ O
applying -X- _ O
the -X- _ O
perturbation -X- _ O
into -X- _ O
embeddings -X- _ O
in -X- _ O
the -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
encoder -X- _ O
- -X- _ O
side -X- _ O
only -X- _ O
, -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
decoder -X- _ O
- -X- _ O
side -X- _ O
only -X- _ O
, -X- _ O
and -X- _ O
( -X- _ O
3 -X- _ O
) -X- _ O
both -X- _ O
encoder -X- _ O
and -X- _ O
decoder -X- _ O
sides -X- _ O
. -X- _ O

For -X- _ O
example -X- _ O
, -X- _ O
let -X- _ O
r0 -X- _ O
j -X- _ O
2 -X- _ O
R -X- _ O
D -X- _ O
be -X- _ O
an -X- _ O
adversarial -X- _ B-MethodName
perturbation -X- _ I-MethodName
vector -X- _ O
for -X- _ O
the -X- _ O
j -X- _ O
- -X- _ O
th -X- _ O
word -X- _ O
in -X- _ O
output -X- _ O
Y -X- _ O
. -X- _ O

This -X- _ O
fact -X- _ O
immediately -X- _ O
offers -X- _ O
us -X- _ O
also -X- _ O
to -X- _ O
consider -X- _ O
applying -X- _ O
the -X- _ O
adversarial -X- _ B-MethodName
perturbation -X- _ I-MethodName
into -X- _ O
the -X- _ O
decoder -X- _ O
- -X- _ O
side -X- _ O
embeddings -X- _ O
f -X- _ O
j -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
NMT -X- _ B-TaskName
models -X- _ O
generally -X- _ O
have -X- _ O
another -X- _ O
embedding -X- _ O
layer -X- _ O
in -X- _ O
the -X- _ O
decoder -X- _ O
- -X- _ O
side -X- _ O
, -X- _ O
as -X- _ O
we -X- _ O
explained -X- _ O
in -X- _ O
Eq -X- _ O
. -X- _ O
2 -X- _ O
. -X- _ O

Adversarial -X- _ B-MethodName
Regularization -X- _ I-MethodName
in -X- _ O
NMT -X- _ B-TaskName
. -X- _ O

It -X- _ O
is -X- _ O
worth -X- _ O
noting -X- _ O
here -X- _ O
that -X- _ O
, -X- _ O
in -X- _ O
our -X- _ O
experiments -X- _ O
, -X- _ O
we -X- _ O
never -X- _ O
applied -X- _ O
the -X- _ O
semi -X- _ O
- -X- _ O
supervised -X- _ O
learning -X- _ O
, -X- _ O
but -X- _ O
used -X- _ O
the -X- _ O
above -X- _ O
equation -X- _ O
for -X- _ O
calculating -X- _ O
perturbation -X- _ O
as -X- _ O
the -X- _ O
replacement -X- _ O
of -X- _ O
standard -X- _ O
adversarial -X- _ B-MethodName
regularization -X- _ I-MethodName
. -X- _ O

` -X- _ O
KL -X- _ O
( -X- _ O
X -X- _ O
, -X- _ O
r -X- _ O
, -X- _ O
‚Ä¢ -X- _ O
, -X- _ O
‚á• -X- _ O
) -X- _ O
= -X- _ O
KL -X- _ O
p(‚Ä¢ -X- _ O
|X -X- _ O
, -X- _ O
‚á• -X- _ O
) -X- _ O
||p(‚Ä¢ -X- _ O
|X -X- _ O
, -X- _ O
r -X- _ O
, -X- _ O
‚á• -X- _ O
) -X- _ O
, -X- _ O
( -X- _ O
11 -X- _ O
) -X- _ O
where -X- _ O
KL(‚Ä¢||‚Ä¢ -X- _ O
) -X- _ O
denotes -X- _ O
the -X- _ O
KL -X- _ B-MetricName
divergence -X- _ I-MetricName
. -X- _ O

This -X- _ O
section -X- _ O
briefly -X- _ O
describes -X- _ O
the -X- _ O
adversarial -X- _ B-MethodName
regularization -X- _ I-MethodName
technique -X- _ O
applied -X- _ O
to -X- _ O
the -X- _ O
text -X- _ B-TaskName
classification -X- _ I-TaskName
tasks -X- _ O
proposed -X- _ O
in -X- _ O
Miyato -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

Adversarial -X- _ B-MethodName
Regularization -X- _ I-MethodName
. -X- _ O

We -X- _ O
generally -X- _ O
use -X- _ O
a -X- _ O
K -X- _ B-MethodName
- -X- _ I-MethodName
best -X- _ I-MethodName
beam -X- _ I-MethodName
search -X- _ I-MethodName
to -X- _ O
generate -X- _ O
an -X- _ O
output -X- _ O
sentence -X- _ O
with -X- _ O
the -X- _ O
( -X- _ O
approximated -X- _ O
) -X- _ O
K -X- _ O
- -X- _ O
highest -X- _ O
probability -X- _ O
given -X- _ O
input -X- _ O
sentence -X- _ O
X -X- _ O
in -X- _ O
the -X- _ O
generation -X- _ O
( -X- _ O
test -X- _ O
) -X- _ O
phase -X- _ O
. -X- _ O

For -X- _ O
training -X- _ O
, -X- _ O
we -X- _ O
generally -X- _ O
seek -X- _ O
the -X- _ O
optimal -X- _ O
parameters -X- _ O
‚á• -X- _ O
that -X- _ O
can -X- _ O
minimize -X- _ O
the -X- _ O
following -X- _ O
optimization -X- _ O
problem -X- _ O
: -X- _ O
‚á• -X- _ O
= -X- _ O
argmin -X- _ O
‚á• -X- _ O
J -X- _ O
( -X- _ O
D -X- _ O
, -X- _ O
‚á• -X- _ O
) -X- _ O
, -X- _ O
( -X- _ O
3 -X- _ O
) -X- _ O
J -X- _ O
( -X- _ O
D -X- _ O
, -X- _ O
‚á• -X- _ O
) -X- _ O
= -X- _ O
1 -X- _ O
|D| -X- _ O
X -X- _ O
( -X- _ O
X -X- _ O
, -X- _ O
Y -X- _ O
) -X- _ O
2D -X- _ O
` -X- _ O
( -X- _ O
X -X- _ O
, -X- _ O
Y -X- _ O
, -X- _ O
‚á• -X- _ O
) -X- _ O
, -X- _ O
( -X- _ O
4 -X- _ O
) -X- _ O
` -X- _ O
( -X- _ O
X -X- _ O
, -X- _ O
Y -X- _ O
, -X- _ O
‚á• -X- _ O
) -X- _ O
= -X- _ O
log -X- _ O
p(Y -X- _ O
|X -X- _ O
, -X- _ O
‚á• -X- _ O
) -X- _ O
, -X- _ O
( -X- _ O
5 -X- _ O
) -X- _ O
where -X- _ O
‚á• -X- _ O
represents -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
trainable -X- _ O
parameters -X- _ O
in -X- _ O
the -X- _ O
NMT -X- _ B-TaskName
model -X- _ O
. -X- _ O

Thus -X- _ O
, -X- _ O
the -X- _ O
NMT -X- _ B-TaskName
model -X- _ O
approximates -X- _ O
the -X- _ O
following -X- _ O
conditional -X- _ O
probability -X- _ O
: -X- _ O
p(Y -X- _ O
|X -X- _ O
) -X- _ O
= -X- _ O
Y -X- _ O
J+1 -X- _ O
j=1 -X- _ O
p(y -X- _ O
j -X- _ O
|y -X- _ O
0 -X- _ O
: -X- _ O
j -X- _ O
1 -X- _ O
, -X- _ O
X),(1 -X- _ O
) -X- _ O
where -X- _ O
y -X- _ O
0 -X- _ O
and -X- _ O
y -X- _ O
J+1 -X- _ O
represent -X- _ O
one -X- _ O
- -X- _ O
hot -X- _ O
vectors -X- _ O
of -X- _ O
special -X- _ O
beginning -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
sentence -X- _ O
( -X- _ O
BOS -X- _ O
) -X- _ O
and -X- _ O
end -X- _ O
- -X- _ O
ofsentence -X- _ O
( -X- _ O
EOS -X- _ O
) -X- _ O
tokens -X- _ O
, -X- _ O
respectively -X- _ O
, -X- _ O
and -X- _ O
X -X- _ O
= -X- _ O
x -X- _ O
1 -X- _ O
: -X- _ O
I -X- _ O
and -X- _ O
Y -X- _ O
= -X- _ O
y -X- _ O
1 -X- _ O
: -X- _ O
J+1 -X- _ O
. -X- _ O

To -X- _ O
explain -X- _ O
the -X- _ O
NMT -X- _ B-TaskName
model -X- _ O
concisely -X- _ O
, -X- _ O
we -X- _ O
assume -X- _ O
that -X- _ O
its -X- _ O
input -X- _ O
and -X- _ O
output -X- _ O
are -X- _ O
both -X- _ O
sequences -X- _ O
of -X- _ O
one -X- _ O
- -X- _ O
hot -X- _ O
vectors -X- _ O
x -X- _ O
1 -X- _ O
: -X- _ O
I -X- _ O
and -X- _ O
y -X- _ O
1 -X- _ O
: -X- _ O
J -X- _ O
that -X- _ O
correspond -X- _ O
to -X- _ O
input -X- _ O
and -X- _ O
output -X- _ O
sentences -X- _ O
whose -X- _ O
lengths -X- _ O
are -X- _ O
I -X- _ O
and -X- _ O
J -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O

Model -X- _ O
Definition -X- _ O
In -X- _ O
general -X- _ O
, -X- _ O
an -X- _ O
NMT -X- _ B-TaskName
model -X- _ O
receives -X- _ O
a -X- _ O
sentence -X- _ O
as -X- _ O
input -X- _ O
and -X- _ O
returns -X- _ O
a -X- _ O
corresponding -X- _ O
( -X- _ O
translated -X- _ O
) -X- _ O
sentence -X- _ O
as -X- _ O
output -X- _ O
. -X- _ O

Neural -X- _ B-TaskName
Machine -X- _ I-TaskName
Translation -X- _ I-TaskName
Model -X- _ O
. -X- _ O

( -X- _ O
2018 -X- _ O
) -X- _ O
used -X- _ O
virtual -X- _ B-MethodName
adversarial -X- _ I-MethodName
training -X- _ I-MethodName
( -X- _ O
VAT -X- _ B-MethodName
) -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
a -X- _ O
semi -X- _ O
- -X- _ O
supervised -X- _ O
extension -X- _ O
of -X- _ O
the -X- _ O
adversarial -X- _ B-MethodName
regularization -X- _ I-MethodName
technique -X- _ O
originally -X- _ O
proposed -X- _ O
in -X- _ O
Miyato -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

We -X- _ O
investigate -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
the -X- _ O
several -X- _ O
practical -X- _ O
configurations -X- _ O
that -X- _ O
have -X- _ O
not -X- _ O
been -X- _ O
examined -X- _ O
in -X- _ O
their -X- _ O
paper -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
the -X- _ O
combinations -X- _ O
with -X- _ O
VAT -X- _ B-MethodName
and -X- _ O
back -X- _ B-MethodName
- -X- _ I-MethodName
translation -X- _ I-MethodName
. -X- _ O

They -X- _ O
also -X- _ O
demonstrated -X- _ O
the -X- _ O
impacts -X- _ O
of -X- _ O
the -X- _ O
adversarial -X- _ B-MethodName
regularization -X- _ I-MethodName
technique -X- _ O
in -X- _ O
NMT -X- _ B-TaskName
models -X- _ O
. -X- _ O

( -X- _ O
2019 -X- _ O
) -X- _ O
also -X- _ O
investigated -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
the -X- _ O
adversarial -X- _ B-MethodName
regularization -X- _ I-MethodName
technique -X- _ O
in -X- _ O
neural -X- _ B-TaskName
language -X- _ I-TaskName
modeling -X- _ I-TaskName
and -X- _ O
NMT -X- _ B-TaskName
. -X- _ O

Namely -X- _ O
, -X- _ O
they -X- _ O
focused -X- _ O
on -X- _ O
sequential -X- _ O
labeling -X- _ O
, -X- _ O
whereas -X- _ O
we -X- _ O
discuss -X- _ O
NMT -X- _ B-TaskName
models -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
the -X- _ O
main -X- _ O
focus -X- _ O
of -X- _ O
these -X- _ O
methods -X- _ O
is -X- _ O
the -X- _ O
incorporation -X- _ O
of -X- _ O
adversarial -X- _ O
examples -X- _ O
in -X- _ O
the -X- _ O
training -X- _ O
phase -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
orthogonal -X- _ O
to -X- _ O
our -X- _ O
attention -X- _ O
, -X- _ O
adversarial -X- _ B-MethodName
regularization -X- _ I-MethodName
, -X- _ O
as -X- _ O
described -X- _ O
in -X- _ O
Section -X- _ O
1 -X- _ O
. -X- _ O

We -X- _ O
investigate -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
several -X- _ O
possible -X- _ O
configurations -X- _ O
that -X- _ O
can -X- _ O
significantly -X- _ O
and -X- _ O
consistently -X- _ O
improve -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
typical -X- _ O
baseline -X- _ O
NMT -X- _ B-TaskName
models -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
LSTM -X- _ O
- -X- _ O
based -X- _ O
and -X- _ O
Transformer -X- _ O
- -X- _ O
based -X- _ O
models -X- _ O
, -X- _ O
. -X- _ O

Therefore -X- _ O
, -X- _ O
the -X- _ O
goal -X- _ O
of -X- _ O
this -X- _ O
paper -X- _ O
is -X- _ O
to -X- _ O
re -X- _ O
- -X- _ O
veal -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
the -X- _ O
adversarial -X- _ B-MethodName
regularization -X- _ I-MethodName
in -X- _ O
NMT -X- _ B-TaskName
models -X- _ O
and -X- _ O
encourage -X- _ O
researchers -X- _ O
/ -X- _ O
developers -X- _ O
to -X- _ O
apply -X- _ O
the -X- _ O
adversarial -X- _ B-MethodName
regularization -X- _ I-MethodName
as -X- _ O
a -X- _ O
common -X- _ O
technique -X- _ O
for -X- _ O
further -X- _ O
improving -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
their -X- _ O
NMT -X- _ B-TaskName
models -X- _ O
. -X- _ O

Figure -X- _ O
1 -X- _ O
illustrates -X- _ O
the -X- _ O
model -X- _ O
architecture -X- _ O
of -X- _ O
NMT -X- _ B-TaskName
models -X- _ O
with -X- _ O
adversarial -X- _ O
perturbation -X- _ O
. -X- _ O

Unfortunately -X- _ O
, -X- _ O
this -X- _ O
application -X- _ O
is -X- _ O
not -X- _ O
fully -X- _ O
trivial -X- _ O
since -X- _ O
we -X- _ O
potentially -X- _ O
have -X- _ O
several -X- _ O
configurations -X- _ O
for -X- _ O
applying -X- _ O
adversarial -X- _ O
perturbations -X- _ O
into -X- _ O
NMT -X- _ B-TaskName
models -X- _ O
( -X- _ O
see -X- _ O
details -X- _ O
in -X- _ O
Section -X- _ O
5 -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
aim -X- _ O
to -X- _ O
further -X- _ O
leverage -X- _ O
this -X- _ O
promising -X- _ O
methodology -X- _ O
into -X- _ O
more -X- _ O
sophisticated -X- _ O
and -X- _ O
critical -X- _ O
neural -X- _ O
models -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
neural -X- _ B-TaskName
machine -X- _ I-TaskName
translation -X- _ I-TaskName
( -X- _ O
NMT -X- _ B-TaskName
) -X- _ O
models -X- _ O
, -X- _ O
since -X- _ O
NMT -X- _ B-TaskName
models -X- _ O
recently -X- _ O
play -X- _ O
one -X- _ O
of -X- _ O
the -X- _ O
central -X- _ O
roles -X- _ O
in -X- _ O
the -X- _ O
NLP -X- _ O
research -X- _ O
community -X- _ O
; -X- _ O
NMT -X- _ B-TaskName
models -X- _ O
have -X- _ O
been -X- _ O
widely -X- _ O
utilized -X- _ O
for -X- _ O
not -X- _ O
only -X- _ O
NMT -X- _ B-TaskName
but -X- _ O
also -X- _ O
many -X- _ O
other -X- _ O
NLP -X- _ O
tasks -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
text -X- _ B-TaskName
summarization -X- _ I-TaskName
( -X- _ O
Rush -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015;Chopra -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
, -X- _ O
grammatical -X- _ B-TaskName
error -X- _ I-TaskName
correction -X- _ I-TaskName
( -X- _ O
Ji -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
, -X- _ O
dialog -X- _ B-TaskName
generation -X- _ I-TaskName
( -X- _ O
Shang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
parsing -X- _ B-TaskName
( -X- _ O
Vinyals -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015;Suzuki -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
refer -X- _ O
to -X- _ O
this -X- _ O
regularization -X- _ O
technique -X- _ O
as -X- _ O
adversarial -X- _ B-MethodName
regularization -X- _ I-MethodName
. -X- _ O

The -X- _ O
key -X- _ O
idea -X- _ O
of -X- _ O
their -X- _ O
success -X- _ O
is -X- _ O
to -X- _ O
apply -X- _ O
adversarial -X- _ O
perturbations -X- _ O
into -X- _ O
the -X- _ O
input -X- _ O
embedding -X- _ O
layer -X- _ O
instead -X- _ O
of -X- _ O
the -X- _ O
inputs -X- _ O
themselves -X- _ O
as -X- _ O
used -X- _ O
in -X- _ O
image -X- _ B-TaskName
processing -X- _ I-TaskName
tasks -X- _ O
. -X- _ O

and -X- _ O
reported -X- _ O
excellent -X- _ O
performance -X- _ O
improvements -X- _ O
on -X- _ O
multiple -X- _ O
benchmark -X- _ O
datasets -X- _ O
of -X- _ O
text -X- _ B-TaskName
classification -X- _ I-TaskName
task -X- _ O
. -X- _ O

( -X- _ O
2017 -X- _ O
) -X- _ O
overcame -X- _ O
this -X- _ O
problem -X- _ O
1 -X- _ O
Our -X- _ O
code -X- _ O
for -X- _ O
replicating -X- _ O
the -X- _ O
experiments -X- _ O
in -X- _ O
this -X- _ O
paper -X- _ O
is -X- _ O
available -X- _ O
at -X- _ O
the -X- _ O
following -X- _ O
URL -X- _ O
: -X- _ O
https://github.com/ -X- _ O
pfnet -X- _ O
- -X- _ O
research -X- _ O
/ -X- _ O
vat_nmt -X- _ O
Encoder -X- _ O
Decoder -X- _ O
! -X- _ O
" -X- _ O
# -X- _ O
$ -X- _ O
" -X- _ O
! -X- _ O
% -X- _ O
# -X- _ O
$ -X- _ O
% -X- _ O
! -X- _ O
& -X- _ O
# -X- _ O
$ -X- _ O
& -X- _ O
' -X- _ O
( -X- _ O
# -X- _ O
$ -X- _ O
( -X- _ O
) -X- _ O
' -X- _ O
" -X- _ O
# -X- _ O
$ -X- _ O
" -X- _ O
) -X- _ O
' -X- _ O
* -X- _ O
# -X- _ O
$ -X- _ O
+ -X- _ O
) -X- _ O
, -X- _ O
" -X- _ O
, -X- _ O
% -X- _ O
, -X- _ O
+ -X- _ O
- -X- _ O
" -X- _ O
Figure -X- _ O
1 -X- _ O
: -X- _ O
An -X- _ O
intuitive -X- _ O
sketch -X- _ O
that -X- _ O
explains -X- _ O
how -X- _ O
we -X- _ O
add -X- _ O
adversarial -X- _ O
perturbations -X- _ O
to -X- _ O
a -X- _ O
typical -X- _ O
NMT -X- _ B-TaskName
model -X- _ O
structure -X- _ O
for -X- _ O
adversarial -X- _ B-MethodName
regularization -X- _ I-MethodName
. -X- _ O

Thus -X- _ O
, -X- _ O
this -X- _ O
paper -X- _ O
investigates -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
several -X- _ O
possible -X- _ O
configurations -X- _ O
of -X- _ O
applying -X- _ O
the -X- _ O
adversarial -X- _ O
perturbation -X- _ O
and -X- _ O
reveals -X- _ O
that -X- _ O
the -X- _ O
adversarial -X- _ B-MethodName
regularization -X- _ I-MethodName
technique -X- _ O
can -X- _ O
significantly -X- _ O
and -X- _ O
consistently -X- _ O
improve -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
widely -X- _ O
used -X- _ O
NMT -X- _ B-TaskName
models -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
LSTMbased -X- _ O
and -X- _ O
Transformer -X- _ O
- -X- _ O
based -X- _ O
models -X- _ O
. -X- _ O

We -X- _ O
aim -X- _ O
to -X- _ O
further -X- _ O
leverage -X- _ O
this -X- _ O
promising -X- _ O
methodology -X- _ O
into -X- _ O
more -X- _ O
sophisticated -X- _ O
and -X- _ O
critical -X- _ O
neural -X- _ O
models -X- _ O
in -X- _ O
the -X- _ O
natural -X- _ O
language -X- _ O
processing -X- _ O
field -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
neural -X- _ B-TaskName
machine -X- _ I-TaskName
translation -X- _ I-TaskName
( -X- _ O
NMT -X- _ B-TaskName
) -X- _ O
models -X- _ O
. -X- _ O

A -X- _ O
regularization -X- _ O
technique -X- _ O
based -X- _ O
on -X- _ O
adversarial -X- _ B-MethodName
perturbation -X- _ I-MethodName
, -X- _ O
which -X- _ O
was -X- _ O
initially -X- _ O
developed -X- _ O
in -X- _ O
the -X- _ O
field -X- _ O
of -X- _ O
image -X- _ O
processing -X- _ O
, -X- _ O
has -X- _ O
been -X- _ O
successfully -X- _ O
applied -X- _ O
to -X- _ O
text -X- _ B-TaskName
classification -X- _ I-TaskName
tasks -X- _ O
and -X- _ O
has -X- _ O
yielded -X- _ O
attractive -X- _ O
improvements -X- _ O
. -X- _ O

Effective -X- _ O
Adversarial -X- _ B-MethodName
Regularization -X- _ I-MethodName
for -X- _ O
Neural -X- _ B-TaskName
Machine -X- _ I-TaskName
Translation -X- _ I-TaskName
. -X- _ O

We -X- _ O
thank -X- _ O
three -X- _ O
anonymous -X- _ O
reviewers -X- _ O
for -X- _ O
their -X- _ O
helpful -X- _ O
comments -X- _ O
. -X- _ O

Acknowledgments -X- _ O
. -X- _ O

Conclusion -X- _ O
. -X- _ O

Actual -X- _ O
Translation -X- _ O
Examples -X- _ O
Table -X- _ O
. -X- _ O

4 -X- _ O
shows -X- _ O
actual -X- _ O
translation -X- _ O
examples -X- _ O
generated -X- _ O
by -X- _ O
the -X- _ O
models -X- _ O
compared -X- _ O
in -X- _ O
our -X- _ O
German!English -X- _ O
translation -X- _ O
setting -X- _ O
. -X- _ O

We -X- _ O
can -X- _ O
further -X- _ O
improve -X- _ O
the -X- _ O
performance -X- _ O
in -X- _ O
some -X- _ O
cases -X- _ O
, -X- _ O
but -X- _ O
the -X- _ O
improvement -X- _ O
is -X- _ O
not -X- _ O
consistent -X- _ O
among -X- _ O
the -X- _ O
datasets -X- _ O
. -X- _ O

As -X- _ O
discussed -X- _ O
in -X- _ O
Kurakin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

( -X- _ O
2016 -X- _ O
) -X- _ O
. -X- _ O

This -X- _ O
tendency -X- _ O
was -X- _ O
also -X- _ O
observed -X- _ O
in -X- _ O
the -X- _ O
results -X- _ O
reported -X- _ O
by -X- _ O
Miyato -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

Results -X- _ O
. -X- _ O

We -X- _ O
adapted -X- _ O
the -X- _ O
hyper -X- _ O
- -X- _ O
parameters -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
several -X- _ O
recent -X- _ O
previous -X- _ O
papers -X- _ O
5 -X- _ O
. -X- _ O

Model -X- _ O
Configurations -X- _ O
. -X- _ O

We -X- _ O
also -X- _ O
applied -X- _ O
the -X- _ O
byte -X- _ O
- -X- _ O
pair -X- _ O
encoding -X- _ O
( -X- _ O
BPE -X- _ O
) -X- _ O
based -X- _ O
subword -X- _ O
splitting -X- _ O
script -X- _ O
4 -X- _ O
with -X- _ O
16,000 -X- _ O
merge -X- _ O
operations -X- _ O
( -X- _ O
Sennrich -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016b -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
removed -X- _ O
sentences -X- _ O
over -X- _ O
50 -X- _ O
words -X- _ O
from -X- _ O
the -X- _ O
training -X- _ O
set -X- _ O
. -X- _ O

Table -X- _ O
1 -X- _ O
shows -X- _ O
the -X- _ O
statistics -X- _ O
of -X- _ O
datasets -X- _ O
used -X- _ O
in -X- _ O
our -X- _ O
experiments -X- _ O
. -X- _ O

Datasets -X- _ O
. -X- _ O

6 -X- _ O
Experiments -X- _ O
. -X- _ O

In -X- _ O
addition -X- _ O
, -X- _ O
we -X- _ O
need -X- _ O
to -X- _ O
slightly -X- _ O
modify -X- _ O
the -X- _ O
definition -X- _ O
of -X- _ O
r -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
originally -X- _ O
the -X- _ O
concatenation -X- _ O
vector -X- _ O
of -X- _ O
all -X- _ O
r -X- _ O
i -X- _ O
for -X- _ O
all -X- _ O
i -X- _ O
, -X- _ O
to -X- _ O
the -X- _ O
concatenation -X- _ O
vector -X- _ O
of -X- _ O
all -X- _ O
r -X- _ O
i -X- _ O
and -X- _ O
r -X- _ O
0 -X- _ O
j -X- _ O
for -X- _ O
all -X- _ O
i -X- _ O
and -X- _ O
j. -X- _ O

The -X- _ O
perturbed -X- _ O
embedding -X- _ O
f -X- _ O
0 -X- _ O
j -X- _ O
2 -X- _ O
R -X- _ O
D -X- _ O
is -X- _ O
computed -X- _ O
for -X- _ O
each -X- _ O
decoder -X- _ O
time -X- _ O
- -X- _ O
step -X- _ O
j -X- _ O
as -X- _ O
follows -X- _ O
: -X- _ O
f -X- _ O
0 -X- _ O
j -X- _ O
= -X- _ O
F -X- _ O
y -X- _ O
j -X- _ O
1 -X- _ O
+ -X- _ O
r0 -X- _ O
j -X- _ O
.(12 -X- _ O
) -X- _ O
Then -X- _ O
similar -X- _ O
to -X- _ O
Eq -X- _ O
. -X- _ O
8 -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
calculate -X- _ O
r0 -X- _ O
as -X- _ O
: -X- _ O
r0 -X- _ O
j -X- _ O
= -X- _ O
‚úè -X- _ O
b -X- _ O
j -X- _ O
||b|| -X- _ O
2 -X- _ O
, -X- _ O
b -X- _ O
j -X- _ O
= -X- _ O
r -X- _ O
f -X- _ O
j -X- _ O
` -X- _ O
( -X- _ O
X -X- _ O
, -X- _ O
Y -X- _ O
, -X- _ O
‚á• -X- _ O
) -X- _ O
, -X- _ O
( -X- _ O
13 -X- _ O
) -X- _ O
where -X- _ O
b -X- _ O
is -X- _ O
a -X- _ O
concatenated -X- _ O
vector -X- _ O
of -X- _ O
b -X- _ O
j -X- _ O
for -X- _ O
all -X- _ O
j. -X- _ O

As -X- _ O
strictly -X- _ O
following -X- _ O
the -X- _ O
original -X- _ O
definition -X- _ O
of -X- _ O
the -X- _ O
conventional -X- _ O
adversarial -X- _ O
training -X- _ O
, -X- _ O
the -X- _ O
straightforward -X- _ O
approach -X- _ O
to -X- _ O
applying -X- _ O
the -X- _ O
adversarial -X- _ O
perturbation -X- _ O
is -X- _ O
to -X- _ O
add -X- _ O
the -X- _ O
perturbation -X- _ O
into -X- _ O
the -X- _ O
encoderside -X- _ O
embeddings -X- _ O
e -X- _ O
i -X- _ O
as -X- _ O
described -X- _ O
in -X- _ O
Eq -X- _ O
. -X- _ O
6 -X- _ O
. -X- _ O

This -X- _ O
means -X- _ O
that -X- _ O
the -X- _ O
training -X- _ O
data -X- _ O
is -X- _ O
identical -X- _ O
in -X- _ O
both -X- _ O
settings -X- _ O
. -X- _ O

( -X- _ O
9 -X- _ O
) -X- _ O
Finally -X- _ O
, -X- _ O
we -X- _ O
jointly -X- _ O
minimize -X- _ O
the -X- _ O
objective -X- _ O
functions -X- _ O
J -X- _ O
( -X- _ O
D -X- _ O
, -X- _ O
‚á• -X- _ O
) -X- _ O
and -X- _ O
A(D -X- _ O
, -X- _ O
‚á• -X- _ O
): -X- _ O
‚á• -X- _ O
= -X- _ O
argmin -X- _ O
‚á• -X- _ O
n -X- _ O
J -X- _ O
( -X- _ O
D -X- _ O
, -X- _ O
‚á• -X- _ O
) -X- _ O
+ -X- _ O
A(D -X- _ O
, -X- _ O
‚á• -X- _ O
) -X- _ O
o -X- _ O
, -X- _ O
( -X- _ O
10 -X- _ O
) -X- _ O
where -X- _ O
is -X- _ O
a -X- _ O
scalar -X- _ O
hyper -X- _ O
- -X- _ O
parameter -X- _ O
that -X- _ O
controls -X- _ O
the -X- _ O
balance -X- _ O
of -X- _ O
the -X- _ O
two -X- _ O
loss -X- _ O
functions -X- _ O
. -X- _ O

( -X- _ O
8) -X- _ O
Thus -X- _ O
, -X- _ O
based -X- _ O
on -X- _ O
adversarial -X- _ O
perturbation -X- _ O
r -X- _ O
, -X- _ O
the -X- _ O
loss -X- _ O
function -X- _ O
can -X- _ O
be -X- _ O
defined -X- _ O
as -X- _ O
: -X- _ O
A(D -X- _ O
, -X- _ O
‚á• -X- _ O
) -X- _ O
= -X- _ O
1 -X- _ O
|D| -X- _ O
X -X- _ O
( -X- _ O
X -X- _ O
, -X- _ O
Y -X- _ O
) -X- _ O
2D -X- _ O
` -X- _ O
( -X- _ O
X -X- _ O
, -X- _ O
r -X- _ O
, -X- _ O
Y -X- _ O
, -X- _ O
‚á• -X- _ O
) -X- _ O
. -X- _ O

This -X- _ O
approximation -X- _ O
method -X- _ O
induces -X- _ O
the -X- _ O
following -X- _ O
non -X- _ O
- -X- _ O
iterative -X- _ O
solution -X- _ O
for -X- _ O
calculating -X- _ O
ri -X- _ O
for -X- _ O
all -X- _ O
encoder -X- _ O
time -X- _ O
- -X- _ O
step -X- _ O
i -X- _ O
: -X- _ O
ri -X- _ O
= -X- _ O
‚úè -X- _ O
a -X- _ O
i -X- _ O
||a|| -X- _ O
2 -X- _ O
, -X- _ O
a -X- _ O
i -X- _ O
= -X- _ O
r -X- _ O
e -X- _ O
i -X- _ O
` -X- _ O
( -X- _ O
X -X- _ O
, -X- _ O
Y -X- _ O
, -X- _ O
‚á• -X- _ O
) -X- _ O
. -X- _ O

( -X- _ O
2015 -X- _ O
) -X- _ O
, -X- _ O
where -X- _ O
` -X- _ O
( -X- _ O
X -X- _ O
, -X- _ O
Y -X- _ O
, -X- _ O
r -X- _ O
, -X- _ O
‚á• -X- _ O
) -X- _ O
is -X- _ O
linearized -X- _ O
around -X- _ O
X. -X- _ O

As -X- _ O
a -X- _ O
solution -X- _ O
, -X- _ O
an -X- _ O
approximation -X- _ O
method -X- _ O
was -X- _ O
proposed -X- _ O
by -X- _ O
Goodfellow -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
generally -X- _ O
infeasible -X- _ O
to -X- _ O
exactly -X- _ O
estimate -X- _ O
r -X- _ O
in -X- _ O
Eq -X- _ O
. -X- _ O
7 -X- _ O
for -X- _ O
deep -X- _ O
neural -X- _ O
models -X- _ O
. -X- _ O

Here -X- _ O
, -X- _ O
` -X- _ O
( -X- _ O
X -X- _ O
, -X- _ O
r -X- _ O
, -X- _ O
Y -X- _ O
, -X- _ O
‚á• -X- _ O
) -X- _ O
represents -X- _ O
an -X- _ O
extension -X- _ O
of -X- _ O
Eq -X- _ O
. -X- _ O
5 -X- _ O
, -X- _ O
where -X- _ O
the -X- _ O
perturbation -X- _ O
r -X- _ O
i -X- _ O
in -X- _ O
r -X- _ O
is -X- _ O
applied -X- _ O
to -X- _ O
the -X- _ O
position -X- _ O
of -X- _ O
ri -X- _ O
as -X- _ O
described -X- _ O
in -X- _ O
Eq -X- _ O
. -X- _ O
6 -X- _ O
. -X- _ O

To -X- _ O
obtain -X- _ O
the -X- _ O
worst -X- _ O
case -X- _ O
perturbations -X- _ O
as -X- _ O
an -X- _ O
adversarial -X- _ O
perturbation -X- _ O
in -X- _ O
terms -X- _ O
of -X- _ O
minimizing -X- _ O
the -X- _ O
log -X- _ O
- -X- _ O
likelihood -X- _ O
of -X- _ O
given -X- _ O
X -X- _ O
, -X- _ O
we -X- _ O
seek -X- _ O
the -X- _ O
optimal -X- _ O
solution -X- _ O
r -X- _ O
by -X- _ O
maximizing -X- _ O
the -X- _ O
following -X- _ O
equation -X- _ O
: -X- _ O
r -X- _ O
= -X- _ O
argmax -X- _ O
r,||r||Ô£ø -X- _ O
‚úè -X- _ O
n -X- _ O
` -X- _ O
( -X- _ O
X -X- _ O
, -X- _ O
r -X- _ O
, -X- _ O
Y -X- _ O
, -X- _ O
‚á• -X- _ O
) -X- _ O
o -X- _ O
, -X- _ O
( -X- _ O
7 -X- _ O
) -X- _ O
where -X- _ O
‚úè -X- _ O
is -X- _ O
a -X- _ O
scalar -X- _ O
hyper -X- _ O
- -X- _ O
parameter -X- _ O
that -X- _ O
controls -X- _ O
the -X- _ O
norm -X- _ O
of -X- _ O
the -X- _ O
perturbation -X- _ O
, -X- _ O
and -X- _ O
r -X- _ O
represents -X- _ O
a -X- _ O
concatenated -X- _ O
vector -X- _ O
of -X- _ O
r -X- _ O
i -X- _ O
for -X- _ O
all -X- _ O
i. -X- _ O

Adversarial -X- _ O
Training -X- _ O
( -X- _ O
AdvT -X- _ O
) -X- _ O
. -X- _ O

( -X- _ O
6 -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
perturbed -X- _ O
input -X- _ O
embedding -X- _ O
e -X- _ O
0 -X- _ O
i -X- _ O
2 -X- _ O
R -X- _ O
D -X- _ O
is -X- _ O
computed -X- _ O
for -X- _ O
each -X- _ O
encoder -X- _ O
time -X- _ O
- -X- _ O
step -X- _ O
i -X- _ O
as -X- _ O
follows -X- _ O
: -X- _ O
e -X- _ O
0 -X- _ O
i -X- _ O
= -X- _ O
Ex -X- _ O
i -X- _ O
+ -X- _ O
ri -X- _ O
. -X- _ O

Let -X- _ O
ri -X- _ O
2 -X- _ O
R -X- _ O
D -X- _ O
be -X- _ O
an -X- _ O
adversarial -X- _ O
perturbation -X- _ O
vector -X- _ O
for -X- _ O
the -X- _ O
i -X- _ O
- -X- _ O
th -X- _ O
word -X- _ O
in -X- _ O
input -X- _ O
X. -X- _ O

( -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
omit -X- _ O
to -X- _ O
explain -X- _ O
this -X- _ O
part -X- _ O
in -X- _ O
detail -X- _ O
as -X- _ O
our -X- _ O
focus -X- _ O
is -X- _ O
a -X- _ O
regularization -X- _ O
technique -X- _ O
that -X- _ O
is -X- _ O
independent -X- _ O
of -X- _ O
the -X- _ O
generation -X- _ O
phase -X- _ O
. -X- _ O

Generation -X- _ O
Phase -X- _ O
. -X- _ O

Training -X- _ O
Phase -X- _ O
Let -X- _ O
D -X- _ O
be -X- _ O
the -X- _ O
training -X- _ O
data -X- _ O
consisting -X- _ O
of -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
pairs -X- _ O
of -X- _ O
X -X- _ O
n -X- _ O
and -X- _ O
Y -X- _ O
n -X- _ O
, -X- _ O
namely -X- _ O
, -X- _ O
D -X- _ O
= -X- _ O
{ -X- _ O
( -X- _ O
X -X- _ O
n -X- _ O
, -X- _ O
Y -X- _ O
n -X- _ O
) -X- _ O
} -X- _ O
N -X- _ O
n=1 -X- _ O
, -X- _ O
where -X- _ O
N -X- _ O
represents -X- _ O
the -X- _ O
amount -X- _ O
of -X- _ O
training -X- _ O
data -X- _ O
. -X- _ O

Thus -X- _ O
, -X- _ O
p(y -X- _ O
j -X- _ O
|y -X- _ O
0 -X- _ O
: -X- _ O
j -X- _ O
1 -X- _ O
, -X- _ O
X -X- _ O
) -X- _ O
in -X- _ O
Eq -X- _ O
. -X- _ O
1 -X- _ O
is -X- _ O
calculated -X- _ O
as -X- _ O
follows -X- _ O
: -X- _ O
p(y -X- _ O
j -X- _ O
|y -X- _ O
0 -X- _ O
: -X- _ O
j -X- _ O
1 -X- _ O
, -X- _ O
X -X- _ O
) -X- _ O
= -X- _ O
AttDec -X- _ O
f -X- _ O
j -X- _ O
, -X- _ O
h -X- _ O
1 -X- _ O
: -X- _ O
I -X- _ O
, -X- _ O
h -X- _ O
1 -X- _ O
: -X- _ O
I -X- _ O
= -X- _ O
Enc(e -X- _ O
1 -X- _ O
: -X- _ O
I -X- _ O
) -X- _ O
, -X- _ O
f -X- _ O
j -X- _ O
= -X- _ O
F -X- _ O
y -X- _ O
j -X- _ O
1 -X- _ O
, -X- _ O
e -X- _ O
i -X- _ O
= -X- _ O
Ex -X- _ O
i -X- _ O
, -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
where -X- _ O
Enc(‚Ä¢ -X- _ O
) -X- _ O
and -X- _ O
AttDec(‚Ä¢ -X- _ O
) -X- _ O
represent -X- _ O
functions -X- _ O
that -X- _ O
abstract -X- _ O
the -X- _ O
entire -X- _ O
encoder -X- _ O
and -X- _ O
decoder -X- _ O
( -X- _ O
with -X- _ O
an -X- _ O
attention -X- _ O
mechanism -X- _ O
) -X- _ O
procedures -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O

Let -X- _ O
E -X- _ O
2 -X- _ O
R -X- _ O
D -X- _ O
‚á• -X- _ O
|Vs| -X- _ O
and -X- _ O
F -X- _ O
2 -X- _ O
R -X- _ O
D -X- _ O
‚á• -X- _ O
|Vt| -X- _ O
be -X- _ O
the -X- _ O
encoder -X- _ O
and -X- _ O
decoder -X- _ O
embedding -X- _ O
matrices -X- _ O
, -X- _ O
respectively -X- _ O
, -X- _ O
where -X- _ O
D -X- _ O
is -X- _ O
the -X- _ O
dimension -X- _ O
of -X- _ O
the -X- _ O
embedding -X- _ O
vectors -X- _ O
. -X- _ O

, -X- _ O
x -X- _ O
j -X- _ O
) -X- _ O
. -X- _ O

Here -X- _ O
, -X- _ O
we -X- _ O
introduce -X- _ O
a -X- _ O
short -X- _ O
notation -X- _ O
x -X- _ O
i -X- _ O
: -X- _ O
j -X- _ O
for -X- _ O
representing -X- _ O
a -X- _ O
sequence -X- _ O
of -X- _ O
vectors -X- _ O
( -X- _ O
x -X- _ O
i -X- _ O
, -X- _ O
. -X- _ O

x -X- _ O
i -X- _ O
and -X- _ O
y -X- _ O
j -X- _ O
denote -X- _ O
the -X- _ O
one -X- _ O
- -X- _ O
hot -X- _ O
vectors -X- _ O
of -X- _ O
the -X- _ O
i -X- _ O
- -X- _ O
th -X- _ O
and -X- _ O
j -X- _ O
- -X- _ O
th -X- _ O
to -X- _ O
- -X- _ O
kens -X- _ O
in -X- _ O
input -X- _ O
and -X- _ O
output -X- _ O
sentences -X- _ O
, -X- _ O
respectively -X- _ O
, -X- _ O
i.e. -X- _ O
x -X- _ O
i -X- _ O
2 -X- _ O
{ -X- _ O
0 -X- _ O
, -X- _ O
1 -X- _ O
} -X- _ O
|Vs| -X- _ O
and -X- _ O
y -X- _ O
j -X- _ O
2 -X- _ O
{ -X- _ O
0 -X- _ O
, -X- _ O
1 -X- _ O
} -X- _ O
|Vt| -X- _ O
. -X- _ O

Let -X- _ O
V -X- _ O
s -X- _ O
and -X- _ O
V -X- _ O
t -X- _ O
represent -X- _ O
the -X- _ O
vocabularies -X- _ O
of -X- _ O
the -X- _ O
input -X- _ O
and -X- _ O
output -X- _ O
sentences -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O

In -X- _ O
parallel -X- _ O
to -X- _ O
our -X- _ O
work -X- _ O
, -X- _ O
Wang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

Therefore -X- _ O
, -X- _ O
the -X- _ O
focus -X- _ O
of -X- _ O
the -X- _ O
neural -X- _ O
models -X- _ O
differs -X- _ O
from -X- _ O
this -X- _ O
paper -X- _ O
. -X- _ O

( -X- _ O
2016 -X- _ O
) -X- _ O
, -X- _ O
in -X- _ O
their -X- _ O
experiments -X- _ O
to -X- _ O
compare -X- _ O
the -X- _ O
results -X- _ O
with -X- _ O
those -X- _ O
of -X- _ O
their -X- _ O
proposed -X- _ O
method -X- _ O
. -X- _ O

Clark -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

They -X- _ O
utilized -X- _ O
the -X- _ O
generated -X- _ O
( -X- _ O
input -X- _ O
) -X- _ O
sentences -X- _ O
as -X- _ O
additional -X- _ O
training -X- _ O
data -X- _ O
. -X- _ O

( -X- _ O
2017 -X- _ O
) -X- _ O
proposed -X- _ O
methods -X- _ O
that -X- _ O
generate -X- _ O
input -X- _ O
sentences -X- _ O
with -X- _ O
random -X- _ O
character -X- _ O
swaps -X- _ O
. -X- _ O

For -X- _ O
example -X- _ O
, -X- _ O
Belinkov -X- _ O
and -X- _ O
Bisk -X- _ O
( -X- _ O
2018 -X- _ O
) -X- _ O
; -X- _ O
Hosseini -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

Several -X- _ O
studies -X- _ O
have -X- _ O
recently -X- _ O
applied -X- _ O
adversarial -X- _ O
training -X- _ O
to -X- _ O
NLP -X- _ O
tasks -X- _ O
, -X- _ O
e.g. -X- _ O
, -X- _ O
( -X- _ O
Jia -X- _ O
and -X- _ O
Liang -X- _ O
, -X- _ O
2017;Belinkov -X- _ O
and -X- _ O
Bisk -X- _ O
, -X- _ O
2018;Hosseini -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017;Samanta -X- _ O
and -X- _ O
Mehta -X- _ O
, -X- _ O
2017;Miyato -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017;Sato -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
. -X- _ O

Related -X- _ O
Work -X- _ O
. -X- _ O

An -X- _ O
important -X- _ O
implication -X- _ O
of -X- _ O
their -X- _ O
study -X- _ O
is -X- _ O
that -X- _ O
their -X- _ O
method -X- _ O
can -X- _ O
be -X- _ O
interpreted -X- _ O
as -X- _ O
a -X- _ O
regularization -X- _ O
method -X- _ O
, -X- _ O
and -X- _ O
thus -X- _ O
, -X- _ O
they -X- _ O
do -X- _ O
not -X- _ O
focus -X- _ O
on -X- _ O
generating -X- _ O
adversarial -X- _ O
examples -X- _ O
. -X- _ O

Moreover -X- _ O
, -X- _ O
those -X- _ O
of -X- _ O
ri -X- _ O
and -X- _ O
r0 -X- _ O
j -X- _ O
are -X- _ O
in -X- _ O
Eq -X- _ O
. -X- _ O
8 -X- _ O
and -X- _ O
13 -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O

The -X- _ O
definitions -X- _ O
of -X- _ O
e -X- _ O
i -X- _ O
and -X- _ O
f -X- _ O
j -X- _ O
can -X- _ O
be -X- _ O
found -X- _ O
in -X- _ O
Eq -X- _ O
. -X- _ O
2 -X- _ O
. -X- _ O

Recently -X- _ O
, -X- _ O
Miyato -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

Since -X- _ O
it -X- _ O
is -X- _ O
unreasonable -X- _ O
to -X- _ O
add -X- _ O
a -X- _ O
small -X- _ O
perturbation -X- _ O
to -X- _ O
the -X- _ O
symbols -X- _ O
, -X- _ O
applying -X- _ O
the -X- _ O
idea -X- _ O
of -X- _ O
adversarial -X- _ O
training -X- _ O
to -X- _ O
NLP -X- _ O
tasks -X- _ O
has -X- _ O
been -X- _ O
recognized -X- _ O
as -X- _ O
a -X- _ O
challenging -X- _ O
problem -X- _ O
. -X- _ O

In -X- _ O
the -X- _ O
field -X- _ O
of -X- _ O
natural -X- _ O
language -X- _ O
processing -X- _ O
( -X- _ O
NLP -X- _ O
) -X- _ O
, -X- _ O
the -X- _ O
input -X- _ O
is -X- _ O
a -X- _ O
sequence -X- _ O
of -X- _ O
discrete -X- _ O
symbols -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
words -X- _ O
or -X- _ O
sentences -X- _ O
. -X- _ O

This -X- _ O
learning -X- _ O
framework -X- _ O
is -X- _ O
referred -X- _ O
to -X- _ O
as -X- _ O
adversarial -X- _ O
training -X- _ O
. -X- _ O

( -X- _ O
2015 -X- _ O
) -X- _ O
proposed -X- _ O
a -X- _ O
learning -X- _ O
framework -X- _ O
that -X- _ O
simultaneously -X- _ O
leverages -X- _ O
adversarial -X- _ O
examples -X- _ O
as -X- _ O
additional -X- _ O
training -X- _ O
data -X- _ O
for -X- _ O
reducing -X- _ O
the -X- _ O
prediction -X- _ O
errors -X- _ O
. -X- _ O

Subsequently -X- _ O
, -X- _ O
Goodfellow -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

Such -X- _ O
perturbed -X- _ O
inputs -X- _ O
are -X- _ O
often -X- _ O
referred -X- _ O
to -X- _ O
as -X- _ O
adversarial -X- _ O
examples -X- _ O
in -X- _ O
the -X- _ O
literature -X- _ O
. -X- _ O

The -X- _ O
existence -X- _ O
of -X- _ O
( -X- _ O
small -X- _ O
) -X- _ O
perturbations -X- _ O
that -X- _ O
induce -X- _ O
a -X- _ O
critical -X- _ O
prediction -X- _ O
error -X- _ O
in -X- _ O
machine -X- _ O
learning -X- _ O
models -X- _ O
was -X- _ O
first -X- _ O
discovered -X- _ O
and -X- _ O
discussed -X- _ O
in -X- _ O
the -X- _ O
field -X- _ O
of -X- _ O
image -X- _ O
processing -X- _ O
( -X- _ O
Szegedy -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
. -X- _ O

Introduction -X- _ O
. -X- _ O

1 -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
not -X- _ O
trivial -X- _ O
to -X- _ O
apply -X- _ O
this -X- _ O
methodology -X- _ O
to -X- _ O
such -X- _ O
models -X- _ O
. -X- _ O

