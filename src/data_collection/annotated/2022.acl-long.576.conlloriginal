-DOCSTART- -X- O
Considering -X- _ O
the -X- _ O
instability -X- _ O
of -X- _ O
the -X- _ O
few -X- _ O
- -X- _ O
shot -X- _ O
learning -X- _ O
, -X- _ O
we -X- _ O
run -X- _ O
each -X- _ O
experiment -X- _ O
5 -X- _ O
times -X- _ O
on -X- _ O
the -X- _ O
random -X- _ O
seed -X- _ O
[ -X- _ O
10,20,30,40,50 -X- _ O
] -X- _ O
and -X- _ O
report -X- _ O
the -X- _ O
averaged -X- _ O
performance -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
the -X- _ O
standard -X- _ O
deviation -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
present -X- _ O
PPT -X- _ B-MethodName
, -X- _ O
a -X- _ O
framework -X- _ O
that -X- _ O
improves -X- _ O
prompt -X- _ B-MethodName
tuning -X- _ I-MethodName
for -X- _ O
few -X- _ O
- -X- _ O
shot -X- _ O
learning -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
study -X- _ O
few -X- _ O
- -X- _ O
shot -X- _ O
learning -X- _ O
on -X- _ O
large -X- _ O
- -X- _ O
scale -X- _ O
11B -X- _ O
PLMs -X- _ O
. -X- _ O
Conclusion -X- _ O
and -X- _ O
Future -X- _ O
Work -X- _ O
. -X- _ O

There -X- _ O
is -X- _ O
also -X- _ O
work -X- _ O
( -X- _ O
IV -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
pointing -X- _ O
out -X- _ O
the -X- _ O
low -X- _ O
performance -X- _ O
of -X- _ O
PT -X- _ B-MethodName
for -X- _ O
few -X- _ O
- -X- _ O
shot -X- _ O
learning -X- _ O
. -X- _ O

Few -X- _ O
- -X- _ O
shot -X- _ O
Learning -X- _ O
with -X- _ O
PLMs -X- _ O
Since -X- _ O
long -X- _ O
- -X- _ O
tail -X- _ O
distribution -X- _ O
is -X- _ O
common -X- _ O
in -X- _ O
real -X- _ O
- -X- _ O
world -X- _ O
applications -X- _ O
, -X- _ O
few -X- _ O
- -X- _ O
shot -X- _ O
learning -X- _ O
is -X- _ O
quite -X- _ O
meaningful -X- _ O
for -X- _ O
the -X- _ O
stable -X- _ O
and -X- _ O
effective -X- _ O
use -X- _ O
of -X- _ O
PLMs -X- _ O
, -X- _ O
thereby -X- _ O
attracts -X- _ O
much -X- _ O
attention -X- _ O
recently -X- _ O
. -X- _ O

Few -X- _ O
- -X- _ O
shot -X- _ O
learning -X- _ O
is -X- _ O
notorious -X- _ O
for -X- _ O
its -X- _ O
instability -X- _ O
, -X- _ O
which -X- _ O
becomes -X- _ O
very -X- _ O
obvious -X- _ O
in -X- _ O
Vanilla -X- _ B-MethodName
PT -X- _ I-MethodName
. -X- _ O

In -X- _ O
this -X- _ O
section -X- _ O
, -X- _ O
we -X- _ O
present -X- _ O
pilot -X- _ O
experiments -X- _ O
of -X- _ O
PT -X- _ B-MethodName
for -X- _ O
few -X- _ O
- -X- _ O
shot -X- _ O
learning -X- _ O
. -X- _ O

Experiments -X- _ O
show -X- _ O
that -X- _ O
PPT -X- _ B-MethodName
can -X- _ O
not -X- _ O
only -X- _ O
improve -X- _ O
PT -X- _ B-MethodName
by -X- _ O
a -X- _ O
large -X- _ O
margin -X- _ O
, -X- _ O
reaching -X- _ O
or -X- _ O
even -X- _ O
outperforming -X- _ O
FT -X- _ B-MethodName
methods -X- _ O
, -X- _ O
but -X- _ O
also -X- _ O
reduce -X- _ O
the -X- _ O
variance -X- _ O
of -X- _ O
few -X- _ O
- -X- _ O
shot -X- _ O
learning -X- _ O
. -X- _ O

Hence -X- _ O
, -X- _ O
in -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
explore -X- _ O
how -X- _ O
to -X- _ O
use -X- _ O
PLMs -X- _ O
for -X- _ O
few -X- _ O
- -X- _ O
shot -X- _ O
learning -X- _ O
in -X- _ O
an -X- _ O
efficient -X- _ O
and -X- _ O
effective -X- _ O
manner -X- _ O
through -X- _ O
PT -X- _ B-MethodName
. -X- _ O

PPT -X- _ B-MethodName
: -X- _ O
Pre -X- _ B-MethodName
- -X- _ I-MethodName
trained -X- _ I-MethodName
Prompt -X- _ I-MethodName
Tuning -X- _ I-MethodName
for -X- _ O
Few -X- _ O
- -X- _ O
shot -X- _ O
Learning -X- _ O
. -X- _ O

Results -X- _ O
have -X- _ O
shown -X- _ O
that -X- _ O
task -X- _ B-MethodName
- -X- _ I-MethodName
oriented -X- _ I-MethodName
fine -X- _ I-MethodName
- -X- _ I-MethodName
tuning -X- _ I-MethodName
can -X- _ O
outperform -X- _ O
models -X- _ O
trained -X- _ O
from -X- _ O
scratch -X- _ O
on -X- _ O
a -X- _ O
series -X- _ O
of -X- _ O
NLP -X- _ O
tasks -X- _ O
. -X- _ O

Then -X- _ O
, -X- _ O
all -X- _ O
parameters -X- _ O
of -X- _ O
both -X- _ O
PLMs -X- _ O
and -X- _ O
additional -X- _ O
heads -X- _ O
are -X- _ O
tuned -X- _ O
using -X- _ O
task -X- _ O
- -X- _ O
specific -X- _ O
data -X- _ O
. -X- _ O

To -X- _ O
adapt -X- _ O
these -X- _ O
PLMs -X- _ O
to -X- _ O
downstream -X- _ O
NLP -X- _ O
tasks -X- _ O
, -X- _ O
task -X- _ B-TaskName
- -X- _ I-TaskName
oriented -X- _ I-TaskName
fine -X- _ I-TaskName
- -X- _ I-TaskName
tuning -X- _ I-TaskName
has -X- _ O
been -X- _ O
proposed -X- _ O
, -X- _ O
where -X- _ O
researchers -X- _ O
use -X- _ O
PLMs -X- _ O
as -X- _ O
the -X- _ O
backbone -X- _ O
and -X- _ O
add -X- _ O
some -X- _ O
task -X- _ O
- -X- _ O
specific -X- _ O
heads -X- _ O
to -X- _ O
optimize -X- _ O
task -X- _ O
- -X- _ O
specific -X- _ O
objectives -X- _ O
. -X- _ O

Given -X- _ O
the -X- _ O
input -X- _ O
x -X- _ O
= -X- _ O
( -X- _ O
s -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
have -X- _ O
: -X- _ O
Based -X- _ O
on -X- _ O
the -X- _ O
PVP -X- _ O
pre -X- _ O
i -X- _ O
, -X- _ O
the -X- _ O
design -X- _ O
of -X- _ O
PVP -X- _ O
k -X- _ O
i -X- _ O
is -X- _ O
similar -X- _ O
to -X- _ O
that -X- _ O
of -X- _ O
English -X- _ O
tasks -X- _ O
. -X- _ O

Sentence -X- _ B-TaskName
- -X- _ I-TaskName
Pair -X- _ I-TaskName
Classification -X- _ I-TaskName
Given -X- _ O
the -X- _ O
input -X- _ O
x -X- _ O
= -X- _ O
( -X- _ O
s -X- _ O
1 -X- _ O
, -X- _ O
s -X- _ O
2 -X- _ O
) -X- _ O
, -X- _ O
the -X- _ O
label -X- _ O
list -X- _ O
Y -X- _ O
= -X- _ O
[ -X- _ O
0 -X- _ O
, -X- _ O
1 -X- _ O
, -X- _ O
2 -X- _ O
] -X- _ O
, -X- _ O
we -X- _ O
have -X- _ O
: -X- _ O
Multiple -X- _ B-TaskName
- -X- _ I-TaskName
Choice -X- _ I-TaskName
Classification -X- _ I-TaskName
Given -X- _ O
a -X- _ O
input -X- _ O
x -X- _ O
consisting -X- _ O
of -X- _ O
a -X- _ O
query -X- _ O
and -X- _ O
six -X- _ O
candidates -X- _ O
: -X- _ O
x -X- _ O
= -X- _ O
( -X- _ O
s -X- _ O
q -X- _ O
, -X- _ O
s -X- _ O
1 -X- _ O
, -X- _ O
s -X- _ O
2 -X- _ O
, -X- _ O
• -X- _ O
• -X- _ O
• -X- _ O
, -X- _ O
s -X- _ O
6 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
convert -X- _ O
x -X- _ O
to -X- _ O
a -X- _ O
language -X- _ O
sequence -X- _ O
by -X- _ O
defining -X- _ O
the -X- _ O
PVP -X- _ O
pre -X- _ O
i -X- _ O
as -X- _ O
follows -X- _ O
: -X- _ O
Single -X- _ B-TaskName
- -X- _ I-TaskName
Sentence -X- _ I-TaskName
Classification -X- _ I-TaskName
Similar -X- _ O
to -X- _ O
the -X- _ O
English -X- _ O
scenario -X- _ O
, -X- _ O
we -X- _ O
take -X- _ O
sentiment -X- _ O
classification -X- _ O
as -X- _ O
an -X- _ O
example -X- _ O
. -X- _ O

Just -X- _ O
like -X- _ O
English -X- _ O
scenarios -X- _ O
, -X- _ O
all -X- _ O
these -X- _ O
PVPs -X- _ O
are -X- _ O
simple -X- _ O
and -X- _ O
intuitive -X- _ O
. -X- _ O

We -X- _ O
describe -X- _ O
the -X- _ O
PVP -X- _ O
pre -X- _ O
i -X- _ O
for -X- _ O
Chinese -X- _ O
datasets -X- _ O
in -X- _ O
this -X- _ O
section -X- _ O
. -X- _ O

B -X- _ O
PVPs -X- _ O
for -X- _ O
Chinese -X- _ O
Tasks -X- _ O
. -X- _ O

For -X- _ O
Chinese -X- _ O
experiments -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
four -X- _ O
datasets -X- _ O
from -X- _ O
CLUE -X- _ B-DatasetName
( -X- _ O
Xu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
( -X- _ O
CMNLI -X- _ B-DatasetName
3 -X- _ I-DatasetName
, -X- _ O
OCNLI -X- _ B-DatasetName
( -X- _ O
Hu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
, -X- _ O
TNews -X- _ B-DatasetName
3 -X- _ I-DatasetName
, -X- _ O
C -X- _ B-DatasetName
3 -X- _ I-DatasetName
( -X- _ O
Sun -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
) -X- _ O
, -X- _ O
two -X- _ O
sentiment -X- _ B-TaskName
analysis -X- _ I-TaskName
datasets -X- _ O
( -X- _ O
ChnSent -X- _ B-DatasetName
4 -X- _ I-DatasetName
and -X- _ O
Amazon -X- _ B-DatasetName
Reviews -X- _ I-DatasetName
4 -X- _ I-DatasetName
) -X- _ O
, -X- _ O
and -X- _ O
one -X- _ O
extra -X- _ O
natural -X- _ B-TaskName
language -X- _ I-TaskName
inference -X- _ I-TaskName
dataset -X- _ O
LCQMC -X- _ B-DatasetName
( -X- _ O
Liu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
. -X- _ O

( -X- _ O
2021 -X- _ O
) -X- _ O
to -X- _ O
use -X- _ O
original -X- _ O
validation -X- _ O
sets -X- _ O
for -X- _ O
testing -X- _ O
. -X- _ O

( -X- _ O
2021 -X- _ O
) -X- _ O
and -X- _ O
Gao -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

Since -X- _ O
some -X- _ O
of -X- _ O
the -X- _ O
test -X- _ O
sets -X- _ O
of -X- _ O
the -X- _ O
datasets -X- _ O
we -X- _ O
used -X- _ O
is -X- _ O
not -X- _ O
publicly -X- _ O
available -X- _ O
, -X- _ O
we -X- _ O
follow -X- _ O
Zhang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

A -X- _ O
Dataset -X- _ O
Information -X- _ O
. -X- _ O

Appendices -X- _ O
. -X- _ O

This -X- _ O
work -X- _ O
was -X- _ O
also -X- _ O
supported -X- _ O
by -X- _ O
the -X- _ O
Guoqiang -X- _ O
Institute -X- _ O
of -X- _ O
Tsinghua -X- _ O
University -X- _ O
, -X- _ O
with -X- _ O
Grant -X- _ O
No -X- _ O
. -X- _ O
2019GQG1 -X- _ O
and -X- _ O
2020GQG0005 -X- _ O
. -X- _ O

This -X- _ O
work -X- _ O
was -X- _ O
supported -X- _ O
by -X- _ O
the -X- _ O
National -X- _ O
Science -X- _ O
Foundation -X- _ O
for -X- _ O
Distinguished -X- _ O
Young -X- _ O
Scholars -X- _ O
( -X- _ O
with -X- _ O
No -X- _ O
. -X- _ O
62125604 -X- _ O
) -X- _ O
and -X- _ O
the -X- _ O
NSFC -X- _ O
projects -X- _ O
( -X- _ O
Key -X- _ O
project -X- _ O
with -X- _ O
No -X- _ O
. -X- _ O
61936010 -X- _ O
and -X- _ O
regular -X- _ O
project -X- _ O
with -X- _ O
No -X- _ O
. -X- _ O
61876096 -X- _ O
) -X- _ O
. -X- _ O

Acknowledgements -X- _ O
. -X- _ O

E -X- _ O
Training -X- _ O
Consumption -X- _ O
. -X- _ O

The -X- _ O
hard -X- _ O
prompts -X- _ O
corresponding -X- _ O
to -X- _ O
each -X- _ O
task -X- _ O
format -X- _ O
are -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
7 -X- _ O
. -X- _ O

For -X- _ O
simplicity -X- _ O
, -X- _ O
we -X- _ O
choose -X- _ O
the -X- _ O
best -X- _ O
hard -X- _ O
prompts -X- _ O
for -X- _ O
each -X- _ O
task -X- _ O
format -X- _ O
( -X- _ O
e.g. -X- _ O
sentence -X- _ B-TaskName
- -X- _ I-TaskName
pair -X- _ I-TaskName
classification -X- _ I-TaskName
, -X- _ O
multiple -X- _ B-TaskName
- -X- _ I-TaskName
choice -X- _ I-TaskName
classification -X- _ I-TaskName
, -X- _ O
and -X- _ O
single -X- _ O
- -X- _ O
sentence -X- _ O
classification -X- _ O
) -X- _ O
based -X- _ O
on -X- _ O
PT -X- _ B-MethodName
in -X- _ O
pilot -X- _ O
experiments -X- _ O
and -X- _ O
directly -X- _ O
use -X- _ O
them -X- _ O
in -X- _ O
Hybrid -X- _ B-MethodName
PPT -X- _ I-MethodName
. -X- _ O

In -X- _ O
this -X- _ O
section -X- _ O
, -X- _ O
we -X- _ O
describe -X- _ O
the -X- _ O
hard -X- _ O
prompts -X- _ O
we -X- _ O
use -X- _ O
in -X- _ O
Hybrid -X- _ B-MethodName
PT -X- _ I-MethodName
and -X- _ O
Hybrid -X- _ B-MethodName
PPT -X- _ I-MethodName
. -X- _ O

D -X- _ O
Hard -X- _ O
Prompts -X- _ O
. -X- _ O

The -X- _ O
thresholds -X- _ O
of -X- _ O
the -X- _ O
label -X- _ O
0 -X- _ O
∼ -X- _ O
4 -X- _ O
are -X- _ O
[ -X- _ O
0.95 -X- _ O
, -X- _ O
0.50 -X- _ O
, -X- _ O
0.50 -X- _ O
, -X- _ O
0.50 -X- _ O
, -X- _ O
0.70 -X- _ O
] -X- _ O
. -X- _ O

We -X- _ O
set -X- _ O
different -X- _ O
minimal -X- _ O
classification -X- _ O
confidence -X- _ O
thresholds -X- _ O
for -X- _ O
the -X- _ O
5 -X- _ O
labels -X- _ O
to -X- _ O
control -X- _ O
annotation -X- _ O
quality -X- _ O
and -X- _ O
balance -X- _ O
the -X- _ O
label -X- _ O
. -X- _ O

We -X- _ O
choose -X- _ O
the -X- _ O
checkpoint -X- _ O
with -X- _ O
the -X- _ O
highest -X- _ O
accuracy -X- _ O
on -X- _ O
the -X- _ O
validation -X- _ O
set -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
70.53 -X- _ O
at -X- _ O
the -X- _ O
5 -X- _ O
- -X- _ O
th -X- _ O
epoch -X- _ O
, -X- _ O
to -X- _ O
annotate -X- _ O
the -X- _ O
label -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
learning -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
1e-4 -X- _ B-HyperparameterValue
, -X- _ O
batch -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
16 -X- _ B-HyperparameterValue
, -X- _ O
warm -X- _ B-HyperparameterName
- -X- _ I-HyperparameterName
up -X- _ I-HyperparameterName
rate -X- _ I-HyperparameterName
0.01 -X- _ B-HyperparameterValue
, -X- _ O
and -X- _ O
train -X- _ O
the -X- _ O
model -X- _ O
for -X- _ O
10 -X- _ B-HyperparameterValue
epochs -X- _ B-HyperparameterName
. -X- _ O

Single -X- _ B-TaskName
- -X- _ I-TaskName
Sentence -X- _ I-TaskName
Classification -X- _ I-TaskName
We -X- _ O
use -X- _ O
the -X- _ O
RoBERTa -X- _ B-MethodName
BASE -X- _ I-MethodName
model -X- _ O
trained -X- _ O
on -X- _ O
the -X- _ O
Yelp-5 -X- _ B-DatasetName
dataset -X- _ O
to -X- _ O
annotate -X- _ O
pseudo -X- _ O
labels -X- _ O
on -X- _ O
the -X- _ O
unlabeled -X- _ O
data -X- _ O
. -X- _ O

The -X- _ O
input -X- _ O
configurations -X- _ O
of -X- _ O
different -X- _ O
option -X- _ O
numbers -X- _ O
is -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
9 -X- _ O
. -X- _ O

To -X- _ O
fit -X- _ O
in -X- _ O
the -X- _ O
max -X- _ O
input -X- _ O
length -X- _ O
, -X- _ O
we -X- _ O
truncate -X- _ O
the -X- _ O
query -X- _ O
sentence -X- _ O
to -X- _ O
389 -X- _ O
tokens -X- _ O
and -X- _ O
the -X- _ O
options -X- _ O
to -X- _ O
86 -X- _ O
tokens -X- _ O
. -X- _ O

We -X- _ O
also -X- _ O
filter -X- _ O
out -X- _ O
the -X- _ O
sentences -X- _ O
with -X- _ O
less -X- _ O
than -X- _ O
5 -X- _ O
tokens -X- _ O
. -X- _ O

Sentence -X- _ B-TaskName
- -X- _ I-TaskName
Pair -X- _ I-TaskName
Classification -X- _ I-TaskName
In -X- _ O
the -X- _ O
next -X- _ O
sentence -X- _ O
prediction -X- _ O
task -X- _ O
, -X- _ O
we -X- _ O
set -X- _ O
the -X- _ O
two -X- _ O
sentences -X- _ O
next -X- _ O
to -X- _ O
each -X- _ O
other -X- _ O
as -X- _ O
label -X- _ O
2 -X- _ O
, -X- _ O
those -X- _ O
from -X- _ O
the -X- _ O
same -X- _ O
document -X- _ O
but -X- _ O
not -X- _ O
true -X- _ O
next -X- _ O
sentence -X- _ O
as -X- _ O
1 -X- _ O
, -X- _ O
and -X- _ O
those -X- _ O
from -X- _ O
different -X- _ O
documents -X- _ O
as -X- _ O
0 -X- _ O
. -X- _ O

The -X- _ O
details -X- _ O
of -X- _ O
constructing -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
data -X- _ O
for -X- _ O
each -X- _ O
task -X- _ O
are -X- _ O
as -X- _ O
follows -X- _ O
. -X- _ O

We -X- _ O
evaluate -X- _ O
the -X- _ O
performance -X- _ O
on -X- _ O
the -X- _ O
validation -X- _ O
set -X- _ O
every -X- _ O
2,000 -X- _ O
steps -X- _ O
and -X- _ O
choose -X- _ O
the -X- _ O
prompt -X- _ O
with -X- _ O
the -X- _ O
lowest -X- _ O
validation -X- _ O
loss -X- _ O
. -X- _ O

We -X- _ O
split -X- _ B-HyperparameterName
5 -X- _ B-HyperparameterValue
% -X- _ I-HyperparameterValue
data -X- _ O
for -X- _ O
validation -X- _ O
and -X- _ O
the -X- _ O
rest -X- _ O
for -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
. -X- _ O

We -X- _ O
set -X- _ O
the -X- _ O
batch -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
as -X- _ O
256 -X- _ B-HyperparameterValue
, -X- _ O
the -X- _ O
max -X- _ B-HyperparameterName
input -X- _ I-HyperparameterName
length -X- _ I-HyperparameterName
as -X- _ O
512 -X- _ B-HyperparameterValue
, -X- _ O
and -X- _ O
train -X- _ O
the -X- _ O
prompts -X- _ O
for -X- _ O
at -X- _ O
most -X- _ O
200,000 -X- _ B-HyperparameterValue
steps -X- _ B-HyperparameterName
. -X- _ O

Across -X- _ O
all -X- _ O
tasks -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
" -X- _ O
inverse -X- _ O
square -X- _ O
root -X- _ O
" -X- _ O
learning -X- _ O
rate -X- _ O
scheduler -X- _ O
( -X- _ O
Raffel -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
and -X- _ O
set -X- _ O
the -X- _ O
learning -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
in -X- _ O
this -X- _ O
scheduler -X- _ O
as -X- _ O
0.1 -X- _ B-HyperparameterValue
with -X- _ O
no -X- _ O
warmup -X- _ O
steps -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
the -X- _ O
sampled -X- _ O
10 -X- _ O
GB -X- _ O
data -X- _ O
to -X- _ O
construct -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
data -X- _ O
for -X- _ O
each -X- _ O
task -X- _ O
format -X- _ O
for -X- _ O
prompt -X- _ B-MethodName
pre -X- _ I-MethodName
- -X- _ I-MethodName
training -X- _ I-MethodName
. -X- _ O

C.3 -X- _ O
Prompt -X- _ O
Pre -X- _ O
- -X- _ O
Training -X- _ O
. -X- _ O

Therefore -X- _ O
, -X- _ O
we -X- _ O
search -X- _ O
for -X- _ O
the -X- _ O
learning -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
in -X- _ O
[ -X- _ O
5e-3 -X- _ B-HyperparameterValue
, -X- _ O
1e-2 -X- _ B-HyperparameterValue
, -X- _ O
2e-2 -X- _ B-HyperparameterValue
, -X- _ O
5e-2 -X- _ B-HyperparameterValue
] -X- _ O
and -X- _ O
choose -X- _ O
the -X- _ O
model -X- _ O
with -X- _ O
the -X- _ O
best -X- _ O
performance -X- _ O
on -X- _ O
the -X- _ O
validation -X- _ O
set -X- _ O
. -X- _ O

steps -X- _ O
. -X- _ O

Generally -X- _ O
, -X- _ O
small -X- _ O
models -X- _ O
prefer -X- _ O
large -X- _ O
learning -X- _ B-HyperparameterName
rates -X- _ I-HyperparameterName
. -X- _ O

Similar -X- _ O
to -X- _ O
FT -X- _ B-MethodName
, -X- _ O
we -X- _ O
fix -X- _ O
the -X- _ O
batch -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
as -X- _ O
16 -X- _ B-HyperparameterValue
and -X- _ O
train -X- _ O
the -X- _ O
model -X- _ O
for -X- _ O
50 -X- _ B-HyperparameterValue
epochs -X- _ B-HyperparameterName
, -X- _ O
while -X- _ O
evaluating -X- _ O
the -X- _ O
model -X- _ O
every -X- _ O
6 -X- _ O
Model -X- _ O
Size -X- _ O
Searching -X- _ O
Interval -X- _ O
Small -X- _ O
2e-4 -X- _ B-HyperparameterValue
, -X- _ O
5e-4 -X- _ B-HyperparameterValue
, -X- _ O
1e-3 -X- _ B-HyperparameterValue
Base -X- _ O
2e-4 -X- _ B-HyperparameterValue
, -X- _ O
5e-4 -X- _ B-HyperparameterValue
, -X- _ O
1e-3 -X- _ B-HyperparameterValue
Large -X- _ O
5e-5 -X- _ B-HyperparameterValue
, -X- _ O
1e-4 -X- _ B-HyperparameterValue
, -X- _ O
2e-4 -X- _ B-HyperparameterValue
XL -X- _ O
3e-5 -X- _ B-HyperparameterValue
, -X- _ O
5e-5 -X- _ B-HyperparameterValue
, -X- _ O
1e-4 -X- _ B-HyperparameterValue
XXL -X- _ O
3e-6 -X- _ B-HyperparameterValue
, -X- _ O
5e-6 -X- _ B-HyperparameterValue
, -X- _ O
1e-5 -X- _ B-HyperparameterValue
Table -X- _ O
8 -X- _ O
: -X- _ O
The -X- _ O
searching -X- _ O
intervals -X- _ O
of -X- _ O
learning -X- _ B-HyperparameterName
rates -X- _ I-HyperparameterName
for -X- _ O
the -X- _ O
models -X- _ O
with -X- _ O
different -X- _ O
sizes -X- _ O
. -X- _ O

When -X- _ O
adapting -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
downstream -X- _ O
tasks -X- _ O
, -X- _ O
we -X- _ O
only -X- _ O
tune -X- _ O
the -X- _ O
soft -X- _ O
prompts -X- _ O
with -X- _ O
the -X- _ O
entire -X- _ O
model -X- _ O
fixed -X- _ O
. -X- _ O

We -X- _ O
choose -X- _ O
the -X- _ O
model -X- _ O
performing -X- _ O
the -X- _ O
best -X- _ O
on -X- _ O
the -X- _ O
validation -X- _ O
set -X- _ O
and -X- _ O
evaluate -X- _ O
it -X- _ O
on -X- _ O
the -X- _ O
test -X- _ O
set -X- _ O
. -X- _ O

We -X- _ O
train -X- _ O
the -X- _ O
model -X- _ O
for -X- _ O
50 -X- _ B-HyperparameterValue
epochs -X- _ B-HyperparameterName
and -X- _ O
do -X- _ O
evaluation -X- _ O
every -X- _ O
6 -X- _ O
optimization -X- _ O
steps -X- _ O
. -X- _ O

Therefore -X- _ O
, -X- _ O
we -X- _ O
search -X- _ O
for -X- _ O
the -X- _ O
learning -X- _ B-HyperparameterName
rates -X- _ I-HyperparameterName
in -X- _ O
varied -X- _ O
intervals -X- _ O
and -X- _ O
show -X- _ O
each -X- _ O
model -X- _ O
size -X- _ O
and -X- _ O
its -X- _ O
corresponding -X- _ O
searching -X- _ O
interval -X- _ O
in -X- _ O
Table -X- _ O
8 -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
way -X- _ O
, -X- _ O
we -X- _ O
train -X- _ O
the -X- _ O
largest -X- _ O
11B -X- _ O
model -X- _ O
with -X- _ O
16 -X- _ O
NVIDIA -X- _ O
V100 -X- _ O
32 -X- _ O
G -X- _ O
GPUs -X- _ O
. -X- _ O
We -X- _ O
find -X- _ O
that -X- _ O
different -X- _ O
sized -X- _ O
models -X- _ O
prefer -X- _ O
significantly -X- _ O
different -X- _ O
learning -X- _ B-HyperparameterName
rates -X- _ I-HyperparameterName
. -X- _ O

For -X- _ O
all -X- _ O
models -X- _ O
, -X- _ O
we -X- _ O
fix -X- _ O
the -X- _ O
batch -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
as -X- _ O
16 -X- _ B-HyperparameterValue
. -X- _ O

We -X- _ O
describe -X- _ O
the -X- _ O
details -X- _ O
of -X- _ O
the -X- _ O
training -X- _ O
hyper -X- _ O
- -X- _ O
parameters -X- _ O
in -X- _ O
the -X- _ O
following -X- _ O
sections -X- _ O
. -X- _ O

For -X- _ O
models -X- _ O
in -X- _ O
other -X- _ O
sizes -X- _ O
, -X- _ O
we -X- _ O
all -X- _ O
use -X- _ O
full -X- _ O
- -X- _ O
precision -X- _ O
training -X- _ O
. -X- _ O

We -X- _ O
also -X- _ O
use -X- _ O
mixedprecision -X- _ O
training -X- _ O
( -X- _ O
Micikevicius -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
and -X- _ O
ZeRO -X- _ O
( -X- _ O
Rajbhandari -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
stage-1 -X- _ O
provided -X- _ O
in -X- _ O
DeepSpeed -X- _ O
( -X- _ O
Rasley -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
to -X- _ O
reduce -X- _ O
GPU -X- _ O
memory -X- _ O
usage -X- _ O
. -X- _ O

Due -X- _ O
to -X- _ O
the -X- _ O
resource -X- _ O
limit -X- _ O
, -X- _ O
for -X- _ O
11B -X- _ O
models -X- _ O
, -X- _ O
we -X- _ O
adopt -X- _ O
model -X- _ O
parallelism -X- _ O
( -X- _ O
Shoeybi -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
and -X- _ O
store -X- _ O
a -X- _ O
model -X- _ O
with -X- _ O
4 -X- _ O
GPU -X- _ O
devices -X- _ O
. -X- _ O

C -X- _ O
Training -X- _ O
Details -X- _ O
. -X- _ O

( -X- _ O
3 -X- _ O
) -X- _ O
Beyond -X- _ O
the -X- _ O
soft -X- _ O
prompt -X- _ O
, -X- _ O
studying -X- _ O
whether -X- _ O
unified -X- _ O
task -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
helps -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
language -X- _ O
models -X- _ O
itself -X- _ O
. -X- _ O

( -X- _ O
2 -X- _ O
) -X- _ O
Evaluating -X- _ O
the -X- _ O
few -X- _ O
- -X- _ O
shot -X- _ O
performance -X- _ O
of -X- _ O
other -X- _ O
parameter -X- _ B-MethodName
- -X- _ I-MethodName
efficient -X- _ I-MethodName
tuning -X- _ I-MethodName
approaches -X- _ O
( -X- _ O
He -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
and -X- _ O
adapting -X- _ O
unified -X- _ O
task -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
to -X- _ O
them -X- _ O
. -X- _ O

There -X- _ O
are -X- _ O
three -X- _ O
important -X- _ O
directions -X- _ O
for -X- _ O
future -X- _ O
work -X- _ O
: -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
Designing -X- _ O
unified -X- _ O
task -X- _ O
formats -X- _ O
and -X- _ O
the -X- _ O
corresponding -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
objectives -X- _ O
for -X- _ O
other -X- _ O
kinds -X- _ O
of -X- _ O
tasks -X- _ O
such -X- _ O
as -X- _ O
language -X- _ O
generation -X- _ O
and -X- _ O
relation -X- _ O
extraction -X- _ O
. -X- _ O

Then -X- _ O
, -X- _ O
we -X- _ O
design -X- _ O
self -X- _ O
- -X- _ O
supervised -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
tasks -X- _ O
for -X- _ O
each -X- _ O
format -X- _ O
and -X- _ O
pre -X- _ O
- -X- _ O
train -X- _ O
prompts -X- _ O
on -X- _ O
these -X- _ O
tasks -X- _ O
. -X- _ O

We -X- _ O
propose -X- _ O
to -X- _ O
firstly -X- _ O
unify -X- _ O
downstream -X- _ O
tasks -X- _ O
to -X- _ O
several -X- _ O
formats -X- _ O
. -X- _ O

But -X- _ O
they -X- _ O
mostly -X- _ O
focus -X- _ O
on -X- _ O
PLMs -X- _ O
with -X- _ O
fewer -X- _ O
than -X- _ O
400 -X- _ O
M -X- _ O
parameters -X- _ O
. -X- _ O

( -X- _ O
2021 -X- _ O
) -X- _ O
also -X- _ O
discuss -X- _ O
reasonable -X- _ O
fewshot -X- _ O
settings -X- _ O
by -X- _ O
restricting -X- _ O
the -X- _ O
size -X- _ O
of -X- _ O
validation -X- _ O
set -X- _ O
and -X- _ O
proposing -X- _ O
a -X- _ O
unified -X- _ O
framework -X- _ O
to -X- _ O
evaluate -X- _ O
few -X- _ O
- -X- _ O
shot -X- _ O
performance -X- _ O
. -X- _ O

( -X- _ O
2021 -X- _ O
) -X- _ O
; -X- _ O
Bragg -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

Apart -X- _ O
from -X- _ O
GPT-3 -X- _ B-MethodName
( -X- _ O
Brown -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
and -X- _ O
PET -X- _ B-MethodName
( -X- _ O
Schick -X- _ O
and -X- _ O
Schütze -X- _ O
, -X- _ O
2021a -X- _ O
) -X- _ O
which -X- _ O
demonstrates -X- _ O
the -X- _ O
superiority -X- _ O
of -X- _ O
PLMs -X- _ O
in -X- _ O
few -X- _ O
- -X- _ O
shot -X- _ O
scenarios -X- _ O
, -X- _ O
some -X- _ O
later -X- _ O
works -X- _ O
Perez -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

To -X- _ O
step -X- _ O
forward -X- _ O
, -X- _ O
some -X- _ O
works -X- _ O
( -X- _ O
Li -X- _ O
and -X- _ O
Liang -X- _ O
, -X- _ O
2021;Qin -X- _ O
and -X- _ O
Eisner -X- _ O
, -X- _ O
2021;Lester -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
propose -X- _ O
to -X- _ O
only -X- _ O
tune -X- _ O
soft -X- _ O
prompts -X- _ O
and -X- _ O
fix -X- _ O
the -X- _ O
entire -X- _ O
PLM -X- _ O
parameters -X- _ O
. -X- _ O

Different -X- _ O
from -X- _ O
hard -X- _ O
prompts -X- _ O
using -X- _ O
concrete -X- _ O
and -X- _ O
discrete -X- _ O
tokens -X- _ O
, -X- _ O
soft -X- _ O
prompts -X- _ O
are -X- _ O
composed -X- _ O
of -X- _ O
several -X- _ O
continuous -X- _ O
learnable -X- _ O
embeddings -X- _ O
, -X- _ O
and -X- _ O
these -X- _ O
embeddings -X- _ O
are -X- _ O
randomly -X- _ O
initialized -X- _ O
. -X- _ O

( -X- _ O
2021b -X- _ O
) -X- _ O
explore -X- _ O
to -X- _ O
combine -X- _ O
hard -X- _ O
prompts -X- _ O
and -X- _ O
soft -X- _ O
prompts -X- _ O
. -X- _ O

( -X- _ O
2021 -X- _ O
) -X- _ O
; -X- _ O
Zhong -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

( -X- _ O
2021b -X- _ O
) -X- _ O
; -X- _ O
Hambardzumyan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

( -X- _ O
2021 -X- _ O
) -X- _ O
; -X- _ O
Han -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

To -X- _ O
overcome -X- _ O
the -X- _ O
shortcomings -X- _ O
of -X- _ O
discrete -X- _ O
spaces -X- _ O
, -X- _ O
Li -X- _ O
and -X- _ O
Liang -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
; -X- _ O
Liu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
these -X- _ O
works -X- _ O
still -X- _ O
restrict -X- _ O
auto -X- _ O
- -X- _ O
generated -X- _ O
prompts -X- _ O
to -X- _ O
discrete -X- _ O
spaces -X- _ O
which -X- _ O
are -X- _ O
usually -X- _ O
sub -X- _ O
- -X- _ O
optimal -X- _ O
. -X- _ O

Considering -X- _ O
manually -X- _ O
designing -X- _ O
prompts -X- _ O
is -X- _ O
both -X- _ O
time -X- _ O
- -X- _ O
consuming -X- _ O
and -X- _ O
difficult -X- _ O
to -X- _ O
find -X- _ O
the -X- _ O
best -X- _ O
choice -X- _ O
, -X- _ O
later -X- _ O
works -X- _ O
( -X- _ O
Gao -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021;Jiang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020;Shin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
proposed -X- _ O
to -X- _ O
generate -X- _ O
prompts -X- _ O
automatically -X- _ O
. -X- _ O

These -X- _ O
pioneering -X- _ O
works -X- _ O
demonstrate -X- _ O
that -X- _ O
language -X- _ O
prompts -X- _ O
can -X- _ O
effectively -X- _ O
stimulate -X- _ O
the -X- _ O
knowledge -X- _ O
from -X- _ O
PLMs -X- _ O
. -X- _ O
Encouraged -X- _ O
by -X- _ O
this -X- _ O
, -X- _ O
manually -X- _ O
designing -X- _ O
hard -X- _ O
prompts -X- _ O
consisting -X- _ O
of -X- _ O
discrete -X- _ O
words -X- _ O
is -X- _ O
first -X- _ O
used -X- _ O
in -X- _ O
prompt -X- _ B-MethodName
- -X- _ I-MethodName
oriented -X- _ I-MethodName
fine -X- _ I-MethodName
- -X- _ I-MethodName
tuning -X- _ I-MethodName
Schick -X- _ O
and -X- _ O
Schütze -X- _ O
( -X- _ O
2021a -X- _ O
, -X- _ O
b -X- _ O
) -X- _ O
. -X- _ O

In -X- _ O
knowledge -X- _ O
probing -X- _ O
, -X- _ O
language -X- _ O
triggers -X- _ O
are -X- _ O
widely -X- _ O
used -X- _ O
to -X- _ O
induce -X- _ O
PLMs -X- _ O
to -X- _ O
generate -X- _ O
relational -X- _ O
facts -X- _ O
. -X- _ O

Knowledge -X- _ O
probing -X- _ O
( -X- _ O
Petroni -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019;Trinh -X- _ O
and -X- _ O
Le -X- _ O
, -X- _ O
2018;Davison -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
is -X- _ O
the -X- _ O
seminal -X- _ O
work -X- _ O
that -X- _ O
stimulates -X- _ O
the -X- _ O
development -X- _ O
of -X- _ O
prompts -X- _ O
. -X- _ O

In -X- _ O
prompt -X- _ B-MethodName
- -X- _ I-MethodName
oriented -X- _ I-MethodName
finetuning -X- _ I-MethodName
, -X- _ O
downstream -X- _ O
tasks -X- _ O
are -X- _ O
also -X- _ O
formalized -X- _ O
as -X- _ O
language -X- _ O
modeling -X- _ O
problems -X- _ O
by -X- _ O
inserting -X- _ O
language -X- _ O
prompts -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
results -X- _ O
of -X- _ O
language -X- _ O
modeling -X- _ O
can -X- _ O
correspond -X- _ O
to -X- _ O
the -X- _ O
solutions -X- _ O
of -X- _ O
downstream -X- _ O
tasks -X- _ O
. -X- _ O

To -X- _ O
overcome -X- _ O
the -X- _ O
gap -X- _ O
between -X- _ O
pretraining -X- _ O
and -X- _ O
downstream -X- _ O
tasks -X- _ O
, -X- _ O
prompt -X- _ B-MethodName
- -X- _ I-MethodName
oriented -X- _ I-MethodName
fine -X- _ I-MethodName
- -X- _ I-MethodName
tuning -X- _ I-MethodName
is -X- _ O
introduced -X- _ O
. -X- _ O

Prompt -X- _ B-MethodName
- -X- _ I-MethodName
oriented -X- _ I-MethodName
Fine -X- _ I-MethodName
- -X- _ I-MethodName
tuning -X- _ I-MethodName
Most -X- _ O
existing -X- _ O
PLMs -X- _ O
are -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
with -X- _ O
language -X- _ O
modeling -X- _ O
objectives -X- _ O
, -X- _ O
yet -X- _ O
the -X- _ O
objectives -X- _ O
of -X- _ O
downstream -X- _ O
tasks -X- _ O
are -X- _ O
quite -X- _ O
different -X- _ O
. -X- _ O

PLMs -X- _ O
and -X- _ O
Task -X- _ O
- -X- _ O
oriented -X- _ O
Fine -X- _ O
- -X- _ O
tuning -X- _ O
Recently -X- _ O
, -X- _ O
various -X- _ O
powerful -X- _ O
PLMs -X- _ O
have -X- _ O
been -X- _ O
proposed -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
GPT -X- _ B-MethodName
( -X- _ O
Radford -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
, -X- _ O
BERT -X- _ B-MethodName
( -X- _ O
Devlin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
RoBERTa -X- _ B-MethodName
( -X- _ O
Liu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
and -X- _ O
T5 -X- _ B-MethodName
( -X- _ O
Raffel -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O

Since -X- _ O
PPT -X- _ B-MethodName
still -X- _ O
converges -X- _ O
a -X- _ O
bit -X- _ O
slower -X- _ O
than -X- _ O
FT -X- _ B-MethodName
, -X- _ O
how -X- _ O
to -X- _ O
further -X- _ O
accelerate -X- _ O
the -X- _ O
convergence -X- _ O
of -X- _ O
PT -X- _ B-MethodName
is -X- _ O
worth -X- _ O
studying -X- _ O
in -X- _ O
future -X- _ O
work -X- _ O
. -X- _ O

Related -X- _ O
Works -X- _ O
. -X- _ O

We -X- _ O
give -X- _ O
a -X- _ O
more -X- _ O
detailed -X- _ O
analysis -X- _ O
of -X- _ O
the -X- _ O
training -X- _ O
consumption -X- _ O
in -X- _ O
the -X- _ O
Appendix -X- _ O
E. -X- _ O

As -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
5 -X- _ O
, -X- _ O
with -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
initialization -X- _ O
, -X- _ O
PPT -X- _ B-MethodName
speeds -X- _ O
up -X- _ O
the -X- _ O
convergence -X- _ O
of -X- _ O
Vanilla -X- _ B-MethodName
PT -X- _ I-MethodName
on -X- _ O
both -X- _ O
RACE -X- _ B-DatasetName
- -X- _ I-DatasetName
m -X- _ I-DatasetName
and -X- _ O
CB -X- _ B-DatasetName
datasets -X- _ O
. -X- _ O

We -X- _ O
also -X- _ O
compare -X- _ O
different -X- _ O
tuning -X- _ O
approaches -X- _ O
given -X- _ O
the -X- _ O
full -X- _ O
training -X- _ O
data -X- _ O
. -X- _ O

For -X- _ O
32 -X- _ B-HyperparameterValue
to -X- _ O
128 -X- _ B-HyperparameterValue
samples -X- _ B-HyperparameterName
, -X- _ O
PPT -X- _ B-MethodName
is -X- _ O
consistently -X- _ O
better -X- _ O
than -X- _ O
PT -X- _ B-MethodName
, -X- _ O
and -X- _ O
the -X- _ O
performances -X- _ O
of -X- _ O
the -X- _ O
three -X- _ O
methods -X- _ O
gradually -X- _ O
converge -X- _ O
when -X- _ O
the -X- _ O
number -X- _ O
grows -X- _ O
to -X- _ O
256 -X- _ B-HyperparameterValue
. -X- _ O

In -X- _ O
Figure -X- _ O
4 -X- _ O
, -X- _ O
we -X- _ O
show -X- _ O
the -X- _ O
trend -X- _ O
of -X- _ O
these -X- _ O
methods -X- _ O
on -X- _ O
the -X- _ O
RACE -X- _ B-DatasetName
- -X- _ I-DatasetName
m -X- _ I-DatasetName
and -X- _ O
CB -X- _ B-DatasetName
datasets -X- _ O
. -X- _ O

Sample -X- _ O
Efficiency -X- _ O
. -X- _ O

We -X- _ O
report -X- _ O
the -X- _ O
mean -X- _ O
and -X- _ O
the -X- _ O
standard -X- _ O
deviation -X- _ O
over -X- _ O
3 -X- _ O
random -X- _ O
seeds -X- _ O
on -X- _ O
the -X- _ O
validation -X- _ O
set -X- _ O
. -X- _ O

When -X- _ O
the -X- _ O
number -X- _ O
grows -X- _ O
, -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
these -X- _ O
methods -X- _ O
becomes -X- _ O
closer -X- _ O
. -X- _ O

Based -X- _ O
on -X- _ O
this -X- _ O
observation -X- _ O
, -X- _ O
an -X- _ O
intuitive -X- _ O
extension -X- _ O
of -X- _ O
our -X- _ O
method -X- _ O
is -X- _ O
to -X- _ O
further -X- _ O
pre -X- _ O
- -X- _ O
train -X- _ O
the -X- _ O
entire -X- _ O
model -X- _ O
with -X- _ O
PVP -X- _ O
pre -X- _ O
i -X- _ O
and -X- _ O
fine -X- _ O
- -X- _ O
tune -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
the -X- _ O
corresponding -X- _ O
downstream -X- _ O
tasks -X- _ O
. -X- _ O

Prompt -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
bridges -X- _ O
this -X- _ O
gap -X- _ O
to -X- _ O
some -X- _ O
extend -X- _ O
. -X- _ O

This -X- _ O
indicates -X- _ O
that -X- _ O
there -X- _ O
still -X- _ O
remains -X- _ O
a -X- _ O
gap -X- _ O
between -X- _ O
masked -X- _ O
language -X- _ O
modeling -X- _ O
and -X- _ O
downstream -X- _ O
tasks -X- _ O
. -X- _ O

The -X- _ O
second -X- _ O
is -X- _ O
the -X- _ O
hybrid -X- _ O
strategy -X- _ O
in -X- _ O
Section -X- _ O
2 -X- _ O
. -X- _ O

This -X- _ O
means -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
soft -X- _ O
prompts -X- _ O
and -X- _ O
using -X- _ O
hybrid -X- _ O
prompts -X- _ O
are -X- _ O
complementary -X- _ O
. -X- _ O

First -X- _ O
, -X- _ O
larger -X- _ O
models -X- _ O
achieve -X- _ O
better -X- _ O
overall -X- _ O
performance -X- _ O
, -X- _ O
which -X- _ O
means -X- _ O
increasing -X- _ O
the -X- _ O
model -X- _ O
size -X- _ O
still -X- _ O
helps -X- _ O
under -X- _ O
the -X- _ O
few -X- _ O
- -X- _ O
shot -X- _ O
setting -X- _ O
. -X- _ O

Effectiveness -X- _ O
From -X- _ O
the -X- _ O
Table -X- _ O
4 -X- _ O
we -X- _ O
have -X- _ O
four -X- _ O
observations -X- _ O
. -X- _ O

The -X- _ O
main -X- _ O
results -X- _ O
of -X- _ O
English -X- _ O
and -X- _ O
Chinese -X- _ O
datasets -X- _ O
are -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
4 -X- _ O
. -X- _ O

Main -X- _ O
Results -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
the -X- _ O
Yelp-5 -X- _ B-DatasetName
( -X- _ O
Zhang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015a -X- _ O
) -X- _ O
dataset -X- _ O
to -X- _ O
train -X- _ O
the -X- _ O
RoBERTa -X- _ B-MethodName
BASE -X- _ I-MethodName
model -X- _ O
mentioned -X- _ O
in -X- _ O
Section -X- _ O
3.2.3 -X- _ O
. -X- _ O
More -X- _ O
details -X- _ O
of -X- _ O
the -X- _ O
training -X- _ O
hyper -X- _ O
- -X- _ O
parameters -X- _ O
can -X- _ O
be -X- _ O
found -X- _ O
in -X- _ O
the -X- _ O
Appendix -X- _ O
C. -X- _ O

For -X- _ O
prompt -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
, -X- _ O
we -X- _ O
sample -X- _ O
10 -X- _ O
GB -X- _ O
data -X- _ O
from -X- _ O
OpenWebText -X- _ B-DatasetName
( -X- _ O
Gokaslan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
for -X- _ O
English -X- _ O
tasks -X- _ O
and -X- _ O
10 -X- _ O
GB -X- _ O
data -X- _ O
from -X- _ O
WuDaoCorpora -X- _ B-DatasetName
( -X- _ O
Yuan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
for -X- _ O
Chinese -X- _ O
tasks -X- _ O
. -X- _ O

As -X- _ O
a -X- _ O
result -X- _ O
, -X- _ O
the -X- _ O
tunable -X- _ O
parameters -X- _ O
is -X- _ O
only -X- _ O
100×4096 -X- _ O
= -X- _ O
4.1 -X- _ O
× -X- _ O
10 -X- _ O
5 -X- _ O
= -X- _ O
410K. -X- _ O

Since -X- _ O
CPM-2 -X- _ B-MethodName
does -X- _ O
not -X- _ O
provide -X- _ O
other -X- _ O
size -X- _ O
models -X- _ O
, -X- _ O
we -X- _ O
compare -X- _ O
it -X- _ O
with -X- _ O
mT5 -X- _ B-MethodName
( -X- _ O
Xue -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
of -X- _ O
various -X- _ O
sizes -X- _ O
. -X- _ O

Therefore -X- _ O
, -X- _ O
we -X- _ O
randomly -X- _ O
select -X- _ O
8 -X- _ O
samples -X- _ O
for -X- _ O
each -X- _ O
label -X- _ O
. -X- _ O

For -X- _ O
tasks -X- _ O
with -X- _ O
more -X- _ O
than -X- _ O
5 -X- _ O
labels -X- _ O
like -X- _ O
TNews -X- _ O
and -X- _ O
YahooAnswer -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
hard -X- _ O
to -X- _ O
compose -X- _ O
a -X- _ O
dataset -X- _ O
with -X- _ O
label -X- _ O
- -X- _ O
balanced -X- _ O
samples -X- _ O
. -X- _ O

As -X- _ O
described -X- _ O
in -X- _ O
Section -X- _ O
2 -X- _ O
, -X- _ O
for -X- _ O
tasks -X- _ O
with -X- _ O
fewer -X- _ O
than -X- _ O
5 -X- _ O
labels -X- _ O
, -X- _ O
we -X- _ O
construct -X- _ O
D -X- _ O
train -X- _ O
and -X- _ O
D -X- _ O
dev -X- _ O
with -X- _ O
32 -X- _ O
samples -X- _ O
from -X- _ O
the -X- _ O
original -X- _ O
training -X- _ O
data -X- _ O
and -X- _ O
ensure -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
labels -X- _ O
is -X- _ O
balanced -X- _ O
. -X- _ O

We -X- _ O
conduct -X- _ O
experiments -X- _ O
on -X- _ O
both -X- _ O
Chinese -X- _ O
and -X- _ O
English -X- _ O
tasks -X- _ O
( -X- _ O
see -X- _ O
Table -X- _ O
3 -X- _ O
) -X- _ O
. -X- _ O

Setup -X- _ O
. -X- _ O

Experiments -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
the -X- _ O
PVP -X- _ O
in -X- _ O
Section -X- _ O
3.2.2 -X- _ O
for -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
, -X- _ O
and -X- _ O
then -X- _ O
apply -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
soft -X- _ O
prompts -X- _ O
to -X- _ O
cover -X- _ O
the -X- _ O
above -X- _ O
mentioned -X- _ O
three -X- _ O
classification -X- _ O
tasks -X- _ O
. -X- _ O

Since -X- _ O
different -X- _ O
tasks -X- _ O
may -X- _ O
have -X- _ O
different -X- _ O
candidate -X- _ O
numbers -X- _ O
and -X- _ O
lengths -X- _ O
, -X- _ O
we -X- _ O
construct -X- _ O
pretraining -X- _ O
samples -X- _ O
with -X- _ O
option -X- _ O
numbers -X- _ O
varying -X- _ O
from -X- _ O
2 -X- _ O
to -X- _ O
16 -X- _ O
2 -X- _ O
and -X- _ O
option -X- _ O
lengths -X- _ O
from -X- _ O
50 -X- _ O
to -X- _ O
20 -X- _ O
. -X- _ O

They -X- _ O
tune -X- _ O
the -X- _ O
entire -X- _ O
model -X- _ O
with -X- _ O
this -X- _ O
meta -X- _ O
task -X- _ O
on -X- _ O
a -X- _ O
collection -X- _ O
of -X- _ O
QA -X- _ O
datasets -X- _ O
and -X- _ O
then -X- _ O
transfer -X- _ O
to -X- _ O
other -X- _ O
classification -X- _ O
tasks -X- _ O
under -X- _ O
low -X- _ O
- -X- _ O
resource -X- _ O
settings -X- _ O
. -X- _ O

( -X- _ O
2021a -X- _ O
) -X- _ O
use -X- _ O
some -X- _ O
hard -X- _ O
prompts -X- _ O
to -X- _ O
unify -X- _ O
several -X- _ O
tasks -X- _ O
as -X- _ O
a -X- _ O
meta -X- _ O
question -X- _ O
answering -X- _ O
task -X- _ O
. -X- _ O

Recently -X- _ O
, -X- _ O
Zhong -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

Constructing -X- _ O
a -X- _ O
unified -X- _ O
PVP -X- _ O
is -X- _ O
similar -X- _ O
to -X- _ O
the -X- _ O
idea -X- _ O
of -X- _ O
MultiQA -X- _ B-DatasetName
( -X- _ O
Talmor -X- _ O
and -X- _ O
Berant -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
and -X- _ O
Uni -X- _ B-DatasetName
- -X- _ I-DatasetName
fiedQA -X- _ I-DatasetName
( -X- _ O
Khashabi -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O

Note -X- _ O
that -X- _ O
in -X- _ O
this -X- _ O
way -X- _ O
, -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
PVPs -X- _ O
can -X- _ O
be -X- _ O
used -X- _ O
in -X- _ O
single -X- _ B-TaskName
text -X- _ I-TaskName
classification -X- _ I-TaskName
tasks -X- _ O
from -X- _ O
arbitrary -X- _ O
domains -X- _ O
and -X- _ O
with -X- _ O
much -X- _ O
more -X- _ O
labels -X- _ O
. -X- _ O

For -X- _ O
single -X- _ B-TaskName
- -X- _ I-TaskName
sentence -X- _ I-TaskName
classification -X- _ I-TaskName
, -X- _ O
the -X- _ O
query -X- _ O
is -X- _ O
the -X- _ O
input -X- _ O
sentence -X- _ O
and -X- _ O
the -X- _ O
options -X- _ O
are -X- _ O
the -X- _ O
concrete -X- _ O
labels -X- _ O
. -X- _ O

Unifying -X- _ O
Task -X- _ O
Formats -X- _ O
. -X- _ O

Therefore -X- _ O
, -X- _ O
the -X- _ O
method -X- _ O
described -X- _ O
in -X- _ O
the -X- _ O
following -X- _ O
section -X- _ O
is -X- _ O
proposed -X- _ O
to -X- _ O
solve -X- _ O
this -X- _ O
problem -X- _ O
. -X- _ O

Although -X- _ O
the -X- _ O
above -X- _ O
method -X- _ O
improves -X- _ O
the -X- _ O
model -X- _ O
performance -X- _ O
, -X- _ O
we -X- _ O
have -X- _ O
to -X- _ O
point -X- _ O
out -X- _ O
that -X- _ O
it -X- _ O
is -X- _ O
still -X- _ O
limited -X- _ O
to -X- _ O
generalize -X- _ O
to -X- _ O
other -X- _ O
single -X- _ B-TaskName
- -X- _ I-TaskName
text -X- _ I-TaskName
classifications -X- _ I-TaskName
in -X- _ O
different -X- _ O
domains -X- _ O
and -X- _ O
with -X- _ O
different -X- _ O
numbers -X- _ O
of -X- _ O
labels -X- _ O
. -X- _ O

For -X- _ O
those -X- _ O
with -X- _ O
fewer -X- _ O
than -X- _ O
5 -X- _ O
labels -X- _ O
, -X- _ O
we -X- _ O
choose -X- _ O
a -X- _ O
subset -X- _ O
from -X- _ O
v -X- _ O
pre -X- _ O
i -X- _ O
( -X- _ O
Y -X- _ O
) -X- _ O
as -X- _ O
labels -X- _ O
. -X- _ O

( -X- _ O
5 -X- _ O
) -X- _ O
For -X- _ O
sentiment -X- _ O
classification -X- _ O
tasks -X- _ O
with -X- _ O
5 -X- _ O
labels -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
use -X- _ O
PVP -X- _ O
k -X- _ O
i -X- _ O
= -X- _ O
PVP -X- _ O
pre -X- _ O
i -X- _ O
. -X- _ O

X -X- _ O
. -X- _ O
" -X- _ O
, -X- _ O
v -X- _ O
pre -X- _ O
i -X- _ O
( -X- _ O
Y -X- _ O
) -X- _ O
= -X- _ O
[ -X- _ O
terrible -X- _ O
, -X- _ O
bad -X- _ O
, -X- _ O
maybe -X- _ O
, -X- _ O
good -X- _ O
, -X- _ O
great -X- _ O
] -X- _ O
. -X- _ O

( -X- _ O
f -X- _ O
pre -X- _ O
i -X- _ O
, -X- _ O
v -X- _ O
pre -X- _ O
i -X- _ O
) -X- _ O
is -X- _ O
given -X- _ O
as -X- _ O
f -X- _ O
pre -X- _ O
i -X- _ O
( -X- _ O
x -X- _ O
) -X- _ O
= -X- _ O
" -X- _ O
s. -X- _ O

Then -X- _ O
with -X- _ O
a -X- _ O
sentence -X- _ O
s -X- _ O
from -X- _ O
the -X- _ O
corpus -X- _ O
, -X- _ O
we -X- _ O
have -X- _ O
the -X- _ O
input -X- _ O
x -X- _ O
= -X- _ O
( -X- _ O
s -X- _ O
) -X- _ O
and -X- _ O
the -X- _ O
label -X- _ O
set -X- _ O
Y -X- _ O
= -X- _ O
{ -X- _ O
1 -X- _ O
, -X- _ O
2 -X- _ O
, -X- _ O
3 -X- _ O
, -X- _ O
4 -X- _ O
, -X- _ O
5 -X- _ O
} -X- _ O
. -X- _ O

Taking -X- _ O
sentiment -X- _ O
classification -X- _ O
as -X- _ O
an -X- _ O
example -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
another -X- _ O
small -X- _ O
model -X- _ O
to -X- _ O
annotate -X- _ O
sentiment -X- _ O
labels -X- _ O
for -X- _ O
the -X- _ O
sentences -X- _ O
from -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
corpus -X- _ O
and -X- _ O
filter -X- _ O
out -X- _ O
those -X- _ O
with -X- _ O
low -X- _ O
classification -X- _ O
probability -X- _ O
. -X- _ O

For -X- _ O
single -X- _ B-TaskName
- -X- _ I-TaskName
sentence -X- _ I-TaskName
classification -X- _ I-TaskName
, -X- _ O
we -X- _ O
create -X- _ O
pseudo -X- _ O
labels -X- _ O
for -X- _ O
prompt -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
. -X- _ O

Single -X- _ B-TaskName
- -X- _ I-TaskName
Sentence -X- _ I-TaskName
Classification -X- _ I-TaskName
. -X- _ O

We -X- _ O
concatenate -X- _ O
them -X- _ O
to -X- _ O
form -X- _ O
the -X- _ O
query -X- _ O
. -X- _ O

A.s1 -X- _ O
• -X- _ O
• -X- _ O
• -X- _ O
F.s6.Answer -X- _ O
is -X- _ O
X -X- _ O
. -X- _ O
" -X- _ O
, -X- _ O
v -X- _ O
pre -X- _ O
i -X- _ O
( -X- _ O
Y -X- _ O
) -X- _ O
= -X- _ O
[ -X- _ O
A -X- _ O
, -X- _ O
B -X- _ O
, -X- _ O
C -X- _ O
, -X- _ O
D -X- _ O
, -X- _ O
E -X- _ O
, -X- _ O
F].(4 -X- _ O
) -X- _ O
Most -X- _ O
multiple -X- _ B-TaskName
- -X- _ I-TaskName
choice -X- _ I-TaskName
tasks -X- _ O
can -X- _ O
use -X- _ O
{ -X- _ O
f -X- _ O
pre -X- _ O
i -X- _ O
, -X- _ O
v -X- _ O
pre -X- _ O
i -X- _ O
} -X- _ O
directly -X- _ O
as -X- _ O
their -X- _ O
PVPs -X- _ O
. -X- _ O
For -X- _ O
tasks -X- _ O
like -X- _ O
reading -X- _ B-TaskName
comprehension -X- _ I-TaskName
, -X- _ O
the -X- _ O
input -X- _ O
may -X- _ O
contain -X- _ O
a -X- _ O
passage -X- _ O
and -X- _ O
a -X- _ O
question -X- _ O
. -X- _ O

For -X- _ O
x -X- _ O
= -X- _ O
( -X- _ O
s -X- _ O
q -X- _ O
, -X- _ O
s -X- _ O
1 -X- _ O
, -X- _ O
s -X- _ O
2 -X- _ O
, -X- _ O
• -X- _ O
• -X- _ O
• -X- _ O
, -X- _ O
s -X- _ O
6 -X- _ O
) -X- _ O
, -X- _ O
( -X- _ O
f -X- _ O
pre -X- _ O
i -X- _ O
, -X- _ O
v -X- _ O
pre -X- _ O
i -X- _ O
) -X- _ O
is -X- _ O
given -X- _ O
as -X- _ O
f -X- _ O
pre -X- _ O
i -X- _ O
( -X- _ O
x -X- _ O
) -X- _ O
= -X- _ O
" -X- _ O
sq -X- _ O
? -X- _ O

These -X- _ O
candidates -X- _ O
consist -X- _ O
of -X- _ O
the -X- _ O
right -X- _ O
answer -X- _ O
, -X- _ O
one -X- _ O
sentence -X- _ O
from -X- _ O
the -X- _ O
same -X- _ O
document -X- _ O
but -X- _ O
is -X- _ O
not -X- _ O
adjacent -X- _ O
to -X- _ O
the -X- _ O
query -X- _ O
, -X- _ O
and -X- _ O
four -X- _ O
sentences -X- _ O
from -X- _ O
other -X- _ O
documents -X- _ O
. -X- _ O

Given -X- _ O
a -X- _ O
sentence -X- _ O
as -X- _ O
the -X- _ O
query -X- _ O
s -X- _ O
q -X- _ O
, -X- _ O
the -X- _ O
model -X- _ O
is -X- _ O
trained -X- _ O
to -X- _ O
select -X- _ O
the -X- _ O
adjacent -X- _ O
sentence -X- _ O
from -X- _ O
six -X- _ O
candidates -X- _ O
, -X- _ O
denoted -X- _ O
as -X- _ O
s -X- _ O
1 -X- _ O
∼ -X- _ O
s -X- _ O
6 -X- _ O
and -X- _ O
thus -X- _ O
the -X- _ O
label -X- _ O
set -X- _ O
is -X- _ O
Y -X- _ O
= -X- _ O
{ -X- _ O
1 -X- _ O
, -X- _ O
2 -X- _ O
, -X- _ O
3 -X- _ O
, -X- _ O
4 -X- _ O
, -X- _ O
5 -X- _ O
, -X- _ O
6 -X- _ O
} -X- _ O
. -X- _ O

We -X- _ O
design -X- _ O
a -X- _ O
next -X- _ O
sentence -X- _ O
selection -X- _ O
task -X- _ O
to -X- _ O
pre -X- _ O
- -X- _ O
train -X- _ O
the -X- _ O
prompt -X- _ O
. -X- _ O

If -X- _ O
a -X- _ O
task -X- _ O
requires -X- _ O
to -X- _ O
measure -X- _ O
the -X- _ O
similarity -X- _ O
between -X- _ O
two -X- _ O
sentences -X- _ O
, -X- _ O
the -X- _ O
probability -X- _ O
over -X- _ O
{ -X- _ O
no -X- _ O
, -X- _ O
yes -X- _ O
} -X- _ O
can -X- _ O
serve -X- _ O
for -X- _ O
this -X- _ O
task -X- _ O
. -X- _ O

( -X- _ O
3 -X- _ O
) -X- _ O
Designing -X- _ O
PVP -X- _ O
k -X- _ O
i -X- _ O
= -X- _ O
( -X- _ O
f -X- _ O
k -X- _ O
i -X- _ O
, -X- _ O
v -X- _ O
k -X- _ O
i -X- _ O
) -X- _ O
according -X- _ O
k -X- _ O
i -X- _ O
= -X- _ O
v -X- _ O
pre -X- _ O
i -X- _ O
. -X- _ O

PVP -X- _ O
pre -X- _ O
i -X- _ O
= -X- _ O
( -X- _ O
f -X- _ O
pre -X- _ O
i -X- _ O
, -X- _ O
v -X- _ O
pre -X- _ O
i -X- _ O
) -X- _ O
is -X- _ O
given -X- _ O
as -X- _ O
f -X- _ O
pre -X- _ O
i -X- _ O
( -X- _ O
x -X- _ O
) -X- _ O
= -X- _ O
" -X- _ O
s1 -X- _ O
X -X- _ O
.s2 -X- _ O
" -X- _ O
, -X- _ O
v -X- _ O
pre -X- _ O
i -X- _ O
( -X- _ O
Y -X- _ O
) -X- _ O
= -X- _ O
[ -X- _ O
no -X- _ O
, -X- _ O
maybe -X- _ O
, -X- _ O
yes -X- _ O
] -X- _ O
. -X- _ O

We -X- _ O
consider -X- _ O
the -X- _ O
label -X- _ O
set -X- _ O
|Y| -X- _ O
≤ -X- _ O
3 -X- _ O
because -X- _ O
this -X- _ O
covers -X- _ O
most -X- _ O
sentence -X- _ O
pair -X- _ O
tasks -X- _ O
. -X- _ O

To -X- _ O
construct -X- _ O
signal -X- _ O
from -X- _ O
unlabeled -X- _ O
documents -X- _ O
, -X- _ O
we -X- _ O
set -X- _ O
the -X- _ O
two -X- _ O
sentences -X- _ O
next -X- _ O
to -X- _ O
each -X- _ O
other -X- _ O
as -X- _ O
label -X- _ O
2 -X- _ O
, -X- _ O
those -X- _ O
from -X- _ O
the -X- _ O
same -X- _ O
document -X- _ O
but -X- _ O
not -X- _ O
true -X- _ O
next -X- _ O
sentences -X- _ O
as -X- _ O
1 -X- _ O
, -X- _ O
and -X- _ O
those -X- _ O
from -X- _ O
different -X- _ O
documents -X- _ O
as -X- _ O
0 -X- _ O
. -X- _ O

These -X- _ O
labels -X- _ O
in -X- _ O
Y -X- _ O
can -X- _ O
respectively -X- _ O
indicate -X- _ O
that -X- _ O
the -X- _ O
semantic -X- _ O
relation -X- _ O
between -X- _ O
two -X- _ O
sentences -X- _ O
is -X- _ O
coherent -X- _ O
( -X- _ O
with -X- _ O
label -X- _ O
2 -X- _ O
) -X- _ O
, -X- _ O
similar -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
and -X- _ O
irrelevant -X- _ O
( -X- _ O
0 -X- _ O
) -X- _ O
. -X- _ O

( -X- _ O
2019 -X- _ O
) -X- _ O
to -X- _ O
a -X- _ O
3 -X- _ O
- -X- _ O
class -X- _ O
classification -X- _ O
with -X- _ O
labels -X- _ O
Y -X- _ O
= -X- _ O
{ -X- _ O
0 -X- _ O
, -X- _ O
1 -X- _ O
, -X- _ O
2 -X- _ O
} -X- _ O
as -X- _ O
the -X- _ O
pretraining -X- _ O
task -X- _ O
. -X- _ O

To -X- _ O
design -X- _ O
a -X- _ O
PVP -X- _ O
for -X- _ O
these -X- _ O
tasks -X- _ O
, -X- _ O
we -X- _ O
extend -X- _ O
the -X- _ O
next -X- _ O
sentence -X- _ O
prediction -X- _ O
in -X- _ O
Devlin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

Sentence -X- _ B-TaskName
- -X- _ I-TaskName
pair -X- _ I-TaskName
classification -X- _ I-TaskName
tasks -X- _ O
such -X- _ O
as -X- _ O
natural -X- _ O
language -X- _ O
inference -X- _ O
and -X- _ O
sentence -X- _ O
similarity -X- _ O
take -X- _ O
two -X- _ O
sentences -X- _ O
x -X- _ O
= -X- _ O
( -X- _ O
s -X- _ O
1 -X- _ O
, -X- _ O
s -X- _ O
2 -X- _ O
) -X- _ O
as -X- _ O
the -X- _ O
input -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
section -X- _ O
, -X- _ O
we -X- _ O
take -X- _ O
three -X- _ O
typical -X- _ O
classification -X- _ O
tasks -X- _ O
as -X- _ O
examples -X- _ O
to -X- _ O
describe -X- _ O
the -X- _ O
design -X- _ O
of -X- _ O
patternverbalizer -X- _ O
pairs -X- _ O
PVP -X- _ O
pre -X- _ O
i -X- _ O
for -X- _ O
prompt -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
. -X- _ O

Designing -X- _ O
Pattern -X- _ O
- -X- _ O
Verbalizer -X- _ O
Pairs -X- _ O
for -X- _ O
Pre -X- _ O
- -X- _ O
training -X- _ O
. -X- _ O

Then -X- _ O
, -X- _ O
for -X- _ O
each -X- _ O
task -X- _ O
PVP -X- _ O
k -X- _ O
i -X- _ O
in -X- _ O
T -X- _ O
i -X- _ O
, -X- _ O
we -X- _ O
continue -X- _ O
to -X- _ O
optimize -X- _ O
Eq -X- _ O
. -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
by -X- _ O
using -X- _ O
P -X- _ O
i -X- _ O
as -X- _ O
the -X- _ O
soft -X- _ O
prompts -X- _ O
initialization -X- _ O
. -X- _ O

After -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
soft -X- _ O
prompts -X- _ O
on -X- _ O
these -X- _ O
tasks -X- _ O
with -X- _ O
all -X- _ O
model -X- _ O
parameters -X- _ O
fixed -X- _ O
, -X- _ O
we -X- _ O
get -X- _ O
m -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
prompts -X- _ O
{ -X- _ O
P -X- _ O
1 -X- _ O
, -X- _ O
P -X- _ O
2 -X- _ O
, -X- _ O
... -X- _ O
, -X- _ O
P -X- _ O
m -X- _ O
} -X- _ O
. -X- _ O

For -X- _ O
each -X- _ O
group -X- _ O
, -X- _ O
we -X- _ O
design -X- _ O
a -X- _ O
corresponding -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
task -X- _ O
PVP -X- _ O
pre -X- _ O
i -X- _ O
= -X- _ O
( -X- _ O
f -X- _ O
pre -X- _ O
i -X- _ O
, -X- _ O
v -X- _ O
pre -X- _ O
i -X- _ O
) -X- _ O
. -X- _ O

Formally -X- _ O
, -X- _ O
suppose -X- _ O
we -X- _ O
can -X- _ O
divide -X- _ O
downstream -X- _ O
tasks -X- _ O
into -X- _ O
m -X- _ O
groups -X- _ O
{ -X- _ O
T -X- _ O
1 -X- _ O
, -X- _ O
T -X- _ O
2 -X- _ O
, -X- _ O
... -X- _ O
, -X- _ O
T -X- _ O
m -X- _ O
} -X- _ O
, -X- _ O
where -X- _ O
T -X- _ O
i -X- _ O
is -X- _ O
the -X- _ O
set -X- _ O
containing -X- _ O
n -X- _ O
i -X- _ O
downstream -X- _ O
tasks -X- _ O
: -X- _ O
{ -X- _ O
PVP -X- _ O
1 -X- _ O
i -X- _ O
, -X- _ O
PVP -X- _ O
2 -X- _ O
i -X- _ O
, -X- _ O
... -X- _ O
, -X- _ O
PVP -X- _ O
n -X- _ O
i -X- _ O
i -X- _ O
} -X- _ O
, -X- _ O
where -X- _ O
PVP -X- _ O
k -X- _ O
i -X- _ O
= -X- _ O
( -X- _ O
f -X- _ O
k -X- _ O
i -X- _ O
, -X- _ O
v -X- _ O
k -X- _ O
i -X- _ O
) -X- _ O
. -X- _ O

Therefore -X- _ O
, -X- _ O
soft -X- _ O
prompts -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
by -X- _ O
NSP -X- _ O
can -X- _ O
be -X- _ O
a -X- _ O
good -X- _ O
initialization -X- _ O
for -X- _ O
these -X- _ O
sentence -X- _ O
- -X- _ O
pair -X- _ O
tasks -X- _ O
. -X- _ O

As -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
3 -X- _ O
, -X- _ O
these -X- _ O
tasks -X- _ O
all -X- _ O
take -X- _ O
two -X- _ O
sentences -X- _ O
as -X- _ O
input -X- _ O
and -X- _ O
compare -X- _ O
their -X- _ O
semantic -X- _ O
meanings -X- _ O
. -X- _ O

We -X- _ O
notice -X- _ O
that -X- _ O
some -X- _ O
groups -X- _ O
of -X- _ O
downstream -X- _ O
tasks -X- _ O
are -X- _ O
related -X- _ O
to -X- _ O
certain -X- _ O
self -X- _ O
- -X- _ O
supervised -X- _ O
tasks -X- _ O
built -X- _ O
on -X- _ O
unlabeled -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
corpora -X- _ O
. -X- _ O

Inspired -X- _ O
by -X- _ O
this -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
to -X- _ O
pre -X- _ O
- -X- _ O
train -X- _ O
soft -X- _ O
prompts -X- _ O
. -X- _ O

Recently -X- _ O
, -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
has -X- _ O
been -X- _ O
proven -X- _ O
to -X- _ O
be -X- _ O
an -X- _ O
effective -X- _ O
method -X- _ O
to -X- _ O
find -X- _ O
a -X- _ O
good -X- _ O
model -X- _ O
initialization -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
we -X- _ O
find -X- _ O
it -X- _ O
hard -X- _ O
to -X- _ O
learn -X- _ O
effective -X- _ O
soft -X- _ O
prompts -X- _ O
, -X- _ O
which -X- _ O
may -X- _ O
result -X- _ O
in -X- _ O
low -X- _ O
performance -X- _ O
in -X- _ O
various -X- _ O
few -X- _ O
- -X- _ O
shot -X- _ O
scenarios -X- _ O
. -X- _ O

With -X- _ O
f -X- _ O
( -X- _ O
• -X- _ O
) -X- _ O
and -X- _ O
v(• -X- _ O
) -X- _ O
, -X- _ O
a -X- _ O
classification -X- _ O
task -X- _ O
can -X- _ O
be -X- _ O
represented -X- _ O
by -X- _ O
a -X- _ O
pattern -X- _ O
- -X- _ O
verbalizer -X- _ O
pair -X- _ O
( -X- _ O
f -X- _ O
, -X- _ O
v -X- _ O
): -X- _ O
arg -X- _ O
max -X- _ O
θ -X- _ O
x -X- _ O
log -X- _ O
p -X- _ O
y|x -X- _ O
; -X- _ O
θ -X- _ O
= -X- _ O
arg -X- _ O
max -X- _ O
θ -X- _ O
x -X- _ O
log -X- _ O
p -X- _ O
X -X- _ O
= -X- _ O
v(y)|f -X- _ O
( -X- _ O
x -X- _ O
) -X- _ O
; -X- _ O
θ -X- _ O
, -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
where -X- _ O
θ -X- _ O
indicates -X- _ O
all -X- _ O
tunable -X- _ O
parameters -X- _ O
, -X- _ O
especially -X- _ O
the -X- _ O
parameters -X- _ O
of -X- _ O
PLMs -X- _ O
. -X- _ O
For -X- _ O
convenience -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
" -X- _ O
PVP -X- _ O
" -X- _ O
to -X- _ O
denote -X- _ O
this -X- _ O
pattern -X- _ O
- -X- _ O
verbalizer -X- _ O
pair -X- _ O
( -X- _ O
Schick -X- _ O
and -X- _ O
Schütze -X- _ O
, -X- _ O
2021a -X- _ O
) -X- _ O
. -X- _ O

Then -X- _ O
, -X- _ O
a -X- _ O
verbalizer -X- _ O
v -X- _ O
: -X- _ O
Y -X- _ O
→ -X- _ O
V -X- _ O
* -X- _ O
is -X- _ O
used -X- _ O
to -X- _ O
map -X- _ O
y -X- _ O
to -X- _ O
some -X- _ O
label -X- _ O
tokens -X- _ O
v(y -X- _ O
) -X- _ O
. -X- _ O

Taking -X- _ O
classification -X- _ O
for -X- _ O
example -X- _ O
, -X- _ O
given -X- _ O
an -X- _ O
input -X- _ O
sentence -X- _ O
x -X- _ O
∈ -X- _ O
V -X- _ O
* -X- _ O
and -X- _ O
its -X- _ O
label -X- _ O
y -X- _ O
∈ -X- _ O
Y -X- _ O
, -X- _ O
a -X- _ O
pattern -X- _ O
mapping -X- _ O
f -X- _ O
: -X- _ O
V -X- _ O
* -X- _ O
→ -X- _ O
V -X- _ O
* -X- _ O
is -X- _ O
first -X- _ O
applied -X- _ O
to -X- _ O
convert -X- _ O
x -X- _ O
into -X- _ O
a -X- _ O
new -X- _ O
sequence -X- _ O
f -X- _ O
( -X- _ O
x -X- _ O
) -X- _ O
, -X- _ O
where -X- _ O
V -X- _ O
is -X- _ O
the -X- _ O
vocabulary -X- _ O
of -X- _ O
PLMs -X- _ O
. -X- _ O
f -X- _ O
( -X- _ O
x -X- _ O
) -X- _ O
not -X- _ O
only -X- _ O
adds -X- _ O
some -X- _ O
prompt -X- _ O
tokens -X- _ O
as -X- _ O
hints -X- _ O
, -X- _ O
but -X- _ O
also -X- _ O
preserves -X- _ O
the -X- _ O
mask -X- _ O
token -X- _ O
X -X- _ O
to -X- _ O
let -X- _ O
PLMs -X- _ O
predict -X- _ O
tokens -X- _ O
at -X- _ O
the -X- _ O
masked -X- _ O
positions -X- _ O
. -X- _ O

As -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
1 -X- _ O
( -X- _ O
c -X- _ O
) -X- _ O
, -X- _ O
to -X- _ O
reduce -X- _ O
the -X- _ O
objective -X- _ O
gap -X- _ O
between -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
and -X- _ O
downstream -X- _ O
tasks -X- _ O
, -X- _ O
promptoriented -X- _ O
fine -X- _ O
- -X- _ O
tuning -X- _ O
converts -X- _ O
downstream -X- _ O
tasks -X- _ O
into -X- _ O
cloze -X- _ O
- -X- _ O
style -X- _ O
objectives -X- _ O
. -X- _ O

Overview -X- _ O
. -X- _ O

use -X- _ O
these -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
prompts -X- _ O
for -X- _ O
specific -X- _ O
tasks -X- _ O
. -X- _ O

This -X- _ O
suggests -X- _ O
that -X- _ O
observations -X- _ O
on -X- _ O
small -X- _ O
models -X- _ O
can -X- _ O
not -X- _ O
be -X- _ O
directly -X- _ O
adapted -X- _ O
to -X- _ O
large -X- _ O
models -X- _ O
and -X- _ O
finding -X- _ O
a -X- _ O
good -X- _ O
initialization -X- _ O
for -X- _ O
soft -X- _ O
prompts -X- _ O
is -X- _ O
yet -X- _ O
to -X- _ O
be -X- _ O
explored -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
from -X- _ O
the -X- _ O
experiments -X- _ O
on -X- _ O
SST-2 -X- _ B-DatasetName
( -X- _ O
Socher -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2013 -X- _ O
) -X- _ O
and -X- _ O
BoolQ -X- _ B-DatasetName
( -X- _ O
Clark -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
( -X- _ O
Table -X- _ O
2 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
find -X- _ O
that -X- _ O
for -X- _ O
the -X- _ O
11B -X- _ O
model -X- _ O
, -X- _ O
real -X- _ O
word -X- _ O
initialization -X- _ O
has -X- _ O
little -X- _ O
or -X- _ O
even -X- _ O
negative -X- _ O
impact -X- _ O
on -X- _ O
the -X- _ O
performance -X- _ O
in -X- _ O
few -X- _ O
- -X- _ O
shot -X- _ O
scenarios -X- _ O
. -X- _ O

The -X- _ O
effectiveness -X- _ O
of -X- _ O
this -X- _ O
approach -X- _ O
has -X- _ O
been -X- _ O
verified -X- _ O
on -X- _ O
small -X- _ O
PLMs -X- _ O
( -X- _ O
fewer -X- _ O
than -X- _ O
3B -X- _ O
parameters -X- _ O
) -X- _ O
in -X- _ O
previous -X- _ O
works -X- _ O
( -X- _ O
Lester -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
. -X- _ O

In -X- _ O
real -X- _ O
word -X- _ O
initialization -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
embeddings -X- _ O
of -X- _ O
concrete -X- _ O
words -X- _ O
to -X- _ O
initialize -X- _ O
the -X- _ O
soft -X- _ O
prompt -X- _ O
and -X- _ O
test -X- _ O
four -X- _ O
initialization -X- _ O
strategies -X- _ O
. -X- _ O

Real -X- _ O
Word -X- _ O
Initialization -X- _ O
. -X- _ O

In -X- _ O
general -X- _ O
, -X- _ O
common -X- _ O
words -X- _ O
that -X- _ O
explain -X- _ O
the -X- _ O
meaning -X- _ O
of -X- _ O
corresponding -X- _ O
labels -X- _ O
work -X- _ O
well -X- _ O
. -X- _ O

From -X- _ O
Table -X- _ O
1 -X- _ O
we -X- _ O
can -X- _ O
see -X- _ O
that -X- _ O
the -X- _ O
choices -X- _ O
of -X- _ O
verbalizers -X- _ O
influence -X- _ O
the -X- _ O
performance -X- _ O
remarkably -X- _ O
. -X- _ O

( -X- _ O
2021 -X- _ O
in -X- _ O
Figure -X- _ O
1 -X- _ O
( -X- _ O
c -X- _ O
) -X- _ O
and -X- _ O
( -X- _ O
d -X- _ O
) -X- _ O
, -X- _ O
the -X- _ O
verbalizer -X- _ O
maps -X- _ O
the -X- _ O
label -X- _ O
" -X- _ O
Positive -X- _ O
" -X- _ O
to -X- _ O
" -X- _ O
great -X- _ O
" -X- _ O
. -X- _ O

Verbalizer -X- _ O
Selection -X- _ O
Verbalizer -X- _ O
maps -X- _ O
taskspecific -X- _ O
labels -X- _ O
to -X- _ O
concrete -X- _ O
tokens -X- _ O
. -X- _ O

Furthermore -X- _ O
, -X- _ O
different -X- _ O
hard -X- _ O
prompts -X- _ O
affect -X- _ O
the -X- _ O
performance -X- _ O
remarkably -X- _ O
, -X- _ O
therefore -X- _ O
much -X- _ O
human -X- _ O
labor -X- _ O
for -X- _ O
prompt -X- _ O
design -X- _ O
and -X- _ O
selection -X- _ O
is -X- _ O
needed -X- _ O
. -X- _ O

In -X- _ O
Table -X- _ O
1 -X- _ O
, -X- _ O
we -X- _ O
show -X- _ O
the -X- _ O
results -X- _ O
of -X- _ O
combining -X- _ O
soft -X- _ O
prompts -X- _ O
P -X- _ O
with -X- _ O
three -X- _ O
manually -X- _ O
designed -X- _ O
hard -X- _ O
prompts -X- _ O
and -X- _ O
two -X- _ O
auto -X- _ O
- -X- _ O
generated -X- _ O
hard -X- _ O
prompts -X- _ O
( -X- _ O
Gao -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
on -X- _ O
a -X- _ O
sentiment -X- _ B-TaskName
classification -X- _ I-TaskName
task -X- _ O
( -X- _ O
Socher -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2013 -X- _ O
) -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
previous -X- _ O
works -X- _ O
train -X- _ O
soft -X- _ O
prompts -X- _ O
jointly -X- _ O
with -X- _ O
the -X- _ O
entire -X- _ O
model -X- _ O
. -X- _ O

Hybrid -X- _ B-MethodName
Prompt -X- _ I-MethodName
Tuning -X- _ I-MethodName
In -X- _ O
hybrid -X- _ B-MethodName
prompt -X- _ I-MethodName
tuning -X- _ I-MethodName
, -X- _ O
both -X- _ O
soft -X- _ O
and -X- _ O
hard -X- _ O
prompts -X- _ O
are -X- _ O
used -X- _ O
( -X- _ O
Liu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021;Han -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021b -X- _ O
) -X- _ O
. -X- _ O

( -X- _ O
2021 -X- _ O
) -X- _ O
to -X- _ O
use -X- _ O
the -X- _ O
original -X- _ O
validation -X- _ O
set -X- _ O
as -X- _ O
the -X- _ O
test -X- _ O
set -X- _ O
D -X- _ O
test -X- _ O
, -X- _ O
which -X- _ O
means -X- _ O
|D -X- _ O
test -X- _ O
| -X- _ O
|D -X- _ O
train -X- _ O
| -X- _ O
= -X- _ O
|D -X- _ O
dev -X- _ O
| -X- _ O
. -X- _ O

( -X- _ O
2021 -X- _ O
) -X- _ O
and -X- _ O
Gao -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

We -X- _ O
analyze -X- _ O
three -X- _ O
strategies -X- _ O
including -X- _ O
hybrid -X- _ B-MethodName
prompt -X- _ I-MethodName
tuning -X- _ I-MethodName
, -X- _ O
verbalizer -X- _ O
selec- -X- _ O
( -X- _ O
Perez -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
. -X- _ O

To -X- _ O
ensure -X- _ O
the -X- _ O
generalization -X- _ O
of -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
prompts -X- _ O
, -X- _ O
we -X- _ O
group -X- _ O
typical -X- _ O
classification -X- _ O
tasks -X- _ O
into -X- _ O
three -X- _ O
formats -X- _ O
: -X- _ O
sentencepair -X- _ B-TaskName
classification -X- _ I-TaskName
, -X- _ O
multiple -X- _ B-TaskName
- -X- _ I-TaskName
choice -X- _ I-TaskName
classification -X- _ I-TaskName
, -X- _ O
and -X- _ O
single -X- _ B-TaskName
- -X- _ I-TaskName
text -X- _ I-TaskName
classification -X- _ I-TaskName
, -X- _ O
each -X- _ O
format -X- _ O
corresponding -X- _ O
to -X- _ O
one -X- _ O
self -X- _ O
- -X- _ O
supervised -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
task -X- _ O
. -X- _ O

We -X- _ O
follow -X- _ O
Zhang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

To -X- _ O
help -X- _ O
the -X- _ O
model -X- _ O
find -X- _ O
suitable -X- _ O
prompts -X- _ O
, -X- _ O
we -X- _ O
pretrain -X- _ O
these -X- _ O
tokens -X- _ O
with -X- _ O
self -X- _ O
- -X- _ O
supervised -X- _ O
tasks -X- _ O
on -X- _ O
large -X- _ O
- -X- _ O
scale -X- _ O
unlabeled -X- _ O
corpora -X- _ O
. -X- _ O

The -X- _ O
above -X- _ O
observations -X- _ O
reveal -X- _ O
that -X- _ O
prompt -X- _ O
searching -X- _ O
for -X- _ O
PLMs -X- _ O
is -X- _ O
not -X- _ O
trivial -X- _ O
, -X- _ O
and -X- _ O
carefully -X- _ O
initialized -X- _ O
soft -X- _ O
prompt -X- _ O
tokens -X- _ O
is -X- _ O
crucial -X- _ O
. -X- _ O

First -X- _ O
, -X- _ O
soft -X- _ O
prompts -X- _ O
can -X- _ O
be -X- _ O
learned -X- _ O
end -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
end -X- _ O
in -X- _ O
comparison -X- _ O
to -X- _ O
hard -X- _ O
prompts -X- _ O
. -X- _ O

These -X- _ O
continuous -X- _ O
prompts -X- _ O
are -X- _ O
generally -X- _ O
randomly -X- _ O
initialized -X- _ O
and -X- _ O
learned -X- _ O
end -X- _ O
- -X- _ O
toend -X- _ O
. -X- _ O

To -X- _ O
address -X- _ O
this -X- _ O
challenge -X- _ O
, -X- _ O
Lester -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

As -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
1 -X- _ O
, -X- _ O
compared -X- _ O
to -X- _ O
task -X- _ O
- -X- _ O
oriented -X- _ O
finetuning -X- _ O
, -X- _ O
prompt -X- _ O
- -X- _ O
oriented -X- _ O
fine -X- _ O
- -X- _ O
tuning -X- _ O
is -X- _ O
more -X- _ O
similar -X- _ O
to -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
objectives -X- _ O
( -X- _ O
masked -X- _ O
language -X- _ O
modeling -X- _ O
) -X- _ O
, -X- _ O
thereby -X- _ O
helping -X- _ O
to -X- _ O
better -X- _ O
use -X- _ O
knowledge -X- _ O
in -X- _ O
PLMs -X- _ O
and -X- _ O
often -X- _ O
obtaining -X- _ O
better -X- _ O
performance -X- _ O
. -X- _ O

As -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
1 -X- _ O
( -X- _ O
c -X- _ O
) -X- _ O
, -X- _ O
by -X- _ O
adding -X- _ O
the -X- _ O
prompt -X- _ O
" -X- _ O
It -X- _ O
was -X- _ O
X -X- _ O
. -X- _ O
" -X- _ O
to -X- _ O
a -X- _ O
sentence -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
determine -X- _ O
its -X- _ O
sentiment -X- _ O
polarity -X- _ O
with -X- _ O
PLMs -X- _ O
by -X- _ O
predicting -X- _ O
" -X- _ O
great -X- _ O
" -X- _ O
or -X- _ O
" -X- _ O
terrible -X- _ O
" -X- _ O
at -X- _ O
the -X- _ O
mask -X- _ O
position -X- _ O
. -X- _ O

In -X- _ O
promptoriented -X- _ O
fine -X- _ O
- -X- _ O
tuning -X- _ O
, -X- _ O
data -X- _ O
samples -X- _ O
are -X- _ O
converted -X- _ O
to -X- _ O
sequences -X- _ O
containing -X- _ O
prompt -X- _ O
tokens -X- _ O
, -X- _ O
and -X- _ O
downstream -X- _ O
tasks -X- _ O
are -X- _ O
formalized -X- _ O
as -X- _ O
language -X- _ O
modeling -X- _ O
problems -X- _ O
. -X- _ O

The -X- _ O
second -X- _ O
one -X- _ O
is -X- _ O
prompt -X- _ O
- -X- _ O
oriented -X- _ O
finetuning -X- _ O
( -X- _ O
Schick -X- _ O
and -X- _ O
Schütze -X- _ O
, -X- _ O
2021a -X- _ O
) -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
inspired -X- _ O
by -X- _ O
the -X- _ O
recent -X- _ O
works -X- _ O
utilizing -X- _ O
language -X- _ O
prompts -X- _ O
to -X- _ O
probe -X- _ O
the -X- _ O
knowledge -X- _ O
in -X- _ O
PLMs -X- _ O
( -X- _ O
Petroni -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019;Brown -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
first -X- _ O
one -X- _ O
is -X- _ O
task -X- _ O
- -X- _ O
oriented -X- _ O
fine -X- _ O
- -X- _ O
tuning -X- _ O
, -X- _ O
where -X- _ O
a -X- _ O
task -X- _ O
- -X- _ O
specific -X- _ O
head -X- _ O
is -X- _ O
added -X- _ O
on -X- _ O
top -X- _ O
of -X- _ O
PLMs -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
entire -X- _ O
model -X- _ O
is -X- _ O
then -X- _ O
fine -X- _ O
- -X- _ O
tuned -X- _ O
by -X- _ O
optimizing -X- _ O
task -X- _ O
- -X- _ O
specific -X- _ O
objectives -X- _ O
on -X- _ O
corresponding -X- _ O
training -X- _ O
data -X- _ O
. -X- _ O

As -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
1 -X- _ O
( -X- _ O
b -X- _ O
) -X- _ O
and -X- _ O
( -X- _ O
c -X- _ O
) -X- _ O
, -X- _ O
there -X- _ O
are -X- _ O
two -X- _ O
mainstream -X- _ O
FT -X- _ B-MethodName
approaches -X- _ O
. -X- _ O

For -X- _ O
simplicity -X- _ O
, -X- _ O
we -X- _ O
name -X- _ O
this -X- _ O
full -X- _ B-MethodName
- -X- _ I-MethodName
model -X- _ I-MethodName
tuning -X- _ I-MethodName
as -X- _ O
" -X- _ O
FT -X- _ B-MethodName
" -X- _ O
. -X- _ O

various -X- _ O
NLP -X- _ O
tasks -X- _ O
and -X- _ O
outperform -X- _ O
the -X- _ O
approach -X- _ O
of -X- _ O
learning -X- _ O
models -X- _ O
from -X- _ O
scratch -X- _ O
( -X- _ O
Han -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021a -X- _ O
) -X- _ O
. -X- _ O

* -X- _ O
indicates -X- _ O
equal -X- _ O
contribution -X- _ O
. -X- _ O

By -X- _ O
tuning -X- _ O
the -X- _ O
entire -X- _ O
model -X- _ O
parameters -X- _ O
, -X- _ O
the -X- _ O
versatile -X- _ O
knowledge -X- _ O
acquired -X- _ O
from -X- _ O
large -X- _ O
- -X- _ O
scale -X- _ O
unlabeled -X- _ O
corpora -X- _ O
can -X- _ O
be -X- _ O
adapted -X- _ O
to -X- _ O
handling -X- _ O
† -X- _ O
Corresponding -X- _ O
author -X- _ O
. -X- _ O

Fine -X- _ O
- -X- _ O
tuning -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
language -X- _ O
models -X- _ O
( -X- _ O
PLMs -X- _ O
) -X- _ O
( -X- _ O
Devlin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019;Radford -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019;Raffel -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
has -X- _ O
made -X- _ O
great -X- _ O
progress -X- _ O
in -X- _ O
recent -X- _ O
years -X- _ O
. -X- _ O

Introduction -X- _ O
. -X- _ O

Our -X- _ O
approach -X- _ O
is -X- _ O
effective -X- _ O
and -X- _ O
efficient -X- _ O
for -X- _ O
using -X- _ O
large -X- _ O
- -X- _ O
scale -X- _ O
PLMs -X- _ O
in -X- _ O
practice -X- _ O
. -X- _ O

Extensive -X- _ O
experiments -X- _ O
show -X- _ O
that -X- _ O
tuning -X- _ B-MethodName
pre -X- _ I-MethodName
- -X- _ I-MethodName
trained -X- _ I-MethodName
prompts -X- _ I-MethodName
for -X- _ O
downstream -X- _ O
tasks -X- _ O
can -X- _ O
reach -X- _ O
or -X- _ O
even -X- _ O
outperform -X- _ O
full -X- _ B-MethodName
- -X- _ I-MethodName
model -X- _ I-MethodName
fine -X- _ I-MethodName
- -X- _ I-MethodName
tuning -X- _ I-MethodName
under -X- _ O
both -X- _ O
full -X- _ O
- -X- _ O
data -X- _ O
and -X- _ O
few -X- _ O
- -X- _ O
shot -X- _ O
settings -X- _ O
. -X- _ O

Therefore -X- _ O
, -X- _ O
in -X- _ O
this -X- _ O
work -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
to -X- _ O
pre -X- _ O
- -X- _ O
train -X- _ O
prompts -X- _ O
by -X- _ O
adding -X- _ O
soft -X- _ O
prompts -X- _ O
into -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
stage -X- _ O
to -X- _ O
obtain -X- _ O
a -X- _ O
better -X- _ O
initialization -X- _ O
. -X- _ O

We -X- _ O
attribute -X- _ O
this -X- _ O
low -X- _ O
performance -X- _ O
to -X- _ O
the -X- _ O
manner -X- _ O
of -X- _ O
initializing -X- _ O
soft -X- _ O
prompts -X- _ O
. -X- _ O

Prompts -X- _ O
for -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
language -X- _ O
models -X- _ O
( -X- _ O
PLMs -X- _ O
) -X- _ O
have -X- _ O
shown -X- _ O
remarkable -X- _ O
performance -X- _ O
by -X- _ O
bridging -X- _ O
the -X- _ O
gap -X- _ O
between -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
tasks -X- _ O
and -X- _ O
various -X- _ O
downstream -X- _ O
tasks -X- _ O
. -X- _ O

For -X- _ O
English -X- _ O
experiments -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
a -X- _ O
dataset -X- _ O
from -X- _ O
GLUE -X- _ O
( -X- _ O
Wang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019b -X- _ O
) -X- _ O
( -X- _ O
SST-2 -X- _ B-DatasetName
( -X- _ O
Socher -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2013 -X- _ O
) -X- _ O
) -X- _ O
, -X- _ O
datasets -X- _ O
from -X- _ O
Su -X- _ O
- -X- _ O
perGLUE -X- _ O
( -X- _ O
Wang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019a -X- _ O
) -X- _ O
, -X- _ O
( -X- _ O
BoolQ -X- _ B-DatasetName
( -X- _ O
Clark -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
CB -X- _ O
( -X- _ O
De -X- _ O
Marneffe -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
andRTE -X- _ O
( -X- _ O
Dagan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2006 -X- _ O
) -X- _ O
) -X- _ O
, -X- _ O
two -X- _ O
extra -X- _ O
single -X- _ O
- -X- _ O
text -X- _ O
classification -X- _ O
datasets -X- _ O
( -X- _ O
SST-5 -X- _ O
( -X- _ O
Socher -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2013 -X- _ O
) -X- _ O
and -X- _ O
YahooAnswers -X- _ O
( -X- _ O
Zhang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015b -X- _ O
) -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
two -X- _ O
standard -X- _ O
question -X- _ O
answering -X- _ O
datasets -X- _ O
( -X- _ O
RACEmiddle -X- _ O
and -X- _ O
RACE -X- _ O
- -X- _ O
high -X- _ O
) -X- _ O
( -X- _ O
Lai -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
for -X- _ O
multiple -X- _ B-TaskName
- -X- _ I-TaskName
choice -X- _ I-TaskName
classification -X- _ I-TaskName
. -X- _ O

For -X- _ O
PPT -X- _ B-MethodName
, -X- _ O
the -X- _ O
consump- -X- _ O
. -X- _ O

We -X- _ O
analyze -X- _ O
the -X- _ O
time -X- _ O
and -X- _ O
memory -X- _ O
consumption -X- _ O
of -X- _ O
FT -X- _ B-MethodName
and -X- _ O
PT -X- _ B-MethodName
in -X- _ O
this -X- _ O
section -X- _ O
. -X- _ O

For -X- _ O
Unified -X- _ B-MethodName
PPT -X- _ I-MethodName
, -X- _ O
we -X- _ O
uniformly -X- _ O
sample -X- _ O
the -X- _ O
option -X- _ O
numbers -X- _ O
from -X- _ O
2 -X- _ O
to -X- _ O
16 -X- _ O
to -X- _ O
cover -X- _ O
more -X- _ O
downstream -X- _ O
circumstances -X- _ O
. -X- _ O

Multiple -X- _ B-TaskName
- -X- _ I-TaskName
Choice -X- _ I-TaskName
Classification -X- _ I-TaskName
In -X- _ O
the -X- _ O
next -X- _ O
sentence -X- _ O
selection -X- _ O
task -X- _ O
, -X- _ O
giving -X- _ O
a -X- _ O
query -X- _ O
sentence -X- _ O
, -X- _ O
the -X- _ O
options -X- _ O
contain -X- _ O
one -X- _ O
adjacent -X- _ O
sentence -X- _ O
, -X- _ O
one -X- _ O
sentence -X- _ O
from -X- _ O
the -X- _ O
same -X- _ O
document -X- _ O
as -X- _ O
the -X- _ O
query -X- _ O
, -X- _ O
and -X- _ O
four -X- _ O
from -X- _ O
the -X- _ O
different -X- _ O
documents -X- _ O
. -X- _ O

We -X- _ O
filter -X- _ O
out -X- _ O
the -X- _ O
sentences -X- _ O
with -X- _ O
less -X- _ O
than -X- _ O
5 -X- _ O
tokens -X- _ O
and -X- _ O
the -X- _ O
pairs -X- _ O
in -X- _ O
which -X- _ O
the -X- _ O
two -X- _ O
sentences -X- _ O
' -X- _ O
length -X- _ O
ratios -X- _ O
are -X- _ O
larger -X- _ O
than -X- _ O
100 -X- _ B-HyperparameterValue
. -X- _ O

This -X- _ O
observation -X- _ O
also -X- _ O
implies -X- _ O
that -X- _ O
PT -X- _ B-MethodName
is -X- _ O
much -X- _ O
harder -X- _ O
to -X- _ O
train -X- _ O
than -X- _ O
FT -X- _ B-MethodName
, -X- _ O
which -X- _ O
is -X- _ O
consistent -X- _ O
with -X- _ O
the -X- _ O
experiment -X- _ O
results -X- _ O
in -X- _ O
the -X- _ O
main -X- _ O
paper -X- _ O
. -X- _ O

We -X- _ O
find -X- _ O
PT -X- _ B-MethodName
requires -X- _ O
a -X- _ O
much -X- _ O
larger -X- _ O
learning -X- _ O
rate -X- _ O
than -X- _ O
FT -X- _ B-MethodName
. -X- _ O

Since -X- _ O
the -X- _ O
tunable -X- _ O
parameters -X- _ O
are -X- _ O
much -X- _ O
less -X- _ O
in -X- _ O
PT -X- _ B-MethodName
, -X- _ O
8 -X- _ O
NVIDIA -X- _ O
V100 -X- _ O
32 -X- _ O
G -X- _ O
GPUs -X- _ O
are -X- _ O
enough -X- _ O
for -X- _ O
the -X- _ O
training -X- _ O
. -X- _ O

For -X- _ O
Prompt -X- _ B-MethodName
Tuning -X- _ I-MethodName
( -X- _ O
PT -X- _ B-MethodName
) -X- _ O
, -X- _ O
we -X- _ O
add -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
soft -X- _ O
prompts -X- _ O
before -X- _ O
the -X- _ O
input -X- _ O
text -X- _ O
. -X- _ O

C.2 -X- _ O
Prompt -X- _ B-MethodName
Tuning -X- _ I-MethodName
. -X- _ O

For -X- _ O
Full -X- _ B-MethodName
- -X- _ I-MethodName
Model -X- _ I-MethodName
Tuning -X- _ I-MethodName
( -X- _ O
FT -X- _ B-MethodName
) -X- _ O
, -X- _ O
we -X- _ O
tune -X- _ O
the -X- _ O
entire -X- _ O
parameters -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
without -X- _ O
concatenating -X- _ O
soft -X- _ O
prompts -X- _ O
. -X- _ O

C.1 -X- _ O
Full -X- _ B-MethodName
- -X- _ I-MethodName
Model -X- _ I-MethodName
Tuning -X- _ I-MethodName
. -X- _ O

Extensive -X- _ O
experiments -X- _ O
show -X- _ O
that -X- _ O
our -X- _ O
method -X- _ O
significantly -X- _ O
outperforms -X- _ O
other -X- _ O
prompt -X- _ B-MethodName
tuning -X- _ I-MethodName
baselines -X- _ O
, -X- _ O
performing -X- _ O
comparable -X- _ O
or -X- _ O
even -X- _ O
better -X- _ O
than -X- _ O
full -X- _ B-MethodName
- -X- _ I-MethodName
model -X- _ I-MethodName
tuning -X- _ I-MethodName
. -X- _ O

Finally -X- _ O
, -X- _ O
we -X- _ O
do -X- _ O
prompt -X- _ B-MethodName
tuning -X- _ I-MethodName
on -X- _ O
downstream -X- _ O
tasks -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
initialization -X- _ O
. -X- _ O

When -X- _ O
models -X- _ O
are -X- _ O
large -X- _ O
enough -X- _ O
, -X- _ O
this -X- _ O
method -X- _ O
can -X- _ O
be -X- _ O
comparable -X- _ O
to -X- _ O
full -X- _ B-MethodName
- -X- _ I-MethodName
model -X- _ I-MethodName
tuning -X- _ I-MethodName
. -X- _ O

We -X- _ O
argue -X- _ O
that -X- _ O
PPT -X- _ B-MethodName
can -X- _ O
be -X- _ O
an -X- _ O
effective -X- _ O
solution -X- _ O
to -X- _ O
this -X- _ O
problem -X- _ O
. -X- _ O

In -X- _ O
addition -X- _ O
, -X- _ O
we -X- _ O
observe -X- _ O
that -X- _ O
although -X- _ O
PT -X- _ B-MethodName
is -X- _ O
faster -X- _ O
than -X- _ O
FT -X- _ B-MethodName
in -X- _ O
a -X- _ O
single -X- _ O
optimization -X- _ O
step -X- _ O
, -X- _ O
it -X- _ O
converges -X- _ O
much -X- _ O
slower -X- _ O
, -X- _ O
which -X- _ O
results -X- _ O
in -X- _ O
an -X- _ O
even -X- _ O
longer -X- _ O
training -X- _ O
time -X- _ O
. -X- _ O

From -X- _ O
Table -X- _ O
6 -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
see -X- _ O
that -X- _ O
PPT -X- _ B-MethodName
and -X- _ O
Unified -X- _ B-MethodName
PPT -X- _ I-MethodName
still -X- _ O
outperform -X- _ O
the -X- _ O
Vanilla -X- _ B-MethodName
PT -X- _ I-MethodName
on -X- _ O
most -X- _ O
datasets -X- _ O
. -X- _ O

We -X- _ O
discuss -X- _ O
how -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
FT -X- _ B-MethodName
, -X- _ O
PT -X- _ B-MethodName
, -X- _ O
and -X- _ O
PPT -X- _ B-MethodName
varies -X- _ O
when -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
training -X- _ O
samples -X- _ O
increases -X- _ O
. -X- _ O

Table -X- _ O
6 -X- _ O
: -X- _ O
The -X- _ O
performance -X- _ O
of -X- _ O
FT -X- _ B-MethodName
, -X- _ O
PT -X- _ B-MethodName
, -X- _ O
PPT -X- _ B-MethodName
, -X- _ O
and -X- _ O
Unified -X- _ B-MethodName
PPT -X- _ I-MethodName
when -X- _ O
the -X- _ O
full -X- _ O
training -X- _ O
datasets -X- _ O
are -X- _ O
available -X- _ O
. -X- _ O

For -X- _ O
the -X- _ O
small -X- _ O
number -X- _ O
of -X- _ O
samples -X- _ O
, -X- _ O
PPT -X- _ B-MethodName
is -X- _ O
consistently -X- _ O
better -X- _ O
than -X- _ O
Vanilla -X- _ B-MethodName
PT -X- _ I-MethodName
. -X- _ O

However -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
see -X- _ O
that -X- _ O
Unified -X- _ B-MethodName
PPT -X- _ I-MethodName
still -X- _ O
achieves -X- _ O
the -X- _ O
best -X- _ O
performance -X- _ O
, -X- _ O
even -X- _ O
exceeding -X- _ O
FT -X- _ B-MethodName
by -X- _ O
a -X- _ O
large -X- _ O
margin -X- _ O
. -X- _ O

We -X- _ O
do -X- _ O
not -X- _ O
use -X- _ O
PPT -X- _ B-MethodName
for -X- _ O
singlesentence -X- _ O
classification -X- _ O
discussed -X- _ O
in -X- _ O
Section -X- _ O
3.2.3 -X- _ O
because -X- _ O
it -X- _ O
is -X- _ O
hard -X- _ O
to -X- _ O
find -X- _ O
other -X- _ O
suitable -X- _ O
datasets -X- _ O
to -X- _ O
train -X- _ O
the -X- _ O
pseudo -X- _ O
label -X- _ O
annotator -X- _ O
. -X- _ O

PT -X- _ B-MethodName
( -X- _ O
MC -X- _ B-TaskName
) -X- _ O
means -X- _ O
we -X- _ O
solve -X- _ O
the -X- _ O
task -X- _ O
in -X- _ O
a -X- _ O
multiple -X- _ B-TaskName
- -X- _ I-TaskName
choice -X- _ I-TaskName
classification -X- _ I-TaskName
format -X- _ O
without -X- _ O
prompt -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
. -X- _ O

For -X- _ O
some -X- _ O
datasets -X- _ O
like -X- _ O
SST-2 -X- _ B-DatasetName
, -X- _ O
the -X- _ O
variance -X- _ O
reaches -X- _ O
15.5 -X- _ O
which -X- _ O
means -X- _ O
the -X- _ O
model -X- _ O
does -X- _ O
not -X- _ O
perform -X- _ O
better -X- _ O
than -X- _ O
random -X- _ O
guesses -X- _ O
under -X- _ O
some -X- _ O
a -X- _ O
verbalizer -X- _ O
to -X- _ O
map -X- _ O
the -X- _ O
labels -X- _ O
to -X- _ O
the -X- _ O
intuitively -X- _ O
selected -X- _ O
words -X- _ O
. -X- _ O

Fourth -X- _ O
, -X- _ O
PPT -X- _ B-MethodName
results -X- _ O
in -X- _ O
lower -X- _ O
variances -X- _ O
on -X- _ O
most -X- _ O
of -X- _ O
the -X- _ O
datasets -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
since -X- _ O
we -X- _ O
focus -X- _ O
on -X- _ O
PT -X- _ B-MethodName
in -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
leave -X- _ O
this -X- _ O
as -X- _ O
future -X- _ O
work -X- _ O
. -X- _ O

Third -X- _ O
, -X- _ O
PPT -X- _ B-MethodName
outperforms -X- _ O
FT -X- _ B-MethodName
on -X- _ O
all -X- _ O
Chinese -X- _ O
datasets -X- _ O
and -X- _ O
most -X- _ O
English -X- _ O
datasets -X- _ O
. -X- _ O

Similar -X- _ O
phenomenons -X- _ O
are -X- _ O
observed -X- _ O
on -X- _ O
other -X- _ O
datasets -X- _ O
like -X- _ O
RACE -X- _ B-DatasetName
- -X- _ I-DatasetName
m -X- _ I-DatasetName
, -X- _ O
LCQMC -X- _ B-DatasetName
, -X- _ O
and -X- _ O
C -X- _ B-DatasetName
3 -X- _ I-DatasetName
, -X- _ O
where -X- _ O
adding -X- _ O
hard -X- _ O
prompts -X- _ O
to -X- _ O
PPT -X- _ B-MethodName
continues -X- _ O
to -X- _ O
improve -X- _ O
results -X- _ O
. -X- _ O

Although -X- _ O
PPT -X- _ B-MethodName
is -X- _ O
worse -X- _ O
than -X- _ O
Hybrid -X- _ B-MethodName
PT -X- _ I-MethodName
on -X- _ O
BoolQ -X- _ B-DatasetName
, -X- _ O
combining -X- _ O
PPT -X- _ B-MethodName
and -X- _ O
hard -X- _ O
prompts -X- _ O
( -X- _ O
Hybrid -X- _ B-MethodName
PPT -X- _ I-MethodName
) -X- _ O
outperforms -X- _ O
all -X- _ O
baselines -X- _ O
. -X- _ O

Second -X- _ O
, -X- _ O
PPT -X- _ B-MethodName
outperforms -X- _ O
Vanilla -X- _ B-MethodName
PT -X- _ I-MethodName
and -X- _ O
LM -X- _ B-MethodName
Adaption -X- _ I-MethodName
on -X- _ O
most -X- _ O
datasets -X- _ O
significantly -X- _ O
. -X- _ O

Since -X- _ O
CPM-2 -X- _ B-MethodName
outperforms -X- _ O
mT5 -X- _ B-MethodName
- -X- _ I-MethodName
XXL -X- _ I-MethodName
across -X- _ O
all -X- _ O
tasks -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
CPM-2 -X- _ B-MethodName
as -X- _ O
the -X- _ O
base -X- _ O
model -X- _ O
. -X- _ O

Note -X- _ O
that -X- _ O
for -X- _ O
Chinese -X- _ O
experiments -X- _ O
, -X- _ O
CPM-2 -X- _ B-MethodName
and -X- _ O
mT5 -X- _ B-MethodName
- -X- _ I-MethodName
XXL -X- _ I-MethodName
share -X- _ O
the -X- _ O
same -X- _ O
parameter -X- _ O
scale -X- _ O
. -X- _ O

Therefore -X- _ O
, -X- _ O
we -X- _ O
study -X- _ O
PT -X- _ B-MethodName
on -X- _ O
the -X- _ O
large -X- _ O
- -X- _ O
scale -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
model -X- _ O
. -X- _ O

We -X- _ O
test -X- _ O
two -X- _ O
variants -X- _ O
of -X- _ O
PPT -X- _ B-MethodName
: -X- _ O
Hybrid -X- _ B-MethodName
PPT -X- _ I-MethodName
, -X- _ O
in -X- _ O
which -X- _ O
carefully -X- _ O
designed -X- _ O
hard -X- _ O
prompts -X- _ O
are -X- _ O
combined -X- _ O
with -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
soft -X- _ O
prompt -X- _ O
, -X- _ O
and -X- _ O
Unified -X- _ B-MethodName
PPT -X- _ I-MethodName
, -X- _ O
in -X- _ O
which -X- _ O
all -X- _ O
tasks -X- _ O
are -X- _ O
unified -X- _ O
in -X- _ O
the -X- _ O
multiple -X- _ B-TaskName
- -X- _ I-TaskName
choice -X- _ I-TaskName
classification -X- _ I-TaskName
format -X- _ O
. -X- _ O

( -X- _ O
2021 -X- _ O
) -X- _ O
in -X- _ O
which -X- _ O
the -X- _ O
T5 -X- _ B-MethodName
model -X- _ O
is -X- _ O
further -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
for -X- _ O
10 -X- _ O
K -X- _ O
steps -X- _ O
with -X- _ O
language -X- _ O
modeling -X- _ O
to -X- _ O
reduce -X- _ O
the -X- _ O
gap -X- _ O
between -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
and -X- _ O
PT -X- _ B-MethodName
. -X- _ O

We -X- _ O
also -X- _ O
consider -X- _ O
LM -X- _ B-MethodName
Adaption -X- _ I-MethodName
used -X- _ O
in -X- _ O
Lester -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

The -X- _ O
first -X- _ O
baseline -X- _ O
is -X- _ O
Vanilla -X- _ B-MethodName
PT -X- _ I-MethodName
, -X- _ O
where -X- _ O
the -X- _ O
soft -X- _ O
prompts -X- _ O
are -X- _ O
randomly -X- _ O
initialized -X- _ O
from -X- _ O
a -X- _ O
normal -X- _ O
distribution -X- _ O
. -X- _ O

In -X- _ O
the -X- _ O
block -X- _ O
PT -X- _ B-MethodName
, -X- _ O
we -X- _ O
show -X- _ O
the -X- _ O
results -X- _ O
of -X- _ O
PPT -X- _ B-MethodName
and -X- _ O
other -X- _ O
baselines -X- _ O
. -X- _ O

In -X- _ O
the -X- _ O
block -X- _ O
FT -X- _ B-MethodName
, -X- _ O
we -X- _ O
present -X- _ O
the -X- _ O
FT -X- _ B-MethodName
results -X- _ O
of -X- _ O
the -X- _ O
T5 -X- _ B-MethodName
model -X- _ O
from -X- _ O
the -X- _ O
size -X- _ O
small -X- _ O
to -X- _ O
XXL -X- _ O
. -X- _ O

Compared -X- _ O
with -X- _ O
the -X- _ O
11B -X- _ O
( -X- _ O
1.1 -X- _ O
× -X- _ O
10 -X- _ O
10 -X- _ O
) -X- _ O
parameters -X- _ O
of -X- _ O
FT -X- _ B-MethodName
, -X- _ O
PT -X- _ B-MethodName
only -X- _ O
needs -X- _ O
to -X- _ O
store -X- _ O
3000 -X- _ O
times -X- _ O
smaller -X- _ O
parameters -X- _ O
for -X- _ O
each -X- _ O
task -X- _ O
. -X- _ O

Consistently -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
100 -X- _ B-HyperparameterValue
soft -X- _ B-HyperparameterName
tokens -X- _ I-HyperparameterName
for -X- _ O
PT -X- _ B-MethodName
. -X- _ O

For -X- _ O
Chinese -X- _ O
datasets -X- _ O
, -X- _ O
we -X- _ O
do -X- _ O
PT -X- _ B-MethodName
based -X- _ O
on -X- _ O
a -X- _ O
11B -X- _ O
model -X- _ O
CPM-2 -X- _ B-MethodName
. -X- _ O

We -X- _ O
also -X- _ O
evaluate -X- _ O
FT -X- _ B-MethodName
on -X- _ O
various -X- _ O
sizes -X- _ O
of -X- _ O
T5 -X- _ B-MethodName
to -X- _ O
verify -X- _ O
that -X- _ O
larger -X- _ O
models -X- _ O
perform -X- _ O
better -X- _ O
and -X- _ O
thus -X- _ O
improving -X- _ O
PT -X- _ B-MethodName
based -X- _ O
on -X- _ O
T5 -X- _ B-MethodName
- -X- _ I-MethodName
XXL -X- _ I-MethodName
is -X- _ O
meaningful -X- _ O
. -X- _ O

For -X- _ O
English -X- _ O
datasets -X- _ O
, -X- _ O
we -X- _ O
conduct -X- _ O
PT -X- _ B-MethodName
based -X- _ O
on -X- _ O
T5 -X- _ B-MethodName
- -X- _ I-MethodName
XXL -X- _ I-MethodName
with -X- _ O
11B -X- _ O
parameters -X- _ O
because -X- _ O
previous -X- _ O
works -X- _ O
( -X- _ O
Lester -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021;Zhang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
have -X- _ O
shown -X- _ O
that -X- _ O
, -X- _ O
T5 -X- _ B-MethodName
- -X- _ I-MethodName
XXL -X- _ I-MethodName
is -X- _ O
comparable -X- _ O
with -X- _ O
FT -X- _ B-MethodName
under -X- _ O
the -X- _ O
full -X- _ O
- -X- _ O
data -X- _ O
setting -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
our -X- _ O
PPT -X- _ B-MethodName
focuses -X- _ O
on -X- _ O
tuning -X- _ O
soft -X- _ O
prompts -X- _ O
with -X- _ O
the -X- _ O
main -X- _ O
body -X- _ O
of -X- _ O
PLMs -X- _ O
fixed -X- _ O
and -X- _ O
our -X- _ O
pretraining -X- _ O
is -X- _ O
conducted -X- _ O
on -X- _ O
fully -X- _ O
unsupervised -X- _ O
data -X- _ O
, -X- _ O
rather -X- _ O
than -X- _ O
the -X- _ O
collection -X- _ O
of -X- _ O
supervised -X- _ O
datasets -X- _ O
. -X- _ O

Specifically -X- _ O
, -X- _ O
for -X- _ O
sentence -X- _ B-TaskName
- -X- _ I-TaskName
pair -X- _ I-TaskName
classification -X- _ I-TaskName
, -X- _ O
the -X- _ O
query -X- _ O
is -X- _ O
the -X- _ O
concatenation -X- _ O
of -X- _ O
the -X- _ O
two -X- _ O
sentences -X- _ O
and -X- _ O
there -X- _ O
are -X- _ O
three -X- _ O
options -X- _ O
: -X- _ O
no -X- _ O
, -X- _ O
maybe -X- _ O
, -X- _ O
and -X- _ O
yes -X- _ O
. -X- _ O

The -X- _ O
above -X- _ O
- -X- _ O
mentioned -X- _ O
PVPs -X- _ O
for -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
can -X- _ O
be -X- _ O
unified -X- _ O
to -X- _ O
a -X- _ O
single -X- _ O
format -X- _ O
: -X- _ O
multiple -X- _ B-TaskName
- -X- _ I-TaskName
choice -X- _ I-TaskName
classification -X- _ I-TaskName
. -X- _ O

In -X- _ O
practice -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
a -X- _ O
RoBERTa -X- _ B-MethodName
BASE -X- _ I-MethodName
( -X- _ O
Liu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
model -X- _ O
fine -X- _ O
- -X- _ O
tuned -X- _ O
on -X- _ O
a -X- _ O
5 -X- _ O
- -X- _ O
class -X- _ O
sentiment -X- _ O
classification -X- _ O
dataset -X- _ O
other -X- _ O
than -X- _ O
the -X- _ O
few -X- _ O
- -X- _ O
shot -X- _ O
datasets -X- _ O
we -X- _ O
evaluate -X- _ O
on -X- _ O
. -X- _ O

Many -X- _ O
tasks -X- _ O
can -X- _ O
be -X- _ O
formulated -X- _ O
as -X- _ O
multiple -X- _ B-TaskName
- -X- _ I-TaskName
choice -X- _ I-TaskName
classification -X- _ I-TaskName
, -X- _ O
which -X- _ O
takes -X- _ O
a -X- _ O
query -X- _ O
and -X- _ O
several -X- _ O
answer -X- _ O
candidates -X- _ O
as -X- _ O
the -X- _ O
input -X- _ O
. -X- _ O

Multiple -X- _ B-TaskName
- -X- _ I-TaskName
Choice -X- _ I-TaskName
Classification -X- _ I-TaskName
. -X- _ O

Sentence -X- _ B-TaskName
- -X- _ I-TaskName
Pair -X- _ I-TaskName
Classification -X- _ I-TaskName
. -X- _ O

For -X- _ O
instance -X- _ O
, -X- _ O
some -X- _ O
tasks -X- _ O
in -X- _ O
the -X- _ O
form -X- _ O
of -X- _ O
sentence -X- _ B-TaskName
- -X- _ I-TaskName
pair -X- _ I-TaskName
classification -X- _ I-TaskName
, -X- _ O
such -X- _ O
as -X- _ O
natural -X- _ B-TaskName
language -X- _ I-TaskName
inference -X- _ I-TaskName
and -X- _ O
sentence -X- _ O
similarity -X- _ O
, -X- _ O
are -X- _ O
similar -X- _ O
to -X- _ O
the -X- _ O
next -X- _ O
sentence -X- _ O
prediction -X- _ O
( -X- _ O
NSP -X- _ O
) -X- _ O
( -X- _ O
Devlin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
task -X- _ O
used -X- _ O
in -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
stage -X- _ O
. -X- _ O

The -X- _ O
parameter -X- _ O
initialization -X- _ O
usually -X- _ O
has -X- _ O
a -X- _ O
large -X- _ O
impact -X- _ O
on -X- _ O
the -X- _ O
difficulty -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
training -X- _ O
and -X- _ O
optimization -X- _ O
, -X- _ O
and -X- _ O
our -X- _ O
pilot -X- _ O
experiments -X- _ O
have -X- _ O
shown -X- _ O
that -X- _ O
existing -X- _ O
initialization -X- _ O
strategies -X- _ O
have -X- _ O
little -X- _ O
or -X- _ O
even -X- _ O
negative -X- _ O
impact -X- _ O
on -X- _ O
the -X- _ O
PT -X- _ B-MethodName
performance -X- _ O
of -X- _ O
large -X- _ O
- -X- _ O
scale -X- _ O
PLMs -X- _ O
. -X- _ O
We -X- _ O
refer -X- _ O
more -X- _ O
details -X- _ O
of -X- _ O
these -X- _ O
pilot -X- _ O
experiments -X- _ O
to -X- _ O
Section -X- _ O
4 -X- _ O
. -X- _ O

By -X- _ O
tuning -X- _ O
P -X- _ O
, -X- _ O
Eq -X- _ O
. -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
is -X- _ O
replaced -X- _ O
by -X- _ O
arg -X- _ O
max -X- _ O
P -X- _ O
x -X- _ O
log -X- _ O
p -X- _ O
X -X- _ O
= -X- _ O
v(y -X- _ O
) -X- _ O
| -X- _ O
[ -X- _ O
P -X- _ O
; -X- _ O
f -X- _ O
( -X- _ O
x -X- _ O
) -X- _ O
] -X- _ O
; -X- _ O
P -X- _ O
.(2 -X- _ O
) -X- _ O
Owing -X- _ O
to -X- _ O
the -X- _ O
power -X- _ O
of -X- _ O
large -X- _ O
- -X- _ O
scale -X- _ O
PLMs -X- _ O
, -X- _ O
Eq -X- _ O
. -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
is -X- _ O
verified -X- _ O
to -X- _ O
be -X- _ O
comparable -X- _ O
to -X- _ O
these -X- _ O
FT -X- _ B-MethodName
methods -X- _ O
under -X- _ O
full -X- _ O
- -X- _ O
data -X- _ O
settings -X- _ O
. -X- _ O

In -X- _ O
PT -X- _ B-MethodName
( -X- _ O
Lester -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
, -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
soft -X- _ O
prompts -X- _ O
P -X- _ O
are -X- _ O
concatenated -X- _ O
to -X- _ O
the -X- _ O
beginning -X- _ O
of -X- _ O
the -X- _ O
sequence -X- _ O
and -X- _ O
the -X- _ O
model -X- _ O
input -X- _ O
becomes -X- _ O
[ -X- _ O
P -X- _ O
; -X- _ O
f -X- _ O
( -X- _ O
x -X- _ O
) -X- _ O
] -X- _ O
, -X- _ O
where -X- _ O
[ -X- _ O
• -X- _ O
; -X- _ O
• -X- _ O
] -X- _ O
is -X- _ O
the -X- _ O
concatenation -X- _ O
operation -X- _ O
. -X- _ O

Following -X- _ O
the -X- _ O
approach -X- _ O
of -X- _ O
T5 -X- _ B-MethodName
( -X- _ O
Raffel -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
and -X- _ O
PT -X- _ B-MethodName
( -X- _ O
Lester -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
solve -X- _ O
all -X- _ O
downstream -X- _ O
tasks -X- _ O
in -X- _ O
a -X- _ O
text -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
text -X- _ O
format -X- _ O
. -X- _ O

In -X- _ O
the -X- _ O
following -X- _ O
sections -X- _ O
, -X- _ O
we -X- _ O
describe -X- _ O
our -X- _ O
PPT -X- _ B-MethodName
framework -X- _ O
and -X- _ O
show -X- _ O
in -X- _ O
experiments -X- _ O
that -X- _ O
PPT -X- _ B-MethodName
not -X- _ O
only -X- _ O
provides -X- _ O
a -X- _ O
good -X- _ O
prompt -X- _ O
initialization -X- _ O
, -X- _ O
but -X- _ O
also -X- _ O
takes -X- _ O
advantage -X- _ O
of -X- _ O
the -X- _ O
good -X- _ O
verbalizer -X- _ O
, -X- _ O
and -X- _ O
is -X- _ O
complementary -X- _ O
to -X- _ O
hybrid -X- _ O
prompts -X- _ O
. -X- _ O

To -X- _ O
summarize -X- _ O
, -X- _ O
although -X- _ O
the -X- _ O
above -X- _ O
enhancement -X- _ O
strategies -X- _ O
can -X- _ O
not -X- _ O
help -X- _ O
PT -X- _ B-MethodName
achieve -X- _ O
comparable -X- _ O
results -X- _ O
with -X- _ O
FT -X- _ B-MethodName
under -X- _ O
few -X- _ O
- -X- _ O
shot -X- _ O
settings -X- _ O
, -X- _ O
they -X- _ O
are -X- _ O
still -X- _ O
the -X- _ O
key -X- _ O
factors -X- _ O
that -X- _ O
influence -X- _ O
the -X- _ O
PT -X- _ B-MethodName
performance -X- _ O
. -X- _ O

This -X- _ O
also -X- _ O
guides -X- _ O
our -X- _ O
verbalizer -X- _ O
selection -X- _ O
for -X- _ O
PPT -X- _ B-MethodName
in -X- _ O
Section -X- _ O
3 -X- _ O
. -X- _ O

For -X- _ O
instance -X- _ O
, -X- _ O
1 -X- _ O
Using -X- _ O
100 -X- _ B-HyperparameterValue
soft -X- _ O
prompt -X- _ O
tokens -X- _ O
achieves -X- _ O
the -X- _ O
best -X- _ O
performance -X- _ O
in -X- _ O
Lester -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

We -X- _ O
can -X- _ O
see -X- _ O
that -X- _ O
hard -X- _ O
prompts -X- _ O
improve -X- _ O
PT -X- _ B-MethodName
, -X- _ O
but -X- _ O
still -X- _ O
under -X- _ O
- -X- _ O
perform -X- _ O
FT -X- _ B-MethodName
. -X- _ O

In -X- _ O
PT -X- _ B-MethodName
where -X- _ O
only -X- _ O
prompt -X- _ O
tokens -X- _ O
are -X- _ O
tunable -X- _ O
, -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
hybrid -X- _ O
prompts -X- _ O
is -X- _ O
under -X- _ O
- -X- _ O
explored -X- _ O
. -X- _ O

Besides -X- _ O
the -X- _ O
effectiveness -X- _ O
, -X- _ O
PPT -X- _ B-MethodName
also -X- _ O
retains -X- _ O
the -X- _ O
parameter -X- _ O
efficiency -X- _ O
of -X- _ O
PT -X- _ B-MethodName
, -X- _ O
which -X- _ O
is -X- _ O
valuable -X- _ O
for -X- _ O
future -X- _ O
applications -X- _ O
on -X- _ O
large -X- _ O
- -X- _ O
scale -X- _ O
PLMs -X- _ O
. -X- _ O
Pilot -X- _ O
Experiments -X- _ O
. -X- _ O

We -X- _ O
evaluate -X- _ O
PPT -X- _ B-MethodName
on -X- _ O
several -X- _ O
datasets -X- _ O
based -X- _ O
on -X- _ O
three -X- _ O
11B -X- _ O
PLMs -X- _ O
: -X- _ O
T5 -X- _ B-MethodName
- -X- _ I-MethodName
XXL -X- _ I-MethodName
( -X- _ O
Raffel -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
, -X- _ O
mT5 -X- _ B-MethodName
- -X- _ I-MethodName
XXL -X- _ I-MethodName
( -X- _ O
Xue -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
and -X- _ O
CPM-2 -X- _ B-MethodName
( -X- _ O
Zhang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
in -X- _ O
few -X- _ O
- -X- _ O
shot -X- _ O
scenarios -X- _ O
. -X- _ O

We -X- _ O
name -X- _ O
this -X- _ O
Pre -X- _ B-MethodName
- -X- _ I-MethodName
trained -X- _ I-MethodName
Prompt -X- _ I-MethodName
Tuning -X- _ I-MethodName
framework -X- _ O
" -X- _ O
PPT -X- _ B-MethodName
" -X- _ O
. -X- _ O

In -X- _ O
addition -X- _ O
, -X- _ O
we -X- _ O
find -X- _ O
multiple -X- _ B-TaskName
- -X- _ I-TaskName
choice -X- _ I-TaskName
classification -X- _ I-TaskName
more -X- _ O
general -X- _ O
among -X- _ O
these -X- _ O
formats -X- _ O
and -X- _ O
we -X- _ O
can -X- _ O
unify -X- _ O
all -X- _ O
classification -X- _ O
tasks -X- _ O
to -X- _ O
this -X- _ O
format -X- _ O
. -X- _ O

Our -X- _ O
discoveries -X- _ O
are -X- _ O
as -X- _ O
follows -X- _ O
: -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
the -X- _ O
verbalizer -X- _ O
choice -X- _ O
has -X- _ O
a -X- _ O
large -X- _ O
impact -X- _ O
on -X- _ O
the -X- _ O
performance -X- _ O
; -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
simply -X- _ O
initializing -X- _ O
soft -X- _ O
prompts -X- _ O
with -X- _ O
concrete -X- _ O
word -X- _ O
embeddings -X- _ O
fails -X- _ O
to -X- _ O
improve -X- _ O
the -X- _ O
performance -X- _ O
, -X- _ O
yet -X- _ O
( -X- _ O
3 -X- _ O
) -X- _ O
combining -X- _ O
soft -X- _ O
and -X- _ O
hard -X- _ O
prompts -X- _ O
is -X- _ O
helpful -X- _ O
; -X- _ O
and -X- _ O
( -X- _ O
4 -X- _ O
) -X- _ O
all -X- _ O
these -X- _ O
methods -X- _ O
can -X- _ O
not -X- _ O
handle -X- _ O
few -X- _ O
- -X- _ O
shot -X- _ O
prompt -X- _ B-MethodName
tuning -X- _ I-MethodName
problems -X- _ O
well -X- _ O
. -X- _ O

Specifically -X- _ O
, -X- _ O
we -X- _ O
con -X- _ O
- -X- _ O
duct -X- _ O
pilot -X- _ O
experiments -X- _ O
to -X- _ O
empirically -X- _ O
analyze -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
PT -X- _ B-MethodName
on -X- _ O
PLMs -X- _ O
in -X- _ O
Section -X- _ O
2 -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
ignored -X- _ O
by -X- _ O
most -X- _ O
existing -X- _ O
works -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
as -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
2(b -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
find -X- _ O
that -X- _ O
PT -X- _ B-MethodName
performs -X- _ O
much -X- _ O
worse -X- _ O
than -X- _ O
FT -X- _ B-MethodName
under -X- _ O
few -X- _ O
- -X- _ O
shot -X- _ O
settings -X- _ O
, -X- _ O
which -X- _ O
may -X- _ O
hinder -X- _ O
the -X- _ O
application -X- _ O
of -X- _ O
PT -X- _ B-MethodName
in -X- _ O
various -X- _ O
low -X- _ O
- -X- _ O
resource -X- _ O
scenarios -X- _ O
. -X- _ O

Second -X- _ O
, -X- _ O
PT -X- _ B-MethodName
is -X- _ O
an -X- _ O
efficient -X- _ O
and -X- _ O
effective -X- _ O
paradigm -X- _ O
for -X- _ O
the -X- _ O
practical -X- _ O
use -X- _ O
of -X- _ O
largescale -X- _ O
PLMs -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
comparable -X- _ O
to -X- _ O
FT -X- _ B-MethodName
when -X- _ O
downstream -X- _ O
data -X- _ O
are -X- _ O
sufficient -X- _ O
( -X- _ O
Figure -X- _ O
2(a -X- _ O
) -X- _ O
) -X- _ O
. -X- _ O

PT -X- _ B-MethodName
has -X- _ O
two -X- _ O
promising -X- _ O
advantages -X- _ O
. -X- _ O

To -X- _ O
avoid -X- _ O
storing -X- _ O
the -X- _ O
entire -X- _ O
model -X- _ O
for -X- _ O
each -X- _ O
downstream -X- _ O
task -X- _ O
, -X- _ O
PT -X- _ B-MethodName
freezes -X- _ O
all -X- _ O
PLM -X- _ O
parameters -X- _ O
and -X- _ O
merely -X- _ O
tunes -X- _ O
soft -X- _ O
prompts -X- _ O
, -X- _ O
without -X- _ O
adding -X- _ O
any -X- _ O
intermediate -X- _ O
layers -X- _ O
and -X- _ O
task -X- _ O
- -X- _ O
specific -X- _ O
components -X- _ O
. -X- _ O

Specifically -X- _ O
, -X- _ O
PT -X- _ B-MethodName
uses -X- _ O
soft -X- _ O
prompts -X- _ O
composed -X- _ O
of -X- _ O
continuous -X- _ O
embeddings -X- _ O
instead -X- _ O
of -X- _ O
hard -X- _ O
prompts -X- _ O
( -X- _ O
discrete -X- _ O
language -X- _ O
phrases -X- _ O
) -X- _ O
. -X- _ O

( -X- _ O
2021 -X- _ O
) -X- _ O
proposes -X- _ O
prompt -X- _ B-MethodName
tuning -X- _ I-MethodName
( -X- _ O
PT -X- _ B-MethodName
) -X- _ O
to -X- _ O
adapt -X- _ O
large -X- _ O
PLMs -X- _ O
to -X- _ O
downstream -X- _ O
tasks -X- _ O
cheaply -X- _ O
, -X- _ O
as -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
1 -X- _ O
( -X- _ O
d -X- _ O
) -X- _ O
. -X- _ O

Although -X- _ O
FT -X- _ B-MethodName
has -X- _ O
shown -X- _ O
promising -X- _ O
results -X- _ O
, -X- _ O
with -X- _ O
the -X- _ O
rapid -X- _ O
growth -X- _ O
of -X- _ O
model -X- _ O
scale -X- _ O
, -X- _ O
fine -X- _ O
- -X- _ O
tuning -X- _ O
and -X- _ O
storing -X- _ O
the -X- _ O
entire -X- _ O
large -X- _ O
model -X- _ O
for -X- _ O
each -X- _ O
downstream -X- _ O
task -X- _ O
becomes -X- _ O
much -X- _ O
more -X- _ O
expensive -X- _ O
. -X- _ O

The -X- _ O
code -X- _ O
is -X- _ O
publicly -X- _ O
available -X- _ O
at -X- _ O
https:// -X- _ O
github.com/thu-coai/PPT -X- _ B-MethodName
. -X- _ O

To -X- _ O
ensure -X- _ O
the -X- _ O
generalization -X- _ O
of -X- _ O
PPT -X- _ B-MethodName
, -X- _ O
we -X- _ O
formulate -X- _ O
similar -X- _ O
classification -X- _ O
tasks -X- _ O
into -X- _ O
a -X- _ O
unified -X- _ O
task -X- _ O
form -X- _ O
and -X- _ O
pre -X- _ O
- -X- _ O
train -X- _ O
soft -X- _ O
prompts -X- _ O
for -X- _ O
this -X- _ O
unified -X- _ O
task -X- _ O
. -X- _ O

We -X- _ O
name -X- _ O
this -X- _ O
Pretrained -X- _ B-MethodName
Prompt -X- _ I-MethodName
Tuning -X- _ I-MethodName
framework -X- _ O
" -X- _ O
PPT -X- _ B-MethodName
" -X- _ O
. -X- _ O

In -X- _ O
our -X- _ O
pilot -X- _ O
experiments -X- _ O
, -X- _ O
we -X- _ O
find -X- _ O
that -X- _ O
prompt -X- _ B-MethodName
tuning -X- _ I-MethodName
performs -X- _ O
comparably -X- _ O
with -X- _ O
conventional -X- _ O
full -X- _ B-MethodName
- -X- _ I-MethodName
model -X- _ I-MethodName
tuning -X- _ I-MethodName
when -X- _ O
downstream -X- _ O
data -X- _ O
are -X- _ O
sufficient -X- _ O
, -X- _ O
whereas -X- _ O
it -X- _ O
is -X- _ O
much -X- _ O
worse -X- _ O
under -X- _ O
fewshot -X- _ O
learning -X- _ O
settings -X- _ O
, -X- _ O
which -X- _ O
may -X- _ O
hinder -X- _ O
the -X- _ O
application -X- _ O
of -X- _ O
prompt -X- _ B-MethodName
tuning -X- _ I-MethodName
. -X- _ O

However -X- _ O
, -X- _ O
prompt -X- _ B-MethodName
tuning -X- _ I-MethodName
is -X- _ O
yet -X- _ O
to -X- _ O
be -X- _ O
fully -X- _ O
explored -X- _ O
. -X- _ O

Among -X- _ O
these -X- _ O
methods -X- _ O
, -X- _ O
prompt -X- _ B-MethodName
tuning -X- _ I-MethodName
, -X- _ O
which -X- _ O
freezes -X- _ O
PLMs -X- _ O
and -X- _ O
only -X- _ O
tunes -X- _ O
soft -X- _ O
prompts -X- _ O
, -X- _ O
provides -X- _ O
an -X- _ O
efficient -X- _ O
and -X- _ O
effective -X- _ O
solution -X- _ O
for -X- _ O
adapting -X- _ O
largescale -X- _ O
PLMs -X- _ O
to -X- _ O
downstream -X- _ O
tasks -X- _ O
. -X- _ O

