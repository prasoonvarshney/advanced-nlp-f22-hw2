-DOCSTART- -X- O
G -X- _ O
denotes -X- _ O
a -X- _ O
feed -X- _ O
- -X- _ O
forward -X- _ O
neural -X- _ O
network -X- _ O
, -X- _ O
and -X- _ O
[ -X- _ O
; -X- _ O
] -X- _ O
denotes -X- _ O
concatenation -X- _ O
. -X- _ O

As -X- _ O
explained -X- _ O
in -X- _ O
section -X- _ O
3 -X- _ O
, -X- _ O
the -X- _ O
Predictor -X- _ O
uses -X- _ O
Decomposable -X- _ B-MethodName
Attention -X- _ I-MethodName
for -X- _ O
prediction -X- _ O
. -X- _ O

Decomposable -X- _ B-MethodName
Attention -X- _ I-MethodName
computes -X- _ O
a -X- _ O
two -X- _ O
- -X- _ O
dimensional -X- _ O
attention -X- _ O
matrix -X- _ O
, -X- _ O
computed -X- _ O
by -X- _ O
two -X- _ O
sets -X- _ O
of -X- _ O
vectors -X- _ O
, -X- _ O
and -X- _ O
thus -X- _ O
, -X- _ O
captures -X- _ O
detailed -X- _ O
information -X- _ O
useful -X- _ O
for -X- _ O
prediction -X- _ O
. -X- _ O

We -X- _ O
verified -X- _ O
these -X- _ O
hypotheses -X- _ O
with -X- _ O
the -X- _ O
Reddit -X- _ B-DatasetName
TIFU -X- _ I-DatasetName
dataset -X- _ O
, -X- _ O
but -X- _ O
not -X- _ O
with -X- _ O
the -X- _ O
email -X- _ O
datasets -X- _ O
, -X- _ O
because -X- _ O
few -X- _ O
emails -X- _ O
included -X- _ O
annotated -X- _ O
summaries -X- _ O
, -X- _ O
and -X- _ O
those -X- _ O
emails -X- _ O
did -X- _ O
not -X- _ O
have -X- _ O
replies -X- _ O
with -X- _ O
quotes -X- _ O
. -X- _ O

We -X- _ O
evaluated -X- _ O
our -X- _ O
model -X- _ O
with -X- _ O
two -X- _ O
mail -X- _ O
datasets -X- _ O
, -X- _ O
ECS -X- _ B-DatasetName
and -X- _ O
EPS -X- _ B-DatasetName
, -X- _ O
and -X- _ O
one -X- _ O
social -X- _ O
media -X- _ O
dataset -X- _ O
TIFU -X- _ B-DatasetName
, -X- _ O
using -X- _ O
ROUGE -X- _ B-MetricName
as -X- _ O
an -X- _ O
evaluation -X- _ O
metric -X- _ O
, -X- _ O
and -X- _ O
validated -X- _ O
that -X- _ O
our -X- _ O
model -X- _ O
is -X- _ O
useful -X- _ O
for -X- _ O
summarization -X- _ B-TaskName
. -X- _ O

This -X- _ O
paper -X- _ O
proposes -X- _ O
Implicit -X- _ B-MethodName
Quote -X- _ I-MethodName
Extractor -X- _ I-MethodName
, -X- _ O
a -X- _ O
model -X- _ O
that -X- _ O
extracts -X- _ B-TaskName
implicit -X- _ I-TaskName
quotes -X- _ I-TaskName
as -X- _ O
summaries -X- _ O
. -X- _ O

appear -X- _ O
only -X- _ O
once -X- _ O
in -X- _ O
the -X- _ O
source -X- _ O
text -X- _ O
; -X- _ O
thus -X- _ O
TextRank -X- _ B-MethodName
fails -X- _ O
to -X- _ O
capture -X- _ O
the -X- _ O
salient -X- _ O
sentences -X- _ O
. -X- _ O

The -X- _ O
sample -X- _ O
is -X- _ O
from -X- _ O
the -X- _ O
EPS -X- _ B-DatasetName
dataset -X- _ O
. -X- _ O

Figure -X- _ O
3 -X- _ O
shows -X- _ O
the -X- _ O
correlation -X- _ O
between -X- _ O
the -X- _ O
maximum -X- _ O
PageRank -X- _ O
in -X- _ O
each -X- _ O
post -X- _ O
of -X- _ O
ECS -X- _ B-DatasetName
/ -X- _ O
EPS -X- _ B-DatasetName
and -X- _ O
ROUGE-1 -X- _ B-MetricName
- -X- _ I-MetricName
F -X- _ I-MetricName
scores -X- _ O
Table -X- _ O
8 -X- _ O
shows -X- _ O
a -X- _ O
demonstrative -X- _ O
example -X- _ O
of -X- _ O
extracted -X- _ O
summaries -X- _ O
of -X- _ O
IQE -X- _ B-MethodName
and -X- _ O
TextRank -X- _ B-MethodName
. -X- _ O

Comparing -X- _ O
with -X- _ O
TextRank -X- _ B-MethodName
, -X- _ O
we -X- _ O
verify -X- _ O
that -X- _ O
our -X- _ O
method -X- _ O
can -X- _ O
capture -X- _ O
salient -X- _ O
sentences -X- _ O
that -X- _ O
the -X- _ O
centrality -X- _ O
- -X- _ O
based -X- _ O
method -X- _ O
fails -X- _ O
to -X- _ O
. -X- _ O

TextRank -X- _ B-MethodName
is -X- _ O
a -X- _ O
typical -X- _ O
example -X- _ O
; -X- _ O
TextRank -X- _ B-MethodName
is -X- _ O
a -X- _ O
centrality -X- _ O
- -X- _ O
based -X- _ O
method -X- _ O
that -X- _ O
extracts -X- _ O
sentences -X- _ O
with -X- _ O
high -X- _ O
PageRank -X- _ O
as -X- _ O
the -X- _ O
summary -X- _ O
. -X- _ O

As -X- _ O
explained -X- _ O
in -X- _ O
the -X- _ O
Introduction -X- _ O
, -X- _ O
most -X- _ O
conventional -X- _ O
unsupervised -X- _ B-TaskName
summarization -X- _ I-TaskName
methods -X- _ O
are -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
assumption -X- _ O
that -X- _ O
important -X- _ O
topics -X- _ O
appear -X- _ O
frequently -X- _ O
in -X- _ O
a -X- _ O
document -X- _ O
. -X- _ O

Without -X- _ O
pretraining -X- _ O
, -X- _ O
the -X- _ O
accuracy -X- _ B-MetricName
decreased -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
on -X- _ O
the -X- _ O
Reddit -X- _ B-DatasetName
TIFU -X- _ I-DatasetName
dataset -X- _ O
, -X- _ O
NER -X- _ B-TaskName
did -X- _ O
not -X- _ O
affect -X- _ O
the -X- _ O
accuracy -X- _ O
. -X- _ O

To -X- _ O
validate -X- _ O
the -X- _ O
effect -X- _ O
of -X- _ O
NER -X- _ B-TaskName
, -X- _ O
we -X- _ O
experiment -X- _ O
without -X- _ O
replacing -X- _ O
named -X- _ O
entities -X- _ O
. -X- _ O

Effect -X- _ O
of -X- _ O
replacing -X- _ O
named -X- _ O
entities -X- _ O
As -X- _ O
explained -X- _ O
in -X- _ O
the -X- _ O
section -X- _ O
4.3 -X- _ O
, -X- _ O
our -X- _ O
models -X- _ O
shown -X- _ O
in -X- _ O
Tables -X- _ O
2 -X- _ O
, -X- _ O
3 -X- _ O
and -X- _ O
4 -X- _ O
all -X- _ O
use -X- _ O
the -X- _ O
Stanford -X- _ B-MethodName
NER -X- _ I-MethodName
. -X- _ O

The -X- _ O
results -X- _ O
of -X- _ O
the -X- _ O
two -X- _ O
analyses -X- _ O
support -X- _ O
the -X- _ O
claim -X- _ O
that -X- _ O
our -X- _ O
model -X- _ O
is -X- _ O
more -X- _ O
likely -X- _ O
to -X- _ O
extract -X- _ O
quotes -X- _ O
and -X- _ O
that -X- _ O
the -X- _ O
ability -X- _ O
of -X- _ O
extracting -X- _ B-TaskName
quotes -X- _ I-TaskName
leads -X- _ O
to -X- _ O
better -X- _ O
summarization -X- _ B-TaskName
. -X- _ O

The -X- _ O
result -X- _ O
in -X- _ O
the -X- _ O
Table -X- _ O
6 -X- _ O
shows -X- _ O
ROUGE -X- _ B-MetricName
scores -X- _ O
are -X- _ O
higher -X- _ O
when -X- _ O
the -X- _ O
extracted -X- _ O
sentence -X- _ O
coincides -X- _ O
with -X- _ O
a -X- _ O
quote -X- _ O
. -X- _ O

IQEquote -X- _ B-MethodName
indicates -X- _ O
the -X- _ O
data -X- _ O
where -X- _ O
the -X- _ O
extracted -X- _ O
sentence -X- _ O
coincides -X- _ O
with -X- _ O
a -X- _ O
quote -X- _ O
, -X- _ O
and -X- _ O
IQEnonquote -X- _ B-MethodName
vice -X- _ O
versa -X- _ O
. -X- _ O

We -X- _ O
compute -X- _ O
ROUGE -X- _ B-MetricName
scores -X- _ O
when -X- _ O
our -X- _ O
model -X- _ O
succeeds -X- _ O
or -X- _ O
fails -X- _ O
in -X- _ O
quote -X- _ B-TaskName
extraction -X- _ I-TaskName
( -X- _ O
which -X- _ O
means -X- _ O
when -X- _ O
MRR -X- _ B-MetricName
equals -X- _ O
1 -X- _ B-MetricValue
or -X- _ O
otherwise -X- _ O
) -X- _ O
. -X- _ O

Next -X- _ O
, -X- _ O
we -X- _ O
validate -X- _ O
whether -X- _ O
the -X- _ O
ROUGE -X- _ B-MetricName
scores -X- _ O
become -X- _ O
better -X- _ O
when -X- _ O
our -X- _ O
model -X- _ O
succeeded -X- _ O
in -X- _ O
extracting -X- _ O
quotes -X- _ O
. -X- _ O

IQE -X- _ B-MethodName
is -X- _ O
more -X- _ O
likely -X- _ O
to -X- _ O
extract -X- _ O
quotes -X- _ O
than -X- _ O
TextRank -X- _ B-MethodName
, -X- _ O
LexRank -X- _ B-MethodName
and -X- _ O
Random -X- _ B-MethodName
. -X- _ O

For -X- _ O
each -X- _ O
data -X- _ O
, -X- _ O
we -X- _ O
compute -X- _ O
MRR -X- _ B-MetricName
and -X- _ O
use -X- _ O
the -X- _ O
mean -X- _ B-MetricName
value -X- _ I-MetricName
as -X- _ O
a -X- _ O
result -X- _ O
. -X- _ O

Thus -X- _ O
we -X- _ O
set -X- _ O
the -X- _ O
threshold -X- _ B-HyperparameterName
at -X- _ O
four -X- _ B-HyperparameterValue
; -X- _ O
if -X- _ O
R(q -X- _ O
) -X- _ O
is -X- _ O
larger -X- _ O
than -X- _ O
4 -X- _ O
we -X- _ O
set -X- _ O
MRR -X- _ B-MethodName
0 -X- _ O
. -X- _ O

Therefore -X- _ O
, -X- _ O
the -X- _ O
MRR -X- _ B-MethodName
in -X- _ O
our -X- _ O
study -X- _ O
indicates -X- _ O
the -X- _ O
capability -X- _ O
of -X- _ O
a -X- _ O
model -X- _ O
to -X- _ O
extract -X- _ O
quotes -X- _ O
. -X- _ O

MRR -X- _ B-MethodName
= -X- _ O
1 -X- _ O
R(q -X- _ O
) -X- _ O
( -X- _ O
R(q -X- _ O
) -X- _ O
â‰¤ -X- _ O
4 -X- _ O
) -X- _ O
0 -X- _ O
( -X- _ O
R(q -X- _ O
) -X- _ O
> -X- _ O
4)(12 -X- _ O
) -X- _ O
The -X- _ O
function -X- _ O
R -X- _ O
denotes -X- _ O
the -X- _ O
rank -X- _ O
of -X- _ O
the -X- _ O
saliency -X- _ O
scores -X- _ O
a -X- _ O
model -X- _ O
computes -X- _ O
; -X- _ O
our -X- _ O
model -X- _ O
does -X- _ O
not -X- _ O
compute -X- _ O
the -X- _ O
scores -X- _ O
but -X- _ O
sequentially -X- _ O
extracts -X- _ O
sentences -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
order -X- _ O
is -X- _ O
regarded -X- _ O
as -X- _ O
the -X- _ O
rank -X- _ O
here -X- _ O
. -X- _ O

We -X- _ O
compute -X- _ O
MRR -X- _ B-MethodName
as -X- _ O
follows -X- _ O
. -X- _ O

To -X- _ O
assess -X- _ O
the -X- _ O
ability -X- _ O
of -X- _ O
quote -X- _ O
extraction -X- _ O
, -X- _ O
we -X- _ O
regard -X- _ O
the -X- _ O
extraction -X- _ O
of -X- _ O
quotes -X- _ O
as -X- _ O
an -X- _ O
information -X- _ B-TaskName
retrieval -X- _ I-TaskName
task -X- _ O
and -X- _ O
evaluate -X- _ O
with -X- _ B-MetricName
Mean -X- _ I-MetricName
Reciprocal -X- _ I-MetricName
Rank -X- _ I-MetricName
( -X- _ O
MRR -X- _ B-MetricName
) -X- _ O
. -X- _ O

297 -X- _ O
Model -X- _ O
ROUGE-1 -X- _ B-MetricName
- -X- _ I-MetricName
F -X- _ I-MetricName
ROUGE-2 -X- _ B-MetricName
- -X- _ I-MetricName
F -X- _ I-MetricName
ROUGE -X- _ B-MetricName
- -X- _ I-MetricName
L -X- _ I-MetricName
- -X- _ I-MetricName
F -X- _ I-MetricName
# -X- _ O
of -X- _ O
For -X- _ O
the -X- _ O
experiments -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
Reddit -X- _ B-DatasetName
TIFU -X- _ I-DatasetName
dataset -X- _ O
and -X- _ O
replies -X- _ O
extracted -X- _ O
via -X- _ O
praw -X- _ O
as -X- _ O
described -X- _ O
in -X- _ O
4.2 -X- _ O
. -X- _ O
From -X- _ O
the -X- _ O
dataset -X- _ O
, -X- _ O
we -X- _ O
extract -X- _ O
replies -X- _ O
that -X- _ O
contain -X- _ O
quotes -X- _ O
, -X- _ O
which -X- _ O
start -X- _ O
with -X- _ O
the -X- _ O
symbol -X- _ O
" -X- _ O
> -X- _ O
" -X- _ O
. -X- _ O

Second -X- _ O
, -X- _ O
following -X- _ O
Carenini -X- _ O
's -X- _ O
work -X- _ O
( -X- _ O
Carenini -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2007;Oya -X- _ O
and -X- _ O
Carenini -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
assumed -X- _ O
quotes -X- _ O
were -X- _ O
useful -X- _ O
for -X- _ O
summarization -X- _ O
but -X- _ O
it -X- _ O
is -X- _ O
not -X- _ O
clear -X- _ O
whether -X- _ O
the -X- _ O
quote -X- _ B-TaskName
extraction -X- _ I-TaskName
leads -X- _ O
to -X- _ O
better -X- _ O
results -X- _ O
of -X- _ O
summarization -X- _ B-TaskName
. -X- _ O

The -X- _ O
Performance -X- _ O
of -X- _ O
Summarization -X- _ B-TaskName
and -X- _ O
Quote -X- _ B-TaskName
Extraction -X- _ I-TaskName
. -X- _ O

IQE -X- _ B-MethodName
did -X- _ O
not -X- _ O
outperform -X- _ O
TextRank -X- _ B-MethodName
on -X- _ O
TIFU -X- _ B-DatasetName
dataset -X- _ O
. -X- _ O

Baseline -X- _ O
models -X- _ O
such -X- _ O
as -X- _ O
LexRank -X- _ B-DatasetName
and -X- _ O
TextRank -X- _ B-DatasetName
compute -X- _ O
similarity -X- _ O
of -X- _ O
sentences -X- _ O
using -X- _ O
the -X- _ O
co -X- _ O
- -X- _ O
occurrence -X- _ O
of -X- _ O
words -X- _ O
. -X- _ O

The -X- _ O
average -X- _ O
number -X- _ O
of -X- _ O
words -X- _ O
each -X- _ O
sentence -X- _ O
has -X- _ O
is -X- _ O
smaller -X- _ O
in -X- _ O
EPS -X- _ B-DatasetName
. -X- _ O

Our -X- _ O
model -X- _ O
outperforms -X- _ O
the -X- _ O
baseline -X- _ O
models -X- _ O
more -X- _ O
with -X- _ O
the -X- _ O
EPS -X- _ B-DatasetName
dataset -X- _ O
than -X- _ O
the -X- _ O
ECS -X- _ B-DatasetName
dataset -X- _ O
. -X- _ O

IQE -X- _ B-MethodName
- -X- _ I-MethodName
TextRank -X- _ I-MethodName
performed -X- _ O
worse -X- _ O
than -X- _ O
IQE -X- _ B-MethodName
with -X- _ O
the -X- _ O
mail -X- _ O
datasets -X- _ O
. -X- _ O

PacSum -X- _ B-MethodName
significantly -X- _ O
outperformed -X- _ O
TextRank -X- _ B-MethodName
on -X- _ O
the -X- _ O
news -X- _ O
article -X- _ O
dataset -X- _ O
( -X- _ O
Zheng -X- _ O
and -X- _ O
Lapata -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
but -X- _ O
does -X- _ O
not -X- _ O
work -X- _ O
well -X- _ O
on -X- _ O
our -X- _ O
datasets -X- _ O
where -X- _ O
the -X- _ O
sentence -X- _ O
position -X- _ O
is -X- _ O
not -X- _ O
an -X- _ O
important -X- _ O
factor -X- _ O
. -X- _ O

Reranking -X- _ O
improves -X- _ O
the -X- _ O
accuracy -X- _ O
on -X- _ O
ECS -X- _ B-DatasetName
and -X- _ O
TIFU -X- _ B-DatasetName
but -X- _ O
not -X- _ O
on -X- _ O
EPS -X- _ B-DatasetName
. -X- _ O

On -X- _ O
Reddit -X- _ B-DatasetName
TIFU -X- _ I-DatasetName
dataset -X- _ O
, -X- _ O
IQE -X- _ B-MethodName
with -X- _ O
reranking -X- _ O
outperforms -X- _ O
most -X- _ O
baseline -X- _ O
models -X- _ O
except -X- _ O
TextRank -X- _ B-MethodName
. -X- _ O

Our -X- _ O
model -X- _ O
outperforms -X- _ O
baseline -X- _ O
models -X- _ O
on -X- _ O
the -X- _ O
mail -X- _ O
datasets -X- _ O
( -X- _ O
ECS -X- _ B-DatasetName
and -X- _ O
EPS -X- _ B-DatasetName
) -X- _ O
in -X- _ O
most -X- _ O
metrics -X- _ O
. -X- _ O

As -X- _ O
another -X- _ O
baseline -X- _ O
, -X- _ O
we -X- _ O
employ -X- _ O
IQETextRank -X- _ B-MethodName
; -X- _ O
the -X- _ O
TextRank -X- _ B-MethodName
model -X- _ O
that -X- _ O
leverages -X- _ O
cosine -X- _ O
similarities -X- _ O
of -X- _ O
sentence -X- _ O
vectors -X- _ O
of -X- _ O
IQE -X- _ B-MethodName
's -X- _ O
Encoder -X- _ O
as -X- _ O
similarities -X- _ O
between -X- _ O
sentences -X- _ O
. -X- _ O

PacSum -X- _ B-MethodName
and -X- _ O
LexRank -X- _ B-MethodName
leverage -X- _ O
idf -X- _ O
. -X- _ O

KLSum -X- _ B-MethodName
employs -X- _ O
the -X- _ O
Kullbuck -X- _ O
- -X- _ O
Leibler -X- _ O
divergence -X- _ O
to -X- _ O
constrain -X- _ O
extracted -X- _ O
sentences -X- _ O
and -X- _ O
the -X- _ O
source -X- _ O
text -X- _ O
to -X- _ O
have -X- _ O
the -X- _ O
similar -X- _ O
word -X- _ O
distribution -X- _ O
. -X- _ O

296 -X- _ O
Model -X- _ O
ROUGE-1 -X- _ B-MetricName
- -X- _ I-MetricName
F -X- _ I-MetricName
ROUGE-2 -X- _ B-MetricName
- -X- _ I-MetricName
F -X- _ I-MetricName
ROUGE -X- _ B-MetricName
- -X- _ I-MetricName
L -X- _ I-MetricName
- -X- _ I-MetricName
F -X- _ I-MetricName
# -X- _ O
of -X- _ O
PacSum -X- _ B-MethodName
is -X- _ O
an -X- _ O
improved -X- _ O
model -X- _ O
of -X- _ O
TextRank -X- _ B-MethodName
, -X- _ O
which -X- _ O
harnesses -X- _ O
the -X- _ O
position -X- _ O
of -X- _ O
sentences -X- _ O
as -X- _ O
a -X- _ O
feature -X- _ O
. -X- _ O

TextRank -X- _ B-MethodName
and -X- _ O
LexRank -X- _ B-MethodName
are -X- _ O
graph -X- _ O
- -X- _ O
centrality -X- _ O
based -X- _ O
methods -X- _ O
that -X- _ O
have -X- _ O
long -X- _ O
been -X- _ O
considered -X- _ O
as -X- _ O
strong -X- _ O
methods -X- _ O
for -X- _ O
unsupervised -X- _ B-TaskName
summarization -X- _ I-TaskName
. -X- _ O

As -X- _ O
baseline -X- _ O
models -X- _ O
, -X- _ O
we -X- _ O
employ -X- _ O
TextRank -X- _ B-MethodName
( -X- _ O
Mihalcea -X- _ O
and -X- _ O
Tarau -X- _ O
, -X- _ O
2004 -X- _ O
) -X- _ O
, -X- _ O
LexRank -X- _ B-MethodName
( -X- _ O
Erkan -X- _ O
and -X- _ O
Radev -X- _ O
, -X- _ O
2004 -X- _ O
) -X- _ O
, -X- _ O
KLSum -X- _ B-MethodName
( -X- _ O
Haghighi -X- _ O
and -X- _ O
Vanderwende -X- _ O
, -X- _ O
2009 -X- _ O
) -X- _ O
, -X- _ O
PacSum -X- _ B-MethodName
( -X- _ O
Zheng -X- _ O
and -X- _ O
Lapata -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
Lead -X- _ B-MethodName
, -X- _ O
and -X- _ O
Random -X- _ B-MethodName
. -X- _ O

As -X- _ O
a -X- _ O
validation -X- _ O
metric -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
an -X- _ O
average -X- _ O
of -X- _ O
ROUGE-1 -X- _ B-MetricName
- -X- _ I-MetricName
F -X- _ I-MetricName
, -X- _ O
ROUGE-2 -X- _ B-MetricName
- -X- _ I-MetricName
F -X- _ I-MetricName
, -X- _ O
and -X- _ O
ROUGE -X- _ B-MetricName
- -X- _ I-MetricName
L -X- _ I-MetricName
- -X- _ I-MetricName
F. -X- _ I-MetricName

For -X- _ O
ROUGE -X- _ B-MetricName
computation -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
ROUGE -X- _ B-MetricName
2.0 -X- _ I-MetricName
( -X- _ O
Ganesan -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
the -X- _ O
first -X- _ O
20 -X- _ B-HyperparameterValue
, -X- _ O
40 -X- _ B-HyperparameterValue
, -X- _ O
and -X- _ O
60 -X- _ B-HyperparameterValue
words -X- _ B-HyperparameterName
of -X- _ O
the -X- _ O
extracted -X- _ O
sentences -X- _ O
. -X- _ O

Following -X- _ O
previous -X- _ O
work -X- _ O
, -X- _ O
we -X- _ O
report -X- _ O
the -X- _ O
average -X- _ B-MetricName
F1 -X- _ I-MetricName
of -X- _ O
ROUGE-1 -X- _ B-MetricName
, -X- _ O
ROUGE-2 -X- _ B-MetricName
, -X- _ O
and -X- _ O
ROUGE -X- _ B-MetricName
- -X- _ I-MetricName
L -X- _ I-MetricName
for -X- _ O
the -X- _ O
evaluation -X- _ O
( -X- _ O
Lin -X- _ O
, -X- _ O
2004 -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
pretrain -X- _ O
word -X- _ O
embeddings -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
with -X- _ O
Skipgram -X- _ B-MethodName
, -X- _ O
using -X- _ O
the -X- _ O
same -X- _ O
data -X- _ O
as -X- _ O
the -X- _ O
training -X- _ O
. -X- _ O

We -X- _ O
replace -X- _ O
the -X- _ O
named -X- _ O
entities -X- _ O
on -X- _ O
the -X- _ O
text -X- _ O
data -X- _ O
with -X- _ O
tags -X- _ O
( -X- _ O
person -X- _ O
, -X- _ O
location -X- _ O
, -X- _ O
and -X- _ O
organization -X- _ O
) -X- _ O
using -X- _ O
the -X- _ O
Stanford -X- _ B-MethodName
Named -X- _ I-MethodName
Entity -X- _ I-MethodName
Recognizer -X- _ I-MethodName
( -X- _ O
NER -X- _ B-TaskName
) -X- _ O
4 -X- _ O
, -X- _ O
to -X- _ O
prevent -X- _ O
the -X- _ O
model -X- _ O
from -X- _ O
simply -X- _ O
using -X- _ O
named -X- _ O
entities -X- _ O
as -X- _ O
a -X- _ O
hint -X- _ O
for -X- _ O
the -X- _ O
prediction -X- _ O
. -X- _ O

During -X- _ O
training -X- _ O
, -X- _ O
L -X- _ B-HyperparameterName
, -X- _ O
the -X- _ B-HyperparameterName
number -X- _ I-HyperparameterName
of -X- _ I-HyperparameterName
sentences -X- _ I-HyperparameterName
the -X- _ I-HyperparameterName
Extractor -X- _ I-HyperparameterName
extracts -X- _ I-HyperparameterName
is -X- _ O
randomly -X- _ O
set -X- _ O
from -X- _ O
1 -X- _ B-HyperparameterValue
to -X- _ I-HyperparameterValue
4 -X- _ I-HyperparameterValue
, -X- _ O
so -X- _ O
that -X- _ O
the -X- _ O
model -X- _ O
can -X- _ O
extract -X- _ O
an -X- _ O
arbitrary -X- _ O
number -X- _ O
of -X- _ O
sentences -X- _ O
. -X- _ O

We -X- _ O
set -X- _ O
this -X- _ O
threshold -X- _ B-HyperparameterName
as -X- _ O
4 -X- _ B-HyperparameterValue
. -X- _ O

The -X- _ O
epoch -X- _ B-HyperparameterName
size -X- _ O
is -X- _ O
10 -X- _ B-HyperparameterValue
, -X- _ O
and -X- _ O
we -X- _ O
use -X- _ O
Adam -X- _ B-HyperparameterValue
( -X- _ O
Kingma -X- _ O
and -X- _ O
Ba -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
as -X- _ O
an -X- _ O
optimizer -X- _ B-HyperparameterName
. -X- _ O

The -X- _ O
upper -X- _ B-HyperparameterName
limit -X- _ I-HyperparameterName
of -X- _ I-HyperparameterName
the -X- _ I-HyperparameterName
number -X- _ I-HyperparameterName
of -X- _ I-HyperparameterName
sentences -X- _ I-HyperparameterName
is -X- _ O
set -X- _ O
to -X- _ O
30 -X- _ B-HyperparameterValue
, -X- _ O
and -X- _ O
that -X- _ O
of -X- _ O
words -X- _ B-HyperparameterName
in -X- _ I-HyperparameterName
each -X- _ I-HyperparameterName
sentence -X- _ I-HyperparameterName
is -X- _ O
set -X- _ O
to -X- _ O
200 -X- _ B-HyperparameterValue
. -X- _ O

We -X- _ O
tokenize -X- _ O
each -X- _ O
email -X- _ O
or -X- _ O
post -X- _ O
into -X- _ O
sentences -X- _ O
and -X- _ O
each -X- _ O
sentence -X- _ O
into -X- _ O
words -X- _ O
using -X- _ O
the -X- _ O
nltk -X- _ B-MethodName
tokenizer -X- _ I-MethodName
3 -X- _ O
. -X- _ O

The -X- _ O
size -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
the -X- _ I-HyperparameterName
vocabulary -X- _ I-HyperparameterName
is -X- _ O
set -X- _ O
to -X- _ O
30,000 -X- _ B-HyperparameterValue
. -X- _ O

The -X- _ O
dimensions -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
the -X- _ I-HyperparameterName
embedding -X- _ I-HyperparameterName
layers -X- _ I-HyperparameterName
and -X- _ O
hidden -X- _ B-HyperparameterName
layers -X- _ I-HyperparameterName
of -X- _ O
the -X- _ O
LSTM -X- _ B-MethodName
are -X- _ O
100 -X- _ B-HyperparameterValue
. -X- _ O

An -X- _ O
overview -X- _ O
of -X- _ O
the -X- _ O
TIFU -X- _ B-DatasetName
evaluation -X- _ O
dataset -X- _ O
is -X- _ O
also -X- _ O
summarized -X- _ O
in -X- _ O
Table -X- _ O
1 -X- _ O
. -X- _ O

Because -X- _ O
the -X- _ O
TIFU -X- _ B-DatasetName
dataset -X- _ O
does -X- _ O
not -X- _ O
include -X- _ O
replies -X- _ O
, -X- _ O
we -X- _ O
collected -X- _ O
replies -X- _ O
of -X- _ O
the -X- _ O
posts -X- _ O
included -X- _ O
in -X- _ O
the -X- _ O
TIFU -X- _ B-DatasetName
dataset -X- _ O
using -X- _ O
praw -X- _ O
2 -X- _ O
. -X- _ O

We -X- _ O
preprocess -X- _ O
the -X- _ O
TIFU -X- _ B-DatasetName
dataset -X- _ O
similarly -X- _ O
as -X- _ O
the -X- _ O
mail -X- _ O
datasets -X- _ O
. -X- _ O

On -X- _ O
the -X- _ O
discussion -X- _ O
forum -X- _ O
Reddit -X- _ B-DatasetName
TIFU -X- _ I-DatasetName
, -X- _ O
users -X- _ O
post -X- _ O
a -X- _ O
tldr -X- _ O
along -X- _ O
with -X- _ O
the -X- _ O
post -X- _ O
. -X- _ O

The -X- _ O
Reddit -X- _ B-DatasetName
TIFU -X- _ I-DatasetName
dataset -X- _ O
( -X- _ O
Kim -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
is -X- _ O
a -X- _ O
dataset -X- _ O
that -X- _ O
leverages -X- _ O
tldr -X- _ O
tags -X- _ O
for -X- _ O
the -X- _ O
summarization -X- _ O
task -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
the -X- _ O
abbreviation -X- _ O
of -X- _ O
" -X- _ O
too -X- _ O
long -X- _ O
did -X- _ O
n't -X- _ O
read -X- _ O
" -X- _ O
. -X- _ O

Reddit -X- _ B-DatasetName
TIFU -X- _ I-DatasetName
Dataset -X- _ O
. -X- _ O

For -X- _ O
evaluation -X- _ O
, -X- _ O
we -X- _ O
employ -X- _ O
the -X- _ O
Enron -X- _ B-DatasetName
Summarization -X- _ I-DatasetName
dataset -X- _ O
( -X- _ O
Loza -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
Avocado -X- _ B-DatasetName
collection -X- _ I-DatasetName
is -X- _ O
a -X- _ O
public -X- _ O
dataset -X- _ O
that -X- _ O
comprises -X- _ O
emails -X- _ O
obtained -X- _ O
from -X- _ O
279 -X- _ O
custodians -X- _ O
of -X- _ O
a -X- _ O
defunct -X- _ O
information -X- _ O
technology -X- _ O
company -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
Avocado -X- _ B-DatasetName
collection -X- _ I-DatasetName
1 -X- _ O
for -X- _ O
the -X- _ O
training -X- _ O
. -X- _ O

One -X- _ O
is -X- _ O
a -X- _ O
mail -X- _ O
dataset -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
other -X- _ O
is -X- _ O
a -X- _ O
dataset -X- _ O
from -X- _ O
the -X- _ O
social -X- _ O
media -X- _ O
platform -X- _ O
, -X- _ O
Reddit -X- _ B-DatasetName
. -X- _ O

To -X- _ O
compute -X- _ O
the -X- _ O
relation -X- _ O
between -X- _ O
the -X- _ O
post -X- _ O
and -X- _ O
the -X- _ O
reply -X- _ O
candidate -X- _ O
, -X- _ O
we -X- _ O
employ -X- _ O
Decomposable -X- _ B-MethodName
Attention -X- _ I-MethodName
( -X- _ O
Parikh -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
discretized -X- _ O
attention -X- _ O
weights -X- _ O
Î± -X- _ O
are -X- _ O
computed -X- _ O
as -X- _ O
follows -X- _ O
: -X- _ O
u -X- _ O
i -X- _ O
âˆ¼ -X- _ O
Uniform(0 -X- _ O
, -X- _ O
1 -X- _ O
) -X- _ O
( -X- _ O
3 -X- _ O
) -X- _ O
g -X- _ O
i -X- _ O
= -X- _ O
âˆ’ -X- _ O
log -X- _ O
( -X- _ O
âˆ’ -X- _ O
log -X- _ O
u -X- _ O
i -X- _ O
) -X- _ O
( -X- _ O
4 -X- _ O
) -X- _ O
a -X- _ O
ti -X- _ O
= -X- _ O
c -X- _ O
T -X- _ O
tanh(h -X- _ O
ext -X- _ O
t -X- _ O
+ -X- _ O
h -X- _ O
p -X- _ O
i -X- _ O
) -X- _ O
( -X- _ O
5 -X- _ O
) -X- _ O
Ï€ -X- _ O
ti -X- _ O
= -X- _ O
exp -X- _ O
a -X- _ O
ti -X- _ O
N -X- _ O
k=1 -X- _ O
exp -X- _ O
a -X- _ O
tk -X- _ O
( -X- _ O
6 -X- _ O
) -X- _ O
Î± -X- _ O
ti -X- _ O
= -X- _ O
exp -X- _ O
( -X- _ O
log -X- _ O
Ï€ -X- _ O
ti -X- _ O
+ -X- _ O
g -X- _ O
i -X- _ O
) -X- _ O
/Ï„ -X- _ O
N -X- _ O
k=1 -X- _ O
exp -X- _ O
( -X- _ O
log -X- _ O
Ï€ -X- _ O
tk -X- _ O
+ -X- _ O
g -X- _ O
k -X- _ O
) -X- _ O
/Ï„ -X- _ O
( -X- _ O
7 -X- _ O
) -X- _ O
c -X- _ O
is -X- _ O
a -X- _ O
parameter -X- _ O
vector -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
temperature -X- _ O
Ï„ -X- _ O
is -X- _ O
set -X- _ O
to -X- _ O
0.1 -X- _ O
. -X- _ O
We -X- _ O
input -X- _ O
the -X- _ O
linear -X- _ O
sum -X- _ O
of -X- _ O
the -X- _ O
attention -X- _ O
weights -X- _ O
Î± -X- _ O
and -X- _ O
the -X- _ O
sentence -X- _ O
vectors -X- _ O
h -X- _ O
p -X- _ O
i -X- _ O
to -X- _ O
LSTM -X- _ B-MethodName
and -X- _ O
update -X- _ O
the -X- _ O
hidden -X- _ O
state -X- _ O
of -X- _ O
the -X- _ O
Extractor -X- _ O
. -X- _ O

We -X- _ O
employ -X- _ O
LSTM -X- _ B-MethodName
to -X- _ O
sequentially -X- _ O
compute -X- _ O
features -X- _ O
on -X- _ O
the -X- _ O
Extractor -X- _ O
. -X- _ O

IQE -X- _ B-MethodName
requires -X- _ O
replies -X- _ O
only -X- _ O
during -X- _ O
the -X- _ O
training -X- _ O
and -X- _ O
can -X- _ O
induce -X- _ O
summaries -X- _ O
without -X- _ O
replies -X- _ O
during -X- _ O
the -X- _ O
evaluation -X- _ O
. -X- _ O

We -X- _ O
compute -X- _ O
the -X- _ O
features -X- _ O
of -X- _ O
each -X- _ O
sentence -X- _ O
h -X- _ O
p -X- _ O
i -X- _ O
by -X- _ O
inputting -X- _ O
embedded -X- _ O
vectors -X- _ O
to -X- _ O
Bidirectional -X- _ B-MethodName
Long -X- _ I-MethodName
Short -X- _ I-MethodName
- -X- _ I-MethodName
Term -X- _ I-MethodName
Memory -X- _ I-MethodName
( -X- _ O
BiLSTM -X- _ B-MethodName
) -X- _ O
and -X- _ O
concatenating -X- _ O
the -X- _ O
last -X- _ O
two -X- _ O
hidden -X- _ O
layers -X- _ O
: -X- _ O
h -X- _ O
p -X- _ O
i -X- _ O
= -X- _ O
BiLSTM(X -X- _ O
p -X- _ O
i -X- _ O
) -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
Extractor -X- _ O
The -X- _ O
Extractor -X- _ O
extracts -X- _ O
a -X- _ O
few -X- _ O
sentences -X- _ O
of -X- _ O
a -X- _ O
post -X- _ O
for -X- _ O
prediction -X- _ O
. -X- _ O

We -X- _ O
propose -X- _ O
Implicit -X- _ B-MethodName
Quote -X- _ I-MethodName
Extractor -X- _ I-MethodName
( -X- _ O
IQE -X- _ B-MethodName
) -X- _ O
, -X- _ O
an -X- _ O
unsupervised -X- _ B-TaskName
extractive -X- _ I-TaskName
summarization -X- _ I-TaskName
model -X- _ O
. -X- _ O

A -X- _ O
few -X- _ O
studies -X- _ O
used -X- _ O
these -X- _ O
quotes -X- _ O
as -X- _ O
features -X- _ O
for -X- _ O
summarization -X- _ B-TaskName
. -X- _ O

Quotes -X- _ O
are -X- _ O
also -X- _ O
important -X- _ O
factors -X- _ O
of -X- _ O
summarization -X- _ B-TaskName
. -X- _ O

Despite -X- _ O
the -X- _ O
rise -X- _ O
of -X- _ O
neural -X- _ O
summarization -X- _ O
models -X- _ O
, -X- _ O
most -X- _ O
research -X- _ O
on -X- _ O
conversation -X- _ B-TaskName
summarization -X- _ I-TaskName
is -X- _ O
based -X- _ O
on -X- _ O
non -X- _ O
- -X- _ O
neural -X- _ O
models -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
these -X- _ O
methods -X- _ O
use -X- _ O
pretrained -X- _ O
neural -X- _ O
network -X- _ O
models -X- _ O
as -X- _ O
a -X- _ O
feature -X- _ O
extractor -X- _ O
, -X- _ O
whereas -X- _ O
we -X- _ O
propose -X- _ O
an -X- _ O
end -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
end -X- _ O
neural -X- _ B-TaskName
extractive -X- _ I-TaskName
summarization -X- _ I-TaskName
model -X- _ O
. -X- _ O

A -X- _ O
few -X- _ O
neural -X- _ O
- -X- _ O
network -X- _ O
- -X- _ O
based -X- _ O
unsupervised -X- _ B-TaskName
extractive -X- _ I-TaskName
summarization -X- _ I-TaskName
methods -X- _ O
were -X- _ O
proposed -X- _ O
( -X- _ O
KÃ¥gebÃ¤ck -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2014;Yin -X- _ O
and -X- _ O
Pei -X- _ O
, -X- _ O
2015;Ma -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
. -X- _ O

Despite -X- _ O
the -X- _ O
rise -X- _ O
of -X- _ O
neural -X- _ O
networks -X- _ O
, -X- _ O
conventional -X- _ O
non -X- _ O
- -X- _ O
neural -X- _ O
methods -X- _ O
are -X- _ O
still -X- _ O
powerful -X- _ O
in -X- _ O
the -X- _ O
field -X- _ O
of -X- _ O
unsupervised -X- _ B-TaskName
extractive -X- _ I-TaskName
summarization -X- _ I-TaskName
. -X- _ O

Most -X- _ O
unsupervised -X- _ B-TaskName
summarization -X- _ I-TaskName
methods -X- _ O
proposed -X- _ O
are -X- _ O
extractive -X- _ O
methods -X- _ O
. -X- _ O

Summarization -X- _ B-TaskName
methods -X- _ O
can -X- _ O
be -X- _ O
roughly -X- _ O
grouped -X- _ O
into -X- _ O
two -X- _ O
methods -X- _ O
: -X- _ O
extractive -X- _ B-TaskName
summarization -X- _ I-TaskName
and -X- _ O
abstractive -X- _ O
summarization -X- _ O
. -X- _ O

â€¢ -X- _ O
Using -X- _ O
the -X- _ O
Reddit -X- _ B-DatasetName
dataset -X- _ O
, -X- _ O
we -X- _ O
verified -X- _ O
that -X- _ O
quote -X- _ B-TaskName
extraction -X- _ I-TaskName
leads -X- _ O
to -X- _ O
a -X- _ O
high -X- _ O
performance -X- _ O
of -X- _ O
summarization -X- _ B-TaskName
. -X- _ O

â€¢ -X- _ O
We -X- _ O
proposed -X- _ O
an -X- _ O
unsupervised -X- _ B-DatasetName
extractive -X- _ I-DatasetName
neural -X- _ I-DatasetName
summarization -X- _ I-DatasetName
model -X- _ O
, -X- _ O
Implicit -X- _ B-MethodName
Quote -X- _ I-MethodName
Extractor -X- _ I-MethodName
( -X- _ O
IQE -X- _ B-MethodName
) -X- _ O
, -X- _ O
and -X- _ O
demonstrated -X- _ O
that -X- _ O
the -X- _ O
model -X- _ O
outperformed -X- _ O
or -X- _ O
achieved -X- _ O
results -X- _ O
competitive -X- _ O
to -X- _ O
baseline -X- _ O
models -X- _ O
on -X- _ O
two -X- _ O
mail -X- _ O
datasets -X- _ O
and -X- _ O
a -X- _ O
Reddit -X- _ B-DatasetName
dataset -X- _ O
. -X- _ O

Using -X- _ O
the -X- _ O
Reddit -X- _ B-DatasetName
dataset -X- _ O
where -X- _ O
quotes -X- _ O
are -X- _ O
abundant -X- _ O
, -X- _ O
we -X- _ O
obtain -X- _ O
results -X- _ O
that -X- _ O
supports -X- _ O
the -X- _ O
hypothesis -X- _ O
. -X- _ O

We -X- _ O
also -X- _ O
evaluated -X- _ O
our -X- _ O
model -X- _ O
with -X- _ O
Reddit -X- _ B-DatasetName
TIFU -X- _ I-DatasetName
dataset -X- _ O
( -X- _ O
Kim -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
and -X- _ O
achieved -X- _ O
results -X- _ O
competitive -X- _ O
with -X- _ O
those -X- _ O
of -X- _ O
the -X- _ O
baseline -X- _ O
models -X- _ O
. -X- _ O

We -X- _ O
evaluate -X- _ O
our -X- _ O
model -X- _ O
with -X- _ O
two -X- _ O
datasets -X- _ O
of -X- _ O
Enron -X- _ B-DatasetName
mail -X- _ I-DatasetName
( -X- _ O
Loza -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
, -X- _ O
corporate -X- _ O
and -X- _ O
private -X- _ O
mails -X- _ O
, -X- _ O
and -X- _ O
verify -X- _ O
that -X- _ O
our -X- _ O
model -X- _ O
outperforms -X- _ O
baseline -X- _ O
models -X- _ O
. -X- _ O

Summaries -X- _ O
should -X- _ O
not -X- _ O
depend -X- _ O
on -X- _ O
replies -X- _ O
, -X- _ O
so -X- _ O
IQE -X- _ B-MethodName
does -X- _ O
not -X- _ O
use -X- _ O
reply -X- _ O
features -X- _ O
to -X- _ O
extract -X- _ O
sentences -X- _ O
. -X- _ O

To -X- _ O
predict -X- _ O
accurately -X- _ O
, -X- _ O
IQE -X- _ B-MethodName
has -X- _ O
to -X- _ O
extract -X- _ O
sentences -X- _ O
that -X- _ O
replies -X- _ O
frequently -X- _ O
refer -X- _ O
to -X- _ O
. -X- _ O

IQE -X- _ B-MethodName
extracts -X- _ O
a -X- _ O
few -X- _ O
sentences -X- _ O
of -X- _ O
the -X- _ O
post -X- _ O
as -X- _ O
a -X- _ O
feature -X- _ O
for -X- _ O
prediction -X- _ O
. -X- _ O

The -X- _ O
aim -X- _ O
of -X- _ O
our -X- _ O
model -X- _ O
is -X- _ O
to -X- _ O
extract -X- _ O
these -X- _ O
implicit -X- _ O
quotes -X- _ O
for -X- _ O
extractive -X- _ B-TaskName
summarization -X- _ I-TaskName
. -X- _ O

The -X- _ O
model -X- _ O
is -X- _ O
Implicit -X- _ B-MethodName
Quote -X- _ I-MethodName
Extractor -X- _ I-MethodName
( -X- _ O
IQE -X- _ B-MethodName
) -X- _ O
. -X- _ O

Graph -X- _ O
- -X- _ O
centrality -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
similarity -X- _ O
of -X- _ O
sentences -X- _ O
( -X- _ O
Mihalcea -X- _ O
and -X- _ O
Tarau -X- _ O
, -X- _ O
2004;Erkan -X- _ O
and -X- _ O
Radev -X- _ O
, -X- _ O
2004;Zheng -X- _ O
and -X- _ O
Lapata -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
has -X- _ O
long -X- _ O
been -X- _ O
a -X- _ O
strong -X- _ O
feature -X- _ O
for -X- _ O
unsupervised -X- _ B-TaskName
summarization -X- _ I-TaskName
, -X- _ O
and -X- _ O
is -X- _ O
also -X- _ O
used -X- _ O
to -X- _ O
summarize -X- _ O
conversations -X- _ O
( -X- _ O
Mehdad -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2014;Shang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
. -X- _ O

Previous -X- _ O
research -X- _ O
proposed -X- _ O
diverse -X- _ O
methods -X- _ O
of -X- _ O
unsupervised -X- _ B-TaskName
summarization -X- _ I-TaskName
. -X- _ O

Neuralnetwork -X- _ O
- -X- _ O
based -X- _ O
models -X- _ O
have -X- _ O
achieved -X- _ O
great -X- _ O
performance -X- _ O
on -X- _ O
supervised -X- _ O
summarization -X- _ O
, -X- _ O
but -X- _ O
its -X- _ O
application -X- _ O
to -X- _ O
unsupervised -X- _ B-TaskName
summarization -X- _ I-TaskName
is -X- _ O
not -X- _ O
sufficiently -X- _ O
explored -X- _ O
. -X- _ O

As -X- _ O
the -X- _ O
amount -X- _ O
of -X- _ O
information -X- _ O
exchanged -X- _ O
via -X- _ O
online -X- _ O
conversations -X- _ O
is -X- _ O
growing -X- _ O
rapidly -X- _ O
, -X- _ O
automated -X- _ B-TaskName
summarization -X- _ I-TaskName
of -X- _ I-TaskName
conversations -X- _ I-TaskName
is -X- _ O
in -X- _ O
demand -X- _ O
. -X- _ O

We -X- _ O
evaluate -X- _ O
our -X- _ O
model -X- _ O
on -X- _ O
two -X- _ O
email -X- _ O
datasets -X- _ O
and -X- _ O
one -X- _ O
social -X- _ O
media -X- _ O
dataset -X- _ O
, -X- _ O
and -X- _ O
confirm -X- _ O
that -X- _ O
our -X- _ O
model -X- _ O
is -X- _ O
useful -X- _ O
for -X- _ O
extractive -X- _ B-TaskName
summarization -X- _ I-TaskName
. -X- _ O

Implicit -X- _ B-MethodName
Quote -X- _ I-MethodName
Extractor -X- _ I-MethodName
aims -X- _ O
to -X- _ O
extract -X- _ O
implicit -X- _ O
quotes -X- _ O
as -X- _ O
summaries -X- _ O
. -X- _ O

We -X- _ O
propose -X- _ O
Implicit -X- _ B-MethodName
Quote -X- _ I-MethodName
Extractor -X- _ I-MethodName
, -X- _ O
an -X- _ O
endto -X- _ O
- -X- _ O
end -X- _ O
unsupervised -X- _ B-TaskName
extractive -X- _ I-TaskName
neural -X- _ I-TaskName
summarization -X- _ I-TaskName
model -X- _ O
for -X- _ O
conversational -X- _ O
texts -X- _ O
. -X- _ O

Identifying -X- _ O
Implicit -X- _ O
Quotes -X- _ O
for -X- _ O
Unsupervised -X- _ B-TaskName
Extractive -X- _ I-TaskName
Summarization -X- _ I-TaskName
of -X- _ O
Conversations -X- _ O
. -X- _ O

Finally -X- _ O
, -X- _ O
we -X- _ O
concatenate -X- _ O
v -X- _ O
1 -X- _ O
and -X- _ O
v -X- _ O
2 -X- _ O
and -X- _ O
obtain -X- _ O
binary -X- _ O
- -X- _ O
classification -X- _ O
result -X- _ O
y -X- _ O
through -X- _ O
a -X- _ O
linear -X- _ O
layer -X- _ O
H -X- _ O
and -X- _ O
the -X- _ O
sigmoid -X- _ O
function -X- _ O
. -X- _ O

Next -X- _ O
, -X- _ O
we -X- _ O
separately -X- _ O
compare -X- _ O
the -X- _ O
aligned -X- _ O
phrases -X- _ O
Î² -X- _ O
t -X- _ O
and -X- _ O
x -X- _ O
ext -X- _ O
t -X- _ O
, -X- _ O
Î± -X- _ O
j -X- _ O
and -X- _ O
h -X- _ O
r -X- _ O
j -X- _ O
, -X- _ O
using -X- _ O
a -X- _ O
function -X- _ O
G. -X- _ O

Î² -X- _ O
i -X- _ O
is -X- _ O
a -X- _ O
linear -X- _ O
sum -X- _ O
of -X- _ O
reply -X- _ O
features -X- _ O
h -X- _ O
r -X- _ O
j -X- _ O
that -X- _ O
is -X- _ O
aligned -X- _ O
to -X- _ O
x -X- _ O
ext -X- _ O
t -X- _ O
and -X- _ O
vice -X- _ O
versa -X- _ O
for -X- _ O
Î± -X- _ O
j -X- _ O
. -X- _ O

The -X- _ O
weights -X- _ O
of -X- _ O
the -X- _ O
co -X- _ O
- -X- _ O
attention -X- _ O
matrix -X- _ O
are -X- _ O
normalized -X- _ O
row -X- _ O
- -X- _ O
wise -X- _ O
and -X- _ O
column -X- _ O
- -X- _ O
wise -X- _ O
in -X- _ O
the -X- _ O
equations -X- _ O
( -X- _ O
14 -X- _ O
) -X- _ O
and -X- _ O
( -X- _ O
15 -X- _ O
) -X- _ O
. -X- _ O

First -X- _ O
, -X- _ O
we -X- _ O
compute -X- _ O
a -X- _ O
co -X- _ O
- -X- _ O
attention -X- _ O
matrix -X- _ O
E -X- _ O
as -X- _ O
in -X- _ O
( -X- _ O
13 -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
computation -X- _ O
uses -X- _ O
the -X- _ O
following -X- _ O
equations -X- _ O
: -X- _ O
The -X- _ O
computation -X- _ O
of -X- _ O
x -X- _ O
ext -X- _ O
t -X- _ O
and -X- _ O
h -X- _ O
r -X- _ O
j -X- _ O
are -X- _ O
explained -X- _ O
in -X- _ O
section -X- _ O
3 -X- _ O
. -X- _ O

A.1 -X- _ O
Decomposable -X- _ O
Attention -X- _ O
. -X- _ O

A -X- _ O
Appendices -X- _ O
. -X- _ O

For -X- _ O
future -X- _ O
work -X- _ O
, -X- _ O
we -X- _ O
will -X- _ O
examine -X- _ O
whether -X- _ O
our -X- _ O
hypotheses -X- _ O
are -X- _ O
valid -X- _ O
for -X- _ O
emails -X- _ O
and -X- _ O
other -X- _ O
datasets -X- _ O
. -X- _ O

We -X- _ O
hypothesized -X- _ O
that -X- _ O
our -X- _ O
model -X- _ O
is -X- _ O
more -X- _ O
likely -X- _ O
to -X- _ O
extract -X- _ O
quotes -X- _ O
and -X- _ O
that -X- _ O
ability -X- _ O
improved -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
our -X- _ O
model -X- _ O
. -X- _ O

Conclusion -X- _ O
. -X- _ O

Our -X- _ O
model -X- _ O
, -X- _ O
by -X- _ O
contrast -X- _ O
, -X- _ O
can -X- _ O
capture -X- _ O
them -X- _ O
because -X- _ O
they -X- _ O
are -X- _ O
topics -X- _ O
that -X- _ O
replies -X- _ O
often -X- _ O
refer -X- _ O
to -X- _ O
. -X- _ O

She -X- _ O
run -X- _ O
into -X- _ O
heather -X- _ O
evans -X- _ O
which -X- _ O
she -X- _ O
had -X- _ O
n't -X- _ O
talked -X- _ O
in -X- _ O
10 -X- _ O
years -X- _ O
. -X- _ O

Rachel -X- _ O
is -X- _ O
coming -X- _ O
to -X- _ O
visit -X- _ O
her -X- _ O
in -X- _ O
couple -X- _ O
of -X- _ O
weeks -X- _ O
and -X- _ O
she -X- _ O
is -X- _ O
asking -X- _ O
if -X- _ O
he -X- _ O
/ -X- _ O
she -X- _ O
will -X- _ O
join -X- _ O
for -X- _ O
any -X- _ O
of -X- _ O
the -X- _ O
rodeo -X- _ O
stuff -X- _ O
. -X- _ O

She -X- _ O
is -X- _ O
scared -X- _ O
of -X- _ O
being -X- _ O
a -X- _ O
mother -X- _ O
but -X- _ O
also -X- _ O
pretty -X- _ O
exited -X- _ O
about -X- _ O
it -X- _ O
. -X- _ O

She -X- _ O
is -X- _ O
having -X- _ O
a -X- _ O
baby -X- _ O
due -X- _ O
in -X- _ O
June -X- _ O
. -X- _ O

The -X- _ O
sender -X- _ O
just -X- _ O
move -X- _ O
out -X- _ O
to -X- _ O
Katy -X- _ O
few -X- _ O
months -X- _ O
ago -X- _ O
. -X- _ O

The -X- _ O
sender -X- _ O
wants -X- _ O
to -X- _ O
congratulate -X- _ O
the -X- _ O
recipient -X- _ O
for -X- _ O
his -X- _ O
/ -X- _ O
her -X- _ O
new -X- _ O
promotion -X- _ O
, -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
, -X- _ O
updating -X- _ O
him -X- _ O
/ -X- _ O
her -X- _ O
about -X- _ O
her -X- _ O
life -X- _ O
. -X- _ O

Summary -X- _ O
( -X- _ O
Gold -X- _ O
) -X- _ O
. -X- _ O

Looking -X- _ O
forward -X- _ O
to -X- _ O
hearing -X- _ O
back -X- _ O
from -X- _ O
ya -X- _ O
. -X- _ O

Got -X- _ O
ta -X- _ O
get -X- _ O
back -X- _ O
to -X- _ O
work -X- _ O
. -X- _ O

Anyway -X- _ O
, -X- _ O
I -X- _ O
'll -X- _ O
let -X- _ O
you -X- _ O
go -X- _ O
. -X- _ O

Seems -X- _ O
like -X- _ O
she -X- _ O
's -X- _ O
doing -X- _ O
well -X- _ O
but -X- _ O
I -X- _ O
can -X- _ O
never -X- _ O
really -X- _ O
tell -X- _ O
with -X- _ O
her -X- _ O
. -X- _ O

I -X- _ O
had -X- _ O
n't -X- _ O
talked -X- _ O
to -X- _ O
her -X- _ O
in -X- _ O
about -X- _ O
10 -X- _ O
years -X- _ O
. -X- _ O

It -X- _ O
was -X- _ O
the -X- _ O
weirdest -X- _ O
thing -X- _ O
-heather -X- _ O
evans -X- _ O
. -X- _ O

You -X- _ O
'll -X- _ O
never -X- _ O
guess -X- _ O
who -X- _ O
I -X- _ O
got -X- _ O
in -X- _ O
touch -X- _ O
with -X- _ O
about -X- _ O
a -X- _ O
month -X- _ O
ago -X- _ O
. -X- _ O

You -X- _ O
planning -X- _ O
on -X- _ O
coming -X- _ O
in -X- _ O
for -X- _ O
any -X- _ O
of -X- _ O
the -X- _ O
rodeo -X- _ O
stuff -X- _ O
? -X- _ O

Rachel -X- _ O
is -X- _ O
coming -X- _ O
to -X- _ O
visit -X- _ O
me -X- _ O
in -X- _ O
a -X- _ O
couple -X- _ O
of -X- _ O
weeks -X- _ O
. -X- _ O

I -X- _ O
'm -X- _ O
really -X- _ O
excited -X- _ O
though -X- _ O
. -X- _ O

The -X- _ O
thought -X- _ O
of -X- _ O
me -X- _ O
being -X- _ O
a -X- _ O
mother -X- _ O
is -X- _ O
downright -X- _ O
scary -X- _ O
but -X- _ O
I -X- _ O
figure -X- _ O
since -X- _ O
I -X- _ O
'm -X- _ O
almost -X- _ O
30 -X- _ O
, -X- _ O
I -X- _ O
probably -X- _ O
need -X- _ O
to -X- _ O
start -X- _ O
growing -X- _ O
up -X- _ O
. -X- _ O

I -X- _ O
ca -X- _ O
n't -X- _ O
even -X- _ O
believe -X- _ O
it -X- _ O
myself -X- _ O
. -X- _ O

New -X- _ O
news -X- _ O
from -X- _ O
me -X- _ O
-I'm -X- _ O
having -X- _ O
a -X- _ O
baby -X- _ O
-due -X- _ O
in -X- _ O
June -X- _ O
. -X- _ O

I -X- _ O
love -X- _ O
it -X- _ O
there -X- _ O
-my -X- _ O
parents -X- _ O
live -X- _ O
about -X- _ O
10 -X- _ O
minutes -X- _ O
away -X- _ O
. -X- _ O

My -X- _ O
hubby -X- _ O
and -X- _ O
' -X- _ O
I -X- _ O
moved -X- _ O
out -X- _ O
to -X- _ O
Katy -X- _ O
a -X- _ O
few -X- _ O
months -X- _ O
ago -X- _ O
. -X- _ O

I -X- _ O
'm -X- _ O
sure -X- _ O
it -X- _ O
's -X- _ O
going -X- _ O
to -X- _ O
be -X- _ O
alot -X- _ O
different -X- _ O
for -X- _ O
you -X- _ O
but -X- _ O
it -X- _ O
sounds -X- _ O
like -X- _ O
a -X- _ O
great -X- _ O
deal -X- _ O
. -X- _ O

Congrats -X- _ O
on -X- _ O
your -X- _ O
promotion -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
those -X- _ O
words -X- _ O
Source -X- _ O
Text -X- _ O
Just -X- _ O
got -X- _ O
your -X- _ O
email -X- _ O
address -X- _ O
from -X- _ O
Rachel -X- _ O
. -X- _ O

The -X- _ O
summary -X- _ O
includes -X- _ O
descriptions -X- _ O
regarding -X- _ O
a -X- _ O
promotion -X- _ O
and -X- _ O
that -X- _ O
the -X- _ O
sender -X- _ O
is -X- _ O
having -X- _ O
a -X- _ O
baby -X- _ O
. -X- _ O

We -X- _ O
suspected -X- _ O
that -X- _ O
important -X- _ O
topics -X- _ O
are -X- _ O
not -X- _ O
always -X- _ O
referred -X- _ O
to -X- _ O
frequently -X- _ O
, -X- _ O
and -X- _ O
suggested -X- _ O
another -X- _ O
criterion -X- _ O
: -X- _ O
the -X- _ O
frequency -X- _ O
of -X- _ O
being -X- _ O
referred -X- _ O
to -X- _ O
in -X- _ O
replies -X- _ O
. -X- _ O

A -X- _ O
sentence -X- _ O
having -X- _ O
high -X- _ O
PageRank -X- _ O
indicates -X- _ O
that -X- _ O
the -X- _ O
sentence -X- _ O
has -X- _ O
high -X- _ O
similarity -X- _ O
with -X- _ O
many -X- _ O
other -X- _ O
sentences -X- _ O
, -X- _ O
meaning -X- _ O
that -X- _ O
many -X- _ O
sentences -X- _ O
refer -X- _ O
to -X- _ O
the -X- _ O
same -X- _ O
topic -X- _ O
. -X- _ O

Difference -X- _ O
from -X- _ O
Conventional -X- _ O
Methods -X- _ O
. -X- _ O

This -X- _ O
shows -X- _ O
the -X- _ O
importance -X- _ O
of -X- _ O
the -X- _ O
separate -X- _ O
training -X- _ O
of -X- _ O
each -X- _ O
component -X- _ O
. -X- _ O

Table -X- _ O
7 -X- _ O
shows -X- _ O
the -X- _ O
effect -X- _ O
of -X- _ O
pretraining -X- _ O
. -X- _ O

As -X- _ O
explained -X- _ O
in -X- _ O
the -X- _ O
section -X- _ O
4.3 -X- _ O
, -X- _ O
we -X- _ O
pretrained -X- _ O
the -X- _ O
Predictor -X- _ O
in -X- _ O
the -X- _ O
first -X- _ O
few -X- _ O
epochs -X- _ O
so -X- _ O
that -X- _ O
the -X- _ O
model -X- _ O
can -X- _ O
learn -X- _ O
the -X- _ O
extraction -X- _ O
and -X- _ O
the -X- _ O
prediction -X- _ O
separately -X- _ O
. -X- _ O

Effect -X- _ O
of -X- _ O
pretraining -X- _ O
Predictor -X- _ O
. -X- _ O

Thus -X- _ O
, -X- _ O
named -X- _ O
entities -X- _ O
will -X- _ O
not -X- _ O
be -X- _ O
hints -X- _ O
to -X- _ O
predict -X- _ O
reply -X- _ O
- -X- _ O
relation -X- _ O
. -X- _ O

Reddit -X- _ O
is -X- _ O
an -X- _ O
anonymized -X- _ O
social -X- _ O
media -X- _ O
platform -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
posts -X- _ O
are -X- _ O
less -X- _ O
likely -X- _ O
to -X- _ O
refer -X- _ O
to -X- _ O
people -X- _ O
's -X- _ O
names -X- _ O
. -X- _ O

Ablation -X- _ O
Tests -X- _ O
. -X- _ O

Does -X- _ O
extracting -X- _ O
quotes -X- _ O
lead -X- _ O
to -X- _ O
good -X- _ O
summarization -X- _ O
? -X- _ O

Table -X- _ O
5 -X- _ O
shows -X- _ O
the -X- _ O
results -X- _ O
. -X- _ O

As -X- _ O
explained -X- _ O
in -X- _ O
the -X- _ O
section -X- _ O
4.3 -X- _ O
, -X- _ O
we -X- _ O
trained -X- _ O
our -X- _ O
model -X- _ O
to -X- _ O
extract -X- _ O
up -X- _ O
to -X- _ O
four -X- _ O
sentences -X- _ O
. -X- _ O

If -X- _ O
a -X- _ O
model -X- _ O
extracts -X- _ O
quotes -X- _ O
as -X- _ O
salient -X- _ O
sentences -X- _ O
, -X- _ O
the -X- _ O
rank -X- _ O
becomes -X- _ O
higher -X- _ O
. -X- _ O

How -X- _ O
well -X- _ O
our -X- _ O
model -X- _ O
extracts -X- _ O
quotes -X- _ O
? -X- _ O
. -X- _ O

We -X- _ O
label -X- _ O
sentences -X- _ O
of -X- _ O
the -X- _ O
posts -X- _ O
that -X- _ O
are -X- _ O
quoted -X- _ O
by -X- _ O
the -X- _ O
replies -X- _ O
and -X- _ O
verify -X- _ O
how -X- _ O
accurately -X- _ O
our -X- _ O
model -X- _ O
can -X- _ O
extract -X- _ O
the -X- _ O
quoted -X- _ O
sentences -X- _ O
. -X- _ O

In -X- _ O
total -X- _ O
, -X- _ O
1,969 -X- _ O
posts -X- _ O
have -X- _ O
replies -X- _ O
that -X- _ O
include -X- _ O
quotes -X- _ O
. -X- _ O

To -X- _ O
answer -X- _ O
these -X- _ O
questions -X- _ O
, -X- _ O
we -X- _ O
conduct -X- _ O
two -X- _ O
experiments -X- _ O
. -X- _ O

First -X- _ O
, -X- _ O
because -X- _ O
we -X- _ O
did -X- _ O
not -X- _ O
use -X- _ O
quotes -X- _ O
as -X- _ O
supervision -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
not -X- _ O
clear -X- _ O
how -X- _ O
well -X- _ O
our -X- _ O
model -X- _ O
extracts -X- _ O
quotes -X- _ O
. -X- _ O

Our -X- _ O
model -X- _ O
performed -X- _ O
well -X- _ O
on -X- _ O
the -X- _ O
Mail -X- _ O
datasets -X- _ O
but -X- _ O
two -X- _ O
questions -X- _ O
remain -X- _ O
unclear -X- _ O
. -X- _ O

It -X- _ O
is -X- _ O
conceivable -X- _ O
that -X- _ O
Reddit -X- _ O
users -X- _ O
are -X- _ O
less -X- _ O
likely -X- _ O
to -X- _ O
refer -X- _ O
to -X- _ O
important -X- _ O
topics -X- _ O
on -X- _ O
the -X- _ O
post -X- _ O
, -X- _ O
given -X- _ O
that -X- _ O
anyone -X- _ O
can -X- _ O
reply -X- _ O
. -X- _ O

Thus -X- _ O
, -X- _ O
if -X- _ O
the -X- _ O
lengths -X- _ O
of -X- _ O
sentences -X- _ O
are -X- _ O
short -X- _ O
, -X- _ O
it -X- _ O
fails -X- _ O
to -X- _ O
build -X- _ O
decent -X- _ O
co -X- _ O
- -X- _ O
occurrence -X- _ O
networks -X- _ O
and -X- _ O
to -X- _ O
capture -X- _ O
the -X- _ O
saliency -X- _ O
of -X- _ O
the -X- _ O
sentences -X- _ O
. -X- _ O

The -X- _ O
overview -X- _ O
of -X- _ O
the -X- _ O
datasets -X- _ O
in -X- _ O
Table -X- _ O
1 -X- _ O
explains -X- _ O
the -X- _ O
reason -X- _ O
. -X- _ O

This -X- _ O
indicates -X- _ O
that -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
our -X- _ O
model -X- _ O
does -X- _ O
not -X- _ O
result -X- _ O
from -X- _ O
the -X- _ O
use -X- _ O
of -X- _ O
neural -X- _ O
networks -X- _ O
. -X- _ O

Experimental -X- _ O
results -X- _ O
for -X- _ O
each -X- _ O
evaluation -X- _ O
dataset -X- _ O
are -X- _ O
listed -X- _ O
in -X- _ O
Table -X- _ O
2 -X- _ O
, -X- _ O
3 -X- _ O
and -X- _ O
4 -X- _ O
. -X- _ O

Results -X- _ O
and -X- _ O
Discussion -X- _ O
. -X- _ O

This -X- _ O
is -X- _ O
added -X- _ O
to -X- _ O
verify -X- _ O
that -X- _ O
the -X- _ O
success -X- _ O
of -X- _ O
our -X- _ O
model -X- _ O
is -X- _ O
not -X- _ O
only -X- _ O
because -X- _ O
our -X- _ O
model -X- _ O
uses -X- _ O
neural -X- _ O
networks -X- _ O
. -X- _ O

We -X- _ O
compute -X- _ O
idf -X- _ O
using -X- _ O
the -X- _ O
validation -X- _ O
data -X- _ O
. -X- _ O

Lead -X- _ O
is -X- _ O
a -X- _ O
simple -X- _ O
method -X- _ O
that -X- _ O
extracts -X- _ O
the -X- _ O
first -X- _ O
few -X- _ O
sentences -X- _ O
from -X- _ O
the -X- _ O
source -X- _ O
text -X- _ O
but -X- _ O
is -X- _ O
considered -X- _ O
as -X- _ O
a -X- _ O
strong -X- _ O
baseline -X- _ O
for -X- _ O
the -X- _ O
summarization -X- _ O
of -X- _ O
news -X- _ O
articles -X- _ O
. -X- _ O

Baseline -X- _ O
. -X- _ O

Each -X- _ O
model -X- _ O
extracts -X- _ O
3 -X- _ O
sentences -X- _ O
as -X- _ O
a -X- _ O
summary -X- _ O
. -X- _ O

In -X- _ O
the -X- _ O
evaluation -X- _ O
phase -X- _ O
, -X- _ O
we -X- _ O
only -X- _ O
use -X- _ O
the -X- _ O
Encoder -X- _ O
and -X- _ O
Extractor -X- _ O
and -X- _ O
do -X- _ O
not -X- _ O
use -X- _ O
the -X- _ O
Predictor -X- _ O
. -X- _ O

Evaluation -X- _ O
. -X- _ O

We -X- _ O
conduct -X- _ O
the -X- _ O
same -X- _ O
experiment -X- _ O
five -X- _ O
times -X- _ O
and -X- _ O
use -X- _ O
the -X- _ O
average -X- _ O
of -X- _ O
the -X- _ O
results -X- _ O
to -X- _ O
mitigate -X- _ O
the -X- _ O
effect -X- _ O
of -X- _ O
randomness -X- _ O
rooting -X- _ O
in -X- _ O
initialization -X- _ O
and -X- _ O
optimization -X- _ O
. -X- _ O

Thus -X- _ O
, -X- _ O
we -X- _ O
train -X- _ O
the -X- _ O
Predictor -X- _ O
in -X- _ O
the -X- _ O
first -X- _ O
few -X- _ O
epochs -X- _ O
before -X- _ O
training -X- _ O
the -X- _ O
Extractor -X- _ O
. -X- _ O

Models -X- _ O
with -X- _ O
several -X- _ O
components -X- _ O
generally -X- _ O
achieve -X- _ O
better -X- _ O
results -X- _ O
if -X- _ O
each -X- _ O
component -X- _ O
is -X- _ O
pretrained -X- _ O
separately -X- _ O
( -X- _ O
Hashimoto -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
Extractor -X- _ O
learns -X- _ O
to -X- _ O
extract -X- _ O
proper -X- _ O
sentences -X- _ O
and -X- _ O
the -X- _ O
Predictor -X- _ O
learns -X- _ O
to -X- _ O
predict -X- _ O
the -X- _ O
relation -X- _ O
between -X- _ O
a -X- _ O
post -X- _ O
and -X- _ O
a -X- _ O
reply -X- _ O
candidate -X- _ O
. -X- _ O

This -X- _ O
is -X- _ O
to -X- _ O
train -X- _ O
the -X- _ O
Extractor -X- _ O
and -X- _ O
the -X- _ O
Predictor -X- _ O
efficiently -X- _ O
. -X- _ O

In -X- _ O
the -X- _ O
first -X- _ O
few -X- _ O
epochs -X- _ O
, -X- _ O
we -X- _ O
do -X- _ O
not -X- _ O
use -X- _ O
the -X- _ O
Extractor -X- _ O
; -X- _ O
all -X- _ O
the -X- _ O
post -X- _ O
sentences -X- _ O
are -X- _ O
used -X- _ O
for -X- _ O
the -X- _ O
prediction -X- _ O
of -X- _ O
post -X- _ O
- -X- _ O
reply -X- _ O
relations -X- _ O
. -X- _ O

Training -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
3,000 -X- _ O
posts -X- _ O
and -X- _ O
tldrs -X- _ O
that -X- _ O
are -X- _ O
not -X- _ O
included -X- _ O
in -X- _ O
the -X- _ O
training -X- _ O
dataset -X- _ O
as -X- _ O
the -X- _ O
validation -X- _ O
dataset -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
same -X- _ O
number -X- _ O
of -X- _ O
posts -X- _ O
and -X- _ O
tldrs -X- _ O
as -X- _ O
the -X- _ O
evaluation -X- _ O
dataset -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
that -X- _ O
367,000 -X- _ O
pairs -X- _ O
of -X- _ O
posts -X- _ O
and -X- _ O
replies -X- _ O
as -X- _ O
the -X- _ O
training -X- _ O
dataset -X- _ O
. -X- _ O

As -X- _ O
a -X- _ O
consequence -X- _ O
, -X- _ O
we -X- _ O
obtained -X- _ O
183,500 -X- _ O
correct -X- _ O
pairs -X- _ O
of -X- _ O
posts -X- _ O
and -X- _ O
replies -X- _ O
and -X- _ O
the -X- _ O
same -X- _ O
number -X- _ O
of -X- _ O
wrong -X- _ O
pairs -X- _ O
. -X- _ O

tldr -X- _ O
briefly -X- _ O
explains -X- _ O
what -X- _ O
is -X- _ O
written -X- _ O
in -X- _ O
the -X- _ O
original -X- _ O
post -X- _ O
and -X- _ O
thus -X- _ O
can -X- _ O
be -X- _ O
regarded -X- _ O
as -X- _ O
a -X- _ O
summary -X- _ O
. -X- _ O

Therefore -X- _ O
, -X- _ O
we -X- _ O
have -X- _ O
112,348 -X- _ O
pairs -X- _ O
in -X- _ O
total -X- _ O
. -X- _ O

The -X- _ O
number -X- _ O
of -X- _ O
positive -X- _ O
labels -X- _ O
and -X- _ O
negative -X- _ O
labels -X- _ O
are -X- _ O
equal -X- _ O
. -X- _ O

We -X- _ O
labeled -X- _ O
a -X- _ O
pair -X- _ O
with -X- _ O
an -X- _ O
actual -X- _ O
reply -X- _ O
as -X- _ O
positive -X- _ O
and -X- _ O
a -X- _ O
pair -X- _ O
with -X- _ O
a -X- _ O
wrong -X- _ O
reply -X- _ O
that -X- _ O
is -X- _ O
randomly -X- _ O
sampled -X- _ O
from -X- _ O
the -X- _ O
whole -X- _ O
dataset -X- _ O
as -X- _ O
negative -X- _ O
. -X- _ O

After -X- _ O
the -X- _ O
preprocessing -X- _ O
, -X- _ O
we -X- _ O
have -X- _ O
56,174 -X- _ O
pairs -X- _ O
. -X- _ O

We -X- _ O
exclude -X- _ O
pairs -X- _ O
where -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
words -X- _ O
in -X- _ O
a -X- _ O
post -X- _ O
or -X- _ O
a -X- _ O
reply -X- _ O
is -X- _ O
smaller -X- _ O
than -X- _ O
50 -X- _ O
or -X- _ O
25 -X- _ O
. -X- _ O

From -X- _ O
this -X- _ O
dataset -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
post -X- _ O
- -X- _ O
and -X- _ O
- -X- _ O
reply -X- _ O
pairs -X- _ O
to -X- _ O
train -X- _ O
our -X- _ O
model -X- _ O
. -X- _ O

Mail -X- _ O
Dataset -X- _ O
. -X- _ O

We -X- _ O
train -X- _ O
and -X- _ O
evaluate -X- _ O
the -X- _ O
model -X- _ O
on -X- _ O
two -X- _ O
domains -X- _ O
of -X- _ O
datasets -X- _ O
. -X- _ O

Experiment -X- _ O
. -X- _ O

To -X- _ O
take -X- _ O
advantage -X- _ O
of -X- _ O
our -X- _ O
method -X- _ O
and -X- _ O
conventional -X- _ O
methods -X- _ O
, -X- _ O
we -X- _ O
employ -X- _ O
reranking -X- _ O
; -X- _ O
we -X- _ O
simply -X- _ O
reorder -X- _ O
summaries -X- _ O
( -X- _ O
3 -X- _ O
sentences -X- _ O
) -X- _ O
extracted -X- _ O
by -X- _ O
our -X- _ O
model -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
ranking -X- _ O
of -X- _ O
TextRank -X- _ O
( -X- _ O
Mihalcea -X- _ O
and -X- _ O
Tarau -X- _ O
, -X- _ O
2004 -X- _ O
) -X- _ O
. -X- _ O

L -X- _ O
rep -X- _ O
= -X- _ O
âˆ’t -X- _ O
rep -X- _ O
log -X- _ O
y -X- _ O
âˆ’ -X- _ O
( -X- _ O
1 -X- _ O
âˆ’ -X- _ O
t -X- _ O
rep -X- _ O
) -X- _ O
log -X- _ O
( -X- _ O
1 -X- _ O
âˆ’ -X- _ O
y -X- _ O
) -X- _ O
( -X- _ O
11 -X- _ O
) -X- _ O
Reranking -X- _ O
As -X- _ O
we -X- _ O
mentioned -X- _ O
in -X- _ O
the -X- _ O
Introduction -X- _ O
, -X- _ O
we -X- _ O
are -X- _ O
seeking -X- _ O
for -X- _ O
a -X- _ O
criterion -X- _ O
that -X- _ O
is -X- _ O
different -X- _ O
from -X- _ O
conventional -X- _ O
methods -X- _ O
. -X- _ O

The -X- _ O
loss -X- _ O
of -X- _ O
this -X- _ O
classification -X- _ O
L -X- _ O
rep -X- _ O
is -X- _ O
obtained -X- _ O
by -X- _ O
cross -X- _ O
entropy -X- _ O
as -X- _ O
follows -X- _ O
where -X- _ O
t -X- _ O
rep -X- _ O
is -X- _ O
1 -X- _ O
when -X- _ O
a -X- _ O
reply -X- _ O
candidate -X- _ O
is -X- _ O
an -X- _ O
actual -X- _ O
reply -X- _ O
, -X- _ O
and -X- _ O
otherwise -X- _ O
0 -X- _ O
. -X- _ O

The -X- _ O
detail -X- _ O
of -X- _ O
the -X- _ O
computation -X- _ O
is -X- _ O
described -X- _ O
in -X- _ O
Appendix -X- _ O
A.1 -X- _ O
. -X- _ O
Decomposable -X- _ O
Attention -X- _ O
. -X- _ O

y -X- _ O
= -X- _ O
sigmoid(DA(x -X- _ O
ext -X- _ O
1 -X- _ O
, -X- _ O
... -X- _ O
, -X- _ O
x -X- _ O
ext -X- _ O
Lâˆ’1 -X- _ O
, -X- _ O
h -X- _ O
r -X- _ O
1 -X- _ O
, -X- _ O
... -X- _ O
, -X- _ O
h -X- _ O
r -X- _ O
M -X- _ O
) -X- _ O
) -X- _ O
( -X- _ O
10 -X- _ O
) -X- _ O
where -X- _ O
DA -X- _ O
denotes -X- _ O
Decomposable -X- _ O
Attention -X- _ O
. -X- _ O

From -X- _ O
this -X- _ O
architecture -X- _ O
, -X- _ O
we -X- _ O
obtain -X- _ O
the -X- _ O
probability -X- _ O
of -X- _ O
binary -X- _ O
- -X- _ O
classification -X- _ O
y -X- _ O
through -X- _ O
the -X- _ O
sigmoid -X- _ O
function -X- _ O
. -X- _ O

Sentence -X- _ O
vectors -X- _ O
{ -X- _ O
h -X- _ O
r -X- _ O
j -X- _ O
} -X- _ O
of -X- _ O
each -X- _ O
sentence -X- _ O
{ -X- _ O
s -X- _ O
r -X- _ O
j -X- _ O
} -X- _ O
on -X- _ O
the -X- _ O
reply -X- _ O
are -X- _ O
computed -X- _ O
similarly -X- _ O
to -X- _ O
the -X- _ O
equation -X- _ O
1 -X- _ O
. -X- _ O

Suppose -X- _ O
a -X- _ O
reply -X- _ O
candidate -X- _ O
R -X- _ O
= -X- _ O
{ -X- _ O
s -X- _ O
r -X- _ O
1 -X- _ O
, -X- _ O
s -X- _ O
r -X- _ O
2 -X- _ O
, -X- _ O
... -X- _ O
, -X- _ O
s -X- _ O
r -X- _ O
M -X- _ O
} -X- _ O
has -X- _ O
M -X- _ O
sentences -X- _ O
. -X- _ O

We -X- _ O
labeled -X- _ O
actual -X- _ O
replies -X- _ O
as -X- _ O
positive -X- _ O
, -X- _ O
and -X- _ O
randomly -X- _ O
sampled -X- _ O
posts -X- _ O
as -X- _ O
negative -X- _ O
. -X- _ O

Predictor -X- _ O
Then -X- _ O
, -X- _ O
using -X- _ O
only -X- _ O
the -X- _ O
extracted -X- _ O
sentences -X- _ O
and -X- _ O
a -X- _ O
reply -X- _ O
candidate -X- _ O
, -X- _ O
the -X- _ O
Predictor -X- _ O
predicts -X- _ O
whether -X- _ O
the -X- _ O
candidate -X- _ O
is -X- _ O
an -X- _ O
actual -X- _ O
reply -X- _ O
or -X- _ O
not -X- _ O
. -X- _ O

x -X- _ O
ext -X- _ O
t -X- _ O
= -X- _ O
N -X- _ O
i=1 -X- _ O
Î± -X- _ O
ti -X- _ O
h -X- _ O
p -X- _ O
i -X- _ O
( -X- _ O
1 -X- _ O
â‰¤ -X- _ O
t -X- _ O
â‰¤ -X- _ O
L -X- _ O
) -X- _ O
( -X- _ O
8) -X- _ O
h -X- _ O
ext -X- _ O
t+1 -X- _ O
= -X- _ O
LSTM(x -X- _ O
ext -X- _ O
t -X- _ O
) -X- _ O
( -X- _ O
0 -X- _ O
â‰¤ -X- _ O
t -X- _ O
â‰¤ -X- _ O
L -X- _ O
âˆ’ -X- _ O
1 -X- _ O
) -X- _ O
( -X- _ O
9 -X- _ O
) -X- _ O
The -X- _ O
initial -X- _ O
input -X- _ O
vector -X- _ O
x -X- _ O
ext -X- _ O
0 -X- _ O
of -X- _ O
the -X- _ O
Extractor -X- _ O
is -X- _ O
a -X- _ O
parameter -X- _ O
, -X- _ O
and -X- _ O
L -X- _ O
is -X- _ O
defined -X- _ O
by -X- _ O
a -X- _ O
user -X- _ O
depending -X- _ O
on -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
sentences -X- _ O
required -X- _ O
for -X- _ O
a -X- _ O
summary -X- _ O
. -X- _ O

We -X- _ O
repeat -X- _ O
this -X- _ O
step -X- _ O
L -X- _ O
times -X- _ O
. -X- _ O

By -X- _ O
adding -X- _ O
Gumbel -X- _ O
noise -X- _ O
g -X- _ O
using -X- _ O
noise -X- _ O
u -X- _ O
from -X- _ O
a -X- _ O
uniform -X- _ O
distribution -X- _ O
, -X- _ O
the -X- _ O
attention -X- _ O
weights -X- _ O
a -X- _ O
become -X- _ O
a -X- _ O
one -X- _ O
- -X- _ O
hot -X- _ O
vector -X- _ O
. -X- _ O

During -X- _ O
the -X- _ O
training -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
Gumbel -X- _ O
Softmax -X- _ O
( -X- _ O
Jang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
to -X- _ O
make -X- _ O
this -X- _ O
discrete -X- _ O
process -X- _ O
differentiable -X- _ O
. -X- _ O

The -X- _ O
sentence -X- _ O
with -X- _ O
the -X- _ O
highest -X- _ O
attention -X- _ O
weight -X- _ O
is -X- _ O
extracted -X- _ O
. -X- _ O

h -X- _ O
ext -X- _ O
0 -X- _ O
= -X- _ O
1 -X- _ O
N -X- _ O
N -X- _ O
i=1 -X- _ O
h -X- _ O
p -X- _ O
i -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
The -X- _ O
Extractor -X- _ O
computes -X- _ O
attention -X- _ O
weights -X- _ O
using -X- _ O
the -X- _ O
hidden -X- _ O
states -X- _ O
of -X- _ O
the -X- _ O
Extractor -X- _ O
h -X- _ O
ext -X- _ O
t -X- _ O
and -X- _ O
the -X- _ O
sentence -X- _ O
features -X- _ O
h -X- _ O
p -X- _ O
i -X- _ O
computed -X- _ O
on -X- _ O
the -X- _ O
Encoder -X- _ O
. -X- _ O

We -X- _ O
set -X- _ O
the -X- _ O
mean -X- _ O
vector -X- _ O
of -X- _ O
the -X- _ O
sentence -X- _ O
features -X- _ O
of -X- _ O
the -X- _ O
Encoder -X- _ O
h -X- _ O
p -X- _ O
i -X- _ O
as -X- _ O
the -X- _ O
initial -X- _ O
hidden -X- _ O
state -X- _ O
of -X- _ O
the -X- _ O
Extractor -X- _ O
h -X- _ O
ext -X- _ O
0 -X- _ O
. -X- _ O

This -X- _ O
is -X- _ O
because -X- _ O
summaries -X- _ O
should -X- _ O
not -X- _ O
depend -X- _ O
on -X- _ O
replies -X- _ O
. -X- _ O

Note -X- _ O
that -X- _ O
the -X- _ O
Extractor -X- _ O
does -X- _ O
not -X- _ O
use -X- _ O
reply -X- _ O
features -X- _ O
for -X- _ O
extraction -X- _ O
. -X- _ O

For -X- _ O
accurate -X- _ O
prediction -X- _ O
, -X- _ O
the -X- _ O
Extractor -X- _ O
learns -X- _ O
to -X- _ O
extract -X- _ O
sentences -X- _ O
that -X- _ O
replies -X- _ O
frequently -X- _ O
refer -X- _ O
to -X- _ O
. -X- _ O

.. -X- _ O
, -X- _ O
x -X- _ O
p -X- _ O
iK -X- _ O
i -X- _ O
} -X- _ O
through -X- _ O
word -X- _ O
embedding -X- _ O
layers -X- _ O
. -X- _ O

Words -X- _ O
are -X- _ O
embedded -X- _ O
to -X- _ O
continuous -X- _ O
vectors -X- _ O
X -X- _ O
p -X- _ O
i -X- _ O
= -X- _ O
{ -X- _ O
x -X- _ O
p -X- _ O
i1 -X- _ O
, -X- _ O
x -X- _ O
p -X- _ O
i2 -X- _ O
, -X- _ O
. -X- _ O

Each -X- _ O
sentence -X- _ O
s -X- _ O
p -X- _ O
i -X- _ O
comprises -X- _ O
K -X- _ O
i -X- _ O
words -X- _ O
W -X- _ O
p -X- _ O
i -X- _ O
= -X- _ O
{ -X- _ O
w -X- _ O
p -X- _ O
i1 -X- _ O
, -X- _ O
w -X- _ O
p -X- _ O
i2 -X- _ O
, -X- _ O
... -X- _ O
, -X- _ O
w -X- _ O
p -X- _ O
iK -X- _ O
i -X- _ O
} -X- _ O
. -X- _ O

First -X- _ O
, -X- _ O
the -X- _ O
post -X- _ O
is -X- _ O
split -X- _ O
into -X- _ O
N -X- _ O
sentences -X- _ O
{ -X- _ O
s -X- _ O
p -X- _ O
1 -X- _ O
, -X- _ O
s -X- _ O
p -X- _ O
2 -X- _ O
, -X- _ O
... -X- _ O
, -X- _ O
s -X- _ O
p -X- _ O
N -X- _ O
} -X- _ O
. -X- _ O

Encoder -X- _ O
The -X- _ O
Encoder -X- _ O
computes -X- _ O
features -X- _ O
of -X- _ O
posts -X- _ O
. -X- _ O

We -X- _ O
describe -X- _ O
each -X- _ O
component -X- _ O
below -X- _ O
. -X- _ O

The -X- _ O
Encoder -X- _ O
computes -X- _ O
features -X- _ O
of -X- _ O
posts -X- _ O
, -X- _ O
the -X- _ O
Extractor -X- _ O
extracts -X- _ O
sentences -X- _ O
of -X- _ O
a -X- _ O
post -X- _ O
to -X- _ O
use -X- _ O
for -X- _ O
prediction -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
Predictor -X- _ O
predicts -X- _ O
whether -X- _ O
a -X- _ O
reply -X- _ O
candidate -X- _ O
is -X- _ O
an -X- _ O
actual -X- _ O
reply -X- _ O
or -X- _ O
not -X- _ O
. -X- _ O

The -X- _ O
model -X- _ O
comprises -X- _ O
an -X- _ O
Encoder -X- _ O
, -X- _ O
an -X- _ O
Extractor -X- _ O
, -X- _ O
and -X- _ O
a -X- _ O
Predictor -X- _ O
. -X- _ O

The -X- _ O
training -X- _ O
task -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
is -X- _ O
to -X- _ O
predict -X- _ O
whether -X- _ O
a -X- _ O
reply -X- _ O
candidate -X- _ O
is -X- _ O
true -X- _ O
or -X- _ O
not -X- _ O
. -X- _ O

A -X- _ O
reply -X- _ O
candidate -X- _ O
can -X- _ O
be -X- _ O
either -X- _ O
a -X- _ O
true -X- _ O
or -X- _ O
a -X- _ O
false -X- _ O
reply -X- _ O
to -X- _ O
the -X- _ O
post -X- _ O
. -X- _ O

The -X- _ O
inputs -X- _ O
to -X- _ O
the -X- _ O
model -X- _ O
during -X- _ O
training -X- _ O
are -X- _ O
a -X- _ O
post -X- _ O
and -X- _ O
reply -X- _ O
candidate -X- _ O
. -X- _ O

Figure -X- _ O
2 -X- _ O
shows -X- _ O
the -X- _ O
structure -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
. -X- _ O

Model -X- _ O
. -X- _ O

In -X- _ O
our -X- _ O
research -X- _ O
, -X- _ O
we -X- _ O
solely -X- _ O
focus -X- _ O
on -X- _ O
quotes -X- _ O
, -X- _ O
and -X- _ O
do -X- _ O
not -X- _ O
directly -X- _ O
use -X- _ O
quotes -X- _ O
as -X- _ O
supervision -X- _ O
; -X- _ O
rather -X- _ O
, -X- _ O
we -X- _ O
aim -X- _ O
to -X- _ O
extract -X- _ O
implicit -X- _ O
quotes -X- _ O
. -X- _ O

The -X- _ O
previous -X- _ O
research -X- _ O
used -X- _ O
quotes -X- _ O
as -X- _ O
auxiliary -X- _ O
features -X- _ O
. -X- _ O

Some -X- _ O
previous -X- _ O
work -X- _ O
( -X- _ O
Carenini -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2007;Oya -X- _ O
and -X- _ O
Carenini -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
assigned -X- _ O
weights -X- _ O
to -X- _ O
words -X- _ O
that -X- _ O
appeared -X- _ O
in -X- _ O
quotes -X- _ O
, -X- _ O
and -X- _ O
improved -X- _ O
the -X- _ O
conventional -X- _ O
centroidbased -X- _ O
methods -X- _ O
. -X- _ O

When -X- _ O
we -X- _ O
reply -X- _ O
to -X- _ O
a -X- _ O
post -X- _ O
or -X- _ O
an -X- _ O
email -X- _ O
and -X- _ O
when -X- _ O
we -X- _ O
want -X- _ O
to -X- _ O
emphasize -X- _ O
a -X- _ O
certain -X- _ O
part -X- _ O
of -X- _ O
it -X- _ O
, -X- _ O
we -X- _ O
quote -X- _ O
the -X- _ O
original -X- _ O
text -X- _ O
. -X- _ O

Dialogue -X- _ O
act -X- _ O
classification -X- _ O
is -X- _ O
a -X- _ O
classification -X- _ O
task -X- _ O
that -X- _ O
classifies -X- _ O
sentences -X- _ O
depending -X- _ O
on -X- _ O
what -X- _ O
their -X- _ O
functions -X- _ O
are -X- _ O
( -X- _ O
e.g. -X- _ O
: -X- _ O
questions -X- _ O
, -X- _ O
answers -X- _ O
, -X- _ O
greetings -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
has -X- _ O
also -X- _ O
been -X- _ O
applied -X- _ O
for -X- _ O
summarization -X- _ O
( -X- _ O
Bhatia -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2014;Oya -X- _ O
and -X- _ O
Carenini -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
. -X- _ O

A -X- _ O
few -X- _ O
used -X- _ O
path -X- _ O
scores -X- _ O
of -X- _ O
word -X- _ O
graphs -X- _ O
( -X- _ O
Mehdad -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2014;Shang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
. -X- _ O

! -X- _ O
" -X- _ O
! -X- _ O
! -X- _ O
# -X- _ O
! -X- _ O
! -X- _ O
$ -X- _ O
! -X- _ O
! -X- _ O
" -X- _ O
% -X- _ O
& -X- _ O
' -X- _ O
! -X- _ O
# -X- _ O
% -X- _ O
& -X- _ O
' -X- _ O
! -X- _ O
( -X- _ O
% -X- _ O
& -X- _ O
' -X- _ O
â€¦ -X- _ O
! -X- _ O
" -X- _ O
) -X- _ O
! -X- _ O
# -X- _ O
) -X- _ O
! -X- _ O
* -X- _ O
) -X- _ O
â€¦ -X- _ O
â€¦ -X- _ O
â€¦ -X- _ O
" -X- _ O
" -X- _ O
! -X- _ O
Split -X- _ O
to -X- _ O
sentences -X- _ O
Attention -X- _ O
& -X- _ O
Gumbel -X- _ O
Softmax -X- _ O
# -X- _ O
" -X- _ O
% -X- _ O
& -X- _ O
' -X- _ O
â‰“ -X- _ O
! -X- _ O
+ -X- _ O
! -X- _ O
# -X- _ O
, -X- _ O
% -X- _ O
& -X- _ O
' -X- _ O
# -X- _ O
( -X- _ O
% -X- _ O
& -X- _ O
' -X- _ O
â‰“ -X- _ O
! -X- _ O
- -X- _ O
! -X- _ O
" -X- _ O
# -X- _ O
! -X- _ O
" -X- _ O
$ -X- _ O
! -X- _ O
" -X- _ O
" -X- _ O
) -X- _ O
" -X- _ O
# -X- _ O
) -X- _ O
" -X- _ O
* -X- _ O
) -X- _ O
BiLSTM -X- _ O
BiLSTM -X- _ O
BiLSTM -X- _ O
â€¦ -X- _ O
Research -X- _ O
on -X- _ O
the -X- _ O
summarization -X- _ O
of -X- _ O
online -X- _ O
conversations -X- _ O
such -X- _ O
as -X- _ O
mail -X- _ O
, -X- _ O
chat -X- _ O
, -X- _ O
social -X- _ O
media -X- _ O
, -X- _ O
and -X- _ O
online -X- _ O
discussion -X- _ O
fora -X- _ O
has -X- _ O
been -X- _ O
conducted -X- _ O
for -X- _ O
a -X- _ O
long -X- _ O
time -X- _ O
. -X- _ O

Another -X- _ O
research -X- _ O
employed -X- _ O
a -X- _ O
task -X- _ O
to -X- _ O
reconstruct -X- _ O
masked -X- _ O
sentences -X- _ O
for -X- _ O
summarization -X- _ O
( -X- _ O
Laban -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O

( -X- _ O
2019 -X- _ O
) -X- _ O
( -X- _ O
2019 -X- _ O
) -X- _ O
generated -X- _ O
summaries -X- _ O
from -X- _ O
mean -X- _ O
vectors -X- _ O
of -X- _ O
review -X- _ O
vectors -X- _ O
, -X- _ O
and -X- _ O
Amplayo -X- _ O
and -X- _ O
Lapata -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
employed -X- _ O
the -X- _ O
prior -X- _ O
distribution -X- _ O
of -X- _ O
Variational -X- _ O
Auto -X- _ O
- -X- _ O
Encoder -X- _ O
to -X- _ O
induce -X- _ O
summaries -X- _ O
. -X- _ O

For -X- _ O
review -X- _ O
abstractive -X- _ O
summarization -X- _ O
, -X- _ O
Isonuma -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

( -X- _ O
2019 -X- _ O
) -X- _ O
employed -X- _ O
the -X- _ O
reconstruction -X- _ O
task -X- _ O
of -X- _ O
the -X- _ O
original -X- _ O
sentence -X- _ O
from -X- _ O
a -X- _ O
compressed -X- _ O
one -X- _ O
. -X- _ O

Baziotis -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

For -X- _ O
sentence -X- _ O
compression -X- _ O
, -X- _ O
Fevry -X- _ O
and -X- _ O
Phang -X- _ O
( -X- _ O
2018 -X- _ O
) -X- _ O
employed -X- _ O
the -X- _ O
task -X- _ O
to -X- _ O
reorder -X- _ O
the -X- _ O
shuffled -X- _ O
word -X- _ O
order -X- _ O
of -X- _ O
sentences -X- _ O
. -X- _ O

As -X- _ O
for -X- _ O
end -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
end -X- _ O
unsupervised -X- _ O
neural -X- _ O
models -X- _ O
, -X- _ O
a -X- _ O
few -X- _ O
abstractive -X- _ O
models -X- _ O
have -X- _ O
been -X- _ O
proposed -X- _ O
. -X- _ O

That -X- _ O
is -X- _ O
, -X- _ O
our -X- _ O
model -X- _ O
can -X- _ O
extract -X- _ O
salient -X- _ O
sentences -X- _ O
that -X- _ O
conventional -X- _ O
methods -X- _ O
fail -X- _ O
to -X- _ O
. -X- _ O

These -X- _ O
methods -X- _ O
assume -X- _ O
that -X- _ O
important -X- _ O
topics -X- _ O
appear -X- _ O
frequently -X- _ O
in -X- _ O
a -X- _ O
document -X- _ O
, -X- _ O
but -X- _ O
our -X- _ O
model -X- _ O
focuses -X- _ O
on -X- _ O
a -X- _ O
different -X- _ O
aspect -X- _ O
of -X- _ O
texts -X- _ O
: -X- _ O
the -X- _ O
probability -X- _ O
of -X- _ O
being -X- _ O
quoted -X- _ O
. -X- _ O

Other -X- _ O
models -X- _ O
use -X- _ O
reconstruction -X- _ O
loss -X- _ O
( -X- _ O
He -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2012;Liu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015;Ma -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
, -X- _ O
Kullback -X- _ O
- -X- _ O
Leibler -X- _ O
divergence -X- _ O
( -X- _ O
Haghighi -X- _ O
and -X- _ O
Vanderwende -X- _ O
, -X- _ O
2009 -X- _ O
) -X- _ O
or -X- _ O
path -X- _ O
score -X- _ O
calculation -X- _ O
( -X- _ O
Mehdad -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2014;Shang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
based -X- _ O
on -X- _ O
multi -X- _ O
- -X- _ O
sentence -X- _ O
compression -X- _ O
algorithm -X- _ O
( -X- _ O
Filippova -X- _ O
, -X- _ O
2010 -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
graph -X- _ O
- -X- _ O
centrality -X- _ O
- -X- _ O
based -X- _ O
method -X- _ O
( -X- _ O
Mihalcea -X- _ O
and -X- _ O
Tarau -X- _ O
, -X- _ O
2004;Erkan -X- _ O
and -X- _ O
Radev -X- _ O
, -X- _ O
2004;Zheng -X- _ O
and -X- _ O
Lapata -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
and -X- _ O
centroid -X- _ O
- -X- _ O
based -X- _ O
method -X- _ O
( -X- _ O
Gholipour -X- _ O
Ghalandari -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
have -X- _ O
been -X- _ O
major -X- _ O
methods -X- _ O
in -X- _ O
this -X- _ O
field -X- _ O
. -X- _ O

Related -X- _ O
Works -X- _ O
. -X- _ O

The -X- _ O
contributions -X- _ O
of -X- _ O
our -X- _ O
research -X- _ O
are -X- _ O
as -X- _ O
follows -X- _ O
: -X- _ O
â€¢ -X- _ O
We -X- _ O
verified -X- _ O
that -X- _ O
" -X- _ O
the -X- _ O
possibility -X- _ O
of -X- _ O
being -X- _ O
quoted -X- _ O
" -X- _ O
is -X- _ O
useful -X- _ O
for -X- _ O
summarization -X- _ O
, -X- _ O
and -X- _ O
demonstrated -X- _ O
that -X- _ O
it -X- _ O
reflects -X- _ O
an -X- _ O
important -X- _ O
aspect -X- _ O
of -X- _ O
saliency -X- _ O
that -X- _ O
conventional -X- _ O
methods -X- _ O
do -X- _ O
not -X- _ O
. -X- _ O

Furthermore -X- _ O
, -X- _ O
we -X- _ O
both -X- _ O
quantitatively -X- _ O
and -X- _ O
qualitatively -X- _ O
analyzed -X- _ O
that -X- _ O
our -X- _ O
model -X- _ O
can -X- _ O
capture -X- _ O
salient -X- _ O
sentences -X- _ O
that -X- _ O
conventional -X- _ O
frequency -X- _ O
- -X- _ O
based -X- _ O
methods -X- _ O
can -X- _ O
not -X- _ O
. -X- _ O

Our -X- _ O
model -X- _ O
is -X- _ O
based -X- _ O
on -X- _ O
a -X- _ O
hypothesis -X- _ O
that -X- _ O
the -X- _ O
ability -X- _ O
of -X- _ O
extracting -X- _ O
quotes -X- _ O
leads -X- _ O
to -X- _ O
a -X- _ O
good -X- _ O
result -X- _ O
. -X- _ O

The -X- _ O
model -X- _ O
requires -X- _ O
replies -X- _ O
only -X- _ O
during -X- _ O
the -X- _ O
training -X- _ O
and -X- _ O
not -X- _ O
during -X- _ O
the -X- _ O
evaluation -X- _ O
. -X- _ O

The -X- _ O
training -X- _ O
task -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
is -X- _ O
to -X- _ O
predict -X- _ O
if -X- _ O
a -X- _ O
reply -X- _ O
candidate -X- _ O
is -X- _ O
an -X- _ O
actual -X- _ O
reply -X- _ O
to -X- _ O
the -X- _ O
post -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
pairs -X- _ O
of -X- _ O
a -X- _ O
post -X- _ O
and -X- _ O
reply -X- _ O
candidate -X- _ O
to -X- _ O
train -X- _ O
the -X- _ O
model -X- _ O
. -X- _ O

As -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
1 -X- _ O
, -X- _ O
implicit -X- _ O
quotes -X- _ O
are -X- _ O
sentences -X- _ O
of -X- _ O
posts -X- _ O
that -X- _ O
are -X- _ O
not -X- _ O
explicitly -X- _ O
quoted -X- _ O
in -X- _ O
replies -X- _ O
, -X- _ O
but -X- _ O
are -X- _ O
those -X- _ O
the -X- _ O
replies -X- _ O
most -X- _ O
likely -X- _ O
refer -X- _ O
to -X- _ O
. -X- _ O

We -X- _ O
propose -X- _ O
a -X- _ O
model -X- _ O
that -X- _ O
can -X- _ O
be -X- _ O
trained -X- _ O
without -X- _ O
explicit -X- _ O
labels -X- _ O
of -X- _ O
quotes -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
most -X- _ O
replies -X- _ O
do -X- _ O
not -X- _ O
include -X- _ O
quotes -X- _ O
, -X- _ O
so -X- _ O
it -X- _ O
is -X- _ O
difficult -X- _ O
to -X- _ O
use -X- _ O
quotes -X- _ O
as -X- _ O
the -X- _ O
training -X- _ O
labels -X- _ O
of -X- _ O
neural -X- _ O
models -X- _ O
. -X- _ O

Previous -X- _ O
research -X- _ O
assigned -X- _ O
weights -X- _ O
to -X- _ O
words -X- _ O
that -X- _ O
appear -X- _ O
in -X- _ O
quotes -X- _ O
, -X- _ O
and -X- _ O
improved -X- _ O
the -X- _ O
centroidbased -X- _ O
summarization -X- _ O
( -X- _ O
Carenini -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2007;Oya -X- _ O
and -X- _ O
Carenini -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
. -X- _ O

Thus -X- _ O
, -X- _ O
we -X- _ O
aim -X- _ O
to -X- _ O
extract -X- _ O
quotes -X- _ O
as -X- _ O
summaries -X- _ O
. -X- _ O

If -X- _ O
we -X- _ O
can -X- _ O
predict -X- _ O
quoted -X- _ O
parts -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
extract -X- _ O
important -X- _ O
sentences -X- _ O
irrespective -X- _ O
of -X- _ O
how -X- _ O
frequently -X- _ O
the -X- _ O
same -X- _ O
topic -X- _ O
appears -X- _ O
in -X- _ O
the -X- _ O
text -X- _ O
. -X- _ O

The -X- _ O
reply -X- _ O
on -X- _ O
the -X- _ O
bottom -X- _ O
includes -X- _ O
a -X- _ O
quote -X- _ O
, -X- _ O
which -X- _ O
generally -X- _ O
starts -X- _ O
with -X- _ O
a -X- _ O
symbol -X- _ O
" -X- _ O
> -X- _ O
" -X- _ O
. -X- _ O

When -X- _ O
one -X- _ O
replies -X- _ O
to -X- _ O
an -X- _ O
email -X- _ O
or -X- _ O
a -X- _ O
post -X- _ O
, -X- _ O
a -X- _ O
quote -X- _ O
is -X- _ O
used -X- _ O
to -X- _ O
highlight -X- _ O
the -X- _ O
important -X- _ O
parts -X- _ O
of -X- _ O
the -X- _ O
text -X- _ O
; -X- _ O
an -X- _ O
example -X- _ O
is -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
1 -X- _ O
. -X- _ O

As -X- _ O
an -X- _ O
alternative -X- _ O
aspect -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
" -X- _ O
the -X- _ O
probability -X- _ O
of -X- _ O
being -X- _ O
quoted -X- _ O
" -X- _ O
. -X- _ O

For -X- _ O
more -X- _ O
accurate -X- _ O
summarization -X- _ O
, -X- _ O
relying -X- _ O
solely -X- _ O
on -X- _ O
the -X- _ O
frequency -X- _ O
is -X- _ O
not -X- _ O
sufficient -X- _ O
and -X- _ O
we -X- _ O
need -X- _ O
to -X- _ O
focus -X- _ O
on -X- _ O
other -X- _ O
aspects -X- _ O
of -X- _ O
texts -X- _ O
. -X- _ O

Therefore -X- _ O
, -X- _ O
if -X- _ O
important -X- _ O
topics -X- _ O
appear -X- _ O
only -X- _ O
a -X- _ O
few -X- _ O
times -X- _ O
, -X- _ O
these -X- _ O
methods -X- _ O
fail -X- _ O
to -X- _ O
capture -X- _ O
salient -X- _ O
sentences -X- _ O
. -X- _ O

The -X- _ O
premise -X- _ O
of -X- _ O
these -X- _ O
methods -X- _ O
is -X- _ O
that -X- _ O
important -X- _ O
topics -X- _ O
appear -X- _ O
frequently -X- _ O
in -X- _ O
a -X- _ O
document -X- _ O
. -X- _ O

Apart -X- _ O
from -X- _ O
centrality -X- _ O
, -X- _ O
centroid -X- _ O
of -X- _ O
vectors -X- _ O
( -X- _ O
Gholipour -X- _ O
Ghalandari -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
, -X- _ O
Kullback -X- _ O
- -X- _ O
Leibler -X- _ O
divergence -X- _ O
( -X- _ O
Haghighi -X- _ O
and -X- _ O
Vanderwende -X- _ O
, -X- _ O
2009 -X- _ O
) -X- _ O
, -X- _ O
reconstruction -X- _ O
loss -X- _ O
( -X- _ O
He -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2012;Liu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015;Ma -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
path -X- _ O
scores -X- _ O
of -X- _ O
word -X- _ O
graphs -X- _ O
( -X- _ O
Mehdad -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2014;Shang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
, -X- _ O
are -X- _ O
leveraged -X- _ O
for -X- _ O
summarization -X- _ O
. -X- _ O

Because -X- _ O
it -X- _ O
is -X- _ O
not -X- _ O
realistic -X- _ O
to -X- _ O
prepare -X- _ O
such -X- _ O
large -X- _ O
datasets -X- _ O
for -X- _ O
every -X- _ O
domain -X- _ O
, -X- _ O
there -X- _ O
is -X- _ O
a -X- _ O
growing -X- _ O
requirement -X- _ O
for -X- _ O
unsupervised -X- _ O
methods -X- _ O
. -X- _ O

Supervised -X- _ O
summarization -X- _ O
requires -X- _ O
tens -X- _ O
of -X- _ O
thousands -X- _ O
of -X- _ O
human -X- _ O
- -X- _ O
annotated -X- _ O
summaries -X- _ O
. -X- _ O

Introduction -X- _ O
. -X- _ O

We -X- _ O
further -X- _ O
discuss -X- _ O
two -X- _ O
topics -X- _ O
; -X- _ O
one -X- _ O
is -X- _ O
whether -X- _ O
quote -X- _ O
extraction -X- _ O
is -X- _ O
an -X- _ O
important -X- _ O
factor -X- _ O
for -X- _ O
summarization -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
other -X- _ O
is -X- _ O
whether -X- _ O
our -X- _ O
model -X- _ O
can -X- _ O
capture -X- _ O
salient -X- _ O
sentences -X- _ O
that -X- _ O
conventional -X- _ O
methods -X- _ O
can -X- _ O
not -X- _ O
. -X- _ O

To -X- _ O
predict -X- _ O
accurately -X- _ O
, -X- _ O
the -X- _ O
model -X- _ O
learns -X- _ O
to -X- _ O
extract -X- _ O
sentences -X- _ O
that -X- _ O
replies -X- _ O
frequently -X- _ O
refer -X- _ O
to -X- _ O
. -X- _ O

For -X- _ O
prediction -X- _ O
, -X- _ O
the -X- _ O
model -X- _ O
has -X- _ O
to -X- _ O
choose -X- _ O
a -X- _ O
few -X- _ O
sentences -X- _ O
from -X- _ O
the -X- _ O
post -X- _ O
. -X- _ O

The -X- _ O
training -X- _ O
task -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
is -X- _ O
to -X- _ O
predict -X- _ O
whether -X- _ O
a -X- _ O
reply -X- _ O
candidate -X- _ O
is -X- _ O
a -X- _ O
true -X- _ O
reply -X- _ O
to -X- _ O
a -X- _ O
post -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
even -X- _ O
if -X- _ O
it -X- _ O
is -X- _ O
not -X- _ O
explicitly -X- _ O
shown -X- _ O
, -X- _ O
replies -X- _ O
always -X- _ O
refer -X- _ O
to -X- _ O
certain -X- _ O
parts -X- _ O
of -X- _ O
texts -X- _ O
; -X- _ O
we -X- _ O
call -X- _ O
them -X- _ O
implicit -X- _ O
quotes -X- _ O
. -X- _ O

Most -X- _ O
replies -X- _ O
do -X- _ O
not -X- _ O
explicitly -X- _ O
include -X- _ O
quotes -X- _ O
, -X- _ O
so -X- _ O
it -X- _ O
is -X- _ O
difficult -X- _ O
to -X- _ O
use -X- _ O
quotes -X- _ O
as -X- _ O
supervision -X- _ O
. -X- _ O

We -X- _ O
aim -X- _ O
to -X- _ O
extract -X- _ O
quoted -X- _ O
sentences -X- _ O
as -X- _ O
summaries -X- _ O
. -X- _ O

When -X- _ O
we -X- _ O
reply -X- _ O
to -X- _ O
posts -X- _ O
, -X- _ O
quotes -X- _ O
are -X- _ O
used -X- _ O
to -X- _ O
highlight -X- _ O
important -X- _ O
part -X- _ O
of -X- _ O
texts -X- _ O
. -X- _ O

