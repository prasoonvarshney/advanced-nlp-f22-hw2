For	O
example	O
,	O
word	O
"	O
allocation	O
"	O
has	O
a	O
new	O
distance-2	O
word	O
"	O
topics	O
"	O
after	O
merging	O
.	O

Experiments	O
on	O
two	O
datasets	O
show	O
EGTRF	B-MethodName
achieves	O
better	O
performance	O
than	O
GTRF	B-MethodName
and	O
LDA	B-MethodName
,	O
which	O
confirm	O
our	O
assumption	O
that	O
adding	O
topical	O
dependency	O
of	O
distance-2	B-HyperparameterName
words	O
and	O
incorporating	O
word	O
similarity	O
information	O
can	O
improve	O
model	O
performance	O
.	O

Word	O
topics	O
are	O
drawed	O
by	O
Extended	B-MethodName
Global	I-MethodName
Random	I-MethodName
Field(EGRF	I-MethodName
)	O
instead	O
of	O
Multinomial	O
,	O
the	O
conditional	O
independence	O
of	O
word	O
topic	O
assignment	O
is	O
thus	O
relaxed	O
.	O

In	O
this	O
paper	O
,	O
we	O
extended	O
Global	B-MethodName
Topic	I-MethodName
Random	I-MethodName
Field(GTRF	I-MethodName
)	O
and	O
proposed	O
a	O
novel	O
topic	O
model	O
Extended	B-MethodName
Global	I-MethodName
Topic	I-MethodName
Random	I-MethodName
Field(EGTRF	I-MethodName
)	O
which	O
can	O
model	O
dependency	O
relation	O
between	O
adjacent	O
words	O
and	O
distance-2	B-HyperparameterName
words	O
.	O

And	O
we	O
define	O
the	O
topic	B-TaskName
model	I-TaskName
based	O
on	O
EGRF	B-MethodName
as	O
Extended	B-MethodName
Global	I-MethodName
Topic	I-MethodName
Random	I-MethodName
Field(EGTRF	I-MethodName
)	O
.	O

We	O
define	O
the	O
random	O
field	O
as	O
in	O
Equation	O
(	O
1	O
)	O
a	O
Extended	B-MethodName
Global	I-MethodName
Random	I-MethodName
Field	I-MethodName
(	O
EGRF	B-MethodName
)	O
.	O

In	O
this	O
section	O
,	O
we	O
first	O
present	O
Extended	B-MethodName
Global	I-MethodName
Random	I-MethodName
Field(EGRF	I-MethodName
)	O
in	O
section	O
2.1	O
,	O
then	O
show	O
how	O
to	O
model	O
topical	O
dependencies	O
using	O
EGRF	B-MethodName
in	O
section	O
2.2	O
.	O
We	O
incorporate	O
word	O
similarity	O
information	O
into	O
model	O
in	O
section	O
2.3	O
.	O
1	O
https://code.google.com/p/word2vec/.	O

In	O
this	O
paper	O
,	O
we	O
extend	O
GTRF	B-MethodName
model	O
and	O
present	O
a	O
novel	O
model	O
Extended	B-MethodName
Global	I-MethodName
Topic	I-MethodName
Random	I-MethodName
Field	I-MethodName
(	O
EGTRF	B-MethodName
)	O
to	O
exploit	O
topical	O
dependency	O
between	O
words	O
.	O

In	O
Global	B-MethodName
Topic	I-MethodName
Random	I-MethodName
Field(GTRF	I-MethodName
)	O
model	O
(	O
Li	O
et	O
al	O
,	O
2014	O
)	O
,	O
sentences	O
of	O
a	O
document	O
are	O
parsed	O
into	O
dependency	O
trees	O
(	O
Marneffe	O
et	O
al	O
,	O
2008	O
)	O
(	O
Manning	O
et	O
al	O
,	O
2014	O
)	O
(	O
Marneffe	O
et	O
al	O
,	O
2006	O
)	O
.	O

Probabilistic	O
topic	O
model	O
such	O
as	O
Latent	B-MethodName
Dirichlet	I-MethodName
Allocation(LDA	I-MethodName
)	O
(	O
Blei	O
et	O
al	O
,	O
2003	O
)	O
has	O
been	O
widely	O
used	O
for	O
discovering	B-TaskName
latent	I-TaskName
topics	I-TaskName
from	O
document	O
collections	O
by	O
capturing	O
words	O
'	O
cooccuring	O
relation	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
new	O
model	O
Extended	B-MethodName
Global	I-MethodName
Topic	I-MethodName
Random	I-MethodName
Field	I-MethodName
(	O
EGTRF	B-MethodName
)	O
to	O
model	O
non	O
-	O
linear	O
dependencies	O
between	O
words	O
.	O

Topic	O
Model	O
such	O
as	O
Latent	B-MethodName
Dirichlet	I-MethodName
Allocation(LDA	I-MethodName
)	O
makes	O
assumption	O
that	O
topic	O
assignment	O
of	O
different	O
words	O
are	O
conditionally	O
independent	O
.	O

As	O
we	O
discussed	O
in	O
section	O
1	O
,	O
word	O
similarity	O
information	O
S	O
w	O
1	O
,	O
w	O
2	O
works	O
as	O
a	O
confidence	O
score	O
to	O
model	O
how	O
likely	O
two	O
words	O
on	O
an	O
edge	O
have	O
same	O
topic	O
.	O

S	O
w	O
1	O
,	O
w	O
2	O
is	O
the	O
similarity	O
measure	O
between	O
word	O
w	O
1	O
and	O
w	O
2	O
.	O

And	O
we	O
make	O
assumption	O
that	O
two	O
words	O
are	O
more	O
likely	O
to	O
have	O
same	O
topic	O
if	O
they	O
have	O
a	O
higher	O
similarity	O
score	O
.	O

To	O
get	O
the	O
similarity	O
score	O
between	O
words	O
,	O
we	O
use	O
word2vec	O
tool	O
to	O
learn	O
the	O
word	O
representation	O
of	O
each	O
word	O
from	O
pre	O
-	O
trained	O
model	O
.	O

Normalized	O
similarity	O
between	O
word	O
vectors	O
can	O
be	O
regarded	O
as	O
the	O
confidence	O
score	O
of	O
how	O
possible	O
two	O
words	O
have	O
same	O
topic	O
.	O

E	O
2	O
is	O
distance-2	O
edge	O
set	O
,	O
E	O
2	O
=	O
{	O
(	O
w	O
i	O
,	O
w	O
j	O
)	O
|∃path	O
between	O
w	O
i	O
,	O
w	O
j	O
that	O
length	O
is	O
2	O
}	O
.	O

E	O
1	O
is	O
distance-1	O
edge	O
set	O
,	O
E	O
1	O
=	O
{	O
(	O
w	O
i	O
,	O
w	O
j	O
)	O
|∃path	O
between	O
w	O
i	O
,	O
w	O
j	O
that	O
length	O
is	O
1	O
}	O
.	O

After	O
representing	O
document	O
to	O
undirected	O
graph	O
on	O
previous	O
section	O
,	O
we	O
extend	O
Global	B-MethodName
Random	I-MethodName
Field	I-MethodName
and	O
give	O
the	O
definition	O
of	O
Extended	B-MethodName
Global	I-MethodName
Random	I-MethodName
Field	I-MethodName
to	O
model	O
the	O
graph	O
as	O
below	O
:	O
Given	O
an	O
undirected	O
graph	O
G	O
,	O
word	O
vertex	O
set	O
is	O
denoted	O
as	O
W	O
=	O
{	O
w	O
i	O
|i	O
=	O
1	O
,	O
2	O
,	O
..	O
n	O
}	O
,	O
where	O
w	O
i	O
is	O
a	O
word	O
vertex	O
,	O
and	O
n	O
is	O
the	O
number	O
of	O
unique	O
words	O
in	O
a	O
document	O
.	O

Extended	B-MethodName
Global	I-MethodName
Topic	I-MethodName
Random	I-MethodName
Field	I-MethodName
.	O

We	O
use	O
the	O
pretrained	O
model	O
from	O
Google	B-DatasetName
News	I-DatasetName
dataset(about	O
100	O
billion	O
words	O
)	O
using	O
word2vec	O
1	O
tool	O
to	O
represent	O
each	O
word	O
as	O
a	O
300	O
-	O
dimensional	O
word	O
vector	O
,	O
and	O
apply	O
normalized	O
word	O
similarity	O
as	O
a	O
confidence	O
score	O
to	O
indicate	O
how	O
possible	O
two	O
word	O
vertices	O
share	O
same	O
topic	O
.	O

Theoretically	O
,	O
we	O
can	O
also	O
model	O
the	O
distance	O
further	O
than	O
2	O
,	O
however	O
,	O
it	O
leads	O
to	O
more	O
complicated	O
computation	O
and	O
small	O
increase	O
of	O
performance	O
.	O

In	O
EGTRF	B-MethodName
,	O
the	O
topic	O
assignment	O
of	O
a	O
word	O
is	O
assumed	O
to	O
depend	O
on	O
both	O
distance-1	O
and	O
distance-2	O
word	O
vertices	O
.	O

However	O
,	O
GTRF	B-MethodName
assumes	O
topic	O
assignment	O
of	O
a	O
word	O
vertex	O
depends	O
on	O
the	O
topic	O
mixture	O
of	O
the	O
document	O
and	O
its	O
neighboring	O
word	O
vertices	O
,	O
ignoring	O
the	O
fact	O
that	O
word	O
vertex	O
can	O
also	O
be	O
influenced	O
by	O
the	O
distance-2	O
or	O
further	O
word	O
vertices	O
.	O

Specifically	O
,	O
we	O
parse	O
sentences	O
into	O
dependency	O
trees	O
and	O
represent	O
them	O
as	O
a	O
graph	O
,	O
and	O
assume	O
the	O
topic	B-TaskName
assignment	I-TaskName
of	O
a	O
word	O
is	O
influenced	O
by	O
its	O
adjacent	O
words	O
and	O
distance-2	O
words	O
.	O

Extended	B-MethodName
Topic	I-MethodName
Model	I-MethodName
for	O
Word	O
Dependency	O
.	O

We	O
believe	O
modeling	O
distance-2	B-HyperparameterName
word	O
vertices	O
can	O
exploit	O
more	O
semantically	O
or	O
syntactically	O
word	O
dependencies	O
from	O
document	O
,	O
and	O
word	O
similarity	O
information	O
obtained	O
from	O
large	O
corpus	O
can	O
make	O
up	O
the	O
lack	O
of	O
sufficient	O
information	O
from	O
the	O
original	O
corpus	O
.	O

Therefore	O
,	O
adding	O
the	O
influence	O
of	O
distance-2	B-HyperparameterName
word	O
vertices	O
and	O
word	O
similarity	O
information	O
can	O
improve	O
performance	O
of	O
topic	B-TaskName
modeling	I-TaskName
.	O

Figure	O
2	O
shows	O
the	O
experimental	O
results	O
of	O
four	O
models	O
:	O
lda	B-MethodName
,	O
gtrf	B-MethodName
,	O
egtrf(EGTRF	B-MethodName
without	B-MethodName
word	I-MethodName
similarity	I-MethodName
information	I-MethodName
)	O
,	O
and	O
egtrf+s(EGTRF	B-MethodName
with	B-MethodName
word	I-MethodName
similarity	I-MethodName
information	I-MethodName
)	O
on	O
two	O
datasets	O
.	O

The	O
results	O
show	O
EGTRF	B-MethodName
outperforms	O
LDA	B-MethodName
and	O
GTRF	B-MethodName
in	O
general	O
,	O
and	O
EGTRF	B-MethodName
with	I-MethodName
word	I-MethodName
similarity	I-MethodName
information	I-MethodName
achieves	O
best	O
performance	O
.	O

We	O
choose	O
10	B-HyperparameterValue
,	O
20	B-HyperparameterValue
,	O
30	B-HyperparameterValue
,	O
50	B-HyperparameterValue
topics	B-HyperparameterName
for	O
20	B-HyperparameterValue
news	B-DatasetName
dataset	I-DatasetName
,	O
10	B-HyperparameterValue
,	O
15	B-HyperparameterValue
,	O
20	B-HyperparameterValue
,	O
25	B-HyperparameterValue
topics	O
for	O
NIPS	B-DatasetName
dataset	O
.	O

Word	O
is	O
represented	O
as	O
vector	O
from	O
pretrained	O
Google	B-DatasetName
News	I-DatasetName
dataset	O
,	O
we	O
use	O
the	O
word	O
vector	O
learned	O
from	O
original	O
corpus	O
when	O
the	O
word	O
does	O
not	O
exist	O
in	O
pre	O
-	O
trained	O
Google	B-DatasetName
News	I-DatasetName
dataset	O
.	O

We	O
set	O
λ	B-HyperparameterName
4	I-HyperparameterName
=	O
1.2	B-HyperparameterValue
to	O
give	O
lower(even	O
negative	O
)	O
reward	O
to	O
edges	O
from	O
E	O
2	O
that	O
the	O
two	O
word	O
vertices	O
have	O
same	O
topic	O
in	O
EGTRF	B-MethodName
,	O
since	O
the	O
distance-1	B-HyperparameterName
words	O
are	O
expected	O
to	O
have	O
greater	O
topical	O
affects	O
than	O
distance-2	B-HyperparameterName
words	O
.	O

We	O
implement	O
GTRF	B-MethodName
without	O
adding	O
self	O
defined	O
edges	O
from	O
the	O
original	O
paper	O
,	O
and	O
set	O
λ	B-HyperparameterName
2	I-HyperparameterName
=	O
0.2	B-HyperparameterValue
to	O
give	O
higher	O
reward	O
to	O
edges	O
from	O
E	O
1	O
that	O
the	O
two	O
word	O
vertices	O
have	O
same	O
topic	O
.	O

Lower	O
perplexity	B-MetricName
,	O
higher	O
log	B-MetricName
predictive	I-MetricName
probability	I-MetricName
indicate	O
better	O
generalization	O
performance	O
.	O

We	O
evaluate	O
how	O
well	O
a	O
model	O
fits	O
the	O
data	O
with	O
held	B-MetricName
-	I-MetricName
out	I-MetricName
perplexity	I-MetricName
(	O
Blei	O
et	O
al	O
,	O
2003	O
)	O
and	O
predictive	B-MetricName
distribution	I-MetricName
(	O
Hoffman	O
et	O
al	O
,	O
2013	O
)	O
.	O

•	O
NIPS	B-DatasetName
data	I-DatasetName
(	O
Globerson	O
et	O
al	O
,	O
2004	O
):	O
Spanning	O
from	O
2000	O
to	O
2005	O
.	O

•	O
20	B-DatasetName
News	I-DatasetName
Groups	I-DatasetName
:	O
After	O
processing	O
,	O
it	O
contains	O
13706	O
documents	O
with	O
a	O
vocabulary	O
of	O
5164	O
terms	O
.	O

In	O
this	O
section	O
we	O
study	O
the	O
empirical	O
performance	O
of	O
EGTRF	B-MethodName
on	O
two	O
datasets	O
.	O

The	O
updating	O
rule	O
of	O
α	O
and	O
β	O
are	O
same	O
to	O
LDA	B-MethodName
,	O
γ	O
is	O
updated	O
using	O
Newton	O
method	O
since	O
we	O
can	O
not	O
obtain	O
the	O
direct	O
updating	O
rule	O
for	O
γ	O
.	O

All	O
terms	O
except	O
P	O
(	O
z|θ	O
)	O
in	O
likelihood	O
function	O
are	O
also	O
same	O
to	O
LDA	B-MethodName
,	O
Based	O
on	O
Equation	O
(	O
9	O
)	O
,	O
we	O
obtain	O
:	O
Eq[log	O
P	O
egrf	O
(	O
z	O
|	O
θ	O
)	O
]	O
≈Eq[log	O
(	O
n	O
M	O
ulti(zw	O
n	O
|	O
θ))]+	O
1	O
−	O
λ2	O
ζ1	O
Eq(|	O
E	O
C	O
1	O
|	O
)	O
+	O
1	O
−	O
λ4	O
ζ1	O
Eq(|	O
E	O
C	O
2	O
|)+	O
(	O
|	O
E1	O
|	O
λ2	O
+	O
|	O
E2	O
|	O
λ4	O
ζ1	O
−	O
|	O
E1	O
|	O
+	O
|	O
E2	O
|	O
ζ2	O
)	O
Eq(θ	O
T	O
θ)+	O
log	O
ζ1	O
−	O
log	O
ζ2(11	O
)	O
We	O
get	O
the	O
approximation	O
in	O
Equation	O
(	O
11	O
)	O
from	O
Taylor	O
series	O
,	O
where	O
ζ	O
1	O
and	O
ζ	O
2	O
are	O
Taylor	O
approximation	O
.	O

The	O
variational	O
function	O
q	O
is	O
same	O
to	O
the	O
original	O
LDA	B-MethodName
paper	O
(	O
Blei	O
et	O
al	O
,	O
2003	O
)	O
.	O

The	O
expectation	O
of	O
the	O
number	O
of	O
edges	O
in	O
E	O
c	O
i	O
can	O
be	O
computed	O
as	O
:	O
E(|	O
E	O
C	O
i	O
|	O
)	O
=	O
(	O
w	O
1	O
,	O
w	O
2	O
)	O
∈E	O
i	O
φ	O
T	O
w	O
1	O
φw	O
2	O
Sw	O
1	O
,	O
w	O
2	O
(	O
10	O
)	O
φ	O
is	O
the	O
K	O
dimensional	O
variational	O
multinomial	O
parameters	O
and	O
can	O
be	O
thought	O
as	O
the	O
posterior	O
probability	O
of	O
a	O
word	O
given	O
the	O
topic	B-TaskName
assignment	I-TaskName
.	O

If	O
|E	O
2	O
|	O
=	O
0	O
,	O
|E	O
1	O
|	O
=	O
0	O
,	O
EGTRF	B-MethodName
is	O
equivalent	O
to	O
GTRF	B-MethodName
.	O

If	O
(	O
7	O
)	O
and	O
(	O
8)	O
hold	O
true	O
,	O
Equation	O
(	O
3	O
)	O
is	O
an	O
EGRF	B-MethodName
.	O

In	O
order	O
to	O
model	O
Equation	O
(	O
3	O
)	O
as	O
an	O
EGRF	B-MethodName
,	O
it	O
must	O
satisfy	O
all	O
the	O
four	O
constraints	O
in	O
Equation	O
(	O
1	O
)	O
.	O

According	O
to	O
EGRF	B-MethodName
described	O
in	O
section	O
2.1	O
,	O
we	O
define	O
the	O
probability	O
of	O
topic	O
sequence	O
z	O
as	O
below	O
:	O
P	O
egrf	O
(	O
z	O
|	O
θ	O
)	O
=	O
1	O
|	O
E1	O
|	O
+	O
|	O
E2	O
|	O
w∈V	O
f	O
(	O
zw)×	O
(	O
(	O
w	O
1	O
,	O
w	O
1	O
)	O
∈E	O
1	O
f	O
(	O
1	O
)	O
(	O
z	O
w	O
1	O
,	O
z	O
w	O
1	O
)	O
+	O
(	O
w	O
2	O
,	O
w	O
2	O
)	O
∈E	O
2	O
f	O
(	O
2	O
)	O
(	O
z	O
w	O
2	O
,	O
z	O
w	O
2	O
)	O
)	O
(	O
3	O
)	O
where	O
f	O
(	O
zw	O
)	O
=	O
M	O
ulti(zw|θ	O
)	O
(	O
4	O
)	O
f	O
(	O
1	O
)	O
(	O
z	O
w	O
1	O
,	O
z	O
w	O
1	O
)	O
=	O
σz	O
w	O
1	O
=	O
z	O
w	O
1	O
λ1	O
+	O
σ	O
z	O
w	O
1	O
=	O
z	O
w	O
1	O
λ2	O
(	O
5	O
)	O
f	O
(	O
2	O
)	O
(	O
z	O
w	O
2	O
,	O
z	O
w	O
2	O
)	O
=	O
σz	O
w	O
2	O
=	O
z	O
w	O
2	O
λ3	O
+	O
σ	O
z	O
w	O
2	O
=	O
z	O
w	O
2	O
λ4	O
(	O
6	O
)	O
σ	O
is	O
an	O
indicator	O
function	O
and	O
equals	O
1	O
if	O
the	O
topic	O
assignments	O
of	O
two	O
words	O
on	O
an	O
edge	O
are	O
same	O
.	O

So	O
the	O
word	B-TaskName
topic	I-TaskName
assignment	I-TaskName
is	O
no	O
longer	O
conditionally	O
independent	O
.	O

We	O
define	O
Extended	B-MethodName
Global	I-MethodName
Topic	I-MethodName
Random	I-MethodName
Field	I-MethodName
based	O
on	O
EGRF	B-MethodName
.	O

Topic	B-TaskName
Model	I-TaskName
Using	O
EGRF	B-MethodName
.	O

And	O
EGRF	B-MethodName
does	O
not	O
have	O
normalization	B-HyperparameterName
factor	I-HyperparameterName
,	O
which	O
is	O
much	O
simplier	O
than	O
models	O
with	O
intractable	O
normalizing	B-HyperparameterName
factor	I-HyperparameterName
.	O

The	O
state(topic	B-TaskName
assignment	I-TaskName
)	O
of	O
a	O
word	O
vertex	O
w	O
is	O
generated	O
from	O
Z	O
=	O
{	O
z	O
i	O
|i	O
=	O
1	O
,	O
2	O
,	O
...	O
,	O
k	O
}	O
,	O
k	O
is	O
the	O
number	O
of	O
topics	O
.	O

Another	O
advantage	O
of	O
EGTRF	B-MethodName
is	O
it	O
incorporates	O
word	O
features	O
.	O

Word	O
similarity	O
information	O
learned	O
from	O
large	O
corpus	O
is	O
incorporated	O
into	O
the	O
model	O
.	O

Conclusion	O
.	O

After	O
processing	O
,	O
it	O
contains	O
843	O
documents	O
with	O
a	O
vocabulary	O
of	O
6098	O
terms	O
.	O

Eighty	O
percent	O
data	O
are	O
used	O
for	O
training	O
,	O
others	O
for	O
testing	O
.	O

For	O
each	O
dataset	O
,	O
we	O
remove	O
very	O
short	O
documents	O
,	O
and	O
compute	O
a	O
vocabulary	O
by	O
removing	O
stop	O
words	O
,	O
rare	O
words	O
,	O
frequent	O
words	O
.	O

Experiment	O
.	O

We	O
run	O
such	O
iterations	O
until	O
convergence	O
.	O

At	O
M	O
-	O
step	O
,	O
we	O
update	O
new	O
α	O
and	O
β	O
based	O
on	O
obtained	O
γ	O
and	O
φ	O
.	O

At	O
E	O
-	O
step	O
,	O
we	O
estimate	O
the	O
best	O
γ	O
and	O
φ	O
given	O
current	O
α	O
and	O
β	O
.	O

φ	O
can	O
be	O
approximated	O
as	O
:	O
φw	O
n	O
,	O
i	O
∝	O
βi	O
,	O
vexp(Ψ(γi)+	O
1	O
−	O
λ2	O
ζ1	O
×	O
(	O
wn	O
,	O
wm)∈E	O
1	O
φw	O
m	O
,	O
i	O
Sm	O
,	O
n+	O
1	O
−	O
λ4	O
ζ1	O
×	O
(	O
wn	O
,	O
wp)∈E	O
2	O
φw	O
p	O
,	O
i	O
Sp	O
,	O
n	O
)	O
(	O
12	O
)	O
EM	O
algorithm	O
is	O
applied	O
using	O
above	O
updating	O
rules	O
.	O

E	O
q	O
(	O
|	O
E	O
C	O
i	O
|	O
)	O
is	O
obtained	O
directly	O
from	O
(	O
10	O
)	O
,	O
E	O
q	O
(	O
θ	O
T	O
θ	O
)	O
is	O
from	O
the	O
property	O
of	O
Dirichlet	O
distribution	O
.	O

We	O
derive	O
Variational	O
Inference	O
for	O
posterior	O
inference	O
.	O

Posterior	O
Inference	O
and	O
Parameter	O
Estimation	O
.	O

In	O
this	O
way	O
,	O
knowledge	O
from	O
large	O
corpus	O
other	O
than	O
current	O
document	O
collections	O
is	O
incorporated	O
to	O
guide	O
topic	O
modeling	O
.	O

The	O
word	O
representations	O
are	O
computed	O
using	O
neural	O
networks	O
,	O
and	O
the	O
learned	O
representations	O
explicitly	O
encode	O
many	O
linguistic	O
regularities	O
and	O
patterns	O
from	O
the	O
corpus	O
.	O

Then	O
equation	O
(	O
3	O
)	O
can	O
be	O
represented	O
as	O
below	O
:	O
P	O
egrf	O
(	O
z	O
|	O
θ	O
)	O
=	O
1	O
|	O
E1	O
|	O
+	O
|	O
E2	O
|	O
w∈V	O
M	O
ulti(zw	O
|	O
θ)×	O
(	O
|	O
E	O
C	O
1	O
|	O
λ1	O
+	O
|	O
E	O
N	O
C	O
1	O
|	O
λ2	O
+	O
|	O
E	O
C	O
2	O
|	O
λ3	O
+	O
|	O
E	O
N	O
C	O
2	O
|	O
λ4	O
)	O
=	O
w∈V	O
M	O
ulti(zw	O
|	O
θ	O
)	O
(	O
|	O
E1	O
|	O
+	O
|	O
E2	O
|)θ	O
T	O
θ	O
×	O
(	O
|	O
E	O
C	O
1	O
|	O
(	O
1	O
−	O
λ2)+	O
|	O
E1	O
|	O
λ2θ	O
T	O
θ+	O
|	O
E	O
C	O
2	O
|	O
(	O
1	O
−	O
λ4)+	O
|	O
E2	O
|	O
λ4θ	O
T	O
θ	O
)	O
(	O
9	O
)	O
From	O
the	O
second	O
line	O
to	O
the	O
third	O
line	O
of	O
Equation	O
(	O
9	O
)	O
,	O
we	O
represent	O
λ	O
1	O
,	O
λ	O
3	O
as	O
the	O
function	O
of	O
λ	O
2	O
,	O
λ	O
4	O
based	O
on	O
(	O
7	O
)	O
and	O
(	O
8)	O
.	O

E	O
C	O
i	O
includes	O
all	O
coherent	O
edges	O
,	O
E	O
N	O
C	O
i	O
contains	O
all	O
non	O
-	O
coherent	O
edges	O
.	O

In	O
distance	O
-	O
i	O
edge	O
set	O
,	O
i=	O
1	O
,	O
2	O
.	O

The	O
coherent	O
edge	O
is	O
the	O
edge	O
that	O
the	O
two	O
linked	O
words	O
have	O
same	O
topic	O
.	O

Word	O
Similarity	O
Information	O
.	O

If	O
|E	O
1	O
|	O
=	O
0	O
,	O
|E	O
2	O
|	O
=	O
0	O
,	O
EGTRF	O
is	O
equiva-	O
lent	O
to	O
LDA	O
.	O

Equation	O
(	O
4	O
)	O
defines	O
word	O
vertex	O
as	O
multinomial	O
distribution	O
,	O
and	O
we	O
assign	O
λ	O
1	O
,	O
λ	O
2	O
,	O
λ	O
3	O
and	O
λ	O
4	O
nonzero	O
values	O
,	O
then	O
it	O
is	O
clear	O
to	O
verify	O
constraint	O
Lower	O
λ	O
2	O
,	O
λ	O
4	O
give	O
higher	O
reward	O
to	O
the	O
edge	O
that	O
connects	O
two	O
word	O
vertices	O
with	O
same	O
topic	O
.	O

We	O
obtain	O
the	O
marginal	O
distribution	O
of	O
a	O
document	O
:	O
p(w	O
|	O
α	O
,	O
β	O
)	O
=	O
P	O
(	O
θ	O
|	O
α	O
)	O
z	O
P	O
egrf	O
(	O
z	O
|	O
θ	O
)	O
n	O
P	O
(	O
wn	O
|	O
zw	O
n	O
,	O
β)dθ(2	O
)	O
We	O
can	O
see	O
the	O
marginal	O
distribution	O
is	O
similar	O
to	O
LDA	O
except	O
topic	O
assignment	O
of	O
word	O
is	O
sampled	O
by	O
Extended	O
Global	O
Random	O
Field	O
instead	O
of	O
Multinomial	O
.	O

Given	O
Dirichlet	O
prior	O
α	O
,	O
word	O
distribution	O
of	O
topics	O
β	O
,	O
topic	O
mixture	O
of	O
document	O
θ	O
,	O
topic	O
assignments	O
z	O
and	O
words	O
w.	O

For	O
each	O
of	O
the	O
n	O
words	O
w	O
n	O
in	O
d	O
:	O
Choose	O
topic	O
z	O
n	O
∼	O
P	O
egrf	O
(	O
z	O
|	O
θ	O
)	O
,	O
Choose	O
word	O
w	O
n	O
∼	O
M	O
ulti(β	O
zn	O
,	O
wn	O
)	O
.	O

Choose	O
θ	O
∼	O
Dir(α	O
)	O
.	O

The	O
generative	O
process	O
for	O
word	O
sequence	O
of	O
a	O
document	O
is	O
described	O
as	O
below	O
:	O
For	O
each	O
document	O
d	O
in	O
corpus	O
D	O
:	O
Transform	O
document	O
d	O
into	O
graph	O
.	O

EGTRF	O
is	O
a	O
generative	O
proba	O
-	O
bilistic	O
model	O
,	O
the	O
basic	O
idea	O
is	O
that	O
documents	O
are	O
represented	O
as	O
mixtures	O
of	O
topics	O
,	O
words	O
are	O
generated	O
depending	O
on	O
the	O
topic	O
mixtures	O
and	O
graph	O
structure	O
of	O
current	O
document	O
.	O

If	O
Equation	O
(	O
1	O
)	O
satisfies	O
all	O
the	O
four	O
constraints	O
,	O
it	O
is	O
easy	O
to	O
verify	O
P	O
(	O
G	O
)	O
is	O
also	O
a	O
probability	O
measure	O
since	O
summing	O
over	O
all	O
possible	O
samples	O
g	O
equals	O
to	O
1	O
.	O

g	O
is	O
one	O
sample	O
of	O
word	O
topic	O
assignments	O
from	O
graph	O
G.	O

So	O
f	O
(	O
z	O
)	O
f	O
(	O
z	O
)	O
f	O
(	O
1	O
)	O
(	O
z	O
,	O
z	O
)	O
and	O
f	O
(	O
z	O
)	O
f	O
(	O
z	O
)	O
f	O
(	O
2	O
)	O
(	O
z	O
,	O
z	O
)	O
are	O
probability	O
measure	O
.	O

f	O
(	O
1	O
)	O
and	O
f	O
(	O
2	O
)	O
are	O
not	O
necessarily	O
probability	O
measure	O
,	O
however	O
,	O
summing	O
over	O
all	O
possible	O
states	O
of	O
the	O
product	O
of	O
the	O
edge	O
and	O
the	O
linked	O
word	O
pair	O
should	O
equal	O
to	O
1	O
,	O
which	O
are	O
from	O
constraints	O
3	O
and	O
4	O
.	O

f	O
(	O
1	O
)	O
(	O
z	O
,	O
z	O
)	O
and	O
f	O
(	O
2	O
)	O
(	O
z	O
,	O
z	O
)	O
are	O
the	O
function	O
defined	O
on	O
edge	O
set	O
E	O
1	O
and	O
E	O
2	O
.	O

z	O
,	O
z	O
∈Z	O
f	O
(	O
z	O
)	O
f	O
(	O
z	O
)	O
f	O
(	O
2	O
)	O
(	O
z	O
,	O
z	O
)	O
=	O
1	O
In	O
Equation	O
(	O
1	O
)	O
,	O
f	O
(	O
z	O
)	O
is	O
the	O
function	O
defined	O
on	O
word	O
vertex	O
,	O
which	O
is	O
a	O
probability	O
measure	O
because	O
of	O
the	O
constraints	O
1	O
and	O
2	O
.	O

z	O
,	O
z	O
∈Z	O
f	O
(	O
z	O
)	O
f	O
(	O
z	O
)	O
f	O
(	O
1	O
)	O
(	O
z	O
,	O
z	O
)	O
=	O
1	O
4	O
.	O

z∈Z	O
f	O
(	O
z	O
)	O
=	O
1	O
3	O
.	O

P	O
(	O
G	O
)	O
=	O
f	O
G	O
(	O
g	O
)	O
=	O
1	O
|	O
E1	O
|	O
+	O
|	O
E2	O
|	O
w∈W	O
f	O
(	O
zw)×	O
(	O
(	O
w	O
1	O
,	O
w	O
1	O
)	O
∈E	O
1	O
f	O
(	O
1	O
)	O
(	O
z	O
w	O
1	O
,	O
z	O
w	O
1	O
)	O
+	O
(	O
w	O
2	O
,	O
w	O
2	O
)	O
∈E	O
2	O
f	O
(	O
2	O
)	O
(	O
z	O
w	O
2	O
,	O
z	O
w	O
2	O
)	O
)	O
(	O
1	O
)	O
s.t	O
.	O
1.f	O
(	O
z	O
)	O
>	O
0	O
,	O
f	O
(	O
1	O
)	O
(	O
z	O
,	O
z	O
)	O
>	O
0	O
,	O
f	O
(	O
2	O
)	O
(	O
z	O
,	O
z	O
)	O
>	O
0	O
2	O
.	O

Extended	O
Global	O
Random	O
Field	O
.	O

We	O
organized	O
the	O
paper	O
as	O
below	O
:	O
EGTRF	O
is	O
presented	O
in	O
Section	O
2	O
,	O
variational	O
inference	O
and	O
parameter	O
estimation	O
are	O
derived	O
in	O
Section	O
3	O
,	O
experiments	O
on	O
two	O
datasets	O
are	O
showed	O
in	O
Section	O
4	O
,	O
we	O
conclude	O
the	O
paper	O
in	O
Section	O
5	O
.	O

The	O
word	O
vector	O
representations	O
are	O
very	O
interesting	O
because	O
the	O
learned	O
vectors	O
explicitly	O
encode	O
many	O
linguistic	O
regularities	O
and	O
patterns	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
.	O

Therefore	O
,	O
EGTRF	O
can	O
exploit	O
more	O
semantically	O
or	O
syntactically	O
word	O
dependencies	O
.	O

Some	O
hidden	O
dependency	O
relations	O
can	O
also	O
be	O
extracted	O
by	O
merging	O
dependency	O
trees	O
.	O

The	O
two	O
sentences	O
are	O
parsed	O
into	O
dependency	O
trees	O
respectively	O
,	O
and	O
then	O
merged	O
into	O
a	O
graph	O
.	O

An	O
example	O
of	O
a	O
simple	O
document	O
that	O
has	O
two	O
sentences	O
shows	O
in	O
Figure	O
1	O
.	O

Then	O
they	O
propose	O
GTRF	O
to	O
model	O
non	O
-	O
linear	O
topical	O
dependencies	O
,	O
word	O
topics	O
are	O
sampled	O
based	O
on	O
graph	O
structure	O
instead	O
of	O
"	O
bag	O
of	O
words	O
"	O
representation	O
,	O
the	O
conditional	O
independence	O
of	O
word	O
topic	O
assignment	O
is	O
thus	O
relaxed	O
.	O

They	O
show	O
topics	O
of	O
semantically	O
or	O
syntactically	O
dependent	O
words	O
achieve	O
the	O
highest	O
similarity	O
and	O
are	O
able	O
to	O
provide	O
more	O
useful	O
information	O
in	O
topic	O
modeling	O
,	O
which	O
is	O
also	O
the	O
basic	O
assumption	O
of	O
our	O
model	O
.	O

In	O
Syntactic	O
topic	O
models	O
(	O
Boyd	O
-	O
Graber	O
et	O
al	O
,	O
2009	O
)	O
,	O
each	O
word	O
of	O
a	O
sentence	O
is	O
generated	O
by	O
a	O
distribution	O
that	O
combines	O
document	O
-	O
specific	O
topic	O
weights	O
and	O
parsetree	O
-	O
specific	O
syntactic	O
transitions	O
.	O

Most	O
of	O
the	O
models	O
above	O
are	O
limited	O
to	O
model	O
linear	O
topical	O
dependencies	O
between	O
words	O
,	O
word	O
topical	O
dependencies	O
can	O
also	O
be	O
modeled	O
by	O
a	O
non	O
-	O
linear	O
way	O
.	O

Zhu	O
(	O
Zhu	O
et	O
al	O
,	O
2010	O
)	O
incorporates	O
Markov	O
dependency	O
between	O
topic	O
assignments	O
of	O
neighboring	O
words	O
,	O
and	O
employs	O
a	O
general	O
structure	O
of	O
the	O
GLM	O
to	O
define	O
a	O
conditional	O
distribution	O
of	O
latent	O
topic	O
assignments	O
over	O
words	O
.	O

Gruber	O
(	O
Gruber	O
et	O
al	O
,	O
2007	O
)	O
models	O
the	O
topics	O
of	O
words	O
in	O
the	O
document	O
as	O
a	O
Markov	O
chain	O
,	O
and	O
assumes	O
all	O
words	O
in	O
the	O
same	O
sentence	O
are	O
more	O
likely	O
to	O
have	O
the	O
same	O
topic	O
.	O

Wallach	O
(	O
Wallach	O
,	O
2006	O
)	O
explores	O
a	O
hierarchical	O
generative	O
probabilistic	O
model	O
that	O
incorporates	O
both	O
n	O
-	O
gram	O
statistics	O
and	O
latent	O
topic	O
variables	O
.	O

To	O
relax	O
the	O
"	O
bag	O
of	O
words	O
"	O
assumption	O
,	O
many	O
extended	O
topic	O
models	O
have	O
been	O
proposed	O
to	O
address	O
the	O
limitation	O
of	O
conditional	O
independence	O
.	O

However	O
,	O
the	O
"	O
bag	O
of	O
words	O
"	O
assumption	O
is	O
employed	O
in	O
most	O
existing	O
topic	O
models	O
,	O
it	O
assumes	O
the	O
order	O
of	O
words	O
can	O
be	O
ignored	O
and	O
topic	O
assignment	O
of	O
each	O
word	O
is	O
conditionally	O
independent	O
given	O
the	O
topic	O
mixture	O
of	O
a	O
document	O
.	O

Introduction	O
.	O

Parameters	O
are	O
estimated	O
efficiently	O
by	O
variational	O
inference	O
and	O
experimental	O
results	O
on	O
two	O
datasets	O
show	O
EGTRF	O
achieves	O
lower	O
perplexity	O
and	O
higher	O
log	O
predictive	O
probability	O
.	O

Word	O
similarity	O
information	O
learned	O
from	O
large	O
corpus	O
is	O
incorporated	O
to	O
enhance	O
word	O
topic	O
assignment	O
.	O

