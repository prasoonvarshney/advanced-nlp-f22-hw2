Conclusion	O
and	O
Future	O
Work	O
.	O

We	O
addressed	O
the	O
problem	O
of	O
utilizing	O
GNNs	B-MethodName
to	O
perform	O
relational	B-TaskName
reasoning	I-TaskName
with	I-TaskName
natural	I-TaskName
languages	I-TaskName
.	O

Our	O
proposed	O
model	O
,	O
GP	B-MethodName
-	I-MethodName
GNN	I-MethodName
,	O
solves	O
the	O
relational	B-TaskName
message	I-TaskName
-	I-TaskName
passing	I-TaskName
task	O
by	O
encoding	O
natural	O
language	O
as	O
parameters	O
and	O
performing	O
propagation	O
from	O
layer	O
to	O
layer	O
.	O

Our	O
model	O
can	O
also	O
be	O
considered	O
as	O
a	O
more	O
generic	O
framework	O
for	O
graph	B-TaskName
generation	I-TaskName
problem	I-TaskName
with	I-TaskName
unstructured	I-TaskName
input	I-TaskName
other	O
than	O
text	O
,	O
e.g.	O
image	O
,	O
video	O
,	O
audio	O
.	O

In	O
this	O
work	O
,	O
we	O
demonstrate	O
its	O
effectiveness	O
in	O
predicting	B-TaskName
the	I-TaskName
relationship	I-TaskName
between	I-TaskName
entities	I-TaskName
in	I-TaskName
natural	I-TaskName
language	I-TaskName
and	O
bag	B-TaskName
-	I-TaskName
level	I-TaskName
and	O
show	O
that	O
by	O
considering	O
more	O
hops	O
in	O
reasoning	O
the	O
performance	O
of	O
relation	B-TaskName
extraction	I-TaskName
could	O
be	O
significantly	O
improved	O
.	O

Consequently	O
,	O
Context	B-MethodName
-	I-MethodName
Aware	I-MethodName
RE	I-MethodName
makes	O
a	O
mistake	O
by	O
predicting	O
(	O
Kentucky	O
,	O
share	O
boarder	O
with	O
,	O
Ohio	O
)	O
.	O

Ground	O
truth	O
graphs	O
are	O
the	O
subgraph	O
in	O
Wikidata	B-DatasetName
knowledge	O
graph	O
induced	O
by	O
the	O
sets	O
of	O
entities	O
in	O
the	O
sentences	O
.	O

Table	O
4	O
:	O
Sample	O
predictions	O
from	O
the	O
baseline	O
models	O
and	O
our	O
GP	B-MethodName
-	I-MethodName
GNN	I-MethodName
model	O
.	O

We	O
also	O
find	O
that	O
Context	B-MethodName
-	I-MethodName
Aware	I-MethodName
RE	I-MethodName
tends	O
to	O
predict	O
relations	O
with	O
similar	O
topics	O
.	O

Note	O
that	O
(	O
BankUnited	O
Center	O
,	O
located	O
in	O
,	O
English	O
)	O
is	O
even	O
not	O
in	O
Wikidata	B-DatasetName
,	O
but	O
our	O
model	O
could	O
identify	O
this	O
fact	O
through	O
reasoning	O
.	O

In	O
the	O
first	O
case	O
,	O
GP	B-MethodName
-	I-MethodName
GNN	I-MethodName
implicitly	O
learns	O
a	O
logic	O
rule	O
∃y	O
,	O
x	O
(	O
BankUnited	O
Center	O
,	O
located	O
in	O
,	O
English	O
)	O
.	O

The	O
results	O
show	O
that	O
GP	B-MethodName
-	I-MethodName
GNN	I-MethodName
has	O
the	O
ability	O
to	O
infer	O
the	O
relationship	O
between	O
two	O
entities	O
with	O
reasoning	O
.	O

4	O
shows	O
qualitative	O
results	O
that	O
compare	O
our	O
GP	B-MethodName
-	I-MethodName
GNN	I-MethodName
model	O
and	O
the	O
baseline	O
models	O
.	O

It	O
is	O
probably	O
due	O
to	O
the	O
reason	O
that	O
bag	B-TaskName
-	I-TaskName
level	I-TaskName
relation	I-TaskName
extraction	I-TaskName
is	O
much	O
easier	O
.	O

We	O
could	O
also	O
see	O
that	O
on	O
the	O
human	O
annotated	O
test	O
set	O
3layer	B-HyperparameterValue
version	O
to	O
have	O
a	O
greater	O
improvement	O
over	O
2	B-HyperparameterValue
-	O
layer	B-HyperparameterName
version	O
as	O
compared	O
with	O
2	B-HyperparameterValue
-	O
layer	B-HyperparameterName
version	O
over	O
1	B-HyperparameterValue
-	O
layer	B-HyperparameterName
version	O
.	O

3	O
that	O
as	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
layers	I-HyperparameterName
grows	O
,	O
the	O
curves	O
get	O
higher	O
and	O
higher	O
precision	O
,	O
indicating	O
considering	O
more	O
hops	O
in	O
reasoning	O
leads	O
to	O
better	O
performance	O
.	O

From	O
Table	O
2	O
and	O
Table	O
3	O
,	O
we	O
could	O
see	O
that	O
on	O
all	O
three	O
datasets	O
,	O
3	B-HyperparameterValue
-	O
layer	B-HyperparameterName
version	O
achieves	O
the	O
best	O
.	O

To	O
demonstrate	O
the	O
effects	O
of	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
layers	I-HyperparameterName
,	O
we	O
also	O
compare	O
our	O
models	O
with	O
different	O
numbers	B-HyperparameterName
of	I-HyperparameterName
lay	I-HyperparameterName
-	I-HyperparameterName
ers	I-HyperparameterName
.	O

A	O
K	B-HyperparameterName
-	O
layer	O
version	O
has	O
the	O
ability	O
to	O
infer	O
K	B-HyperparameterName
-	O
hop	O
relations	O
.	O

The	O
number	B-HyperparameterName
of	I-HyperparameterName
layers	I-HyperparameterName
represents	O
the	O
reasoning	O
ability	O
of	O
our	O
models	O
.	O

The	O
Effectiveness	O
of	O
the	O
Number	B-HyperparameterName
of	I-HyperparameterName
Layers	I-HyperparameterName
.	O

(	O
2015	O
)	O
formalize	O
the	O
bag	B-TaskName
-	I-TaskName
level	I-TaskName
relation	I-TaskName
extraction	I-TaskName
as	O
multi	O
-	O
instance	O
learning	O
.	O

To	O
evaluate	O
our	O
models	O
and	O
baseline	O
models	O
in	O
bag	B-TaskName
-	I-TaskName
level	I-TaskName
,	O
we	O
utilize	O
a	O
bag	O
of	O
sentences	O
with	O
a	O
given	O
entity	O
pair	O
to	O
score	O
the	O
relations	O
between	O
them	O
.	O

So	O
far	O
,	O
we	O
have	O
only	O
talked	O
about	O
the	O
way	O
to	O
implement	O
sentence	B-TaskName
-	I-TaskName
level	I-TaskName
relation	I-TaskName
extraction	I-TaskName
.	O

We	O
have	O
also	O
tried	O
two	O
forms	O
of	O
adjacent	B-HyperparameterName
matrices	I-HyperparameterName
:	O
tied	B-HyperparameterValue
-	I-HyperparameterValue
weights	I-HyperparameterValue
(	O
set	O
A	O
(	O
n	O
)	O
=	O
A	O
(	O
n+1	O
)	O
)	O
and	O
untied	B-HyperparameterValue
-	I-HyperparameterValue
weights	I-HyperparameterValue
.	O

We	O
select	O
non	B-HyperparameterName
-	I-HyperparameterName
linear	I-HyperparameterName
activation	I-HyperparameterName
functions	I-HyperparameterName
between	O
relu	B-HyperparameterValue
and	O
tanh	B-HyperparameterValue
,	O
and	O
select	O
d	B-HyperparameterName
n	I-HyperparameterName
among	O
{	O
2	B-HyperparameterValue
,	O
4	B-HyperparameterValue
,	O
8	B-HyperparameterValue
,	O
12	B-HyperparameterValue
,	O
16	B-HyperparameterValue
}	O
9	O
.	O

GP	B-MethodName
-	I-MethodName
GNN	I-MethodName
with	O
K	B-HyperparameterName
=	O
2	B-HyperparameterValue
or	O
K	B-HyperparameterName
=	O
3	B-HyperparameterValue
layers	B-HyperparameterName
.	O

Bidirectional	B-MethodName
LSTM	I-MethodName
(	O
Schuster	O
and	O
Paliwal	O
,	O
1997	O
)	O
could	O
be	O
seen	O
as	O
an	O
1	B-HyperparameterValue
-	O
layer	B-HyperparameterName
variant	O
of	O
our	O
model	O
.	O

LSTM	B-MethodName
or	O
GP	B-MethodName
-	I-MethodName
GNN	I-MethodName
with	O
K	B-HyperparameterName
=	O
1	B-HyperparameterValue
layer	B-HyperparameterName
.	O

For	O
CNN	B-MethodName
and	O
following	O
PCNN	B-MethodName
,	O
the	O
entity	O
markers	O
are	O
the	O
same	O
as	O
originally	O
proposed	O
in	O
Zeng	O
et	O
al	O
.	O

PCNN	B-MethodName
,	O
proposed	O
by	O
Zeng	O
et	O
al	O
.	O

(	O
2014	O
)	O
,	O
our	O
implementation	O
,	O
follows	O
Nguyen	O
and	O
Grishman	O
(	O
2015	O
)	O
,	O
concatenates	O
features	O
extracted	O
by	O
three	O
different	O
window	B-HyperparameterName
sizes	I-HyperparameterName
:	O
3	B-HyperparameterValue
,	O
5	B-HyperparameterValue
,	O
7	B-HyperparameterValue
.	O

Different	O
from	O
the	O
original	O
version	O
of	O
CNN	B-MethodName
proposed	O
in	O
Zeng	O
et	O
al	O
.	O

Multi	B-MethodName
-	I-MethodName
Window	I-MethodName
CNN	I-MethodName
.	O

It	O
was	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
on	O
Wikipedia	B-DatasetName
dataset	I-DatasetName
.	O

Context	B-MethodName
-	I-MethodName
Aware	I-MethodName
RE	I-MethodName
,	O
proposed	O
by	O
Sorokin	O
and	O
Gurevych	O
(	O
2017	O
)	O
.	O

We	O
further	O
split	O
a	O
dense	O
test	O
set	O
from	O
the	O
distantly	B-DatasetName
labeled	I-DatasetName
test	O
set	O
.	O

Dense	O
distantly	B-DatasetName
labeled	I-DatasetName
test	O
set	O
.	O

Human	B-DatasetName
annotated	I-DatasetName
test	O
set	O
Based	O
on	O
the	O
test	O
set	O
provided	O
by	O
(	O
Sorokin	O
and	O
Gurevych	O
,	O
2017	O
)	O
,	O
5	O
annotators	O
6	O
are	O
asked	O
to	O
label	O
the	O
dataset	O
.	O

There	O
is	O
a	O
small	O
difference	O
between	O
our	O
task	O
and	O
theirs	O
:	O
our	O
task	O
is	O
to	O
extract	B-TaskName
the	I-TaskName
relationship	I-TaskName
between	I-TaskName
every	I-TaskName
pair	I-TaskName
of	I-TaskName
entities	I-TaskName
in	I-TaskName
the	I-TaskName
sentence	I-TaskName
,	O
whereas	O
their	O
task	O
is	O
to	O
extract	O
the	O
relationship	O
between	O
the	O
given	O
entity	O
pair	O
and	O
the	O
context	O
entity	O
pairs	O
.	O

Distantly	O
labeled	O
set	O
Sorokin	O
and	O
Gurevych	O
(	O
2017	O
)	O
have	O
proposed	O
a	O
dataset	O
with	O
Wikipedia	B-DatasetName
corpora	I-DatasetName
.	O

In	O
both	O
part	O
(	O
1	O
)	O
and	O
part	O
(	O
2	O
)	O
,	O
we	O
do	O
three	O
subparts	O
of	O
experiments	O
:	O
(	O
i	O
)	O
we	O
will	O
first	O
show	O
that	O
our	O
models	O
could	O
improve	O
instance	B-TaskName
-	I-TaskName
level	I-TaskName
relation	I-TaskName
extraction	I-TaskName
on	O
a	O
human	B-DatasetName
annotated	I-DatasetName
test	O
set	O
,	O
and	O
(	O
ii	O
)	O
then	O
we	O
will	O
show	O
that	O
our	O
models	O
could	O
also	O
help	O
enhance	O
the	O
performance	O
of	O
bag	B-TaskName
-	I-TaskName
level	I-TaskName
relation	I-TaskName
extraction	I-TaskName
on	O
a	O
distantly	B-DatasetName
labeled	I-DatasetName
test	O
set	O
4	O
,	O
and	O
(	O
iii	O
)	O
we	O
also	O
split	O
a	O
subset	O
of	O
distantly	B-DatasetName
labeled	I-DatasetName
test	O
set	O
,	O
where	O
the	O
number	O
of	O
entities	O
and	O
edges	O
is	O
large	O
.	O

Our	O
experiments	O
mainly	O
aim	O
at	O
:	O
(	O
1	O
)	O
showing	O
that	O
our	O
best	O
models	O
could	O
improve	O
the	O
performance	O
of	O
relation	B-TaskName
extraction	I-TaskName
under	O
a	O
variety	O
of	O
settings	O
;	O
(	O
2	O
)	O
illustrating	O
that	O
how	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
layers	I-HyperparameterName
affect	O
the	O
performance	O
of	O
our	O
model	O
;	O
and	O
(	O
3	O
)	O
performing	O
a	O
qualitative	O
investigation	O
to	O
highlight	O
the	O
difference	O
between	O
our	O
models	O
and	O
baseline	O
models	O
.	O

We	O
treat	O
K	B-HyperparameterName
as	O
a	O
hyperparameter	O
,	O
the	O
effectiveness	O
of	O
which	O
will	O
be	O
discussed	O
in	O
detail	O
(	O
Sect	O
.	O

In	O
general	O
graphs	O
,	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
layers	I-HyperparameterName
K	O
is	O
chosen	O
to	O
be	O
of	O
the	O
order	O
of	O
the	O
graph	O
diameter	O
so	O
that	O
all	O
nodes	O
obtain	O
information	O
from	O
the	O
entire	O
graph	O
.	O

Number	B-HyperparameterName
of	I-HyperparameterName
Layers	I-HyperparameterName
.	O

In	O
our	O
experiments	O
,	O
we	O
generalize	O
the	O
idea	O
of	O
Gated	B-MethodName
Graph	I-MethodName
Neural	I-MethodName
Networks	I-MethodName
(	O
Li	O
et	O
al	O
.	O
,	O
2016	O
)	O
by	O
setting	O
a	O
subject	B-HyperparameterName
=	O
[	B-HyperparameterValue
1	I-HyperparameterValue
;	I-HyperparameterValue
0	I-HyperparameterValue
]	I-HyperparameterValue
and	O
a	O
object	B-HyperparameterName
=	O
[	B-HyperparameterValue
0	I-HyperparameterValue
;	I-HyperparameterValue
1	I-HyperparameterValue
]	I-HyperparameterValue
3	O
.	O

To	O
encode	O
the	O
context	O
of	O
entity	O
pairs	O
(	O
or	O
edges	O
in	O
the	O
graph	O
)	O
,	O
we	O
first	O
concatenate	O
the	O
position	O
embeddings	O
with	O
word	O
embeddings	O
in	O
the	O
sentence	O
:	O
E(x	O
i	O
,	O
j	O
t	O
)	O
=	O
[	O
x	O
t	O
;	O
p	O
i	O
,	O
j	O
t	O
]	O
,	O
(	O
4	O
)	O
where	O
x	O
t	O
denotes	O
the	O
word	O
embedding	O
of	O
word	O
x	O
t	O
and	O
p	O
i	O
,	O
j	O
t	O
denotes	O
the	O
position	O
embedding	O
of	O
word	O
position	O
t	O
relative	O
to	O
the	O
entity	O
pair	O
's	O
position	O
i	O
,	O
j	O
(	O
Details	O
of	O
these	O
two	O
embeddings	O
are	O
introduced	O
in	O
the	O
next	O
two	O
paragraphs	O
.	O
)	O
After	O
that	O
,	O
we	O
feed	O
the	O
representations	O
of	O
entity	O
pairs	O
into	O
encoder	O
f	O
(	O
•	O
)	O
which	O
contains	O
a	O
bi	B-MethodName
-	I-MethodName
directional	I-MethodName
LSTM	I-MethodName
and	O
a	O
multilayer	B-MethodName
perceptron	I-MethodName
:	O
A	O
(	O
n	O
)	O
i	O
,	O
j	O
=	O
[	O
MLPn(BiLSTMn((E(x	O
i	O
,	O
j	O
0	O
)	O
,	O
E(x	O
i	O
,	O
j	O
1	O
)	O
,	O
•	O
•	O
•	O
,	O
E(x	O
i	O
,	O
j	O
l−1	O
)	O
)	O
]	O
,	O
(	O
5	O
)	O
where	O
n	O
denotes	O
the	O
index	O
of	O
layer	O
1	O
,	O
[	O
•	O
]	O
means	O
reshaping	O
a	O
vector	O
as	O
a	O
matrix	O
,	O
BiLSTM	B-MethodName
encodes	O
a	O
sequence	O
by	O
concatenating	O
tail	O
hidden	O
states	O
of	O
the	O
forward	O
LSTM	B-MethodName
and	O
head	O
hidden	O
states	O
of	O
the	O
backward	O
LSTM	B-MethodName
together	O
and	O
MLP	B-MethodName
denotes	O
a	O
multilayer	B-MethodName
perceptron	I-MethodName
with	O
non	O
-	O
linear	O
activation	O
σ	O
.	O

In	O
this	O
section	O
,	O
we	O
will	O
introduce	O
how	O
to	O
apply	O
GP	B-MethodName
-	I-MethodName
GNNs	I-MethodName
to	O
relation	B-TaskName
extraction	I-TaskName
.	O

,	O
v	O
|Vs|	O
}	O
,	O
where	O
each	O
v	O
i	O
consists	O
of	O
one	O
or	O
a	O
sequence	O
of	O
tokens	O
,	O
relation	B-TaskName
extraction	I-TaskName
from	O
text	O
is	O
to	O
identify	O
the	O
pairwise	O
relationship	O
r	O
v	O
i	O
,	O
v	O
j	O
∈	O
R	O
between	O
each	O
entity	O
pair	O
(	O
v	O
i	O
,	O
v	O
j	O
)	O
.	O

Relation	B-TaskName
Extraction	I-TaskName
with	O
GP	B-MethodName
-	I-MethodName
GNNs	I-MethodName
.	O
Relation	B-TaskName
extraction	I-TaskName
from	I-TaskName
text	I-TaskName
is	O
a	O
classic	O
natural	B-TaskName
language	I-TaskName
relational	I-TaskName
reasoning	I-TaskName
task	O
.	O

The	O
parameters	O
in	O
GP	B-MethodName
-	I-MethodName
GNNs	I-MethodName
are	O
trained	O
by	O
gradient	O
descent	O
methods	O
.	O

Therefore	O
,	O
the	O
loss	O
of	O
GP	B-MethodName
-	I-MethodName
GNNs	I-MethodName
could	O
be	O
calculated	O
as	O
L	O
=	O
g(h	O
0	O
0:|V|−1	O
,	O
h	O
1	O
0:|V|−1	O
,	O
.	O

The	O
encoding	O
module	O
converts	O
sequences	O
into	O
transition	O
matrices	O
corresponding	O
to	O
edges	O
,	O
i.e.	O
the	O
parameters	O
of	O
the	O
propagation	O
module	O
,	O
by	O
A	O
(	O
n	O
)	O
i	O
,	O
j	O
=	O
f	O
(	O
E(x	O
i	O
,	O
j	O
0	O
)	O
,	O
E(x	O
i	O
,	O
j	O
1	O
)	O
,	O
•	O
•	O
•	O
,	O
E(x	O
i	O
,	O
j	O
l−1	O
)	O
;	O
θ	O
n	O
e	O
)	O
,	O
(	O
1	O
)	O
where	O
f	O
(	O
•	O
)	O
could	O
be	O
any	O
model	O
that	O
could	O
encode	O
sequential	O
data	O
,	O
such	O
as	O
LSTMs	B-MethodName
,	O
GRUs	B-MethodName
,	O
CNNs	B-MethodName
,	O
E(•	O
)	O
indicates	O
an	O
embedding	O
function	O
,	O
and	O
θ	O
n	O
e	O
denotes	O
the	O
parameters	O
of	O
the	O
encoding	O
module	O
of	O
n	O
-	O
th	O
layer	O
.	O

After	O
that	O
,	O
GP	B-MethodName
-	I-MethodName
GNNs	I-MethodName
employ	O
three	O
modules	O
including	O
(	O
1	O
)	O
encoding	O
module	O
,	O
(	O
2	O
)	O
propagation	O
module	O
and	O
(	O
3	O
)	O
classification	O
module	O
to	O
process	O
relational	O
reasoning	O
,	O
as	O
shown	O
in	O
Fig	O
.	O

In	O
this	O
section	O
,	O
we	O
will	O
introduce	O
the	O
general	O
framework	O
of	O
GP	B-MethodName
-	I-MethodName
GNNs	I-MethodName
.	O
GP	B-MethodName
-	I-MethodName
GNNs	I-MethodName
first	O
build	O
a	O
fully	O
-	O
connected	O
graph	O
G	O
=	O
(	O
V	O
,	O
E	O
)	O
,	O
where	O
V	O
is	O
the	O
set	O
of	O
entities	O
,	O
and	O
each	O
edge	O
(	O
v	O
i	O
,	O
v	O
j	O
)	O
∈	O
E	O
,	O
v	O
i	O
,	O
v	O
j	O
∈	O
V	O
corresponds	O
to	O
a	O
sequence	O
s	O
=	O
x	O
i	O
,	O
j	O
0	O
,	O
x	O
i	O
,	O
j	O
1	O
,	O
.	O

Generated	O
Parameters	O
(	O
GP	B-MethodName
-	I-MethodName
GNNs	I-MethodName
)	O
We	O
first	O
define	O
the	O
task	O
of	O
natural	O
language	O
relational	O
reasoning	O
.	O

Graph	B-MethodName
Neural	I-MethodName
Network	I-MethodName
with	O
.	O

(	O
2018	O
)	O
proposed	O
a	O
walk	O
-	O
based	O
model	O
to	O
do	O
relation	B-TaskName
extraction	I-TaskName
.	O

Miwa	O
and	O
Bansal	O
(	O
2016	O
)	O
show	O
the	O
effectiveness	O
of	O
LSTMs	B-MethodName
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
in	O
relation	B-TaskName
extraction	I-TaskName
.	O

(	O
2017	O
)	O
show	O
that	O
the	O
relation	O
path	O
has	O
an	O
important	O
role	O
in	O
relation	B-TaskName
extraction	I-TaskName
.	O

(	O
2017	O
)	O
predict	O
n	O
-	O
ary	O
relations	O
of	O
entities	O
in	O
different	O
sentences	O
with	O
Graph	B-MethodName
LSTMs	I-MethodName
.	O
Le	O
and	O
Titov	O
(	O
2018	O
)	O
treat	O
relations	O
as	O
latent	O
variables	O
which	O
are	O
capable	O
of	O
inducing	O
the	O
relations	O
without	O
any	O
supervision	O
signals	O
.	O

(	O
2016	O
)	O
study	O
an	O
attention	O
mechanism	O
for	O
relation	B-TaskName
extraction	I-TaskName
tasks	O
.	O

Nguyen	O
and	O
Grishman	O
(	O
2015	O
)	O
propose	O
a	O
multi	O
-	O
window	O
version	O
of	O
CNN	B-MethodName
for	O
relation	B-TaskName
extraction	I-TaskName
.	O

(	O
2015	O
)	O
further	O
extends	O
it	O
with	O
piece	B-MethodName
-	I-MethodName
wise	I-MethodName
maxpooling	I-MethodName
.	O

(	O
2014	O
)	O
is	O
one	O
of	O
the	O
earliest	O
works	O
that	O
applies	O
a	O
simple	O
CNN	B-MethodName
to	O
this	O
task	O
,	O
and	O
Zeng	O
et	O
al	O
.	O

In	O
this	O
paper	O
,	O
we	O
focus	O
on	O
the	O
relational	B-TaskName
reasoning	I-TaskName
in	O
the	O
natural	O
language	O
domain	O
.	O

Relational	B-TaskName
Reasoning	I-TaskName
.	O

Relational	B-TaskName
reasoning	I-TaskName
has	O
been	O
explored	O
in	O
various	O
fields	O
.	O

In	O
sharp	O
contrast	O
,	O
this	O
paper	O
focuses	O
on	O
extracting	B-TaskName
relations	I-TaskName
from	I-TaskName
real	I-TaskName
-	I-TaskName
world	I-TaskName
relation	I-TaskName
datasets	O
.	O

Although	O
they	O
also	O
consider	O
applying	O
GNNs	B-MethodName
to	O
natural	O
language	O
processing	O
tasks	O
,	O
they	O
still	O
perform	O
message	O
-	O
passing	O
on	O
predefined	O
graphs	O
.	O

(	O
2018	O
)	O
apply	O
GNNs	B-MethodName
to	O
multi	O
-	O
hop	O
question	O
answering	O
by	O
encoding	O
co	O
-	O
occurence	O
and	O
coreference	O
relationships	O
.	O

(	O
2018	O
)	O
apply	O
GNNs	B-MethodName
to	O
relation	O
extraction	O
by	O
encoding	O
dependency	O
trees	O
,	O
and	O
De	O
Cao	O
et	O
al	O
.	O

(	O
2017	O
)	O
apply	O
GNNs	B-MethodName
to	O
knowledge	O
base	O
completion	O
tasks	O
.	O

For	O
example	O
,	O
Marcheggiani	O
and	O
Titov	O
(	O
2017	O
)	O
propose	O
to	O
apply	O
GNNs	B-MethodName
to	O
semantic	O
role	O
labeling	O
and	O
Schlichtkrull	O
et	O
al	O
.	O

There	O
are	O
relatively	O
fewer	O
papers	O
discussing	O
how	O
to	O
adapt	O
GNNs	B-MethodName
to	O
natural	O
language	O
tasks	O
.	O

Garcia	O
and	O
Bruna	O
(	O
2018	O
)	O
shows	O
how	O
to	O
use	O
GNNs	B-MethodName
to	O
learn	O
classifiers	O
on	O
image	O
datasets	O
in	O
a	O
few	O
-	O
shot	O
manner	O
.	O

(	O
2017	O
)	O
propose	O
to	O
apply	O
GNNs	B-MethodName
to	O
molecular	O
property	O
prediction	O
tasks	O
.	O

(	O
2016	O
)	O
replace	O
the	O
Almeida	O
-	O
Pineda	O
algorithm	O
with	O
the	O
more	O
generic	O
backpropagation	O
and	O
demonstrate	O
its	O
effectiveness	O
empirically	O
.	O

GNNs	B-MethodName
were	O
first	O
proposed	O
in	O
(	O
Scarselli	O
et	O
al	O
.	O
,	O
2009	O
)	O
and	O
are	O
trained	O
via	O
the	O
Almeida	O
-	O
Pineda	O
algorithm	O
(	O
Almeida	O
,	O
1987	O
)	O
.	O

Graph	B-MethodName
Neural	I-MethodName
Networks	I-MethodName
(	O
GNNs	B-MethodName
)	O
.	O

(	O
2	O
)	O
We	O
verify	O
our	O
GP	B-MethodName
-	I-MethodName
GNNs	I-MethodName
on	O
the	O
task	O
of	O
relation	B-TaskName
extraction	I-TaskName
from	I-TaskName
text	I-TaskName
,	O
which	O
demonstrates	O
its	O
ability	O
on	O
multi	O
-	O
hop	O
relational	O
reasoning	O
as	O
compared	O
to	O
those	O
models	O
which	O
extract	O
relationships	O
separately	O
.	O

Our	O
main	O
contributions	O
are	O
in	O
two	O
-	O
fold	O
:	O
(	O
1	O
)	O
We	O
extend	O
a	O
novel	O
graph	B-MethodName
neural	I-MethodName
network	I-MethodName
model	I-MethodName
with	I-MethodName
generated	I-MethodName
parameters	I-MethodName
,	O
to	O
enable	O
relational	O
message	O
-	O
passing	O
with	O
rich	O
text	O
information	O
,	O
which	O
could	O
be	O
applied	O
to	O
process	B-TaskName
relational	I-TaskName
reasoning	I-TaskName
on	O
unstructured	O
inputs	O
such	O
as	O
natural	O
language	O
.	O

We	O
carry	O
out	O
experiments	O
on	O
Wikipedia	B-DatasetName
corpus	I-DatasetName
aligned	I-DatasetName
with	I-DatasetName
Wikidata	I-DatasetName
knowledge	I-DatasetName
base	I-DatasetName
(	O
Vrandečić	O
and	O
Krötzsch	O
,	O
2014	O
)	O
and	O
build	O
a	O
human	O
annotated	O
test	O
set	O
as	O
well	O
as	O
two	O
distantly	O
labeled	O
test	O
sets	O
with	O
different	O
levels	O
of	O
denseness	O
.	O
Experiment	O
results	O
show	O
that	O
our	O
model	O
outperforms	O
other	O
models	O
on	O
relation	B-TaskName
extraction	I-TaskName
task	I-TaskName
by	O
considering	O
multi	B-MethodName
-	I-MethodName
hop	I-MethodName
relational	I-MethodName
reasoning	I-MethodName
.	O

relation	B-TaskName
extraction	I-TaskName
from	O
text	O
.	O

Given	O
a	O
sentence	O
with	O
several	O
entities	O
marked	O
,	O
we	O
model	O
the	O
interaction	O
between	O
these	O
entities	O
by	O
generating	O
the	O
weights	O
of	O
graph	B-MethodName
neural	I-MethodName
networks	I-MethodName
.	O

Language	O
Spoken	O
Language	O
Cast	O
member	O
Figure	O
1	O
:	O
An	O
example	O
of	O
relation	B-TaskName
extraction	I-TaskName
from	O
plain	O
text	O
.	O

In	O
the	O
experiments	O
,	O
we	O
apply	O
GP	B-MethodName
-	I-MethodName
GNNs	I-MethodName
to	O
a	O
classic	O
natural	O
language	O
relational	O
reasoning	O
task	O
:	O
Léon	O
:	O
The	O
Professional	O
is	O
a	O
1996	O
English	O
-	O
language	O
French	O
thriller	O
film	O
directed	O
by	O
Luc	O
Besson	O
.	O

As	O
compared	O
to	O
traditional	O
GNNs	B-MethodName
,	O
GP	B-MethodName
-	I-MethodName
GNNs	I-MethodName
could	O
learn	O
edge	O
parameters	O
from	O
natural	O
languages	O
,	O
extending	O
it	O
from	O
performing	O
inference	O
on	O
only	O
non	O
-	O
relational	O
graphs	O
or	O
graphs	O
with	O
a	O
limited	O
number	O
of	O
edge	O
types	O
to	O
unstructured	O
inputs	O
such	O
as	O
texts	O
.	O

After	O
that	O
,	O
it	O
employs	O
three	O
modules	O
to	O
process	O
relational	B-TaskName
reasoning	I-TaskName
:	O
(	O
1	O
)	O
an	O
encoding	O
module	O
which	O
enables	O
edges	O
to	O
encode	O
rich	O
information	O
from	O
natural	O
languages	O
,	O
(	O
2	O
)	O
a	O
propagation	O
module	O
which	O
propagates	O
relational	O
information	O
among	O
various	O
nodes	O
,	O
and	O
(	O
3	O
)	O
a	O
classification	O
module	O
which	O
makes	O
predictions	O
with	O
node	O
representations	O
.	O

GP	B-MethodName
-	I-MethodName
GNNs	I-MethodName
first	O
constructs	O
a	O
fullyconnected	O
graph	O
with	O
the	O
entities	O
in	O
the	O
sequence	O
of	O
text	O
.	O

To	O
address	O
this	O
issue	O
,	O
in	O
this	O
paper	O
,	O
we	O
propose	O
graph	B-MethodName
neural	I-MethodName
networks	I-MethodName
with	I-MethodName
generated	I-MethodName
parameters	I-MethodName
(	O
GP	B-MethodName
-	I-MethodName
GNNs	I-MethodName
)	O
,	O
to	O
adapt	O
graph	B-MethodName
neural	I-MethodName
networks	I-MethodName
to	O
solve	O
the	O
natural	B-TaskName
language	I-TaskName
relational	I-TaskName
reasoning	I-TaskName
task	O
.	O

However	O
,	O
most	O
existing	O
GNNs	B-MethodName
can	O
only	O
process	O
multi	O
-	O
hop	O
relational	O
reasoning	O
on	O
pre	O
-	O
defined	O
graphs	O
and	O
can	O
not	O
be	O
directly	O
applied	O
in	O
natural	O
language	O
relational	O
reasoning	O
.	O

1	O
,	O
existing	O
relation	B-TaskName
extraction	I-TaskName
models	O
could	O
easily	O
extract	O
the	O
facts	O
that	O
Luc	O
Besson	O
directed	O
a	O
film	O
Léon	O
:	O
The	O
Professional	O
and	O
that	O
the	O
film	O
is	O
in	O
English	O
,	O
but	O
fail	O
to	O
infer	O
the	O
relationship	O
between	O
Luc	O
Besson	O
and	O
English	O
without	O
multi	O
-	O
hop	O
relational	O
reasoning	O
.	O

These	O
works	O
have	O
demonstrated	O
GNNs	B-MethodName
'	O
strong	O
power	O
to	O
process	O
relational	O
reasoning	O
on	O
graphs	O
.	O

In	O
recent	O
years	O
,	O
graph	B-MethodName
neural	I-MethodName
networks	I-MethodName
(	O
GNNs	B-MethodName
)	O
have	O
been	O
applied	O
to	O
various	O
fields	O
of	O
machine	O
learning	O
,	O
including	O
node	O
classification	O
(	O
Kipf	O
and	O
Welling	O
,	O
2016	O
)	O
,	O
relation	O
classification	O
(	O
Schlichtkrull	O
et	O
al	O
.	O
,	O
2017	O
)	O
,	O
molecular	O
property	O
prediction	O
(	O
Gilmer	O
et	O
al	O
.	O
,	O
2017	O
)	O
,	O
few	O
-	O
shot	O
learning	O
(	O
Garcia	O
and	O
Bruna	O
,	O
2018	O
)	O
,	O
and	O
achieved	O
promising	O
results	O
on	O
these	O
tasks	O
.	O

We	O
verify	O
GP	B-MethodName
-	I-MethodName
GNNs	I-MethodName
in	O
relation	B-TaskName
extraction	I-TaskName
from	O
text	O
,	O
both	O
on	O
bag	O
-	O
and	O
instancesettings	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
graph	B-MethodName
neural	I-MethodName
network	I-MethodName
with	I-MethodName
generated	I-MethodName
parameters	I-MethodName
(	O
GP	B-MethodName
-	I-MethodName
GNNs	I-MethodName
)	O
.	O

Graph	B-MethodName
Neural	I-MethodName
Networks	I-MethodName
with	I-MethodName
Generated	I-MethodName
Parameters	I-MethodName
for	O
Relation	B-TaskName
Extraction	I-TaskName
.	O

However	O
,	O
in	O
our	O
model	O
,	O
since	O
Ohio	O
and	O
Johnson	O
County	O
have	O
no	O
relationship	O
,	O
this	O
wrong	O
relation	O
is	O
not	O
predicted	O
.	O

As	O
we	O
have	O
discussed	O
before	O
,	O
this	O
is	O
due	O
to	O
its	O
mechanism	O
to	O
model	O
cooccurrence	O
of	O
multiple	O
relations	O
.	O

ritory	O
issues	O
.	O

Although	O
"	O
No	O
Relation	O
"	O
is	O
also	O
be	O
seen	O
as	O
a	O
type	O
of	O
relation	O
,	O
we	O
only	O
show	O
other	O
relation	O
types	O
in	O
the	O
graphs	O
.	O

The	O
models	O
take	O
sentences	O
and	O
entity	O
markers	O
as	O
input	O
and	O
produce	O
a	O
graph	O
containing	O
entities	O
(	O
colored	O
and	O
bold	O
)	O
and	O
relations	O
between	O
them	O
.	O

Hao	O
Zhu	O
is	O
supported	O
by	O
Tsinghua	O
Initiative	O
Research	O
Program	O
.	O

This	O
work	O
10	O
http://thunlp.org	O
is	O
jointly	O
supported	O
by	O
the	O
NSFC	O
project	O
under	O
the	O
grant	O
No	O
.	O
61661146007	O
and	O
the	O
NExT++	O
project	O
,	O
the	O
National	O
Research	O
Foundation	O
,	O
Prime	O
Ministers	O
Office	O
,	O
Singapore	O
under	O
its	O
IRC@Singapore	O
Funding	O
Initiative	O
.	O

The	O
authors	O
thank	O
the	O
members	O
of	O
Tsinghua	O
NLP	O
lab	O
10	O
for	O
their	O
thoughtful	O
suggestions	O
.	O

Acknowledgement	O
.	O

For	O
example	O
,	O
in	O
the	O
third	O
case	O
,	O
share	O
border	O
with	O
and	O
located	O
in	O
are	O
both	O
relations	O
about	O
ter-	O
.	O

Tab	O
.	O

Qualitative	O
Results	O
:	O
Case	O
Study	O
.	O

We	O
leave	O
these	O
explorations	O
for	O
future	O
work	O
.	O

In	O
real	O
applications	O
,	O
different	O
variants	O
could	O
be	O
selected	O
for	O
different	O
kind	O
of	O
sentences	O
or	O
we	O
can	O
also	O
ensemble	O
the	O
prediction	O
from	O
different	O
models	O
.	O

This	O
observation	O
reveals	O
that	O
the	O
reasoning	O
mechanism	O
could	O
help	O
us	O
identify	O
relations	O
especially	O
on	O
sentences	O
where	O
there	O
are	O
more	O
entities	O
.	O

However	O
,	O
the	O
improvement	O
of	O
the	O
third	O
layer	O
is	O
much	O
smaller	O
on	O
the	O
overall	O
distantly	O
supervised	O
test	O
set	O
than	O
the	O
one	O
on	O
the	O
dense	O
subset	O
.	O

We	O
could	O
also	O
see	O
from	O
Fig	O
.	O

Here	O
,	O
we	O
fol-	O
P@5	O
%	O
P@10	O
%	O
P@15	O
%	O
P@20	O
%	O
P@5	O
%	O
P@10	O
%	O
P@15	O
%	O
P@20	O
%	O
.	O

Zeng	O
et	O
al	O
.	O

Evaluation	O
Details	O
.	O

Table	O
1	O
shows	O
our	O
best	O
hyper	O
-	O
parameter	O
settings	O
,	O
which	O
are	O
used	O
in	O
all	O
of	O
our	O
experiments	O
.	O

We	O
select	O
the	O
best	O
parameters	O
for	O
the	O
validation	O
set	O
.	O

Hyper	O
-	O
parameters	O
.	O

These	O
models	O
are	O
capable	O
of	O
performing	O
2	O
-	O
hop	O
reasoning	O
and	O
3	O
-	O
hop	O
reasoning	O
,	O
respectively	O
.	O

(	O
,	O
2015	O
)	O
)	O
.	O

(	O
2014Zeng	O
et	O
al	O
.	O

This	O
model	O
divides	O
the	O
whole	O
sentence	O
into	O
three	O
pieces	O
and	O
applies	O
max	O
-	O
pooling	O
after	O
convolution	O
layer	O
piece	O
-	O
wisely	O
.	O

(	O
2015	O
)	O
.	O

(	O
2014	O
)	O
utilize	O
convolutional	O
neural	O
networks	O
to	O
classify	O
relations	O
.	O

Zeng	O
et	O
al	O
.	O

This	O
baseline	O
is	O
implemented	O
by	O
ourselves	O
based	O
on	O
authors	O
'	O
public	O
repo	O
8	O
.	O

This	O
model	O
utilizes	O
attention	O
mechanism	O
to	O
encode	O
the	O
context	O
relations	O
for	O
predicting	O
target	O
relations	O
.	O

We	O
select	O
the	O
following	O
models	O
for	O
comparison	O
,	O
the	O
first	O
four	O
of	O
which	O
are	O
our	O
baseline	O
models	O
.	O

Models	O
for	O
Comparison	O
.	O

There	O
are	O
1,350	O
sentences	O
and	O
more	O
than	O
17,915	O
triples	O
and	O
7,906	O
relational	O
facts	O
in	O
this	O
test	O
set	O
.	O

This	O
test	O
set	O
could	O
be	O
used	O
to	O
test	O
our	O
methods	O
'	O
performance	O
on	O
sentences	O
with	O
the	O
complex	O
interaction	O
between	O
entities	O
.	O

Our	O
criteria	O
are	O
:	O
(	O
1	O
)	O
the	O
number	O
of	O
entities	O
should	O
be	O
strictly	O
larger	O
than	O
2	O
;	O
and	O
(	O
2	O
)	O
there	O
must	O
be	O
at	O
least	O
one	O
circle	O
(	O
with	O
at	O
least	O
three	O
entities	O
)	O
in	O
the	O
ground	O
-	O
truth	O
label	O
of	O
the	O
sentence	O
7	O
.	O

There	O
are	O
350	O
sentences	O
and	O
1,230	O
triples	O
in	O
this	O
test	O
set	O
.	O

Only	O
the	O
instances	O
accepted	O
by	O
all	O
5	O
annotators	O
are	O
incorporated	O
into	O
the	O
human	O
annotated	O
test	O
set	O
.	O

They	O
are	O
asked	O
to	O
decide	O
whether	O
or	O
not	O
the	O
distant	O
supervision	O
is	O
right	O
for	O
every	O
pair	O
of	O
entities	O
.	O

5	O
We	O
use	O
the	O
same	O
training	O
set	O
for	O
all	O
of	O
the	O
experiments	O
.	O

Therefore	O
,	O
we	O
need	O
to	O
modify	O
their	O
dataset	O
:	O
(	O
1	O
)	O
We	O
added	O
reversed	O
edges	O
if	O
they	O
are	O
missing	O
from	O
a	O
given	O
triple	O
,	O
e.g.	O
if	O
triple	O
(	O
Earth	O
,	O
part	O
of	O
,	O
Solar	O
System	O
)	O
exists	O
in	O
the	O
sentence	O
,	O
we	O
add	O
a	O
reversed	O
label	O
,	O
(	O
Solar	O
System	O
,	O
has	O
a	O
member	O
,	O
Earth	O
)	O
,	O
to	O
it	O
;	O
(	O
2	O
)	O
For	O
all	O
of	O
the	O
entity	O
pairs	O
with	O
no	O
relations	O
,	O
we	O
added	O
"	O
NA	O
"	O
labels	O
to	O
them	O
.	O

Datasets	O
.	O

Experiment	O
Settings	O
.	O

Experiments	O
.	O

To	O
make	O
it	O
more	O
efficient	O
,	O
we	O
avoid	O
using	O
loop	O
-	O
based	O
,	O
scalar	O
-	O
oriented	O
code	O
by	O
matrix	O
and	O
vector	O
operations	O
.	O

We	O
use	O
PyTorch	O
(	O
Paszke	O
et	O
al	O
.	O
,	O
2017	O
)	O
to	O
implement	O
our	O
models	O
.	O

In	O
practice	O
,	O
we	O
stack	O
the	O
embeddings	O
for	O
every	O
target	O
entity	O
pairs	O
together	O
to	O
infer	O
the	O
underlying	O
relationship	O
between	O
each	O
pair	O
of	O
entities	O
.	O

We	O
use	O
cross	O
entropy	O
here	O
as	O
the	O
classification	O
loss	O
L	O
=	O
s∈S	O
i	O
=	O
j	O
log	O
P(rv	O
i	O
,	O
v	O
j	O
|i	O
,	O
j	O
,	O
s),(8	O
)	O
where	O
r	O
v	O
i	O
,	O
v	O
j	O
denotes	O
the	O
relation	O
label	O
for	O
entity	O
pair	O
(	O
v	O
i	O
,	O
v	O
j	O
)	O
and	O
S	O
denotes	O
the	O
whole	O
corpus	O
.	O

This	O
could	O
be	O
used	O
for	O
classification	O
:	O
P(rv	O
i	O
,	O
v	O
j	O
|h	O
,	O
t	O
,	O
s	O
)	O
=	O
softmax(MLP(rv	O
i	O
,	O
v	O
j	O
)	O
)	O
,	O
(	O
7	O
)	O
where	O
r	O
v	O
i	O
,	O
v	O
j	O
∈	O
R	O
,	O
and	O
MLP	O
denotes	O
a	O
multi	O
-	O
layer	O
perceptron	O
module	O
.	O

;	O
[	O
h	O
(	O
K	O
)	O
v	O
i	O
h	O
(	O
K	O
)	O
v	O
j	O
]	O
]	O
,	O
(	O
6	O
)	O
where	O
represents	O
element	O
-	O
wise	O
multiplication	O
.	O

The	O
output	O
module	O
takes	O
the	O
embeddings	O
of	O
the	O
target	O
entity	O
pair	O
(	O
v	O
i	O
,	O
v	O
j	O
)	O
as	O
input	O
,	O
which	O
are	O
first	O
converted	O
by	O
:	O
rv	O
i	O
,	O
v	O
j	O
=	O
[	O
[	O
h	O
(	O
1	O
)	O
v	O
i	O
h	O
(	O
1	O
)	O
v	O
j	O
]	O
;	O
[	O
h	O
(	O
2	O
)	O
v	O
i	O
h	O
(	O
2	O
)	O
v	O
j	O
]	O
;	O
.	O

Classification	O
Module	O
.	O

5.4	O
)	O
.	O

In	O
our	O
context	O
,	O
however	O
,	O
since	O
the	O
graph	O
is	O
densely	O
connected	O
,	O
the	O
depth	O
is	O
interpreted	O
simply	O
as	O
giving	O
the	O
model	O
more	O
expressive	O
power	O
.	O

Annotators	O
a	O
subject	O
and	O
a	O
object	O
could	O
also	O
carry	O
the	O
prior	O
knowledge	O
about	O
subject	O
entity	O
and	O
object	O
entity	O
.	O

We	O
set	O
special	O
values	O
for	O
the	O
head	O
and	O
tail	O
entity	O
's	O
initial	O
embeddings	O
as	O
a	O
kind	O
of	O
"	O
flag	O
"	O
messages	O
which	O
we	O
expect	O
to	O
be	O
passed	O
through	O
propagation	O
.	O

The	O
Initial	O
Embeddings	O
of	O
Nodes	O
Suppose	O
we	O
are	O
focusing	O
on	O
extracting	O
the	O
relationship	O
between	O
entity	O
v	O
i	O
and	O
entity	O
v	O
j	O
,	O
the	O
initial	O
embeddings	O
of	O
them	O
are	O
annotated	O
as	O
h	O
(	O
0	O
)	O
v	O
i	O
=	O
a	O
subject	O
,	O
and	O
h	O
(	O
0	O
)	O
v	O
j	O
=	O
a	O
object	O
,	O
while	O
the	O
initial	O
embeddings	O
of	O
other	O
entities	O
are	O
set	O
to	O
all	O
zeros	O
.	O

Next	O
,	O
we	O
use	O
Eq	O
.	O
(	O
2	O
)	O
to	O
propagate	O
information	O
among	O
nodes	O
where	O
the	O
initial	O
embeddings	O
of	O
nodes	O
and	O
number	O
of	O
layers	O
are	O
further	O
specified	O
as	O
follows	O
.	O

Propagation	O
Module	O
.	O

We	O
use	O
notation	O
p	O
i	O
,	O
j	O
t	O
to	O
represent	O
the	O
position	O
embedding	O
for	O
x	O
t	O
corresponding	O
to	O
entity	O
pair	O
(	O
v	O
i	O
,	O
v	O
j	O
)	O
.	O

Each	O
position	O
marker	O
is	O
also	O
mapped	O
to	O
a	O
d	O
p	O
-dimensional	O
vector	O
by	O
a	O
position	O
embedding	O
matrix	O
P	O
∈	O
R	O
3×dp	O
.	O

In	O
this	O
work	O
,	O
we	O
consider	O
a	O
simple	O
entity	O
marking	O
scheme	O
2	O
:	O
we	O
mark	O
each	O
token	O
in	O
the	O
sentence	O
as	O
either	O
belonging	O
to	O
the	O
first	O
entity	O
v	O
i	O
,	O
the	O
second	O
entity	O
v	O
j	O
or	O
to	O
neither	O
of	O
those	O
.	O

Position	O
Embedding	O
.	O

Throughout	O
this	O
paper	O
,	O
we	O
stick	O
to	O
50	O
-	O
dimensional	O
GloVe	O
embeddings	O
pre	O
-	O
trained	O
on	O
a	O
6	O
-	O
billion	O
-	O
word	O
corpus	O
(	O
Pennington	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

,	O
x	O
l−1	O
}	O
to	O
a	O
kdimensional	O
embedding	O
vector	O
x	O
t	O
using	O
a	O
word	O
embedding	O
matrix	O
W	O
e	O
∈	O
R	O
|V	O
|×dw	O
,	O
where	O
|V	O
|	O
is	O
the	O
size	O
of	O
the	O
vocabulary	O
.	O

We	O
first	O
map	O
each	O
token	O
x	O
t	O
of	O
sentence	O
{	O
x	O
0	O
,	O
x	O
1	O
,	O
.	O

Word	O
Representations	O
.	O

Encoding	O
Module	O
.	O

Module	O
Classification	O
Module	O
h	O
(	O
n	O
)	O
1	O
h	O
(	O
n	O
)	O
2	O
h	O
(	O
n	O
)	O
3	O
A	O
(	O
n	O
)	O
1,2	O
A	O
(	O
n	O
)	O
2,3	O
A	O
(	O
n	O
)	O
3,1	O
x	O
1,2	O
3	O
x	O
1,2	O
4	O
x	O
1,2	O
2	O
x	O
1,2	O
1	O
x	O
1,2	O
0	O
Figure	O
2	O
:	O
Overall	O
architecture	O
:	O
an	O
encoding	O
module	O
takes	O
a	O
sequence	O
of	O
vector	O
representations	O
as	O
inputs	O
,	O
and	O
output	O
a	O
transition	O
matrix	O
as	O
output	O
;	O
a	O
propagation	O
module	O
propagates	O
the	O
hidden	O
states	O
from	O
nodes	O
to	O
its	O
neighbours	O
with	O
the	O
generated	O
transition	O
matrix	O
;	O
a	O
classification	O
module	O
provides	O
task	O
-	O
related	O
predictions	O
according	O
to	O
nodes	O
representations	O
.	O

Propagation	O
.	O

Encoding	O
Module	O
.	O

,	O
x	O
l−1	O
)	O
,	O
a	O
set	O
of	O
relations	O
R	O
and	O
a	O
set	O
of	O
entities	O
in	O
this	O
sentence	O
V	O
s	O
=	O
{	O
v	O
1	O
,	O
v	O
2	O
,	O
.	O

Given	O
a	O
sentence	O
s	O
=	O
(	O
x	O
0	O
,	O
x	O
1	O
,	O
.	O

,	O
h	O
K	O
0:|V|−1	O
,	O
Y	O
;	O
θ	O
c	O
)	O
,	O
(	O
3	O
)	O
where	O
θ	O
c	O
denotes	O
the	O
parameters	O
of	O
the	O
classification	O
module	O
,	O
K	O
is	O
the	O
number	O
of	O
layers	O
in	O
propagation	O
module	O
and	O
Y	O
denotes	O
the	O
ground	O
truth	O
label	O
.	O

Generally	O
,	O
the	O
classification	O
module	O
takes	O
node	O
representations	O
as	O
inputs	O
and	O
outputs	O
predictions	O
.	O

Classification	O
Module	O
.	O

Given	O
representations	O
of	O
layer	O
n	O
,	O
the	O
representations	O
of	O
layer	O
n	O
+	O
1	O
are	O
calculated	O
by	O
h	O
(	O
n+1	O
)	O
i	O
=	O
v	O
j	O
∈N	O
(	O
v	O
i	O
)	O
σ(A	O
(	O
n	O
)	O
i	O
,	O
j	O
h	O
(	O
n	O
)	O
j	O
)	O
,	O
(	O
2	O
)	O
where	O
N	O
(	O
v	O
i	O
)	O
denotes	O
the	O
neighbours	O
of	O
node	O
v	O
i	O
in	O
graph	O
G	O
and	O
σ(•	O
)	O
denotes	O
a	O
non	O
-	O
linear	O
activation	O
function	O
.	O

The	O
initial	O
embeddings	O
of	O
nodes	O
,	O
i.e.	O
the	O
representations	O
of	O
layer	O
0	O
,	O
are	O
task	O
-	O
related	O
,	O
which	O
could	O
be	O
embeddings	O
that	O
encode	O
features	O
of	O
nodes	O
or	O
just	O
one	O
-	O
hot	O
embeddings	O
.	O

The	O
propagation	O
module	O
learns	O
representations	O
for	O
nodes	O
layer	O
by	O
layer	O
.	O

Propagation	O
Module	O
.	O

Encoding	O
Module	O
.	O

2	O
.	O

,	O
x	O
i	O
,	O
j	O
l−1	O
extracted	O
from	O
the	O
text	O
.	O

Given	O
a	O
sequence	O
of	O
text	O
with	O
m	O
entities	O
,	O
it	O
aims	O
to	O
reason	O
on	O
both	O
the	O
text	O
and	O
entities	O
and	O
make	O
a	O
prediction	O
of	O
the	O
labels	O
of	O
the	O
entities	O
or	O
entity	O
pairs	O
.	O

The	O
drawback	O
of	O
existing	O
approaches	O
is	O
that	O
they	O
could	O
not	O
make	O
full	O
use	O
of	O
the	O
multihop	O
inference	O
patterns	O
among	O
multiple	O
entity	O
pairs	O
and	O
their	O
relations	O
within	O
the	O
sentence	O
.	O

The	O
most	O
related	O
work	O
is	O
Sorokin	O
and	O
Gurevych	O
(	O
2017	O
)	O
,	O
where	O
the	O
proposed	O
model	O
incorporates	O
contextual	O
relations	O
with	O
an	O
attention	O
mechanism	O
when	O
predicting	O
the	O
relation	O
of	O
a	O
target	O
entity	O
pair	O
.	O

Christopoulou	O
et	O
al	O
.	O

Zeng	O
et	O
al	O
.	O

Peng	O
et	O
al	O
.	O

Lin	O
et	O
al	O
.	O

For	O
example	O
,	O
Zeng	O
et	O
al	O
.	O

Existing	O
works	O
(	O
Zeng	O
et	O
al	O
.	O
,	O
2014(Zeng	O
et	O
al	O
.	O
,	O
,	O
2015;;Lin	O
et	O
al	O
.	O
,	O
2016	O
)	O
have	O
demonstrated	O
that	O
neural	O
networks	O
are	O
capa	O
-	O
ble	O
of	O
capturing	O
the	O
pair	O
-	O
wise	O
relationship	O
between	O
entities	O
in	O
certain	O
situations	O
.	O

(	O
2018	O
)	O
model	O
the	O
interaction	O
of	O
physical	O
objects	O
.	O

(	O
2017	O
)	O
build	O
up	O
a	O
scene	O
graph	O
according	O
to	O
an	O
image	O
,	O
and	O
Kipf	O
et	O
al	O
.	O

(	O
2017	O
)	O
propose	O
a	O
simple	O
neural	O
network	O
to	O
reason	O
the	O
relationship	O
of	O
objects	O
in	O
a	O
picture	O
,	O
Xu	O
et	O
al	O
.	O

For	O
example	O
,	O
Santoro	O
et	O
al	O
.	O

Johnson	O
(	O
2017	O
)	O
introduces	O
a	O
novel	O
neural	O
architecture	O
to	O
generate	O
a	O
graph	O
based	O
on	O
the	O
textual	O
input	O
and	O
dynamically	O
update	O
the	O
relationship	O
during	O
the	O
learning	O
process	O
.	O

Zhang	O
et	O
al	O
.	O

(	O
2017	O
)	O
apply	O
message	O
-	O
passing	O
on	O
a	O
graph	O
constructed	O
by	O
coreference	O
links	O
to	O
answer	O
relational	O
questions	O
.	O

Dhingra	O
et	O
al	O
.	O

(	O
2017	O
)	O
study	O
the	O
effectiveness	O
of	O
message	O
-	O
passing	O
in	O
quantum	O
chemistry	O
.	O

Gilmer	O
et	O
al	O
.	O

Gilmer	O
et	O
al	O
.	O

Later	O
the	O
authors	O
in	O
Li	O
et	O
al	O
.	O

Related	O
Work	O
.	O

Moreover	O
,	O
we	O
also	O
present	O
three	O
datasets	O
,	O
which	O
could	O
help	O
future	O
researchers	O
compare	O
their	O
models	O
in	O
different	O
settings	O
.	O

We	O
also	O
perform	O
a	O
qualitative	O
analysis	O
which	O
shows	O
that	O
our	O
model	O
could	O
discover	O
more	O
relations	O
by	O
reasoning	O
more	O
robustly	O
as	O
compared	O
to	O
baseline	O
models	O
.	O

Modeling	O
the	O
relationship	O
between	O
"	O
Léon	O
"	O
and	O
"	O
English	O
"	O
as	O
well	O
as	O
"	O
Luc	O
Besson	O
"	O
helps	O
discover	O
the	O
relationship	O
between	O
"	O
Luc	O
Besson	O
"	O
and	O
"	O
English	O
"	O
.	O

English	O
Luc	O
Besson	O
.	O

Léon	O
.	O

Enabling	O
multi	O
-	O
hop	O
relational	O
reasoning	O
in	O
natural	O
languages	O
remains	O
an	O
open	O
problem	O
.	O

By	O
considering	O
the	O
reasoning	O
patterns	O
,	O
one	O
can	O
discover	O
that	O
Luc	O
Besson	O
could	O
speak	O
English	O
following	O
a	O
reasoning	O
logic	O
that	O
Luc	O
Besson	O
directed	O
Léon	O
:	O
The	O
Professional	O
and	O
this	O
film	O
is	O
in	O
English	O
indicates	O
Luc	O
Besson	O
could	O
speak	O
English	O
.	O

Consider	O
the	O
example	O
shown	O
in	O
Fig	O
.	O

Besides	O
graphs	O
,	O
relational	O
reasoning	O
is	O
also	O
of	O
great	O
importance	O
in	O
many	O
natural	O
language	O
processing	O
tasks	O
such	O
as	O
question	O
answering	O
,	O
relation	O
extraction	O
,	O
summarization	O
,	O
etc	O
.	O

Relational	O
reasoning	O
aims	O
to	O
abstractly	O
reason	O
about	O
entities	O
/	O
objects	O
and	O
their	O
relations	O
,	O
which	O
is	O
an	O
important	O
part	O
of	O
human	O
intelligence	O
.	O

Introduction	O
.	O

Codes	O
and	O
data	O
are	O
released	O
at	O
https	O
:	O
//github.com	O
/	O
thunlp	O
/	O
gp	O
-	O
gnn	O
.	O

We	O
also	O
perform	O
a	O
qualitative	O
analysis	O
to	O
demonstrate	O
that	O
our	O
model	O
could	O
discover	O
more	O
accurate	O
relations	O
by	O
multi	O
-	O
hop	O
relational	O
reasoning	O
.	O

Experimental	O
results	O
on	O
a	O
humanannotated	O
dataset	O
and	O
two	O
distantly	O
supervised	O
datasets	O
show	O
that	O
multi	O
-	O
hop	O
reasoning	O
mechanism	O
yields	O
significant	O
improvements	O
.	O

The	O
parameters	O
in	O
the	O
propagation	O
module	O
,	O
i.e.	O
the	O
transition	O
matrices	O
used	O
in	O
message	O
passing	O
procedure	O
,	O
are	O
produced	O
by	O
a	O
generator	O
taking	O
natural	O
language	O
sentences	O
as	O
inputs	O
.	O

