After	O
constructing	O
clusters	O
given	O
a	O
document	O
,	O
a	O
word	O
-	O
graph	O
is	O
created	O
for	O
each	O
cluster	O
to	O
get	O
abstractive	O
fusions	O
from	O
these	O
related	O
sentences	O
.	O

We	O
use	O
the	O
NLTK	O
4	O
and	O
BNLP	O
5	O
to	O
preprocess	O
each	O
sentence	O
and	O
obtain	O
a	O
more	O
accurate	O
representation	O
of	O
the	O
information	O
.	O

BenSumm	B-MethodName
Model	O
.	O

The	O
summary	O
of	O
our	O
contributions	O
:	O
•	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
our	O
Bengali	O
Text	B-TaskName
Summarization	I-TaskName
model	O
(	O
BenSumm	O
)	O
is	O
the	O
very	O
first	O
unsupervised	O
model	O
to	O
generate	O
abstractive	O
summary	O
from	O
Bengali	O
text	O
documents	O
while	O
being	O
simple	O
yet	O
robust	O
.	O

A	O
cluster	O
of	O
sentences	O
uses	O
multi	O
-	O
sentence	O
compression	O
(	O
MSC	O
)	O
to	O
summarize	O
into	O
one	O
single	O
sentence	O
originally	O
called	O
sentence	O
fusion	O
(	O
Barzilay	O
and	O
McKeown	O
,	O
2005;Nayeem	O
and	O
Chali	O
,	O
2017b	O
)	O
.	O

A	O
detailed	O
illustration	O
of	O
our	O
BenSumm	B-MethodName
model	O
with	O
outputs	O
from	O
each	O
step	O
for	O
a	O
sample	O
input	O
document	O
is	O
presented	O
in	O
Figure	O
6	O
.	O

We	O
have	O
implemented	O
a	O
graphbased	B-MethodName
model	I-MethodName
to	O
fuse	O
multiple	O
related	O
sentences	O
,	O
requiring	O
only	O
a	O
POS	O
tagger	O
and	O
a	O
pre	O
-	O
trained	O
language	O
model	O
.	O

In	O
this	O
paper	O
,	O
we	O
have	O
developed	O
an	O
unsupervised	B-TaskName
abstractive	I-TaskName
text	I-TaskName
summarization	I-TaskName
system	O
for	O
Bengali	O
text	O
documents	O
.	O

We	O
get	O
an	O
average	O
score	O
of	O
4.41	B-MetricValue
,	O
3.95	B-MetricValue
,	O
and	O
4.2	B-MetricValue
in	O
content	B-MetricName
,	O
readability	B-MetricName
,	O
and	O
overall	B-MetricName
quality	I-MetricName
respectively	O
.	O

Here	O
,	O
content	B-MetricName
means	O
how	O
well	O
the	O
summary	O
can	O
convey	O
the	O
original	O
input	O
document	O
's	O
meaning	O
,	O
and	O
readability	B-MetricName
represents	O
the	O
grammatical	O
correction	O
and	O
the	O
overall	O
summary	O
sentence	O
coherence	O
.	O

They	O
have	O
evaluated	O
each	O
system	O
generated	O
summary	O
with	O
scores	B-MetricName
ranges	O
from	O
1	B-MetricValue
to	O
5	B-MetricValue
,	O
where	O
1	B-MetricValue
represents	O
very	O
poor	O
performance	O
,	O
and	O
5	B-MetricValue
represents	O
very	O
good	O
performance	O
.	O

Therefore	O
,	O
we	O
assign	O
three	O
different	O
evaluators	O
to	O
each	O
summary	O
generated	O
from	O
our	O
abstractive	O
system	O
(	O
BenSumm	B-MethodName
[	O
Abs	O
]	O
)	O
considering	O
three	O
different	O
aspects	O
,	O
i.e.	O
,	O
Content	B-MetricName
,	O
Readability	B-MetricName
,	O
and	O
Overall	B-MetricName
Quality	I-MetricName
.	O

10	O
Human	O
Evaluation	O
Though	O
ROUGE	B-MetricName
(	O
Lin	O
,	O
2004	O
)	O
has	O
been	O
shown	O
to	O
correlate	O
well	O
with	O
human	O
judgments	O
,	O
it	O
is	O
biased	O
towards	O
surface	O
level	O
lexical	O
similarities	O
,	O
and	O
this	O
makes	O
it	O
inappropriate	O
for	O
the	O
evaluation	O
of	O
abstractive	O
summaries	O
.	O

We	O
get	O
better	O
scores	O
in	O
terms	O
of	O
R1	B-MetricName
and	O
RL	B-MetricName
compared	O
to	O
the	O
baselines	O
.	O

Moreover	O
,	O
we	O
compare	O
our	O
extractive	O
version	O
of	O
our	O
model	O
BenSumm	B-MethodName
without	O
the	O
sentence	O
fusion	O
component	O
.	O

According	O
to	O
Table	O
2	O
,	O
our	O
abstractive	O
summarization	O
model	O
outperforms	O
all	O
the	O
extractive	O
baselines	O
in	O
terms	O
of	O
all	O
the	O
ROUGE	B-MetricName
metrics	O
even	O
though	O
the	O
dataset	O
itself	O
is	O
highly	O
abstractive	B-MetricName
(	O
reference	O
summary	O
contains	O
almost	O
73	B-MetricValue
%	I-MetricValue
new	O
words	O
)	O
.	O

Results	O
We	O
report	O
our	O
model	O
's	O
performance	O
compared	O
with	O
the	O
baselines	O
in	O
terms	O
of	O
F1	B-MetricName
scores	O
of	O
R-1	B-MetricName
,	O
R-2	B-MetricName
,	O
and	O
R	B-MetricName
-	I-MetricName
L	I-MetricName
in	O
Table	O
2	O
.	O

Baseline	O
Systems	O
We	O
compare	O
our	O
system	O
with	O
various	O
well	O
established	O
baseline	O
systems	O
like	O
LexRank	B-MethodName
(	O
Erkan	O
and	O
Radev	O
,	O
2004	O
)	O
,	O
TextRank	B-MethodName
(	O
Mihalcea	O
and	O
Tarau	O
,	O
2004	O
)	O
,	O
GreedyKL	B-MethodName
(	O
Haghighi	O
and	O
Vanderwende	O
,	O
2009	O
)	O
,	O
and	O
SumBasic	B-MethodName
(	O
Nenkova	O
and	O
Vanderwende	O
,	O
2005	O
)	O
.	O

Since	O
ROUGE	B-MetricName
computes	O
scores	O
based	O
on	O
the	O
lexical	O
overlap	O
at	O
the	O
surface	O
level	O
,	O
there	O
is	O
no	O
difference	O
in	O
implementation	O
for	O
summary	O
evaluation	O
of	O
the	O
Bengali	O
language	O
.	O

We	O
report	O
unigram	O
and	O
bigram	O
overlap	O
(	O
ROUGE-1	B-MetricName
and	O
ROUGE-2	B-MetricName
)	O
to	O
measure	O
informativeness	O
and	O
the	O
longest	O
common	O
subsequence	O
(	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
)	O
to	O
measure	O
the	O
summaries	O
'	O
fluency	O
.	O

Automatic	O
Evaluation	O
We	O
evaluate	O
our	O
system	O
(	O
BenSumm	B-MethodName
)	O
using	O
an	O
automatic	O
evaluation	O
metric	O
ROUGE	B-TaskName
F1	O
(	O
Lin	O
,	O
2004	O
)	O
without	O
any	O
limit	O
of	O
words	O
.	O

We	O
remove	O
the	O
abstractive	B-MethodName
sentence	I-MethodName
fusion	I-MethodName
part	O
to	O
compare	O
with	O
the	O
baselines	O
for	O
the	O
extractive	O
evaluation	O
.	O

Moreover	O
,	O
to	O
provide	O
our	O
proposed	O
framework	O
's	O
effectiveness	O
,	O
we	O
also	O
experiment	O
with	O
an	O
extractive	O
dataset	O
BNLPC	B-DatasetName
7	O
(	O
Haque	O
et	O
al	O
.	O
,	O
2015	O
)	O
.	O

From	O
the	O
dataset	O
,	O
we	O
measure	O
the	O
copy	B-MetricName
rate	I-MetricName
between	O
the	O
source	O
document	O
and	O
the	O
human	O
summaries	O
.	O

6	O
We	O
collected	O
the	O
human	O
written	O
document	O
-	O
summary	O
pairs	O
from	O
the	O
several	O
printed	O
copy	O
of	O
NCTB	B-DatasetName
books	I-DatasetName
.	O

We	O
conduct	O
experiments	O
on	O
our	O
dataset	O
which	O
consists	O
of	O
139	O
samples	O
of	O
human	B-TaskName
-	I-TaskName
written	I-TaskName
abstractive	I-TaskName
document	I-TaskName
-	I-TaskName
summary	I-TaskName
pairs	I-TaskName
written	O
professional	O
summary	O
writers	O
of	O
the	O
National	O
Curriculum	O
and	O
Textbook	O
Board	O
(	O
NCTB	O
)	O
.	O

This	O
section	O
presents	O
our	O
experimental	O
details	O
for	O
assessing	O
the	O
performance	O
of	O
the	O
proposed	O
Ben	B-MethodName
-	I-MethodName
Summ	I-MethodName
model	O
.	O

Word	B-MethodName
Graph	I-MethodName
Generation	O
Sentence	B-MethodName
Fusion	I-MethodName
.	O

Word	B-MethodName
Graph	I-MethodName
Generation	O
Sentence	B-MethodName
Fusion	I-MethodName
.	O

We	O
get	O
multiple	O
weighted	O
sentences	O
(	O
see	O
Figure	O
2	O
)	O
form	O
the	O
clusters	O
using	O
the	O
ranking	B-MethodName
strategy	O
(	O
Boudin	O
and	O
Morin	O
,	O
2013	O
)	O
.	O

Word	B-MethodName
Graph	I-MethodName
(	O
WG	B-MethodName
)	O
Construction	O
.	O

We	O
chose	O
to	O
build	O
an	O
abstractive	O
summarizer	O
with	O
a	O
sentence	B-MethodName
fusion	I-MethodName
technique	O
by	O
generating	O
word	O
graphs	O
(	O
Filippova	O
,	O
2010;Boudin	O
and	O
Morin	O
,	O
2013	O
)	O
for	O
the	O
Bengali	O
Language	O
.	O

We	O
measure	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
clusters	I-HyperparameterName
for	O
a	O
given	O
document	O
using	O
the	O
silhouette	O
value	O
.	O

There	O
will	O
be	O
a	O
minimum	B-HyperparameterName
of	O
2	B-HyperparameterValue
and	O
a	O
maximum	B-HyperparameterName
of	O
n	B-HyperparameterValue
−	I-HyperparameterValue
1	I-HyperparameterValue
clusters	B-HyperparameterName
.	O

We	O
use	O
hierarchical	B-MethodName
agglomerative	I-MethodName
clustering	I-MethodName
with	O
the	O
ward	B-MethodName
's	I-MethodName
method	I-MethodName
(	O
Murtagh	O
and	O
Legendre	O
,	O
2014	O
)	O
.	O

Therefore	O
,	O
we	O
calculate	O
the	O
cosine	O
similarity	O
between	O
the	O
sentence	O
vectors	O
obtained	O
from	O
ULMfit	B-MethodName
pre	O
-	O
trained	O
language	O
model	O
(	O
Howard	O
and	O
Ruder	O
,	O
2018	O
)	O
.	O

The	O
Term	B-MethodName
Frequency	I-MethodName
-	I-MethodName
Inverse	I-MethodName
Document	I-MethodName
Frequency	I-MethodName
(	O
TF	B-MethodName
-	I-MethodName
IDF	I-MethodName
)	O
measure	O
does	O
not	O
work	O
well	O
(	O
Aggarwal	O
and	O
Zhai	O
,	O
2012	O
)	O
.	O

We	O
here	O
describe	O
each	O
of	O
the	O
steps	O
involved	O
in	O
our	O
Bengali	B-TaskName
Unsupervised	I-TaskName
Abstractive	I-TaskName
Text	I-TaskName
Summarization	I-TaskName
model	O
(	O
BenSumm	O
)	O
for	O
single	O
document	O
setting	O
.	O

(	O
2018	O
)	O
developed	O
an	O
unsupervised	B-TaskName
abstractive	I-TaskName
summarization	I-TaskName
system	O
that	O
jointly	O
performs	O
sentence	O
fusion	O
and	O
paraphrasing	O
.	O

Clarke	O
and	O
Lapata	O
(	O
2008	O
)	O
;	O
Filippova	O
(	O
2010	O
)	O
showed	O
a	O
first	O
intermediate	O
step	O
towards	O
abstractive	B-TaskName
summarization	I-TaskName
,	O
which	O
compresses	O
original	O
sentences	O
for	O
a	O
summary	O
generation	O
.	O

Potential	O
utility	O
for	O
extractive	O
text	B-TaskName
summarization	I-TaskName
made	O
SC	O
very	O
popular	O
for	O
single	O
or	O
multi	O
-	O
document	O
summarization	O
(	O
Nenkova	O
and	O
McKeown	O
,	O
2012	O
)	O
.	O

Moreover	O
,	O
there	O
is	O
no	O
human	O
-	O
annotated	O
dataset	O
to	O
compare	O
abstractive	B-TaskName
summarization	I-TaskName
methods	O
of	O
this	O
language	O
.	O

(	O
,	O
2015	O
)	O
)	O
worked	O
on	O
extractive	O
Bengali	B-TaskName
text	I-TaskName
summarization	I-TaskName
using	O
pronoun	O
replacement	O
,	O
sentence	O
ranking	O
with	O
term	O
frequency	O
,	O
numerical	O
figures	O
,	O
and	O
overlapping	O
of	O
title	O
words	O
with	O
the	O
document	O
sentences	O
.	O

Nevertheless	O
,	O
very	O
few	O
attempts	O
have	O
been	O
made	O
for	O
Bengali	B-TaskName
Text	I-TaskName
summarization	I-TaskName
despite	O
Bangla	O
being	O
the	O
7	O
th	O
most	O
spoken	O
language	O
.	O

Many	O
researchers	O
have	O
worked	O
on	O
text	B-TaskName
summarization	I-TaskName
and	O
introduced	O
different	O
extractive	O
and	O
abstractive	O
methods	O
.	O

Our	O
model	O
requires	O
only	O
POS	O
tagger	O
and	O
a	O
pre	O
-	O
trained	O
language	O
model	O
,	O
which	O
is	O
easily	O
reproducible	O
.	O

2	O
•	O
We	O
design	O
an	O
unsupervised	B-TaskName
abstractive	I-TaskName
sentence	I-TaskName
generation	I-TaskName
model	O
that	O
performs	O
sentence	B-MethodName
fusion	I-MethodName
on	O
Bengali	B-DatasetName
texts	I-DatasetName
.	O

The	O
success	O
of	O
neural	O
sequence	O
-	O
tosequence	O
(	O
seq2seq	O
)	O
models	O
with	O
attention	O
(	O
Bahdanau	O
et	O
al	O
.	O
,	O
2015;Luong	O
et	O
al	O
.	O
,	O
2015	O
)	O
provides	O
an	O
effective	O
way	O
for	O
text	O
generation	O
which	O
has	O
been	O
extensively	O
applied	O
in	O
the	O
case	O
of	O
abstractive	O
summarization	O
of	O
English	O
language	O
documents	O
(	O
Rush	O
et	O
al	O
.	O
,	O
2015;Chopra	O
et	O
al	O
.	O
,	O
2016;Nallapati	O
et	O
al	O
.	O
,	O
2016;Miao	O
and	O
Blunsom	O
,	O
2016;Paulus	O
et	O
al	O
.	O
,	O
2018;Nayeem	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

Compared	O
to	O
extractive	O
,	O
abstractive	B-TaskName
summary	I-TaskName
generation	I-TaskName
is	O
indeed	O
a	O
challenging	O
task	O
.	O

Extractive	B-TaskName
summarization	I-TaskName
is	O
about	O
ranking	O
important	O
sentences	O
from	O
the	O
original	O
text	O
.	O

The	O
process	O
of	O
shortening	O
a	O
large	O
text	O
document	O
with	O
the	O
most	O
relevant	O
information	O
of	O
the	O
source	O
is	O
known	O
as	O
automatic	B-TaskName
text	I-TaskName
summarization	I-TaskName
.	O

Our	O
unsupervised	B-TaskName
abstractive	I-TaskName
summarization	I-TaskName
model	O
outperforms	O
the	O
baselines	O
without	O
being	O
exposed	O
to	O
any	O
human	O
-	O
annotated	O
reference	O
summaries	O
.	O

We	O
conduct	O
experiments	O
on	O
this	O
dataset	O
and	O
compare	O
our	O
system	O
with	O
several	O
well	O
-	O
established	O
unsupervised	B-TaskName
extractive	I-TaskName
summarization	I-TaskName
systems	O
.	O

We	O
also	O
provide	O
a	O
human	B-MetricName
-	I-MetricName
annotated	I-MetricName
dataset	I-MetricName
with	O
document	O
-	O
summary	O
pairs	O
to	O
evaluate	O
our	O
abstractive	O
model	O
and	O
to	O
support	O
the	O
comparison	O
of	O
future	O
abstractive	B-TaskName
summarization	I-TaskName
systems	O
of	O
the	O
Bengali	O
Language	O
.	O

To	O
overcome	O
this	O
problem	O
,	O
we	O
propose	O
a	O
graph	O
-	O
based	O
unsupervised	B-TaskName
abstractive	I-TaskName
summarization	I-TaskName
system	O
in	O
the	O
single	O
-	O
document	O
setting	O
for	O
Bengali	O
text	O
documents	O
,	O
which	O
requires	O
only	O
a	O
Part	O
-	O
Of	O
-	O
Speech	O
(	O
POS	O
)	O
tagger	O
and	O
a	O
pre	O
-	O
trained	O
language	O
model	O
trained	O
on	O
Bengali	O
texts	O
.	O

Abstractive	B-TaskName
summarization	I-TaskName
systems	O
generally	O
rely	O
on	O
large	O
collections	O
of	O
documentsummary	O
pairs	O
.	O

Unsupervised	B-TaskName
Abstractive	I-TaskName
Summarization	I-TaskName
of	O
Bengali	O
Text	O
Documents	O
.	O

A	O
Appendix	O
.	O

We	O
want	O
to	O
thank	O
all	O
the	O
anonymous	O
reviewers	O
for	O
their	O
thoughtful	O
comments	O
and	O
constructive	O
suggestions	O
for	O
future	O
improvements	O
to	O
this	O
work	O
.	O

Acknowledgments	O
.	O

In	O
the	O
future	O
,	O
we	O
would	O
like	O
to	O
jointly	O
model	O
multi	O
-	O
sentence	O
compression	O
and	O
paraphrasing	O
in	O
our	O
system	O
.	O

One	O
of	O
the	O
limitations	O
of	O
our	O
model	O
is	O
that	O
it	O
can	O
not	O
generate	O
new	O
words	O
.	O

We	O
design	O
a	O
Bengali	O
Document	O
Summarization	O
tool	O
to	O
provide	O
both	O
extractive	O
and	O
abstractive	O
summary	O
of	O
a	O
given	O
document	O
.	O

Experimental	O
results	O
on	O
our	O
proposed	O
dataset	O
demonstrate	O
the	O
superiority	O
of	O
our	O
approach	O
against	O
strong	O
extractive	O
baselines	O
.	O

Conclusion	O
and	O
Future	O
Work	O
.	O

Moreover	O
,	O
We	O
design	O
a	O
Bengali	O
Document	O
Summarization	O
tool	O
(	O
see	O
Figure	O
5	O
)	O
capable	O
of	O
providing	O
both	O
extractive	O
and	O
abtractive	O
summary	O
for	O
an	O
input	O
document	O
.	O

Finally	O
,	O
we	O
present	O
an	O
example	O
of	O
our	O
model	O
output	O
in	O
Figure	O
4	O
.	O

On	O
the	O
other	O
hand	O
,	O
our	O
model	O
is	O
unsupervised	O
and	O
abstractive	O
.	O

It	O
is	O
important	O
to	O
note	O
that	O
these	O
summarizers	O
are	O
completely	O
extractive	O
and	O
designed	O
for	O
English	O
language	O
.	O

We	O
use	O
an	O
open	O
source	O
implementation	O
9	O
of	O
these	O
summarizers	O
and	O
adapted	O
it	O
for	O
Bengali	O
language	O
.	O

8	O
We	O
extract	O
3	O
-	O
best	O
sentences	O
from	O
our	O
system	O
and	O
the	O
systems	O
we	O
compare	O
as	O
baselines	O
.	O

It	O
's	O
clearly	O
visible	O
from	O
the	O
table	O
that	O
our	O
dataset	O
is	O
highly	O
abstractive	O
and	O
will	O
serve	O
as	O
a	O
robust	O
benchmark	O
for	O
this	O
task	O
's	O
future	O
works	O
.	O

The	O
overall	O
statistics	O
of	O
the	O
datasets	O
are	O
presented	O
in	O
Table	O
1	O
.	O

The	O
majority	O
of	O
Bangladeshi	O
schools	O
follow	O
these	O
books	O
.	O

The	O
NCTB	O
is	O
responsible	O
for	O
the	O
development	O
of	O
the	O
curriculum	O
and	O
distribution	O
of	O
textbooks	O
.	O

Dataset	O
.	O

Experiments	O
.	O

Do	O
n't	O
be	O
happy	O
to	O
see	O
the	O
beautiful	O
faces	O
.	O
]	O
Human	O
Reference	O
.	O

We	O
need	O
hard	O
work	O
and	O
pursuit	O
to	O
form	O
the	O
nature	O
,	O
otherwise	O
it	O
is	O
not	O
possible	O
to	O
defeat	O
the	O
devil	O
.	O

People	O
hate	O
his	O
nature	O
,	O
his	O
touch	O
,	O
his	O
customs	O
.	O

[	O
Evil	O
people	O
are	O
fascinated	O
by	O
human	O
form	O
and	O
enjoy	O
its	O
fruits	O
.	O

দ	O
ুঃস্বভাবের	O
মান	O
ষ	O
মান	O
বষর	O
রূপ	O
দদবে	O
ম	O
গ্ধ	O
হয়	O
এেং	O
তার	O
ফল	O
দভাগ	O
কবর	O
।	O
যার	O
স্বভাে	O
,	O
তার	O
স্পর্	O
শ	O
,	O
তার	O
রীততনীততবক	O
মান	O
ষ	O
ঘৃ	O
ণা	O
কবর	O
।	O
স্বভাে	O
গঠবন	O
কঠঠন	O
পতরশ্রম	O
ও	O
সাধনা	O
চাই	O
,	O
নইবল	O
র্য়তানবক	O
পরাজিত	O
করা	O
সম্ভে	O
নয়	O
।	O
তার	O
স	O
ন্দর	O
ম	O
ে	O
দদবে	O
আনজন্দত	O
হবয়া	O
না	O
।	O
.	O

System	O
Summary	O
.	O

Ranking	O
.	O

Ranking	O
.	O

Cluster	O
n	O
Cluster	O
1	O
.	O

Clustering	O
.	O

Preprocessing	O
.	O

Single	O
Document	O
.	O

We	O
also	O
present	O
a	O
detailed	O
illustration	O
of	O
our	O
framework	O
with	O
an	O
example	O
source	O
document	O
in	O
the	O
Appendix	O
.	O

The	O
overall	O
process	O
is	O
presented	O
in	O
Figure	O
3	O
.	O

We	O
generate	O
the	O
final	O
summary	O
by	O
merging	O
all	O
the	O
topranked	O
sentences	O
.	O

We	O
take	O
the	O
top	O
-	O
ranked	O
sentence	O
from	O
each	O
cluster	O
to	O
present	O
the	O
summary	O
.	O

Start	O
.	O

Figure	O
1	O
illustrates	O
an	O
example	O
WG	O
for	O
these	O
two	O
sentences	O
.	O

Figure	O
2	O
presents	O
two	O
sentences	O
,	O
which	O
is	O
one	O
of	O
the	O
source	O
document	O
clusters	O
,	O
and	O
the	O
possible	O
paths	O
with	O
their	O
weighted	O
values	O
are	O
generated	O
using	O
the	O
word	O
-	O
graph	O
approach	O
.	O

After	O
constructing	O
the	O
word	O
-	O
graph	O
,	O
we	O
can	O
generate	O
M	O
-shortest	O
paths	O
from	O
the	O
dummy	O
start	O
node	O
to	O
the	O
end	O
node	O
in	O
the	O
word	O
graph	O
(	O
see	O
Figure	O
1	O
)	O
.	O

Each	O
sentence	O
of	O
the	O
cluster	O
is	O
connected	O
to	O
a	O
dummy	O
start	O
and	O
end	O
node	O
to	O
mark	O
the	O
beginning	O
and	O
ending	O
sentences	O
.	O

After	O
the	O
first	O
sentence	O
is	O
added	O
to	O
the	O
graph	O
as	O
word	O
nodes	O
(	O
punctuation	O
included	O
)	O
,	O
words	O
from	O
the	O
other	O
related	O
sentences	O
are	O
mapped	O
onto	O
a	O
node	O
in	O
the	O
graph	O
with	O
the	O
same	O
POS	O
tag	O
.	O

Directed	O
edges	O
are	O
formed	O
by	O
connecting	O
the	O
adjacent	O
words	O
from	O
the	O
sentences	O
.	O

The	O
words	O
are	O
represented	O
as	O
vertices	O
along	O
with	O
the	O
parts	O
-	O
of	O
-	O
speech	O
(	O
POS	O
)	O
tags	O
.	O

Let	O
,	O
a	O
set	O
of	O
related	O
sentences	O
S	O
=	O
{	O
s	O
1	O
,	O
s	O
2	O
,	O
...	O
,	O
s	O
n	O
}	O
,	O
we	O
construct	O
a	O
graph	O
G	O
=	O
(	O
V	O
,	O
E	O
)	O
by	O
iteratively	O
adding	O
sentences	O
to	O
it	O
.	O

Given	O
a	O
cluster	O
of	O
related	O
sentences	O
,	O
we	O
construct	O
a	O
word	O
-	O
graph	O
following	O
(	O
Filippova	O
,	O
2010;Boudin	O
and	O
Morin	O
,	O
2013	O
)	O
.	O

This	O
method	O
is	O
entirely	O
unsupervised	O
and	O
needs	O
only	O
a	O
POS	O
tagger	O
,	O
which	O
is	O
highly	O
suitable	O
for	O
the	O
low	O
-	O
resource	O
setting	O
.	O

Textual	O
graphs	O
to	O
generate	O
abstractive	O
summaries	O
provide	O
effective	O
results	O
(	O
Ganesan	O
et	O
al	O
.	O
,	O
2010	O
)	O
.	O

The	O
following	O
formula	O
can	O
measure	O
silhouette	O
Score	O
:	O
Silhouette	O
Score	O
=	O
(	O
x	O
−	O
y	O
)	O
max(x	O
,	O
y)(1	O
)	O
where	O
y	O
denotes	O
mean	O
distance	O
to	O
the	O
other	O
instances	O
of	O
intra	O
-	O
cluster	O
and	O
x	O
is	O
the	O
mean	O
distance	O
to	O
the	O
instances	O
of	O
the	O
next	O
closest	O
cluster	O
.	O

The	O
clusters	O
are	O
highly	O
coherent	O
as	O
it	O
has	O
to	O
contain	O
sentences	O
similar	O
to	O
every	O
other	O
sentence	O
in	O
the	O
same	O
cluster	O
even	O
if	O
the	O
clusters	O
are	O
small	O
.	O

Here	O
,	O
n	O
denotes	O
the	O
number	O
of	O
sentences	O
in	O
the	O
document	O
.	O

This	O
step	O
is	O
critical	O
to	O
ensure	O
good	O
coverage	O
of	O
the	O
whole	O
document	O
and	O
avoid	O
redundancy	O
by	O
selecting	O
at	O
most	O
one	O
sentence	O
from	O
each	O
cluster	O
(	O
Nayeem	O
and	O
Chali	O
,	O
2017a	O
)	O
.	O

The	O
clustering	O
step	O
allows	O
us	O
to	O
group	O
similar	O
sentences	O
from	O
a	O
given	O
document	O
.	O

Sentence	O
Clustering	O
.	O

Our	O
preprocessing	O
step	O
includes	O
tokenization	O
,	O
removal	O
of	O
stopwords	O
,	O
Part	O
-	O
Of	O
-	O
Speech	O
(	O
POS	O
)	O
tagging	O
,	O
and	O
filtering	O
of	O
punctuation	O
marks	O
.	O

Nayeem	O
et	O
al	O
.	O

Boudin	O
and	O
Morin	O
(	O
2013	O
)	O
improved	O
Filippova	O
's	O
approach	O
by	O
re	O
-	O
ranking	O
the	O
compression	O
paths	O
according	O
to	O
keyphrases	O
,	O
which	O
resulted	O
in	O
more	O
informative	O
sentences	O
.	O

The	O
Word	O
-	O
Graph	O
based	O
approaches	O
were	O
first	O
proposed	O
by	O
(	O
Filippova	O
,	O
2010	O
)	O
,	O
which	O
require	O
only	O
a	O
POS	O
tagger	O
and	O
a	O
list	O
of	O
stopwords	O
.	O

Tex	O
-	O
tRank	O
(	O
Mihalcea	O
and	O
Tarau	O
,	O
2004	O
)	O
and	O
LexRank	O
(	O
Erkan	O
and	O
Radev	O
,	O
2004	O
)	O
are	O
graph	O
-	O
based	O
methods	O
for	O
extracting	O
important	O
sentences	O
from	O
a	O
document	O
.	O

Jing	O
and	O
McKeown	O
(	O
2000	O
)	O
worked	O
on	O
Sentence	O
Compression	O
(	O
SC	O
)	O
which	O
has	O
received	O
considerable	O
attention	O
in	O
the	O
NLP	O
community	O
.	O

Unfortunately	O
,	O
the	O
methods	O
are	O
limited	O
to	O
extractive	O
summarization	O
,	O
which	O
ranks	O
some	O
important	O
sentences	O
from	O
the	O
document	O
instead	O
of	O
generating	O
new	O
sentences	O
which	O
is	O
challenging	O
for	O
an	O
extremely	O
low	O
resource	O
language	O
like	O
Bengali	O
.	O

(	O
2017Haque	O
et	O
al	O
.	O

Haque	O
et	O
al	O
.	O

3	O
Das	O
and	O
Bandyopadhyay	O
(	O
2010	O
)	O
developed	O
Bengali	O
opinion	O
based	O
text	O
summarizer	O
using	O
given	O
topic	O
which	O
can	O
determine	O
the	O
information	O
on	O
sentiments	O
of	O
the	O
original	O
texts	O
.	O

Related	O
works	O
.	O

•	O
We	O
also	O
introduce	O
a	O
highly	O
abstractive	O
dataset	O
with	O
document	O
-	O
summary	O
pairs	O
to	O
evaluate	O
our	O
model	O
,	O
which	O
is	O
written	O
by	O
professional	O
summary	O
writers	O
of	O
National	O
Curriculum	O
and	O
Textbook	O
Board	O
(	O
NCTB	O
)	O
.	O

Therefore	O
,	O
we	O
choose	O
to	O
create	O
an	O
effective	O
Bengali	O
Text	O
Summarizer	O
with	O
an	O
unsupervised	O
approach	O
.	O

In	O
contrast	O
,	O
the	O
unsupervised	O
approach	O
reduces	O
the	O
human	O
effort	O
and	O
cost	O
for	O
collecting	O
and	O
annotating	O
large	O
amount	O
of	O
paired	O
training	O
data	O
.	O

These	O
models	O
are	O
usually	O
trained	O
with	O
lots	O
of	O
gold	O
summaries	O
,	O
but	O
there	O
is	O
no	O
large	O
-	O
scale	O
human	O
-	O
annotated	O
abstractive	O
summaries	O
available	O
for	O
low	O
-	O
resource	O
language	O
like	O
Bengali	O
.	O

Traditionally	O
used	O
abstractive	O
techniques	O
are	O
sentence	O
compression	O
,	O
syntactic	O
reorganization	O
,	O
sentence	O
fusion	O
,	O
and	O
lexical	O
paraphrasing	O
(	O
Lin	O
and	O
Ng	O
,	O
2019	O
)	O
.	O

The	O
abstractive	O
method	O
generates	O
human	O
-	O
like	O
sentences	O
using	O
natural	O
language	O
generation	O
techniques	O
.	O

There	O
are	O
two	O
types	O
of	O
summarizations	O
:	O
extractive	O
and	O
abstractive	O
.	O

A	O
good	O
summary	O
should	O
be	O
coherent	O
,	O
non	O
-	O
redundant	O
,	O
and	O
grammatically	O
readable	O
while	O
retaining	O
the	O
original	O
document	O
's	O
most	O
important	O
contents	O
(	O
Nenkova	O
and	O
McKeown	O
,	O
2012;Nayeem	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

Introduction	O
.	O

1	O
We	O
make	O
our	O
code	O
&	O
dataset	O
publicly	O
available	O
at	O
https://github.com/tafseer-nayeem/	O
BengaliSummarization	O
for	O
reproduciblity	O
.	O

Listed	O
by	O
alphabetical	O
order	O
.	O

1	O
*	O
Equal	O
contribution	O
.	O

However	O
,	O
the	O
performance	O
of	O
abstractive	O
systems	O
remains	O
a	O
challenge	O
due	O
to	O
the	O
unavailability	O
of	O
the	O
parallel	O
data	O
for	O
low	O
-	O
resource	O
languages	O
like	O
Bengali	O
.	O

