6	O
.	O

Finally	O
,	O
we	O
note	O
that	O
the	O
NER	O
tagging	O
provided	O
by	O
LitNER	B-MethodName
has	O
been	O
integrated	O
into	O
the	O
GutenTag	O
tool	O
(	O
as	O
of	O
version	O
0.1.4	O
)	O
.	O

Our	O
results	O
show	O
that	O
a	O
simple	O
classifier	O
,	O
trained	O
only	O
with	O
noisy	O
examples	O
derived	O
in	O
an	O
unsupervised	O
fashion	O
,	O
can	O
easily	O
beat	O
a	O
general	O
-	O
purpose	O
supervised	O
system	O
,	O
provided	O
it	O
has	O
access	O
to	O
the	O
full	O
context	O
of	O
the	O
text	O
.	O

In	O
this	O
paper	O
,	O
we	O
have	O
presented	O
LitNER	B-MethodName
,	O
an	O
NER	O
system	O
targeted	O
specifically	O
at	O
fiction	O
.	O

Conclusion	O
.	O

When	O
these	O
and	O
non	O
-	O
entities	O
are	O
excluded	O
from	O
OTHER	O
,	O
what	O
remains	O
is	O
eclectic	O
,	O
including	O
names	O
referring	O
to	O
small	O
groups	O
of	O
people	O
(	O
e.g.	O
families	O
)	O
,	O
animals	O
,	O
gods	O
,	O
ships	O
,	O
and	O
titles	O
of	O
other	O
works	O
of	O
literature	O
.	O

The	O
other	O
clusters	O
in	O
Table	O
1	O
reflect	O
word	O
categories	O
which	O
are	O
relatively	O
closed	O
-	O
class	O
and	O
much	O
less	O
central	O
to	O
the	O
fictional	O
narratives	O
as	O
character	O
and	O
setting	O
;	O
we	O
do	O
n't	O
see	O
a	O
compelling	O
case	O
for	O
tagging	O
them	O
.	O

This	O
coarse	O
-	O
grained	O
tag	O
set	O
reflects	O
not	O
only	O
the	O
practical	O
limitations	O
of	O
the	O
method	O
,	O
but	O
also	O
where	O
we	O
believe	O
automatic	O
methods	O
have	O
potential	O
to	O
provide	O
useful	O
information	O
for	O
literary	O
analysis	O
.	O

LitNER	B-MethodName
tags	O
names	O
into	O
only	O
two	O
main	O
classes	O
,	O
PERSON	O
and	O
LOCATION	O
,	O
plus	O
a	O
catch	O
-	O
all	O
OTHER	O
.	O

We	O
also	O
note	O
that	O
relying	O
primarily	O
on	O
contextual	O
classification	O
while	O
eschewing	O
resources	O
such	O
as	O
gazetteers	O
makes	O
much	O
less	O
sense	O
outside	O
the	O
context	O
of	O
fiction	O
;	O
we	O
would	O
expect	O
relatively	O
few	O
fictitious	O
entities	O
in	O
most	O
genres	O
.	O

Except	O
in	O
cases	O
where	O
it	O
is	O
possible	O
to	O
collapse	O
texts	O
into	O
appropriately	O
-	O
sized	O
groups	O
where	O
the	O
use	O
of	O
a	O
particular	O
name	O
is	O
likely	O
to	O
be	O
both	O
common	O
and	O
consistent	O
-	O
an	O
example	O
might	O
be	O
a	O
collection	O
of	O
texts	O
written	O
by	O
a	O
single	O
author	O
,	O
which	O
in	O
social	O
media	O
such	O
as	O
Twitter	O
seems	O
to	O
obey	O
the	O
classic	O
one	O
-	O
sense	O
-	O
per	O
-	O
discourse	O
rule	O
(	O
Gella	O
et	O
al	O
.	O
,	O
2014)-it	O
's	O
not	O
clear	O
that	O
this	O
approach	O
can	O
be	O
applied	O
successfully	O
in	O
cases	O
where	O
texts	O
are	O
relatively	O
short	O
,	O
which	O
is	O
a	O
far	O
more	O
common	O
situation	O
.	O

Also	O
less	O
than	O
promising	O
is	O
the	O
potential	O
for	O
using	O
text	O
-	O
level	O
classification	O
in	O
other	O
genres	O
:	O
whereas	O
the	O
average	O
number	O
of	O
token	O
occurrences	O
of	O
distinct	O
name	O
types	O
within	O
a	O
single	O
text	O
in	O
the	O
Project	B-DatasetName
Gutenberg	I-DatasetName
corpus	I-DatasetName
is	O
5.9	O
,	O
this	O
number	O
is	O
just	O
1.6	O
for	O
the	O
much	O
-	O
shorter	O
texts	O
of	O
the	O
Gigaword	B-DatasetName
corpus	I-DatasetName
.	I-DatasetName

For	O
the	O
latter	O
,	O
there	O
were	O
several	O
clusters	O
in	O
the	O
top	O
10	O
(	O
including	O
the	O
first	O
one	O
)	O
which	O
corresponded	O
to	O
LOCATION	O
,	O
while	O
the	O
first	O
(	O
fairly	O
)	O
clean	O
PERSON	O
cluster	O
was	O
the	O
15th	O
largest	O
;	O
in	O
general	O
,	O
individual	O
people	O
,	O
organizations	O
,	O
and	O
other	O
groupings	O
of	O
people	O
(	O
e.g.	O
by	O
country	O
of	O
origin	O
)	O
were	O
not	O
well	O
distinguished	O
by	O
Brown	B-MethodName
clustering	I-MethodName
in	O
the	O
Gigaword	B-DatasetName
corpus	I-DatasetName
,	O
at	O
least	O
not	O
with	O
the	O
same	O
low	O
number	O
of	O
clusters	O
that	O
worked	O
well	O
in	O
the	O
Project	B-DatasetName
Gutenberg	I-DatasetName
corpus	I-DatasetName
.	O

With	O
respect	O
to	O
the	O
former	O
,	O
the	O
obvious	O
issue	O
is	O
considerably	O
more	O
complex	O
proper	O
nouns	O
phrases	O
such	O
as	O
governmental	O
organizations	O
and	O
related	O
titles	O
.	O

We	O
have	O
found	O
more	O
subtle	O
genre	O
effects	O
as	O
well	O
:	O
for	O
comparison	O
,	O
we	O
applied	O
the	O
preliminary	O
steps	O
of	O
our	O
approach	O
to	O
another	O
corpus	O
of	O
published	O
texts	O
which	O
is	O
of	O
comparable	O
(	O
token	O
)	O
size	O
to	O
the	O
Project	B-DatasetName
Gutenberg	I-DatasetName
corpus	O
,	O
namely	O
the	O
Gigaword	B-DatasetName
newswire	I-DatasetName
corpus	O
(	O
Graff	O
and	O
Cieri	O
,	O
2003	O
)	O
,	O
and	O
noted	O
degraded	O
performance	O
for	O
both	O
segmentation	O
and	O
Brown	B-MethodName
clustering	I-MethodName
.	O

The	O
initial	O
rule	B-MethodName
-	I-MethodName
based	I-MethodName
segmentation	I-MethodName
,	O
for	O
instance	O
,	O
depends	O
on	O
reliable	O
capitalization	O
of	O
names	O
,	O
which	O
is	O
often	O
not	O
present	O
in	O
social	O
media	O
,	O
or	O
in	O
most	O
non	O
-	O
European	O
languages	O
.	O

Aspects	O
of	O
the	O
method	O
presented	O
here	O
could	O
theoretically	O
be	O
applied	O
to	O
NER	B-TaskName
in	O
other	O
genres	O
and	O
other	O
languages	O
,	O
but	O
one	O
important	O
point	O
we	O
wish	O
to	O
make	O
is	O
that	O
our	O
approach	O
clearly	O
takes	O
advantage	O
of	O
specific	O
properties	O
of	O
(	O
English	O
)	O
literature	O
.	O

Discussion	O
.	O

This	O
potential	O
0.002	B-MetricValue
improvement	O
is	O
tiny	O
compared	O
to	O
the	O
0.085	B-MetricValue
difference	O
in	O
accuracy	B-MetricName
between	O
the	O
two	O
systems	O
.	O

Only	O
2	O
of	O
the	O
15	O
were	O
segmented	O
correctly	O
by	O
Stanford	B-MethodName
CoreNLP	I-MethodName
.	O

For	O
the	O
segmentation	O
errors	O
,	O
we	O
compared	O
our	O
corrected	O
segmentations	O
with	O
the	O
segmentation	O
provided	O
by	O
the	O
CRF	O
-	O
based	O
Stanford	B-MethodName
CoreNLP	I-MethodName
system	O
,	O
our	O
best	O
competitor	O
.	O

With	O
regards	O
to	O
different	O
options	O
for	O
LitNER	B-MethodName
,	O
we	O
see	O
a	O
major	O
benefit	O
from	O
considering	O
all	O
occurrences	O
of	O
the	O
name	O
in	O
the	O
texts	O
rather	O
than	O
just	O
the	O
one	O
we	O
are	O
testing	O
on	O
(	O
Section	O
3.3	O
)	O
,	O
and	O
a	O
more	O
modest	O
benefit	O
from	O
using	O
the	O
information	O
on	O
parts	O
of	O
phrases	O
taken	O
from	O
the	O
Brown	B-MethodName
clustering	I-MethodName
(	O
Section	O
3.4	O
)	O
.	O

LitNER	B-MethodName
is	O
also	O
clearly	O
better	O
than	O
the	O
Brown	B-MethodName
clusters	I-MethodName
it	O
was	O
trained	O
on	O
,	O
particularly	O
for	O
F	B-MetricName
M	I-MetricName
(	O
+0.120	B-MetricValue
absolute	O
)	O
.	O

Stanford	B-MethodName
CoreNLP	I-MethodName
is	O
the	O
only	O
competitive	O
off	O
-	O
the	O
-	O
shelf	O
system	O
-	O
the	O
other	O
two	O
are	O
far	O
too	O
conservative	O
when	O
encountering	O
names	O
they	O
have	O
n't	O
seen	O
before	O
.	O

The	O
results	O
in	O
Table	O
2	O
show	O
that	O
our	O
system	O
easily	O
bests	O
the	O
off	O
-	O
the	O
-	O
shelf	O
systems	O
when	O
it	O
is	O
given	O
the	O
contextual	O
information	O
from	O
the	O
entire	O
text	O
;	O
the	O
difference	O
is	O
more	O
stark	O
for	O
accuracy	B-MetricName
(	O
+0.085	B-MetricValue
absolute	O
)	O
,	O
though	O
consistent	O
for	O
F	B-MetricName
M	I-MetricName
(	O
+0.041	B-MetricValue
absolute	O
)	O
.	O

Results	O
.	O

We	O
evaluate	O
using	O
two	O
standard	O
metrics	O
:	O
accuracy	B-MetricName
(	O
"	O
Acc	B-MetricName
"	O
)	O
,	O
and	O
macroaveraged	O
F	B-MetricName
-	I-MetricName
score	I-MetricName
(	O
"	O
F	B-MetricName
M	I-MetricName
"	O
)	O
.	O

For	O
our	O
system	O
(	O
LitNER	B-MethodName
)	O
,	O
we	O
test	O
a	O
version	O
where	O
only	O
the	O
immediate	O
sentence	O
context	O
is	O
used	O
(	O
"	O
sentence	O
"	O
)	O
,	O
and	O
versions	O
based	O
on	O
text	O
context	O
(	O
"	O
text	O
"	O
)	O
with	O
or	O
without	O
our	O
phrase	O
improvement	O
(	O
"	O
Â±phrase	O
"	O
)	O
.	O

Since	O
the	O
exact	O
segmentation	O
guidelines	O
likely	O
varied	O
across	O
these	O
systems	O
-	O
in	O
particular	O
,	O
we	O
found	O
that	O
Stanford	B-MethodName
CoreNLP	I-MethodName
often	O
left	O
off	O
the	O
title	O
in	O
names	O
such	O
as	O
Mr.	O
Smithand	O
we	O
did	O
n't	O
want	O
to	O
focus	O
on	O
these	O
issues	O
,	O
we	O
did	O
not	O
require	O
exact	O
matches	O
of	O
our	O
name	O
segmentation	O
;	O
instead	O
,	O
we	O
consider	O
the	O
entire	O
name	O
as	O
PERSON	O
or	O
LOCATION	O
if	O
any	O
of	O
the	O
tokens	O
were	O
tagged	O
as	O
such	O
(	O
names	O
with	O
both	O
tags	O
were	O
considered	O
OTHER	O
)	O
.	O

OpenNLP	B-MethodName
allowed	O
us	O
to	O
classify	O
only	O
PERSON	O
and	O
LOCATION	O
,	O
but	O
for	O
Stanford	B-MethodName
CoreNLP	I-MethodName
and	O
LingPipe	B-MethodName
we	O
used	O
the	O
existing	O
3	O
-	O
entity	O
systems	O
,	O
with	O
the	O
ORGANI	O
-	O
ZATION	O
tag	O
collapsed	O
into	O
OTHER	O
(	O
as	O
it	O
was	O
in	O
our	O
guidelines	O
;	O
instances	O
of	O
ORGANIZATION	O
are	O
rare	O
in	O
literature	O
)	O
.	O

We	O
compare	O
our	O
system	O
to	O
a	O
selection	O
of	O
publicly	O
available	O
,	O
off	O
-	O
the	O
-	O
shelf	O
NER	O
systems	O
:	O
OpenNLP	B-MethodName
,	O
4	O
LingPipe	B-MethodName
,	O
5	O
and	O
Stanford	B-MethodName
CoreNLP	I-MethodName
(	O
Finkel	O
et	O
al	O
.	O
,	O
2005	O
)	O
,	O
as	O
well	O
as	O
the	O
initial	O
Brown	B-MethodName
clustering	I-MethodName
.	O

We	O
ran	O
a	O
separate	O
two	O
-	O
annotator	O
agreement	O
study	O
over	O
200	O
examples	O
which	O
yielded	O
a	O
Cohen	B-MetricName
's	I-MetricName
Kappa	I-MetricName
of	O
0.84	B-MetricValue
,	O
suggesting	O
high	O
enough	O
reliability	O
that	O
a	O
single	O
annotator	O
was	O
sufficient	O
.	O

The	O
annotator	O
was	O
presented	O
with	O
the	O
sentence	O
and	O
the	O
pre	O
-	O
segmented	O
name	O
of	O
interest	O
,	O
and	O
asked	O
(	O
via	O
written	O
instructions	O
)	O
to	O
categorize	O
the	O
indicated	O
name	O
into	O
PERSON	O
,	O
LOCATION	O
,	O
OTHER	O
,	O
UNCERTAIN	O
due	O
to	O
ambiguity	O
,	O
or	O
segmentation	O
error	O
.	O

These	O
were	O
tagged	O
by	O
a	O
single	O
annotator	O
,	O
an	O
English	O
native	O
speaker	O
with	O
a	O
PhD	O
in	O
English	O
Literature	O
.	O

To	O
this	O
end	O
,	O
we	O
randomly	O
sampled	O
texts	O
,	O
sentences	O
,	O
and	O
then	O
names	O
within	O
those	O
sentences	O
from	O
our	O
name	O
-	O
segmented	O
Project	O
Gutenberg	O
corpus	O
to	O
produce	O
a	O
set	O
of	O
1000	O
examples	O
.	O

Though	O
there	O
are	O
a	O
few	O
novels	O
which	O
have	O
been	O
tagged	O
for	O
characters	O
(	O
Vala	O
et	O
al	O
.	O
,	O
2015	O
)	O
,	O
we	O
wanted	O
to	O
test	O
our	O
system	O
relative	O
to	O
a	O
much	O
wider	O
range	O
of	O
fiction	O
.	O

Our	O
interest	O
is	O
in	O
a	O
general	O
NER	O
system	O
for	O
literature	O
.	O

Evaluation	O
.	O

Once	O
we	O
have	O
calculated	O
p	O
(	O
t|r	O
)	O
for	O
each	O
class	O
,	O
we	O
choose	O
the	O
t	O
with	O
the	O
highest	O
p	O
(	O
t|r	O
)	O
.	O

Note	O
that	O
if	O
r	O
s	O
/	O
â	O
W	O
s	O
,	O
no	O
modification	O
is	O
made	O
,	O
and	O
for	O
the	O
OTHER	O
type	O
p	O
(	O
t|r	O
)	O
=	O
p(t|r	O
)	O
.	O

As	O
such	O
,	O
the	O
total	O
effect	O
on	O
the	O
score	O
can	O
be	O
negative	O
.	O

To	O
avoid	O
applying	O
too	O
much	O
weight	O
to	O
the	O
homogeneous	O
classes	O
,	O
the	O
second	O
term	O
in	O
the	O
summation	O
subtracts	O
the	O
average	O
number	O
of	O
occurrences	O
in	O
the	O
given	O
position	O
for	O
all	O
words	O
in	O
W	O
s	O
.	O

For	O
our	O
two	O
homogenous	O
entity	O
types	O
(	O
PERSON	O
and	O
LOCATION	O
)	O
,	O
we	O
calculate	O
a	O
new	O
score	O
p	O
:	O
p	O
(	O
t|r	O
)	O
=	O
p(t|r	O
)	O
+	O
sâS	O
c(r	O
s	O
,	O
t	O
,	O
s	O
)	O
t	O
âT	O
c(r	O
s	O
,	O
t	O
,	O
s	O
)	O
â	O
w	O
âWs	O
c(w	O
,	O
t	O
,	O
s	O
)	O
t	O
âT	O
c(w	O
,	O
t	O
,	O
s	O
)	O
|W	O
s	O
|	O
(	O
1	O
)	O
The	O
first	O
term	O
in	O
the	O
outermost	O
summation	O
in	O
Equation	O
1	O
is	O
the	O
proportion	O
of	O
occurrences	O
of	O
the	O
given	O
expression	O
in	O
position	O
s	O
which	O
correspond	O
to	O
type	O
t.	O

Let	O
c(w	O
,	O
t	O
,	O
s	O
)	O
be	O
the	O
the	O
number	O
of	O
times	O
a	O
word	O
w	O
â	O
W	O
s	O
appears	O
in	O
the	O
corpus	O
at	O
position	O
s	O
in	O
phrases	O
which	O
were	O
Brown	O
clustered	O
into	O
the	O
entity	O
type	O
t	O
â	O
T	O
,	O
and	O
p(t|r	O
)	O
be	O
the	O
original	O
probability	O
of	O
phrase	O
r	O
being	O
type	O
t	O
as	O
determined	O
by	O
the	O
logistic	O
regression	O
classifier	O
.	O

Our	O
final	O
model	O
addresses	O
this	O
failing	O
somewhat	O
by	O
using	O
more	O
information	O
from	O
our	O
Brown	O
clustering	O
:	O
from	O
each	O
of	O
the	O
initial	O
and	O
final	O
words	O
across	O
all	O
names	O
,	O
we	O
extract	O
a	O
set	O
of	O
words	O
W	O
s	O
that	O
appear	O
at	O
least	O
ten	O
times	O
in	O
position	O
s	O
â	O
S	O
,	O
S	O
=	O
{	O
initial	O
,	O
f	O
inal	O
}	O
across	O
all	O
phrases	O
.	O

In	O
the	O
case	O
of	O
names	O
which	O
are	O
phrases	O
,	O
this	O
is	O
troubling	O
because	O
there	O
are	O
many	O
generalizations	O
to	O
be	O
made	O
;	O
for	O
instance	O
names	O
ending	O
with	O
City	O
are	O
locations	O
.	O

Relative	O
to	O
(	O
true	O
)	O
supervised	O
models	O
,	O
our	O
bootstrapped	O
model	O
suffers	O
from	O
being	O
able	O
to	O
use	O
only	O
context	O
,	O
and	O
not	O
the	O
identity	O
of	O
the	O
name	O
itself	O
.	O

Improved	O
phrase	O
classification	O
.	O

3	O
The	O
only	O
non	O
-	O
standard	O
setting	O
that	O
we	O
use	O
is	O
the	O
"	O
balanced	O
"	O
option	O
,	O
which	O
weights	O
classes	O
by	O
the	O
inverse	O
of	O
their	O
count	O
in	O
the	O
training	O
set	O
,	O
countering	O
the	O
preference	O
for	O
the	O
majority	O
class	O
;	O
we	O
do	O
this	O
because	O
our	O
bootstrapped	O
distribution	O
is	O
an	O
unreliable	O
reflection	O
of	O
the	O
true	O
distribution	O
,	O
and	O
also	O
because	O
it	O
makes	O
it	O
a	O
fairer	O
comparison	O
to	O
off	O
-	O
theshelf	O
models	O
with	O
no	O
access	O
to	O
this	O
distribution	O
.	O

For	O
classification	O
,	O
we	O
use	O
logistic	O
regression	O
from	O
scikit	O
-	O
learn	O
(	O
Pedregosa	O
et	O
al	O
.	O
,	O
2011	O
)	O
trained	O
with	O
SGD	B-HyperparameterValue
using	O
L2	B-HyperparameterValue
regularization	I-HyperparameterValue
(	O
C	B-HyperparameterName
=	O
1	B-HyperparameterValue
)	O
.	O

Note	O
that	O
given	O
our	O
bootstrapping	O
setup	O
,	O
the	O
word	O
type	O
itself	O
can	O
not	O
be	O
used	O
directly	O
as	O
a	O
feature	O
.	O

To	O
be	O
included	O
as	O
features	O
,	O
the	O
n	O
-	O
grams	O
had	O
to	O
occur	O
with	O
â¥	O
10	O
different	O
w	O
0	O
target	O
word	O
types	O
.	O

Across	O
multiple	O
tokens	O
of	O
the	O
same	O
type	O
,	O
we	O
count	O
the	O
same	O
context	O
only	O
once	O
,	O
creating	O
a	O
binary	O
feature	O
vector	O
which	O
was	O
normalized	O
by	O
dividing	O
by	O
the	O
count	O
of	O
all	O
non	O
-	O
zero	O
entries	O
once	O
all	O
contexts	O
were	O
collected	O
.	O

For	O
this	O
we	O
used	O
the	O
name	B-DatasetName
-	I-DatasetName
segmented	I-DatasetName
corpus	I-DatasetName
,	O
and	O
when	O
one	O
of	O
the	O
words	O
in	O
the	O
context	O
was	O
also	O
a	O
name	O
,	O
we	O
take	O
the	O
category	O
from	O
the	O
Brown	O
clustering	O
as	O
the	O
word	O
(	O
so	O
w	O
2	O
for	O
London	O
in	O
from	O
London	O
to	O
New	O
York	O
is	O
LOCATION	O
,	O
not	O
New	O
)	O
.	O

Our	O
feature	O
set	O
consists	O
of	O
filtered	O
word	O
features	O
in	O
a	O
2	O
-	O
word	O
window	O
(	O
w	O
â2	O
w	O
â1	O
w	O
0	O
w	O
+1	O
w	O
+2	O
)	O
around	O
the	O
token	O
occurrences	O
w	O
0	O
of	O
a	O
target	O
type	O
in	O
a	O
given	O
text	O
,	O
made	O
up	O
of	O
position	O
-	O
indexed	O
unigrams	O
(	O
w	O
â2	O
,	O
w	O
â1	O
,	O
w	O
+1	O
and	O
w	O
+2	O
)	O
and	O
bigrams	O
(	O
w	O
â2	O
w	O
â1	O
,	O
w	O
+1	O
w	O
+2	O
and	O
w	O
â1	O
w	O
+1	O
)	O
,	O
excluding	O
unigrams	O
when	O
a	O
subsuming	O
bigram	O
feature	O
matched	O
(	O
e.g.	O
if	O
we	O
match	O
trust	O
in	O
,	O
we	O
do	O
not	O
add	O
trust	O
and	O
in	O
)	O
.	O

The	O
noisy	O
training	O
set	O
thus	O
constructed	O
has	O
about	O
1	O
million	O
examples	O
.	O

Mary	O
is	O
a	O
common	O
name	O
,	O
and	O
may	O
be	O
a	O
major	O
character	O
in	O
one	O
text	O
,	O
but	O
a	O
minor	O
one	O
in	O
another	O
;	O
hence	O
,	O
we	O
build	O
a	O
classifier	O
that	O
deals	O
with	O
both	O
context	O
-	O
rich	O
and	O
context	O
-	O
poor	O
situations	O
.	O

Our	O
rationale	O
here	O
is	O
that	O
the	O
challenging	O
part	O
of	O
NER	O
in	O
literature	O
is	O
names	O
that	O
appear	O
only	O
in	O
one	O
text	O
;	O
by	O
limiting	O
our	O
context	O
for	O
common	O
words	O
to	O
a	O
single	O
text	O
,	O
we	O
simulate	O
the	O
task	O
for	O
rarer	O
words	O
.	O

That	O
is	O
,	O
to	O
build	O
a	O
training	O
set	O
,	O
we	O
pass	O
through	O
the	O
corpus	O
and	O
each	O
time	O
we	O
come	O
across	O
a	O
common	O
name	O
in	O
a	O
particular	O
document	O
,	O
we	O
build	O
a	O
feature	O
vector	O
corresponding	O
to	O
all	O
the	O
contexts	O
in	O
that	O
document	O
,	O
with	O
the	O
label	O
taken	O
from	O
the	O
clustering	O
.	O

It	O
is	O
trained	O
on	O
the	O
(	O
text	O
-	O
level	O
)	O
"	O
instances	O
"	O
of	O
relatively	O
common	O
names	O
(	O
appearing	O
more	O
than	O
100	O
times	O
in	O
the	O
corpus	O
)	O
from	O
the	O
3	O
NE	O
label	O
types	O
derived	O
based	O
on	O
the	O
Brown	O
clustering	O
.	O

By	O
text	O
-	O
level	O
,	O
we	O
mean	O
that	O
it	O
assumes	O
one	O
-	O
sense	O
-	O
perdocument	O
,	O
classifying	O
a	O
name	O
for	O
an	O
entire	O
document	O
,	O
based	O
on	O
all	O
instances	O
of	O
the	O
name	O
in	O
the	O
document	O
(	O
Gale	O
et	O
al	O
.	O
,	O
1992	O
)	O
.	O

The	O
central	O
element	O
of	O
our	O
NER	O
system	O
is	O
a	O
textlevel	O
classifier	O
of	O
names	O
based	O
on	O
context	O
.	O

Text	O
-	O
level	O
context	O
classifier	O
.	O

To	O
avoid	O
confusion	O
,	O
authors	O
will	O
generally	O
preserve	O
one	O
-	O
sense	O
-	O
per	O
-	O
document	O
,	O
but	O
this	O
is	O
not	O
true	O
at	O
the	O
corpus	O
level	O
.	O

Another	O
problem	O
with	O
Brown	O
clustering	O
is	O
that	O
ignores	O
possible	O
sense	O
distinctions	O
:	O
for	O
instance	O
,	O
Florence	O
is	O
both	O
a	O
city	O
and	O
a	O
person	O
name	O
.	O

Fiction	O
,	O
though	O
,	O
has	O
many	O
rare	O
names	O
and	O
locations	O
,	O
since	O
authors	O
will	O
often	O
invent	O
them	O
.	O

In	O
any	O
case	O
,	O
Brown	O
clustering	O
works	O
fairly	O
well	O
for	O
common	O
names	O
,	O
but	O
for	O
rarer	O
ones	O
,	O
the	O
clustering	O
is	O
haphazard	O
.	O

The	O
other	O
clusters	O
are	O
messier	O
,	O
but	O
still	O
in-	O
(	O
Vala	O
et	O
al	O
.	O
,	O
2015	O
)	O
.	O

The	O
most	O
common	O
words	O
in	O
the	O
first	O
two	O
clusters	O
are	O
mostly	O
what	O
we	O
would	O
expect	O
,	O
though	O
there	O
is	O
a	O
bit	O
of	O
noise	O
,	O
e.g.	O
Him	O
included	O
as	O
a	O
place	O
.	O

2	O
Note	O
the	O
presence	O
of	O
the	O
multiword	O
name	O
New	O
York	O
in	O
the	O
second	O
cluster	O
,	O
as	O
a	O
result	O
of	O
the	O
segmentation	O
.	O

The	O
top-5	O
clusters	O
by	O
token	O
count	O
of	O
names	O
are	O
given	O
in	O
Table	O
1	O
.	O

Alternatively	O
,	O
we	O
could	O
have	O
set	O
c	B-HyperparameterName
higher	O
and	O
manually	O
grouped	O
the	O
clusters	O
based	O
on	O
the	O
common	O
words	O
in	O
the	O
clusters	O
,	O
adding	O
a	O
thin	O
layer	O
of	O
supervision	O
to	O
the	O
process	O
;	O
with	O
a	O
low	O
c	B-HyperparameterName
,	O
however	O
,	O
this	O
was	O
unnecessary	O
since	O
the	O
composition	O
and	O
ranking	O
of	O
the	O
clusters	O
conformed	O
exactly	O
to	O
our	O
expectations	O
.	O

To	O
automatically	O
extract	O
a	O
seed	O
list	O
of	O
people	O
and	O
locations	O
,	O
we	O
ranked	O
the	O
clusters	O
by	O
the	O
total	O
(	O
token	O
)	O
count	O
of	O
names	O
(	O
as	O
identified	O
by	O
GutenTag	O
)	O
,	O
and	O
took	O
the	O
first	O
cluster	O
to	O
be	O
PER	O
-	O
SON	O
,	O
and	O
the	O
second	O
to	O
be	O
LOCATION	O
;	O
all	O
other	O
clusters	O
are	O
considered	O
OTHER	O
,	O
our	O
third	O
,	O
catchall	O
category	O
.	O

We	O
did	O
not	O
tune	O
this	O
number	O
,	O
except	O
to	O
observe	O
that	O
larger	O
numbers	O
(	O
e.g.	O
100	B-HyperparameterValue
or	O
200	B-HyperparameterValue
)	O
resulted	O
in	O
increasingly	O
fragmented	O
clusters	O
for	O
our	O
entities	O
of	O
interest	O
.	O

The	O
rationale	O
for	O
such	O
a	O
small	O
cluster	B-HyperparameterName
size	I-HyperparameterName
-	O
the	O
default	O
is	O
1000	B-HyperparameterValue
,	O
and	O
NER	O
systems	O
which	O
use	O
Brown	O
clusters	O
as	O
features	O
do	O
better	O
with	O
even	O
more	O
(	O
Derczynski	O
et	O
al	O
.	O
,	O
2015)-is	O
that	O
we	O
want	O
to	O
have	O
clusters	O
that	O
correspond	O
to	O
major	O
noun	O
categories	O
(	O
e.g.	O
PERSON	O
and	O
LOCATION	O
)	O
,	O
which	O
we	O
consider	O
the	O
next	O
most	O
fundamental	O
division	O
beyond	O
part	O
-	O
of	O
-	O
speech	O
;	O
50	O
was	O
selected	O
because	O
it	O
is	O
roughly	O
comparable	O
to	O
the	O
size	O
of	O
the	O
Penn	B-DatasetName
Treebank	I-DatasetName
tagset	I-DatasetName
(	O
Marcus	O
et	O
al	O
.	O
,	O
1993	O
)	O
.	O

We	O
used	O
default	O
settings	O
except	O
for	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
clusters	I-HyperparameterName
(	O
c	B-HyperparameterName
):	O
50	B-HyperparameterValue
.	O

Note	O
that	O
using	O
information	O
from	O
Brown	O
clusters	O
is	O
a	O
well	O
established	O
technique	O
in	O
NER	B-TaskName
,	O
but	O
more	O
typically	O
as	O
features	O
within	O
a	O
supervised	O
framework	O
(	O
Miller	O
et	O
al	O
.	O
,	O
2004;Liang	O
,	O
2005;Ritter	O
et	O
al	O
.	O
,	O
2011	O
)	O
;	O
we	O
are	O
unaware	O
of	O
any	O
work	O
using	O
them	O
directly	O
as	O
a	O
source	O
of	O
bootstrapped	O
training	O
examples	O
.	O

Briefly	O
,	O
Brown	O
clusters	O
are	O
formed	O
using	O
an	O
agglomerative	O
hierarchical	O
cluster	O
of	O
terms	O
based	O
on	O
their	O
immediate	O
context	O
,	O
placing	O
terms	O
into	O
categories	O
to	O
maximize	O
the	O
probability	O
of	O
consecutive	O
terms	O
over	O
the	O
entire	O
corpus	O
.	O

The	O
next	O
step	O
is	O
to	O
induce	O
Brown	O
clusters	O
(	O
Brown	O
et	O
al	O
.	O
,	O
1992	O
)	O
over	O
the	O
pre	O
-	O
segmented	O
corpus	O
(	O
including	O
potential	O
names	O
)	O
,	O
using	O
the	O
tool	O
of	O
Liang	O
(	O
2005	O
)	O
.	O

Brown	O
clustering	O
.	O

Though	O
not	O
our	O
primary	O
concern	O
,	O
we	O
return	O
to	O
evaluate	O
the	O
quality	O
of	O
the	O
initial	O
segmentation	O
in	O
Section	O
5	O
.	O

For	O
this	O
work	O
,	O
however	O
,	O
we	O
remove	O
those	O
restrictions	O
to	O
maximize	O
recall	B-MetricName
.	O

To	O
improve	O
precision	O
,	O
the	O
name	O
tagger	O
in	O
the	O
version	O
of	O
GutenTag	O
used	O
for	O
this	O
paper	O
(	O
0.1.3	O
)	O
has	O
lower	O
bounds	O
on	O
token	O
count	O
(	O
at	O
least	O
10	O
)	O
and	O
an	O
upper	O
bound	O
on	O
the	O
length	O
of	O
names	O
(	O
no	O
longer	O
than	O
3	O
words	O
)	O
.	O

It	O
largely	O
(	O
but	O
not	O
entirely	O
)	O
overcomes	O
the	O
problem	O
of	O
sentenceinitial	O
capitalization	O
in	O
English	O
by	O
generalizing	O
over	O
an	O
entire	O
text	O
;	O
as	O
long	O
as	O
a	O
capitalized	O
word	O
or	O
phrase	O
appears	O
in	O
a	O
non	O
-	O
sentence	O
initial	O
position	O
at	O
least	O
once	O
in	O
a	O
text	O
,	O
it	O
will	O
be	O
tagged	O
in	O
the	O
sentence	O
-	O
initial	O
position	O
as	O
well	O
.	O

GutenTag	O
also	O
provides	O
an	O
initial	O
segmentation	O
of	O
tokens	O
into	O
potential	O
names	O
,	O
using	O
a	O
simple	O
rule	O
-	O
based	O
system	O
which	O
segments	O
contiguous	O
capitalized	O
words	O
,	O
potentially	O
with	O
common	O
intervening	O
function	O
words	O
like	O
of	O
as	O
well	O
as	O
leading	O
the	O
(	O
e.g.	O
the	O
King	O
of	O
Westeros	O
)	O
.	O

The	O
final	O
corpus	O
size	O
is	O
10844	O
texts	O
.	O

We	O
focus	O
here	O
only	O
on	O
fiction	O
texts	O
(	O
i.e.	O
novels	O
and	O
short	O
stories	O
)	O
;	O
other	O
kinds	O
of	O
literature	O
(	O
e.g.	O
plays	O
)	O
are	O
rare	O
in	O
the	O
corpus	O
and	O
have	O
very	O
different	O
properties	O
in	O
terms	O
of	O
the	O
distribution	O
of	O
names	O
.	O

We	O
access	O
the	O
texts	O
via	O
the	O
GutenTag	O
tool	O
(	O
Brooke	O
et	O
al	O
.	O
,	O
2015	O
)	O
,	O
which	O
allows	O
both	O
filtering	O
of	O
texts	O
by	O
genre	O
as	O
well	O
as	O
within	O
-	O
text	O
filtering	O
to	O
remove	O
Project	O
Gutenberg	O
copyright	O
information	O
,	O
front	O
and	O
back	O
matter	O
(	O
e.g.	O
table	O
of	O
contents	O
)	O
,	O
and	O
headers	O
.	O

The	O
corpus	O
we	O
use	O
for	O
building	O
and	O
testing	O
our	O
NER	O
system	O
is	O
the	O
2010	O
image	O
of	O
the	O
(	O
US	O
)	O
Project	O
Gutenberg	O
corpus	O
,	O
1	O
a	O
reasonably	O
comprehensive	O
collection	O
of	O
out	O
-	O
of	O
-	O
copyright	O
English	O
literary	O
texts	O
,	O
to	O
our	O
knowledge	O
the	O
largest	O
that	O
is	O
publicly	O
available	O
in	O
a	O
machine	O
-	O
readable	O
,	O
full	O
-	O
text	O
format	O
.	O

Corpus	O
preparation	O
and	O
segmentation	O
.	O

Method	O
.	O

(	O
2015	O
)	O
identify	O
some	O
of	O
the	O
failures	O
of	O
off	O
-	O
the	O
-	O
shelf	O
NER	O
with	O
regards	O
to	O
character	O
identification	O
,	O
and	O
attempt	O
to	O
fix	O
them	O
;	O
their	O
efforts	O
are	O
focused	O
,	O
however	O
,	O
on	O
characters	O
that	O
are	O
referred	O
to	O
by	O
description	O
rather	O
than	O
names	O
or	O
aliases	O
.	O

Vala	O
et	O
al	O
.	O

In	O
addition	O
to	O
NER	B-TaskName
,	O
character	B-TaskName
identifica	I-TaskName
-	I-TaskName
tion	I-TaskName
also	O
involves	O
clustering	O
multiple	O
aliases	O
of	O
the	O
same	O
character	O
,	O
and	O
discarding	O
person	O
names	O
that	O
do	O
n't	O
correspond	O
to	O
characters	O
.	O

We	O
also	O
note	O
that	O
although	O
supervised	B-TaskName
NER	I-TaskName
is	O
the	O
norm	O
,	O
there	O
is	O
a	O
smaller	O
body	O
of	O
work	O
in	O
semi	O
-	O
supervised	O
and	O
unsupervised	O
approaches	O
to	O
NER	O
and	O
semantic	O
lexicon	O
induction	O
,	O
for	O
instance	O
pattern	O
bootstrapping	O
(	O
Nadeau	O
et	O
al	O
.	O
,	O
2006;Thelen	O
and	O
Riloff	O
,	O
2002;McIntosh	O
et	O
al	O
.	O
,	O
2011	O
)	O
as	O
well	O
as	O
generative	O
approaches	O
(	O
Elsner	O
et	O
al	O
.	O
,	O
2009	O
)	O
.	O

In	O
the	O
context	O
of	O
literature	O
,	O
the	O
most	O
closely	O
related	O
task	O
is	O
character	O
identification	O
(	O
Vala	O
et	O
al	O
.	O
,	O
2015	O
)	O
,	O
which	O
is	O
itself	O
an	O
intermediate	O
task	O
for	O
character	O
speech	O
identification	O
(	O
He	O
et	O
al	O
.	O
,	O
2013	O
)	O
,	O
analysis	O
of	O
characterization	O
(	O
Bamman	O
et	O
al	O
.	O
,	O
2014	O
)	O
,	O
and	O
analysis	O
of	O
social	O
networks	O
(	O
Elson	O
et	O
al	O
.	O
,	O
2010;Agarwal	O
et	O
al	O
.	O
,	O
2013;Ardanuy	O
and	O
Sporleder	O
,	O
2015	O
)	O
.	O

Relevant	O
to	O
the	O
present	O
work	O
is	O
the	O
fact	O
that	O
,	O
despite	O
there	O
being	O
some	O
work	O
on	O
enforcing	O
tag	O
consistency	O
across	O
multiple	O
instances	O
of	O
the	O
same	O
token	O
(	O
Finkel	O
et	O
al	O
.	O
,	O
2005	O
)	O
and	O
the	O
use	O
of	O
non	O
-	O
local	O
features	O
(	O
Ratinov	O
and	O
Roth	O
,	O
2009	O
)	O
to	O
improve	O
supervised	O
sequential	O
models	O
,	O
the	O
consensus	O
seems	O
to	O
be	O
that	O
this	O
nonlocal	O
information	O
has	O
a	O
relatively	O
modest	O
effect	O
on	O
performance	O
in	O
standard	O
datasets	O
,	O
and	O
as	O
a	O
result	O
off	O
-	O
the	O
-	O
shelf	O
NER	O
systems	O
in	O
practice	O
treat	O
each	O
sentence	O
as	O
a	O
separate	O
document	O
,	O
with	O
multiple	O
instances	O
of	O
the	O
same	O
token	O
in	O
different	O
sentences	O
viewed	O
as	O
entirely	O
independent	O
classification	O
problems	O
.	O

The	O
standard	O
approach	O
to	O
NER	B-TaskName
is	O
to	O
treat	O
it	O
as	O
a	O
supervised	O
sequential	O
classification	O
problem	O
,	O
typically	O
using	O
conditional	O
random	O
fields	O
or	O
similar	O
models	O
,	O
based	O
on	O
local	O
context	O
features	O
as	O
well	O
as	O
properties	O
of	O
the	O
token	O
itself	O
.	O

Related	O
work	O
.	O

Notably	O
,	O
we	O
do	O
this	O
without	O
any	O
hand	O
-	O
labelled	O
data	O
whatsoever	O
,	O
bootstrapping	O
a	O
text	O
-	O
level	O
context	O
classifier	O
from	O
a	O
low	O
-	O
dimensional	O
Brown	O
clustering	O
of	O
the	O
Project	B-DatasetName
Gutenberg	I-DatasetName
corpus	O
.	O

In	O
this	O
paper	O
,	O
we	O
show	O
that	O
it	O
is	O
possible	O
to	O
take	O
advantage	O
of	O
the	O
properties	O
of	O
fiction	O
texts	O
,	O
in	O
particular	O
the	O
repetition	O
of	O
names	O
,	O
to	O
build	O
a	O
high	O
-	O
performing	O
3	O
-	O
class	O
NER	O
system	O
which	O
distinguishes	O
people	O
and	O
locations	O
from	O
other	O
capitalized	O
words	O
and	O
phrases	O
.	O

There	O
are	O
not	O
,	O
to	O
our	O
knowledge	O
,	O
any	O
NER	O
systems	O
that	O
are	O
specifically	O
targeted	O
at	O
literature	O
,	O
and	O
most	O
related	O
work	O
has	O
used	O
Stanford	O
CoreNLP	O
as	O
an	O
off	O
-	O
the	O
-	O
shelf	O
solution	O
(	O
Bamman	O
et	O
al	O
.	O
,	O
2014;Vala	O
et	O
al	O
.	O
,	O
2015	O
)	O
.	O

Named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
NER	B-TaskName
)	O
of	O
person	O
names	O
is	O
generally	O
the	O
first	O
step	O
in	O
identifying	O
characters	O
;	O
locations	O
are	O
also	O
a	O
prevalent	O
NE	O
type	O
,	O
and	O
can	O
be	O
useful	O
when	O
tracking	O
different	O
plot	O
threads	O
(	O
Wallace	O
,	O
2012	O
)	O
,	O
or	O
trends	O
in	O
the	O
settings	O
of	O
fiction	O
.	O

Much	O
of	O
the	O
work	O
on	O
applying	O
NLP	O
to	O
the	O
analysis	O
of	O
literature	O
has	O
focused	O
on	O
literary	O
figures	O
/	O
characters	O
in	O
the	O
text	O
,	O
e.g.	O
in	O
the	O
context	O
of	O
social	O
network	O
analysis	O
(	O
Elson	O
et	O
al	O
.	O
,	O
2010;Agarwal	O
et	O
al	O
.	O
,	O
2013;Ardanuy	O
and	O
Sporleder	O
,	O
2015	O
)	O
or	O
analysis	O
of	O
characterization	O
(	O
Bamman	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

Introduction	O
.	O

Our	O
experiments	O
show	O
it	O
to	O
substantially	O
outperform	O
off	O
-	O
the	O
-	O
shelf	O
supervised	O
NER	O
systems	O
.	O

Relative	O
to	O
more	O
traditional	O
approaches	O
,	O
LitNER	B-MethodName
has	O
two	O
important	O
properties	O
:	O
(	O
1	O
)	O
it	O
makes	O
no	O
use	O
of	O
handtagged	O
data	O
or	O
gazetteers	O
,	O
instead	O
it	O
bootstraps	O
a	O
model	O
from	O
term	O
clusters	O
;	O
and	O
(	O
2	O
)	O
it	O
leverages	O
multiple	O
instances	O
of	O
the	O
same	O
name	O
in	O
a	O
text	O
.	O

We	O
present	O
a	O
named	O
entity	O
recognition	O
(	O
NER	O
)	O
system	O
for	O
tagging	O
fiction	O
:	O
LitNER	B-MethodName
.	O

Bootstrapped	B-MethodName
Text	I-MethodName
-	I-MethodName
level	I-MethodName
Named	I-MethodName
Entity	I-MethodName
Recognition	I-MethodName
for	O
Literature	O
.	O

