-DOCSTART- -X- O
This -X- _ O
work -X- _ O
was -X- _ O
supported -X- _ O
by -X- _ O
EPSRC -X- _ O
grant -X- _ O
EP -X- _ O
/ -X- _ O
L027623/1 -X- _ O
. -X- _ O

We -X- _ O
propose -X- _ O
these -X- _ O
techniques -X- _ O
as -X- _ O
practical -X- _ O
approaches -X- _ O
to -X- _ O
including -X- _ O
target -X- _ O
syntax -X- _ O
in -X- _ O
NMT -X- _ B-TaskName
. -X- _ O

We -X- _ O
further -X- _ O
improve -X- _ O
on -X- _ O
the -X- _ O
individual -X- _ O
results -X- _ O
via -X- _ O
a -X- _ O
decoding -X- _ O
strategy -X- _ O
allowing -X- _ O
ensembling -X- _ B-MethodName
of -X- _ I-MethodName
models -X- _ I-MethodName
producing -X- _ O
different -X- _ O
output -X- _ O
representations -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
subword -X- _ O
units -X- _ O
and -X- _ O
syntax -X- _ O
. -X- _ O

We -X- _ O
train -X- _ O
these -X- _ O
models -X- _ O
using -X- _ O
a -X- _ O
delayed -X- _ B-MethodName
SGD -X- _ I-MethodName
update -X- _ I-MethodName
training -X- _ I-MethodName
procedure -X- _ I-MethodName
that -X- _ O
is -X- _ O
especially -X- _ O
effective -X- _ O
for -X- _ O
the -X- _ O
long -X- _ O
representations -X- _ O
that -X- _ O
arise -X- _ O
from -X- _ O
including -X- _ O
target -X- _ O
language -X- _ O
syntactic -X- _ O
information -X- _ O
in -X- _ O
the -X- _ O
output -X- _ O
. -X- _ O

We -X- _ O
report -X- _ O
strong -X- _ O
performance -X- _ O
with -X- _ O
individual -X- _ O
models -X- _ O
that -X- _ O
meets -X- _ O
or -X- _ O
improves -X- _ O
over -X- _ O
the -X- _ O
recent -X- _ O
best -X- _ O
WAT -X- _ B-MethodName
Ja -X- _ I-MethodName
- -X- _ I-MethodName
En -X- _ I-MethodName
ensemble -X- _ I-MethodName
results -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
we -X- _ O
found -X- _ O
that -X- _ O
this -X- _ O
gives -X- _ O
little -X- _ O
improvement -X- _ O
in -X- _ O
BLEU -X- _ B-MetricName
over -X- _ O
unconstrained -X- _ O
decoding -X- _ O
although -X- _ O
it -X- _ O
remains -X- _ O
an -X- _ O
interesting -X- _ O
line -X- _ O
of -X- _ O
research -X- _ O
. -X- _ O

We -X- _ O
find -X- _ O
that -X- _ O
the -X- _ O
syntax -X- _ B-MethodName
model -X- _ I-MethodName
is -X- _ O
often -X- _ O
more -X- _ O
grammatical -X- _ O
, -X- _ O
even -X- _ O
when -X- _ O
the -X- _ O
plain -X- _ B-MethodName
BPE -X- _ I-MethodName
model -X- _ O
may -X- _ O
share -X- _ O
more -X- _ O
vocabulary -X- _ O
with -X- _ O
the -X- _ O
reference -X- _ O
( -X- _ O
Table -X- _ O
2 -X- _ O
) -X- _ O
. -X- _ O

To -X- _ O
highlight -X- _ O
these -X- _ O
, -X- _ O
we -X- _ O
examine -X- _ O
hypotheses -X- _ O
generated -X- _ O
by -X- _ O
the -X- _ O
plain -X- _ B-MethodName
BPE -X- _ I-MethodName
and -X- _ O
linearized -X- _ B-MethodName
derivation -X- _ I-MethodName
models -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
an -X- _ O
ensemble -X- _ B-MethodName
of -X- _ I-MethodName
models -X- _ I-MethodName
producing -X- _ O
plain -X- _ B-MethodName
BPE -X- _ I-MethodName
and -X- _ O
linearized -X- _ B-MethodName
derivations -X- _ I-MethodName
improves -X- _ O
by -X- _ O
0.5 -X- _ B-MetricValue
BLEU -X- _ B-MetricName
over -X- _ O
the -X- _ O
plain -X- _ B-MethodName
BPE -X- _ I-MethodName
baseline -X- _ O
. -X- _ O

Ensembles -X- _ B-MethodName
of -X- _ I-MethodName
two -X- _ I-MethodName
identical -X- _ I-MethodName
models -X- _ I-MethodName
trained -X- _ O
with -X- _ O
different -X- _ O
seeds -X- _ O
only -X- _ O
slightly -X- _ O
improve -X- _ O
over -X- _ O
the -X- _ O
single -X- _ B-MethodName
model -X- _ I-MethodName
( -X- _ O
Table -X- _ O
5 -X- _ O
) -X- _ O
. -X- _ O

It -X- _ O
has -X- _ O
been -X- _ O
suggested -X- _ O
that -X- _ O
decaying -X- _ O
the -X- _ O
learning -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
can -X- _ O
have -X- _ O
a -X- _ O
similar -X- _ O
effect -X- _ O
to -X- _ O
large -X- _ O
batch -X- _ O
training -X- _ O
( -X- _ O
Smith -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
, -X- _ O
but -X- _ O
reducing -X- _ O
the -X- _ O
initial -X- _ O
learning -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
by -X- _ O
a -X- _ O
factor -X- _ B-HyperparameterValue
of -X- _ I-HyperparameterValue
8 -X- _ I-HyperparameterValue
alone -X- _ O
did -X- _ O
not -X- _ O
give -X- _ O
the -X- _ O
same -X- _ O
improvements -X- _ O
. -X- _ O

Accumulating -X- _ O
the -X- _ O
gradient -X- _ O
over -X- _ O
8 -X- _ B-HyperparameterValue
batches -X- _ B-HyperparameterName
of -X- _ O
size -X- _ O
4096 -X- _ B-HyperparameterValue
gives -X- _ O
a -X- _ O
3 -X- _ B-MetricValue
BLEU -X- _ B-MetricName
improvement -X- _ O
for -X- _ O
the -X- _ O
linear -X- _ B-MethodName
derivation -X- _ I-MethodName
model -X- _ O
. -X- _ O

English -X- _ O
constituency -X- _ O
trees -X- _ O
are -X- _ O
obtained -X- _ O
using -X- _ O
CKYlark -X- _ B-MethodName
( -X- _ O
Oda -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
, -X- _ O
with -X- _ O
words -X- _ O
replaced -X- _ O
by -X- _ O
BPE -X- _ O
subwords -X- _ O
. -X- _ O

We -X- _ O
report -X- _ O
all -X- _ O
experiments -X- _ O
for -X- _ O
Japanese -X- _ O
- -X- _ O
English -X- _ O
, -X- _ O
using -X- _ O
the -X- _ O
first -X- _ B-HyperparameterValue
1 -X- _ I-HyperparameterValue
M -X- _ I-HyperparameterValue
training -X- _ B-HyperparameterName
sentences -X- _ I-HyperparameterName
of -X- _ O
the -X- _ O
Japanese -X- _ O
- -X- _ O
English -X- _ O
ASPEC -X- _ B-DatasetName
data -X- _ O
( -X- _ O
Nakazawa -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
. -X- _ O

For -X- _ O
these -X- _ O
models -X- _ O
we -X- _ O
use -X- _ O
embedding -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
400 -X- _ B-HyperparameterValue
, -X- _ O
a -X- _ O
single -X- _ B-HyperparameterValue
BiLSTM -X- _ B-HyperparameterName
layer -X- _ I-HyperparameterName
of -X- _ O
size -X- _ O
750 -X- _ B-HyperparameterValue
, -X- _ O
and -X- _ O
batch -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
80 -X- _ B-HyperparameterValue
. -X- _ O

For -X- _ O
comparison -X- _ O
with -X- _ O
earlier -X- _ O
target -X- _ O
syntax -X- _ O
work -X- _ O
, -X- _ O
we -X- _ O
also -X- _ O
train -X- _ O
two -X- _ O
RNN -X- _ B-MethodName
attention -X- _ I-MethodName
- -X- _ I-MethodName
based -X- _ I-MethodName
seq2seq -X- _ I-MethodName
models -X- _ O
( -X- _ O
Bahdanau -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
with -X- _ O
normal -X- _ O
SGD -X- _ O
to -X- _ O
produce -X- _ O
plain -X- _ O
BPE -X- _ O
sequences -X- _ O
and -X- _ O
linearized -X- _ O
derivations -X- _ O
. -X- _ O

In -X- _ O
all -X- _ O
cases -X- _ O
we -X- _ O
decode -X- _ O
using -X- _ O
SGNMT -X- _ B-MethodName
( -X- _ O
Stahlberg -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
with -X- _ O
beam -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
4 -X- _ B-HyperparameterValue
, -X- _ O
using -X- _ O
the -X- _ O
average -X- _ O
of -X- _ O
the -X- _ O
final -X- _ O
20 -X- _ B-HyperparameterValue
checkpoints -X- _ B-HyperparameterName
. -X- _ O

All -X- _ O
Transformer -X- _ O
architectures -X- _ O
are -X- _ O
Ten -X- _ B-MethodName
- -X- _ I-MethodName
sor2Tensor -X- _ I-MethodName
's -X- _ O
base -X- _ O
Transformer -X- _ O
model -X- _ O
( -X- _ O
Vaswani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
with -X- _ O
a -X- _ O
batch -X- _ O
size -X- _ O
of -X- _ O
4096 -X- _ O
. -X- _ O

Each -X- _ O
multirepresentation -X- _ B-MethodName
ensemble -X- _ I-MethodName
consists -X- _ O
of -X- _ O
the -X- _ O
plain -X- _ O
BPE -X- _ O
model -X- _ O
and -X- _ O
one -X- _ O
other -X- _ O
individual -X- _ O
model -X- _ O
. -X- _ O

We -X- _ O
decode -X- _ O
with -X- _ O
individual -X- _ O
models -X- _ O
and -X- _ O
two -X- _ B-MethodName
- -X- _ I-MethodName
model -X- _ I-MethodName
ensembles -X- _ I-MethodName
, -X- _ O
comparing -X- _ O
results -X- _ O
for -X- _ O
single -X- _ O
- -X- _ O
representation -X- _ O
and -X- _ O
multi -X- _ B-MethodName
- -X- _ I-MethodName
representation -X- _ I-MethodName
ensembles -X- _ I-MethodName
. -X- _ O

To -X- _ O
compare -X- _ O
target -X- _ O
representations -X- _ O
we -X- _ O
train -X- _ O
Transformer -X- _ B-MethodName
models -X- _ I-MethodName
with -X- _ O
target -X- _ O
representations -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
, -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
, -X- _ O
( -X- _ O
4 -X- _ O
) -X- _ O
and -X- _ O
( -X- _ O
5 -X- _ O
) -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
1 -X- _ O
, -X- _ O
using -X- _ O
delayed -X- _ B-MethodName
SGD -X- _ I-MethodName
updates -X- _ I-MethodName
every -X- _ O
8 -X- _ B-HyperparameterValue
batches -X- _ B-HyperparameterName
. -X- _ O

We -X- _ O
first -X- _ O
explore -X- _ O
the -X- _ O
effect -X- _ O
of -X- _ O
our -X- _ O
delayed -X- _ B-MethodName
SGD -X- _ I-MethodName
update -X- _ I-MethodName
training -X- _ O
scheme -X- _ O
on -X- _ O
single -X- _ O
models -X- _ O
, -X- _ O
contrasting -X- _ O
updates -X- _ O
every -X- _ O
batch -X- _ O
with -X- _ O
accumulated -X- _ O
updates -X- _ O
every -X- _ O
8 -X- _ B-HyperparameterValue
batches -X- _ B-HyperparameterName
. -X- _ O

This -X- _ O
lets -X- _ O
us -X- _ O
effectively -X- _ O
use -X- _ O
very -X- _ O
large -X- _ O
batch -X- _ B-HyperparameterName
sizes -X- _ I-HyperparameterName
without -X- _ O
requiring -X- _ O
multiple -X- _ O
GPUs -X- _ O
. -X- _ O
Ensembling -X- _ O
Representations -X- _ O
. -X- _ O

We -X- _ O
accumulate -X- _ O
gradients -X- _ O
over -X- _ O
a -X- _ O
fixed -X- _ O
number -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
batches -X- _ I-HyperparameterName
before -X- _ O
using -X- _ O
the -X- _ O
accumulated -X- _ O
gradients -X- _ O
to -X- _ O
update -X- _ O
the -X- _ O
model -X- _ O
1 -X- _ O
. -X- _ O

Our -X- _ O
strategy -X- _ O
avoids -X- _ O
this -X- _ O
problem -X- _ O
by -X- _ O
using -X- _ O
delayed -X- _ B-MethodName
SGD -X- _ I-MethodName
updates -X- _ I-MethodName
. -X- _ O

During -X- _ O
NMT -X- _ B-TaskName
training -X- _ O
, -X- _ O
by -X- _ O
default -X- _ O
, -X- _ O
the -X- _ O
gradients -X- _ O
used -X- _ O
to -X- _ O
update -X- _ O
model -X- _ O
parameters -X- _ O
are -X- _ O
calculated -X- _ O
over -X- _ O
individual -X- _ O
batches -X- _ O
. -X- _ O

The -X- _ O
Ten -X- _ B-MethodName
- -X- _ I-MethodName
sor2Tensor -X- _ I-MethodName
framework -X- _ O
( -X- _ O
Vaswani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
defines -X- _ O
batch -X- _ O
size -X- _ O
as -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
tokens -X- _ O
per -X- _ O
batch -X- _ O
, -X- _ O
so -X- _ O
batches -X- _ O
will -X- _ O
contain -X- _ O
fewer -X- _ O
sequences -X- _ O
if -X- _ O
their -X- _ O
average -X- _ O
length -X- _ O
increases -X- _ O
. -X- _ O

Delayed -X- _ B-MethodName
SGD -X- _ I-MethodName
Update -X- _ I-MethodName
Training -X- _ O
for -X- _ O
Long -X- _ O
Sequences -X- _ O
. -X- _ O

Garmash -X- _ O
and -X- _ O
Monz -X- _ O
( -X- _ O
2016 -X- _ O
) -X- _ O
show -X- _ O
translation -X- _ O
improvements -X- _ O
with -X- _ O
multi -X- _ B-MethodName
- -X- _ I-MethodName
source -X- _ I-MethodName
- -X- _ I-MethodName
language -X- _ I-MethodName
NMT -X- _ I-MethodName
ensembles -X- _ I-MethodName
. -X- _ O

Hokamp -X- _ O
( -X- _ O
2017 -X- _ O
) -X- _ O
shows -X- _ O
improvements -X- _ O
in -X- _ O
the -X- _ O
quality -X- _ B-TaskName
estimation -X- _ I-TaskName
task -X- _ O
using -X- _ O
ensembles -X- _ O
of -X- _ O
NMT -X- _ B-TaskName
models -X- _ O
with -X- _ O
multiple -X- _ O
input -X- _ O
representations -X- _ O
which -X- _ O
share -X- _ O
an -X- _ O
output -X- _ O
representation -X- _ O
. -X- _ O

( -X- _ O
2017 -X- _ O
) -X- _ O
combine -X- _ O
recurrent -X- _ B-MethodName
neural -X- _ I-MethodName
network -X- _ I-MethodName
grammar -X- _ I-MethodName
( -X- _ I-MethodName
RNNG -X- _ I-MethodName
) -X- _ I-MethodName
models -X- _ O
( -X- _ O
Dyer -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
with -X- _ O
attention -X- _ B-MethodName
- -X- _ I-MethodName
based -X- _ I-MethodName
models -X- _ O
to -X- _ O
produce -X- _ O
well -X- _ O
- -X- _ O
formed -X- _ O
dependency -X- _ O
trees -X- _ O
. -X- _ O

( -X- _ O
2017 -X- _ O
) -X- _ O
perform -X- _ O
NMT -X- _ B-TaskName
with -X- _ O
syntax -X- _ O
annotation -X- _ O
in -X- _ O
the -X- _ O
form -X- _ O
of -X- _ O
Combinatory -X- _ O
Categorial -X- _ O
Grammar -X- _ O
( -X- _ O
CCG -X- _ O
) -X- _ O
supertags -X- _ O
. -X- _ O

Long -X- _ O
sequences -X- _ O
make -X- _ O
training -X- _ O
more -X- _ O
difficult -X- _ O
( -X- _ O
Bahdanau -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
, -X- _ O
which -X- _ O
we -X- _ O
address -X- _ O
with -X- _ O
an -X- _ O
adjusted -X- _ O
training -X- _ O
procedure -X- _ O
for -X- _ O
the -X- _ O
Transformer -X- _ O
architecture -X- _ O
( -X- _ O
Vaswani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
, -X- _ O
using -X- _ O
delayed -X- _ B-MethodName
SGD -X- _ I-MethodName
updates -X- _ I-MethodName
which -X- _ O
accumulate -X- _ O
gradients -X- _ O
over -X- _ O
multiple -X- _ O
batches -X- _ O
. -X- _ O

As -X- _ O
part -X- _ O
of -X- _ O
our -X- _ O
investigation -X- _ O
we -X- _ O
suggest -X- _ O
strategies -X- _ O
for -X- _ O
practical -X- _ O
NMT -X- _ B-TaskName
with -X- _ O
very -X- _ O
long -X- _ O
target -X- _ O
sequences -X- _ O
. -X- _ O

We -X- _ O
hypothesize -X- _ O
that -X- _ O
an -X- _ O
NMT -X- _ B-MethodName
ensemble -X- _ I-MethodName
would -X- _ O
be -X- _ O
strengthened -X- _ O
if -X- _ O
its -X- _ O
component -X- _ O
models -X- _ O
were -X- _ O
complementary -X- _ O
in -X- _ O
this -X- _ O
way -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
ensembling -X- _ B-MethodName
often -X- _ O
requires -X- _ O
component -X- _ O
models -X- _ O
to -X- _ O
make -X- _ O
predictions -X- _ O
relating -X- _ O
to -X- _ O
the -X- _ O
same -X- _ O
output -X- _ O
sequence -X- _ O
position -X- _ O
at -X- _ O
each -X- _ O
time -X- _ O
step -X- _ O
. -X- _ O

Previous -X- _ O
work -X- _ O
has -X- _ O
observed -X- _ O
that -X- _ O
NMT -X- _ B-TaskName
models -X- _ O
trained -X- _ O
to -X- _ O
generate -X- _ O
target -X- _ O
syntax -X- _ O
can -X- _ O
exhibit -X- _ O
improved -X- _ O
sentence -X- _ O
structure -X- _ O
( -X- _ O
Aharoni -X- _ O
and -X- _ O
Goldberg -X- _ O
, -X- _ O
2017;Eriguchi -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
relative -X- _ O
to -X- _ O
those -X- _ O
trained -X- _ O
on -X- _ O
plain -X- _ O
- -X- _ O
text -X- _ O
, -X- _ O
while -X- _ O
plain -X- _ O
- -X- _ O
text -X- _ O
models -X- _ O
produce -X- _ O
shorter -X- _ O
sequences -X- _ O
and -X- _ O
so -X- _ O
may -X- _ O
encode -X- _ O
lexical -X- _ O
information -X- _ O
more -X- _ O
easily -X- _ O
( -X- _ O
Nadejde -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O

Ensembles -X- _ O
of -X- _ O
multiple -X- _ O
NMT -X- _ B-TaskName
models -X- _ O
consistently -X- _ O
and -X- _ O
significantly -X- _ O
improve -X- _ O
over -X- _ O
single -X- _ O
models -X- _ O
( -X- _ O
Garmash -X- _ O
and -X- _ O
Monz -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
formulate -X- _ O
beam -X- _ O
search -X- _ O
over -X- _ O
such -X- _ O
ensembles -X- _ O
using -X- _ O
WFSTs -X- _ B-MethodName
, -X- _ O
and -X- _ O
describe -X- _ O
a -X- _ O
delayed -X- _ B-MethodName
SGD -X- _ I-MethodName
update -X- _ I-MethodName
training -X- _ I-MethodName
procedure -X- _ I-MethodName
that -X- _ O
is -X- _ O
especially -X- _ O
effective -X- _ O
for -X- _ O
long -X- _ O
representations -X- _ O
like -X- _ O
linearized -X- _ O
syntax -X- _ O
. -X- _ O

We -X- _ O
explore -X- _ O
strategies -X- _ O
for -X- _ O
incorporating -X- _ O
target -X- _ O
syntax -X- _ O
into -X- _ O
Neural -X- _ B-TaskName
Machine -X- _ I-TaskName
Translation -X- _ I-TaskName
. -X- _ O

Multi -X- _ O
- -X- _ O
representation -X- _ O
Ensembles -X- _ O
and -X- _ O
Delayed -X- _ O
SGD -X- _ O
Updates -X- _ O
Improve -X- _ O
Syntax -X- _ B-TaskName
- -X- _ I-TaskName
based -X- _ I-TaskName
NMT -X- _ I-TaskName
. -X- _ O

Acknowledgments -X- _ O
. -X- _ O

Conclusions -X- _ O
. -X- _ O

It -X- _ O
is -X- _ O
also -X- _ O
possible -X- _ O
to -X- _ O
constrain -X- _ O
decoding -X- _ O
of -X- _ O
linearized -X- _ O
trees -X- _ O
and -X- _ O
derivations -X- _ O
to -X- _ O
wellformed -X- _ O
outputs -X- _ O
. -X- _ O

Our -X- _ O
solution -X- _ O
was -X- _ O
to -X- _ O
penalise -X- _ O
scores -X- _ O
of -X- _ O
non -X- _ O
- -X- _ O
terminals -X- _ O
under -X- _ O
the -X- _ O
syntax -X- _ O
model -X- _ O
by -X- _ O
a -X- _ O
constant -X- _ O
factor -X- _ O
. -X- _ O

In -X- _ O
ensembling -X- _ O
plain -X- _ O
- -X- _ O
text -X- _ O
with -X- _ O
a -X- _ O
syntax -X- _ O
external -X- _ O
representation -X- _ O
we -X- _ O
observed -X- _ O
that -X- _ O
in -X- _ O
a -X- _ O
small -X- _ O
proportion -X- _ O
of -X- _ O
cases -X- _ O
non -X- _ O
- -X- _ O
terminals -X- _ O
were -X- _ O
over -X- _ O
- -X- _ O
generated -X- _ O
, -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
mismatch -X- _ O
in -X- _ O
target -X- _ O
sequence -X- _ O
lengths -X- _ O
. -X- _ O

By -X- _ O
ensembling -X- _ O
syntax -X- _ O
and -X- _ O
plain -X- _ O
- -X- _ O
text -X- _ O
we -X- _ O
hope -X- _ O
to -X- _ O
benefit -X- _ O
from -X- _ O
their -X- _ O
complementary -X- _ O
strengths -X- _ O
. -X- _ O

Our -X- _ O
syntax -X- _ O
models -X- _ O
achieve -X- _ O
similar -X- _ O
results -X- _ O
despite -X- _ O
producing -X- _ O
much -X- _ O
longer -X- _ O
sequences -X- _ O
. -X- _ O

Our -X- _ O
plain -X- _ O
BPE -X- _ O
baseline -X- _ O
( -X- _ O
Table -X- _ O
4 -X- _ O
) -X- _ O
outperforms -X- _ O
the -X- _ O
current -X- _ O
best -X- _ O
system -X- _ O
on -X- _ O
WAT -X- _ O
Ja -X- _ O
- -X- _ O
En -X- _ O
, -X- _ O
an -X- _ O
8 -X- _ O
- -X- _ O
model -X- _ O
ensemble -X- _ O
( -X- _ O
Morishita -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O

Our -X- _ O
first -X- _ O
results -X- _ O
in -X- _ O
Table -X- _ O
3 -X- _ O
show -X- _ O
that -X- _ O
large -X- _ O
batch -X- _ O
training -X- _ O
can -X- _ O
significantly -X- _ O
improve -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
single -X- _ O
Transformers -X- _ O
, -X- _ O
particularly -X- _ O
when -X- _ O
trained -X- _ O
to -X- _ O
produce -X- _ O
longer -X- _ O
sequences -X- _ O
. -X- _ O

Results -X- _ O
and -X- _ O
Discussion -X- _ O
. -X- _ O

The -X- _ O
linearized -X- _ O
derivation -X- _ O
uses -X- _ O
additional -X- _ O
tokens -X- _ O
for -X- _ O
non -X- _ O
- -X- _ O
terminals -X- _ O
with -X- _ O
< -X- _ O
/R -X- _ O
> -X- _ O
. -X- _ O

Non -X- _ O
- -X- _ O
terminals -X- _ O
are -X- _ O
included -X- _ O
as -X- _ O
separate -X- _ O
tokens -X- _ O
. -X- _ O

We -X- _ O
train -X- _ O
separate -X- _ O
Japanese -X- _ O
( -X- _ O
lowercased -X- _ O
) -X- _ O
and -X- _ O
English -X- _ O
( -X- _ O
cased -X- _ O
) -X- _ O
BPE -X- _ O
vocabularies -X- _ O
on -X- _ O
the -X- _ O
plain -X- _ O
- -X- _ O
text -X- _ O
, -X- _ O
with -X- _ O
30 -X- _ O
K -X- _ O
merges -X- _ O
each -X- _ O
. -X- _ O

All -X- _ O
models -X- _ O
use -X- _ O
plain -X- _ O
BPE -X- _ O
Japanese -X- _ O
source -X- _ O
sentences -X- _ O
. -X- _ O

Experiments -X- _ O
. -X- _ O

Symbols -X- _ O
in -X- _ O
the -X- _ O
internal -X- _ O
representation -X- _ O
are -X- _ O
consumed -X- _ O
as -X- _ O
needed -X- _ O
to -X- _ O
stay -X- _ O
synchronized -X- _ O
with -X- _ O
the -X- _ O
external -X- _ O
representation -X- _ O
, -X- _ O
as -X- _ O
illustrated -X- _ O
in -X- _ O
Figure -X- _ O
1 -X- _ O
; -X- _ O
epsilons -X- _ O
are -X- _ O
consumed -X- _ O
with -X- _ O
a -X- _ O
probability -X- _ O
of -X- _ O
1 -X- _ O
. -X- _ O

As -X- _ O
search -X- _ O
proceeds -X- _ O
, -X- _ O
each -X- _ O
model -X- _ O
score -X- _ O
is -X- _ O
updated -X- _ O
separately -X- _ O
with -X- _ O
its -X- _ O
appropriate -X- _ O
representation -X- _ O
. -X- _ O

This -X- _ O
leads -X- _ O
to -X- _ O
an -X- _ O
outer -X- _ O
beam -X- _ O
search -X- _ O
over -X- _ O
external -X- _ O
representations -X- _ O
with -X- _ O
inner -X- _ O
beam -X- _ O
searches -X- _ O
for -X- _ O
the -X- _ O
best -X- _ O
matching -X- _ O
internal -X- _ O
representations -X- _ O
. -X- _ O

The -X- _ O
ensembled -X- _ O
score -X- _ O
of -X- _ O
h -X- _ O
is -X- _ O
then -X- _ O
: -X- _ O
P -X- _ O
( -X- _ O
h -X- _ O
j -X- _ O
|h -X- _ O
< -X- _ O
j -X- _ O
) -X- _ O
= -X- _ O
P -X- _ O
o -X- _ O
( -X- _ O
h -X- _ O
j -X- _ O
|h -X- _ O
< -X- _ O
j -X- _ O
) -X- _ O
× -X- _ O
( -X- _ O
3 -X- _ O
) -X- _ O
max -X- _ O
( -X- _ O
x -X- _ O
, -X- _ O
y)∈M -X- _ O
( -X- _ O
h -X- _ O
) -X- _ O
P -X- _ O
i -X- _ O
( -X- _ O
i(y)|i(x -X- _ O
) -X- _ O
) -X- _ O
The -X- _ O
max -X- _ O
performed -X- _ O
for -X- _ O
each -X- _ O
partial -X- _ O
hypothesis -X- _ O
h -X- _ O
is -X- _ O
itself -X- _ O
approximated -X- _ O
by -X- _ O
a -X- _ O
beam -X- _ O
search -X- _ O
. -X- _ O

The -X- _ O
set -X- _ O
of -X- _ O
partial -X- _ O
paths -X- _ O
yielding -X- _ O
h -X- _ O
are -X- _ O
: -X- _ O
M -X- _ O
( -X- _ O
h -X- _ O
) -X- _ O
= -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
{ -X- _ O
( -X- _ O
x -X- _ O
, -X- _ O
y)|xyz -X- _ O
∈ -X- _ O
P -X- _ O
, -X- _ O
o(x -X- _ O
) -X- _ O
= -X- _ O
h -X- _ O
< -X- _ O
j -X- _ O
, -X- _ O
o(xy -X- _ O
) -X- _ O
= -X- _ O
h -X- _ O
} -X- _ O
2 -X- _ O
See -X- _ O
the -X- _ O
tokenization -X- _ O
wrappers -X- _ O
in -X- _ O
https:// -X- _ O
github.com/ucam-smt/sgnmt -X- _ O
Here -X- _ O
z -X- _ O
is -X- _ O
the -X- _ O
path -X- _ O
suffix -X- _ O
. -X- _ O

h -X- _ O
j -X- _ O
be -X- _ O
a -X- _ O
partial -X- _ O
hypothesis -X- _ O
in -X- _ O
the -X- _ O
output -X- _ O
representation -X- _ O
. -X- _ O

Let -X- _ O
h -X- _ O
= -X- _ O
h -X- _ O
1 -X- _ O
. -X- _ O

In -X- _ O
practice -X- _ O
, -X- _ O
beam -X- _ O
decoding -X- _ O
is -X- _ O
performed -X- _ O
in -X- _ O
the -X- _ O
external -X- _ O
representation -X- _ O
, -X- _ O
i.e. -X- _ O
over -X- _ O
projections -X- _ O
of -X- _ O
paths -X- _ O
in -X- _ O
P -X- _ O
2 -X- _ O
. -X- _ O

A -X- _ O
path -X- _ O
( -X- _ O
Sennrich -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
. -X- _ O

Let -X- _ O
P -X- _ O
be -X- _ O
the -X- _ O
paths -X- _ O
in -X- _ O
T -X- _ O
leading -X- _ O
from -X- _ O
the -X- _ O
start -X- _ O
state -X- _ O
to -X- _ O
any -X- _ O
final -X- _ O
state -X- _ O
. -X- _ O

Mapping -X- _ O
from -X- _ O
word -X- _ O
to -X- _ O
BPE -X- _ O
representations -X- _ O
is -X- _ O
straightforward -X- _ O
, -X- _ O
and -X- _ O
mapping -X- _ O
from -X- _ O
( -X- _ O
linearized -X- _ O
) -X- _ O
syntax -X- _ O
to -X- _ O
plain -X- _ O
- -X- _ O
text -X- _ O
simply -X- _ O
deletes -X- _ O
non -X- _ O
- -X- _ O
terminals -X- _ O
. -X- _ O

The -X- _ O
complexity -X- _ O
of -X- _ O
the -X- _ O
transduction -X- _ O
depends -X- _ O
on -X- _ O
the -X- _ O
representations -X- _ O
. -X- _ O

To -X- _ O
formulate -X- _ O
an -X- _ O
ensembling -X- _ O
decoder -X- _ O
over -X- _ O
pairs -X- _ O
of -X- _ O
these -X- _ O
representations -X- _ O
, -X- _ O
we -X- _ O
assume -X- _ O
we -X- _ O
have -X- _ O
a -X- _ O
transducer -X- _ O
T -X- _ O
that -X- _ O
maps -X- _ O
from -X- _ O
one -X- _ O
representation -X- _ O
to -X- _ O
the -X- _ O
other -X- _ O
representation -X- _ O
. -X- _ O

Table -X- _ O
1 -X- _ O
shows -X- _ O
several -X- _ O
different -X- _ O
representations -X- _ O
of -X- _ O
the -X- _ O
same -X- _ O
hypothesis -X- _ O
. -X- _ O

Training -X- _ O
on -X- _ O
multiple -X- _ O
GPUs -X- _ O
is -X- _ O
one -X- _ O
way -X- _ O
to -X- _ O
increase -X- _ O
the -X- _ O
amount -X- _ O
of -X- _ O
data -X- _ O
used -X- _ O
to -X- _ O
estimate -X- _ O
gradients -X- _ O
, -X- _ O
but -X- _ O
it -X- _ O
requires -X- _ O
significant -X- _ O
resources -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
with -X- _ O
such -X- _ O
large -X- _ O
batches -X- _ O
the -X- _ O
model -X- _ O
size -X- _ O
may -X- _ O
exceed -X- _ O
available -X- _ O
GPU -X- _ O
memory -X- _ O
. -X- _ O

Previous -X- _ O
research -X- _ O
has -X- _ O
used -X- _ O
very -X- _ O
large -X- _ O
batches -X- _ O
to -X- _ O
improve -X- _ O
training -X- _ O
convergence -X- _ O
while -X- _ O
requiring -X- _ O
fewer -X- _ O
model -X- _ O
updates -X- _ O
( -X- _ O
Smith -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017;Neishi -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O

A -X- _ O
possible -X- _ O
consequence -X- _ O
is -X- _ O
that -X- _ O
batches -X- _ O
containing -X- _ O
fewer -X- _ O
sequences -X- _ O
per -X- _ O
update -X- _ O
may -X- _ O
have -X- _ O
' -X- _ O
noisier -X- _ O
' -X- _ O
estimated -X- _ O
gradients -X- _ O
than -X- _ O
batches -X- _ O
with -X- _ O
more -X- _ O
sequences -X- _ O
. -X- _ O

We -X- _ O
suggest -X- _ O
a -X- _ O
training -X- _ O
strategy -X- _ O
for -X- _ O
the -X- _ O
Transformer -X- _ O
model -X- _ O
( -X- _ O
Vaswani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
which -X- _ O
gives -X- _ O
improved -X- _ O
performance -X- _ O
for -X- _ O
long -X- _ O
sequences -X- _ O
, -X- _ O
like -X- _ O
syntax -X- _ O
representations -X- _ O
, -X- _ O
without -X- _ O
requiring -X- _ O
additional -X- _ O
GPU -X- _ O
memory -X- _ O
. -X- _ O

We -X- _ O
map -X- _ O
words -X- _ O
to -X- _ O
subwords -X- _ O
as -X- _ O
described -X- _ O
in -X- _ O
Section -X- _ O
3 -X- _ O
. -X- _ O

The -X- _ O
original -X- _ O
tree -X- _ O
can -X- _ O
be -X- _ O
directly -X- _ O
reproduced -X- _ O
from -X- _ O
the -X- _ O
sequence -X- _ O
, -X- _ O
so -X- _ O
that -X- _ O
structure -X- _ O
information -X- _ O
is -X- _ O
maintained -X- _ O
. -X- _ O

Our -X- _ O
linearized -X- _ O
derivation -X- _ O
representation -X- _ O
( -X- _ O
( -X- _ O
4 -X- _ O
) -X- _ O
in -X- _ O
Table -X- _ O
1 -X- _ O
) -X- _ O
consists -X- _ O
of -X- _ O
the -X- _ O
derivation -X- _ O
's -X- _ O
right -X- _ O
- -X- _ O
hand -X- _ O
side -X- _ O
tokens -X- _ O
with -X- _ O
an -X- _ O
end -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
rule -X- _ O
marker -X- _ O
, -X- _ O
< -X- _ O
/R -X- _ O
> -X- _ O
, -X- _ O
marking -X- _ O
the -X- _ O
last -X- _ O
non -X- _ O
- -X- _ O
terminal -X- _ O
in -X- _ O
each -X- _ O
rule -X- _ O
. -X- _ O

We -X- _ O
therefore -X- _ O
propose -X- _ O
a -X- _ O
derivation -X- _ O
- -X- _ O
based -X- _ O
representation -X- _ O
which -X- _ O
is -X- _ O
much -X- _ O
more -X- _ O
compact -X- _ O
than -X- _ O
a -X- _ O
linearized -X- _ O
parse -X- _ O
tree -X- _ O
( -X- _ O
examples -X- _ O
in -X- _ O
Table -X- _ O
1 -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
wish -X- _ O
to -X- _ O
ensemble -X- _ O
using -X- _ O
models -X- _ O
which -X- _ O
generate -X- _ O
linearized -X- _ O
constituency -X- _ O
trees -X- _ O
but -X- _ O
these -X- _ O
representations -X- _ O
can -X- _ O
be -X- _ O
very -X- _ O
long -X- _ O
and -X- _ O
difficult -X- _ O
to -X- _ O
model -X- _ O
. -X- _ O

Ensembles -X- _ O
of -X- _ O
Syntax -X- _ O
Models -X- _ O
. -X- _ O

Previous -X- _ O
approaches -X- _ O
to -X- _ O
ensembling -X- _ O
diverse -X- _ O
models -X- _ O
focus -X- _ O
on -X- _ O
model -X- _ O
inputs -X- _ O
. -X- _ O

( -X- _ O
2017 -X- _ O
) -X- _ O
similarly -X- _ O
produce -X- _ O
both -X- _ O
words -X- _ O
and -X- _ O
arcstandard -X- _ O
algorithm -X- _ O
actions -X- _ O
( -X- _ O
Nivre -X- _ O
, -X- _ O
2004 -X- _ O
) -X- _ O
. -X- _ O

Wu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

Eriguchi -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

They -X- _ O
demonstrate -X- _ O
improved -X- _ O
target -X- _ O
language -X- _ O
reordering -X- _ O
when -X- _ O
producing -X- _ O
syntax -X- _ O
. -X- _ O

Aharoni -X- _ O
and -X- _ O
Goldberg -X- _ O
( -X- _ O
2017 -X- _ O
) -X- _ O
translate -X- _ O
from -X- _ O
source -X- _ O
BPE -X- _ O
into -X- _ O
target -X- _ O
linearized -X- _ O
parse -X- _ O
trees -X- _ O
, -X- _ O
but -X- _ O
omit -X- _ O
POS -X- _ O
tags -X- _ O
to -X- _ O
reduce -X- _ O
sequence -X- _ O
length -X- _ O
. -X- _ O

Nadejde -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

Related -X- _ O
Work -X- _ O
. -X- _ O

We -X- _ O
also -X- _ O
suggest -X- _ O
a -X- _ O
syntax -X- _ O
representation -X- _ O
which -X- _ O
results -X- _ O
in -X- _ O
much -X- _ O
shorter -X- _ O
sequences -X- _ O
. -X- _ O

These -X- _ O
long -X- _ O
sequences -X- _ O
may -X- _ O
arise -X- _ O
through -X- _ O
the -X- _ O
use -X- _ O
of -X- _ O
linearized -X- _ O
constituency -X- _ O
trees -X- _ O
and -X- _ O
can -X- _ O
be -X- _ O
much -X- _ O
longer -X- _ O
than -X- _ O
their -X- _ O
plain -X- _ O
byte -X- _ O
pair -X- _ O
encoded -X- _ O
( -X- _ O
BPE -X- _ O
) -X- _ O
equivalent -X- _ O
representations -X- _ O
( -X- _ O
Table -X- _ O
1 -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
propose -X- _ O
an -X- _ O
approach -X- _ O
to -X- _ O
decoding -X- _ O
ensembles -X- _ O
of -X- _ O
models -X- _ O
generating -X- _ O
different -X- _ O
representations -X- _ O
, -X- _ O
focusing -X- _ O
on -X- _ O
models -X- _ O
generating -X- _ O
syntax -X- _ O
. -X- _ O

Models -X- _ O
producing -X- _ O
different -X- _ O
sentence -X- _ O
representations -X- _ O
are -X- _ O
necessarily -X- _ O
synchronized -X- _ O
to -X- _ O
enable -X- _ O
this -X- _ O
. -X- _ O

Introduction -X- _ O
. -X- _ O

Our -X- _ O
approach -X- _ O
gives -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
performance -X- _ O
on -X- _ O
a -X- _ O
difficult -X- _ O
Japanese -X- _ O
- -X- _ O
English -X- _ O
task -X- _ O
. -X- _ O

We -X- _ O
specifically -X- _ O
focus -X- _ O
on -X- _ O
syntax -X- _ O
in -X- _ O
ensembles -X- _ O
containing -X- _ O
multiple -X- _ O
sentence -X- _ O
representations -X- _ O
. -X- _ O

