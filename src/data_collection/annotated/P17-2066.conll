Since	O
most	O
available	O
caption	O
datasets	O
have	O
been	O
constructed	O
for	O
English	O
language	O
,	O
there	O
are	O
few	O
datasets	O
for	O
Japanese	O
.	O

In	O
this	O
paper	O
,	O
we	O
particularly	O
consider	O
generating	O
Japanese	O
captions	O
for	O
images	O
.	O

As	O
a	O
result	O
,	O
we	O
showed	O
the	O
necessity	O
of	O
STAIR	B-DatasetName
Captions	I-DatasetName
.	O

In	O
addition	O
,	O
we	O
confirmed	O
that	O
Japanese	O
captions	O
can	O
be	O
generated	O
simply	O
by	O
adapting	O
the	O
existing	O
caption	B-TaskName
generation	I-TaskName
method	O
.	O

Moreover	O
,	O
by	O
using	O
both	O
Japanese	O
and	O
English	O
captions	O
,	O
we	O
will	O
develop	O
multi	O
-	O
lingual	O
caption	B-TaskName
generation	I-TaskName
models	O
.	O

In	O
our	O
experiment	O
,	O
we	O
compared	O
the	O
performance	O
of	O
Japanese	O
caption	O
generation	O
by	O
a	O
neural	O
network	O
-	O
based	O
model	O
with	O
and	O
without	O
STAIR	B-DatasetName
Captions	I-DatasetName
to	O
highlight	O
the	O
necessity	O
of	O
Japanese	O
captions	O
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
STAIR	B-DatasetName
Captions	I-DatasetName
is	O
currently	O
the	O
largest	O
Japanese	O
image	O
caption	O
dataset	O
.	O

In	O
STAIR	B-DatasetName
Captions	I-DatasetName
,	O
Japanese	O
captions	O
are	O
provided	O
for	O
all	O
the	O
images	O
of	O
MS	B-DatasetName
-	I-DatasetName
COCO	I-DatasetName
.	O

In	O
this	O
paper	O
,	O
we	O
constructed	O
a	O
new	O
Japanese	O
image	O
caption	O
dataset	O
called	O
STAIR	B-DatasetName
Captions	I-DatasetName
.	O

In	O
the	O
example	O
at	O
the	O
bottom	O
of	O
the	O
table	O
,	O
En	B-MethodName
-	I-MethodName
generator	I-MethodName
→	I-MethodName
MT	I-MethodName
yielded	O
the	O
incorrect	O
caption	O
by	O
translating	O
"	O
A	O
bunch	O
of	O
food	O
"	O
as	O
"	O
食べ物の束	O
(	O
A	O
bundle	O
of	O
food	O
)	O
.	O
"	O
By	O
contrast	O
,	O
Ja	B-MethodName
-	I-MethodName
generator	I-MethodName
correctly	O
recognized	O
that	O
the	O
food	O
pictured	O
in	O
the	O
image	O
is	O
a	O
donut	O
,	O
and	O
expressed	O
it	O
as	O
"	O
ドーナツがたくさん	O
(	O
A	O
bunch	O
of	O
donuts	O
)	O
.	O
"	O
.	O

By	O
contrast	O
,	O
Jagenerator	B-MethodName
generated	O
"	O
二階建てのバス	O
(	O
two	O
-	O
story	O
bus	O
)	O
,	O
"	O
which	O
is	O
appropriate	O
as	O
the	O
Japanese	O
translation	O
of	O
A	O
double	O
decker	O
bus	O
.	O

In	O
the	O
example	O
at	O
the	O
top	O
in	O
Table	O
3	O
,	O
En	B-MethodName
-	I-MethodName
generator	I-MethodName
first	O
generated	O
the	O
term	O
,	O
"	O
A	O
double	O
decker	O
bus	O
.	O
"	O
MT	O
translated	O
the	O
term	O
into	O
as	O
"	O
二重デッカーバ	O
ス	O
"	O
,	O
but	O
the	O
translation	O
is	O
word	O
-	O
by	O
-	O
word	O
and	O
inappropriate	O
as	O
a	O
Japanese	O
term	O
.	O

Table	O
3	O
shows	O
two	O
examples	O
where	O
Ja	B-MethodName
-	I-MethodName
generator	I-MethodName
generated	O
appropriate	O
captions	O
,	O
whereas	O
Engenerator	B-MethodName
→	I-MethodName
MT	I-MethodName
generated	O
unnatural	O
ones	O
.	O

The	O
results	O
show	O
that	O
Ja	B-MethodName
-	I-MethodName
generator	I-MethodName
,	O
that	O
is	O
,	O
the	O
approach	O
in	O
which	O
Japanese	O
captions	O
were	O
used	O
as	O
training	O
data	O
,	O
outperformed	O
En	B-MethodName
-	I-MethodName
generator	I-MethodName
→	I-MethodName
MT	I-MethodName
,	O
which	O
was	O
trained	O
without	O
Japanese	O
captions	O
.	O

The	O
hyper	O
-	O
parameters	O
of	O
the	O
neural	O
network	O
were	O
tuned	O
based	O
on	O
CIDEr	B-MetricName
scores	O
by	O
using	O
the	O
validation	O
set	O
.	O

With	O
the	O
optimization	B-HyperparameterName
of	O
LSTM	B-MethodName
,	O
we	O
used	O
mini	B-HyperparameterValue
-	I-HyperparameterValue
batch	I-HyperparameterValue
RMSProp	I-HyperparameterValue
,	O
and	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
was	O
set	O
to	O
20	B-HyperparameterValue
.	O

We	O
used	O
VGG	B-MethodName
with	O
16	B-HyperparameterValue
layers	B-HyperparameterName
as	O
CNN	B-MethodName
,	O
where	O
the	O
VGG	B-MethodName
parameters	O
were	O
the	O
pre	O
-	O
trained	O
ones5	O
.	O

In	O
both	O
the	O
methods	O
,	O
following	O
Karpathy	O
and	O
Fei	O
-	O
Fei	O
,	O
we	O
only	O
trained	O
LSTM	B-MethodName
parameters	O
,	O
while	O
CNN	B-MethodName
parameters	O
were	O
fixed	O
.	O

En	B-MethodName
-	I-MethodName
generator	I-MethodName
→	I-MethodName
MT	I-MethodName
:	O
テーブルの上にある食べ物の束	O
。	O
Ja	B-MethodName
-	I-MethodName
generator	I-MethodName
:	O
.	O

En	B-MethodName
-	I-MethodName
generator	I-MethodName
:	O
.	O

ストリートを運転する二重デッカーバス	O
。	O
Ja	B-MethodName
-	I-MethodName
generator	I-MethodName
:	O
.	O

En	B-MethodName
-	I-MethodName
generator	I-MethodName
→	I-MethodName
MT	I-MethodName
:	O
.	O

En	B-MethodName
-	I-MethodName
generator	I-MethodName
:	O
.	O

Ja	B-MethodName
-	I-MethodName
generator	I-MethodName
was	O
trained	O
with	O
Japanese	O
captions	O
.	O

En	B-MethodName
-	I-MethodName
generator	I-MethodName
→	I-MethodName
MT	I-MethodName
is	O
the	O
pipeline	O
method	O
:	O
it	O
first	O
generates	O
English	O
caption	O
and	O
performs	O
machine	O
translation	O
subsequently	O
.	O

En	B-MethodName
-	I-MethodName
generator	I-MethodName
denotes	O
the	O
caption	O
generator	O
trained	O
with	O
MS	B-DatasetName
-	I-DatasetName
COCO	I-DatasetName
.	O

As	O
mentioned	O
in	O
Section	O
4	O
,	O
we	O
used	O
the	O
method	O
proposed	O
by	O
Karpathy	O
and	O
Fei	O
-	O
Fei	O
(	O
2015	O
)	O
as	O
caption	B-TaskName
generation	I-TaskName
models	O
for	O
both	O
En	B-MethodName
-	I-MethodName
generator	I-MethodName
→	I-MethodName
MT	I-MethodName
and	O
Ja	B-MethodName
-	I-MethodName
generator	I-MethodName
.	O

Unlike	O
MS	B-DatasetName
-	I-DatasetName
COCO	I-DatasetName
→	O
MT	O
,	O
this	O
method	O
directly	O
generate	O
a	O
Japanese	O
caption	O
from	O
a	O
given	O
image	O
.	O

•	O
Ja	O
-	O
generator	O
:	O
This	O
method	O
trains	O
a	O
neural	O
network	O
using	O
STAIR	B-DatasetName
Captions	I-DatasetName
.	O

In	O
the	O
test	O
phase	O
,	O
given	O
an	O
image	O
,	O
we	O
first	O
generate	O
an	O
English	O
caption	O
to	O
the	O
image	O
by	O
the	O
trained	O
neural	O
network	O
,	O
and	O
then	O
translate	O
the	O
generated	O
caption	O
into	O
Japanese	O
one	O
by	O
machine	B-TaskName
translation	I-TaskName
.	O

This	O
method	O
trains	O
a	O
neural	O
network	O
,	O
which	O
generates	O
English	O
captions	O
,	O
with	O
MS	B-DatasetName
-	I-DatasetName
COCO	I-DatasetName
.	O

•	O
En	O
-	O
generator	O
→	O
MT	O
:	O
A	O
pipeline	O
method	O
of	O
English	O
caption	B-TaskName
generation	I-TaskName
and	O
English	O
-	O
Japanese	O
machine	B-TaskName
translation	I-TaskName
.	O

Although	O
BLEU	B-MetricName
and	O
ROUGE	B-MetricName
were	O
developed	O
originally	O
for	O
evaluating	O
machine	O
translation	O
and	O
text	O
summarization	O
,	O
we	O
use	O
them	O
here	O
because	O
they	O
are	O
often	O
used	O
for	O
measuring	O
the	O
quality	O
of	O
caption	B-TaskName
generation	I-TaskName
.	O

Following	O
the	O
literature	O
(	O
Chen	O
et	O
al	O
.	O
,	O
2015;Karpathy	O
and	O
Fei	O
-	O
Fei	O
,	O
2015	O
)	O
,	O
we	O
use	O
BLEU	B-MetricName
(	O
Papineni	O
et	O
al	O
.	O
,	O
2002	O
)	O
,	O
ROUGE	B-MetricName
(	O
Lin	O
,	O
2004	O
)	O
,	O
andCIDEr	B-MetricName
(	O
Vedantam	O
et	O
al	O
.	O
,	O
2015	O
)	O
as	O
evaluation	O
measures	O
.	O

In	O
particular	O
,	O
we	O
evaluate	O
quantitatively	O
and	O
qualitatively	O
how	O
fluent	O
Japanese	O
captions	O
can	O
be	O
generated	O
by	O
using	O
a	O
neural	O
network	O
-	O
based	O
caption	B-TaskName
generation	I-TaskName
model	O
trained	O
on	O
STAIR	O
Captions	O
.	O

In	O
this	O
section	O
,	O
we	O
perform	O
an	O
experiment	O
which	O
generates	O
Japanese	O
captions	O
using	O
STAIR	B-DatasetName
Captions	I-DatasetName
.	O

In	O
the	O
training	O
phase	O
,	O
given	O
the	O
training	O
data	O
,	O
we	O
train	O
W	O
(	O
i	O
m	O
)	O
,	O
b	O
(	O
i	O
m	O
)	O
,	O
W	O
*	O
,	O
b	O
*	O
,	O
CNN	B-MethodName
,	O
and	O
LSTM	B-MethodName
parameters	O
,	O
where	O
*	O
represents	O
wild	O
card	O
.	O

The	O
generation	O
process	O
is	O
repeated	O
until	O
LSTM	B-MethodName
outputs	O
the	O
symbol	O
that	O
indicates	O
the	O
end	O
of	O
sentence	O
.	O

Then	O
,	O
caption	O
generation	O
is	O
defined	O
as	O
follows	O
:	O
x	O
(	O
i	O
m	O
)	O
=	O
CNN(I	B-MethodName
)	O
,	O
h	O
0	O
=	O
tanh	O
(	O
W	O
(	O
i	O
m	O
)	O
x	O
(	O
i	O
m	O
)	O
+	O
b	O
(	O
i	O
m	O
)	O
)	O
,	O
c	O
0	O
=	O
0	O
,	O
h	O
t	O
,	O
c	O
t	O
=	O
LSTM	B-MethodName
(	O
x	O
t	O
,	O
h	O
t−1	O
,	O
c	O
t−1	O
)	O
(	O
t	O
≥	O
1	O
)	O
,	O
y	O
t	O
=	O
softmax	O
(	O
W	O
o	O
h	O
t	O
+	O
b	O
o	O
)	O
,	O
where	O
CNN(•	B-MethodName
)	O
is	O
a	O
function	O
that	O
outputs	O
the	O
image	O
features	O
extracted	O
by	O
CNN	B-MethodName
,	O
that	O
is	O
,	O
the	O
final	O
layer	O
of	O
CNN	B-MethodName
,	O
and	O
y	O
t	O
is	O
the	O
tth	O
output	O
word	O
.	O

Specifically	O
,	O
CNN	B-MethodName
first	O
extracts	O
features	O
from	O
a	O
given	O
image	O
,	O
and	O
then	O
,	O
LSTM	B-MethodName
generates	O
a	O
caption	O
from	O
the	O
extracted	O
features	O
.	O

This	O
method	O
consists	O
of	O
a	O
convolutional	B-MethodName
neural	I-MethodName
network	I-MethodName
(	O
CNN	B-MethodName
)	O
and	O
long	B-MethodName
short	I-MethodName
-	I-MethodName
term	I-MethodName
memory	I-MethodName
(	O
LSTM)3	B-MethodName
.	O

In	O
this	O
section	O
,	O
we	O
briefly	O
review	O
the	O
caption	B-TaskName
generation	I-TaskName
method	O
proposed	O
by	O
Karpathy	O
and	O
Fei	O
-	O
Fei	O
(	O
2015	O
)	O
,	O
which	O
is	O
used	O
in	O
our	O
experiments	O
(	O
Section	O
5	O
)	O
.	O

Image	B-TaskName
Caption	I-TaskName
Generation	I-TaskName
.	O

That	O
the	O
numbers	O
of	O
images	O
and	O
captions	O
are	O
large	O
in	O
STAIR	B-DatasetName
Captions	I-DatasetName
is	O
an	O
important	O
point	O
in	O
image	O
caption	O
.	O

In	O
the	O
public	O
part	O
of	O
STAIR	B-DatasetName
Captions	I-DatasetName
,	O
the	O
numbers	O
of	O
images	O
and	O
Japanese	O
captions	O
are	O
4.65x	O
and	O
4.67x	O
greater	O
than	O
those	O
in	O
YJ	B-DatasetName
Captions	I-DatasetName
,	O
respectively	O
.	O

Compared	O
with	O
YJ	B-DatasetName
Captions	I-DatasetName
,	O
overall	O
,	O
the	O
numbers	O
of	O
Japanese	O
captions	O
and	O
images	O
in	O
STAIR	B-DatasetName
Captions	I-DatasetName
are	O
6.23x	O
and	O
6.19x	O
,	O
respectively	O
.	O

In	O
addition	O
,	O
we	O
compare	O
it	O
to	O
YJ	B-DatasetName
Captions	I-DatasetName
(	O
Miyazaki	O
and	O
Shimizu	O
,	O
2016	O
)	O
,	O
a	O
dataset	O
with	O
Japanese	O
captions	O
for	O
the	O
images	O
in	O
MS	B-DatasetName
-	I-DatasetName
COCO	I-DatasetName
like	O
in	O
STAIR	B-DatasetName
Captions	I-DatasetName
.	O

This	O
section	O
introduces	O
the	O
quantitative	O
characteristics	O
of	O
STAIR	B-DatasetName
Captions	I-DatasetName
.	O

To	O
concurrently	O
and	O
inexpensively	O
annotate	O
captions	O
by	O
using	O
the	O
above	O
web	O
system	O
,	O
we	O
asked	O
part	O
-	O
time	O
job	O
workers	O
and	O
crowd	O
-	O
sourcing	O
workers	O
to	O
perform	O
the	O
caption	O
annotation	O
.	O

Following	O
the	O
rules	O
for	O
publishing	O
datasets	O
created	O
based	O
on	O
MS	B-DatasetName
-	I-DatasetName
COCO	I-DatasetName
,	O
the	O
Japanese	O
captions	O
we	O
created	O
for	O
the	O
test	O
images	O
are	O
excluded	O
from	O
the	O
public	O
part	O
of	O
STAIR	B-DatasetName
Captions	I-DatasetName
.	O

We	O
annotated	O
all	O
images	O
(	O
164,062	O
images	O
)	O
in	O
the	O
2014	O
edition	O
of	O
MS	B-DatasetName
-	I-DatasetName
COCO	I-DatasetName
.	O

This	O
section	O
explains	O
how	O
we	O
constructed	O
STAIR	B-DatasetName
Captions	I-DatasetName
.	O

STAIR	B-DatasetName
Captions	I-DatasetName
.	O

In	O
Section	O
3	O
,	O
we	O
highlight	O
this	O
difference	O
by	O
comparing	O
the	O
statistics	O
of	O
STAIR	B-DatasetName
Captions	I-DatasetName
and	O
YJ	B-DatasetName
Captions	I-DatasetName
.	O

The	O
main	O
difference	O
between	O
STAIR	B-DatasetName
Captions	I-DatasetName
and	O
YJ	B-DatasetName
Captions	I-DatasetName
is	O
that	O
STAIR	B-DatasetName
Captions	I-DatasetName
provides	O
Japanese	O
captions	O
for	O
a	O
greater	O
number	O
of	O
images	O
.	O

As	O
in	O
our	O
study	O
,	O
they	O
constructed	O
a	O
Japanese	O
caption	O
dataset	O
called	O
YJ	B-DatasetName
Captions	I-DatasetName
.	O

In	O
addition	O
,	O
many	O
studies	O
have	O
extended	O
MS	B-DatasetName
-	I-DatasetName
COCO	I-DatasetName
by	O
annotating	O
additional	O
information	O
about	O
the	O
images	O
in	O
MS	B-DatasetName
-	I-DatasetName
COCO2	I-DatasetName
.	O

Since	O
its	O
release	O
,	O
MS	B-DatasetName
-	I-DatasetName
COCO	I-DatasetName
has	O
been	O
used	O
as	O
a	O
benchmark	O
dataset	O
for	O
image	B-TaskName
classification	I-TaskName
and	O
caption	B-TaskName
generation	I-TaskName
.	O

MS	B-DatasetName
-	I-DatasetName
COCO	I-DatasetName
is	O
a	O
dataset	O
constructed	O
for	O
research	O
on	O
image	B-TaskName
classification	I-TaskName
,	O
object	B-TaskName
recognition	I-TaskName
,	O
and	O
English	O
caption	B-TaskName
generation	I-TaskName
.	O

Note	O
that	O
when	O
annotating	O
the	O
Japanese	O
captions	O
,	O
we	O
did	O
not	O
refer	O
to	O
the	O
original	O
English	O
captions	O
in	O
MS	B-DatasetName
-	I-DatasetName
COCO	I-DatasetName
.	O

As	O
detailed	O
in	O
Section	O
3	O
,	O
we	O
annotate	O
Japanese	O
captions	O
for	O
the	O
images	O
in	O
MS	B-DatasetName
-	I-DatasetName
COCO	I-DatasetName
.	O

STAIR	B-DatasetName
Captions	I-DatasetName
is	O
available	O
for	O
download	O
from	O
http://captions.stair.center	O
.	O

•	O
We	O
confirmed	O
that	O
quantitatively	O
and	O
qualitatively	O
better	O
Japanese	O
captions	O
than	O
the	O
ones	O
translated	O
from	O
English	O
captions	O
can	O
be	O
generated	O
by	O
applying	O
a	O
neural	O
network	O
-	O
based	O
image	O
caption	O
generation	O
model	O
learned	O
on	O
STAIR	B-DatasetName
Captions	I-DatasetName
(	O
Section	O
5	O
)	O
.	O

The	O
contributions	O
of	O
this	O
paper	O
are	O
as	O
follows	O
:	O
•	O
We	O
constructed	O
a	O
large	O
-	O
scale	O
Japanese	O
image	O
caption	O
dataset	O
,	O
STAIR	B-DatasetName
Captions	I-DatasetName
,	O
which	O
consists	O
of	O
Japanese	O
captions	O
for	O
all	O
the	O
images	O
in	O
MS	B-DatasetName
-	I-DatasetName
COCO	I-DatasetName
(	O
Lin	O
et	O
al	O
.	O
,	O
2014	O
)	O
(	O
Section	O
3	O
)	O
.	O

By	O
improving	O
the	O
quality	O
of	O
image	B-TaskName
captioning	I-TaskName
,	O
image	B-TaskName
search	I-TaskName
using	O
natural	O
sentences	O
and	O
image	B-TaskName
recognition	I-TaskName
support	O
for	O
1In	O
recent	O
years	O
it	O
has	O
been	O
held	O
as	O
a	O
joint	O
workshop	O
such	O
as	O
EMNLP	O
and	O
ACL	O
;	O
https://vision.cs.hacettepe	O
.	O

Image	B-TaskName
captioning	I-TaskName
is	O
to	O
automatically	O
generate	O
a	O
caption	O
for	O
a	O
given	O
image	O
.	O

In	O
this	O
research	O
area	O
,	O
methods	O
to	O
automatically	O
generate	O
image	O
descriptions	O
(	O
captions	O
)	O
,	O
that	O
is	O
,	O
image	B-TaskName
captioning	I-TaskName
,	O
have	O
attracted	O
a	O
great	O
deal	O
of	O
attention	O
(	O
Karpathy	O
and	O
Fei	O
-	O
Fei	O
,	O
2015;Donahue	O
et	O
al	O
.	O
,	O
2015;Vinyals	O
et	O
al	O
.	O
,	O
2015;Mao	O
et	O
al	O
.	O
,	O
2015	O
)	O
.	O

In	O
the	O
experiment	O
,	O
we	O
show	O
that	O
a	O
neural	O
network	O
trained	O
using	O
STAIR	B-DatasetName
Captions	I-DatasetName
can	O
generate	O
more	O
natural	O
and	O
better	O
Japanese	O
captions	O
,	O
compared	O
to	O
those	O
generated	O
using	O
English	O
-	O
Japanese	O
machine	B-TaskName
translation	I-TaskName
after	O
generating	O
English	O
captions	O
.	O

STAIR	B-DatasetName
Captions	I-DatasetName
consists	O
of	O
820,310	O
Japanese	O
captions	O
for	O
164,062	O
images	O
.	O

To	O
tackle	O
this	O
problem	O
,	O
we	O
construct	O
a	O
large	O
-	O
scale	O
Japanese	O
image	O
caption	O
dataset	O
based	O
on	O
images	O
from	O
MS	B-DatasetName
-	I-DatasetName
COCO	I-DatasetName
,	O
which	O
is	O
called	O
STAIR	B-DatasetName
Captions	I-DatasetName
.	O

In	O
recent	O
years	O
,	O
automatic	O
generation	O
of	O
image	O
descriptions	O
(	O
captions	O
)	O
,	O
that	O
is	O
,	O
image	B-TaskName
captioning	I-TaskName
,	O
has	O
attracted	O
a	O
great	O
deal	O
of	O
attention	O
.	O

STAIR	B-DatasetName
Captions	I-DatasetName
:	O
Constructing	O
a	O
Large	O
-	O
Scale	O
Japanese	O
Image	O
Caption	O
Dataset	O
.	O

In	O
future	O
work	O
,	O
we	O
will	O
analyze	O
the	O
experimental	O
results	O
in	O
greater	O
detail	O
.	O

The	O
total	O
number	O
of	O
Japanese	O
captions	O
is	O
820,310	O
.	O

Conclusion	O
.	O

Table	O
2	O
summarizes	O
the	O
experimental	O
results	O
.	O

Results	O
.	O

As	O
preprocessing	O
,	O
we	O
applied	O
morphological	O
analysis	O
to	O
the	O
Japanese	O
captions	O
using	O
MeCab6	O
.	O

We	O
divided	O
the	O
dataset	O
into	O
three	O
parts	O
,	O
i.e.	O
,	O
113,287	O
images	O
for	O
the	O
training	O
set	O
,	O
5,000	O
images	O
for	O
the	O
validation	O
set	O
,	O
and	O
5,000	O
images	O
for	O
the	O
test	O
set	O
.	O

Following	O
the	O
experimental	O
setting	O
in	O
the	O
previous	O
studies	O
(	O
Chen	O
et	O
al	O
.	O
,	O
2015;Karpathy	O
and	O
Fei	O
-	O
Fei	O
,	O
2015	O
)	O
,	O
we	O
used	O
123,287	O
images	O
included	O
in	O
the	O
MS	O
-	O
COCO	O
training	O
and	O
validation	O
sets	O
and	O
their	O
corresponding	O
Japanese	O
captions	O
.	O

Dataset	O
Separation	O
.	O

ドーナツがたくさん並んでいる	O
。	O
.	O

A	O
bunch	O
of	O
food	O
that	O
are	O
on	O
a	O
table	O
.	O

二階建てのバスが道路を走っている	O
。	O
.	O

A	O
double	O
decker	O
bus	O
driving	O
down	O
a	O
street	O
.	O

Examples	O
of	O
generated	O
image	O
captions	O
.	O

This	O
method	O
is	O
the	O
baseline	O
.	O

Here	O
,	O
we	O
use	O
Google	O
translate4	O
for	O
machine	O
translation	O
.	O

In	O
this	O
experiment	O
,	O
we	O
evaluate	O
the	O
following	O
caption	O
generation	O
methods	O
.	O

Comparison	O
Methods	O
.	O

Evaluation	O
Measure	O
.	O

Experimental	O
Setup	O
.	O

The	O
aim	O
of	O
this	O
experiment	O
is	O
to	O
show	O
the	O
necessity	O
of	O
a	O
Japanese	O
caption	O
dataset	O
.	O

Experiments	O
.	O

The	O
input	O
x	O
t	O
at	O
time	O
t	O
is	O
substituted	O
by	O
a	O
word	O
embedding	O
vector	O
corresponding	O
to	O
the	O
previous	O
output	O
,	O
that	O
is	O
,	O
y	O
t−1	O
.	O

Let	O
I	O
be	O
an	O
image	O
,	O
and	O
the	O
corresponding	O
caption	O
be	O
Y	O
=	O
(	O
y	O
1	O
,	O
y	O
2	O
,	O
•	O
•	O
•	O
,	O
y	O
n	O
)	O
.	O

Table	O
1	O
summarizes	O
the	O
statistics	O
of	O
the	O
datasets	O
.	O

Statistics	O
.	O

The	O
entire	O
annotation	O
work	O
was	O
completed	O
by	O
about	O
2,100	O
workers	O
in	O
about	O
half	O
a	O
year	O
.	O

To	O
guarantee	O
the	O
quality	O
of	O
the	O
captions	O
created	O
in	O
this	O
manner	O
,	O
we	O
conducted	O
sampling	O
inspection	O
of	O
the	O
annotated	O
captions	O
,	O
and	O
the	O
captions	O
not	O
in	O
line	O
with	O
the	O
guidelines	O
were	O
removed	O
.	O

(	O
5	O
)	O
A	O
caption	O
must	O
not	O
include	O
emotions	O
or	O
opinions	O
about	O
the	O
image	O
.	O

(	O
4	O
)	O
A	O
caption	O
must	O
be	O
a	O
single	O
sentence	O
.	O

(	O
3	O
)	O
A	O
caption	O
must	O
describe	O
only	O
what	O
is	O
happening	O
in	O
an	O
image	O
and	O
the	O
things	O
displayed	O
therein	O
.	O

(	O
2	O
)	O
A	O
caption	O
must	O
follow	O
the	O
da	O
/	O
dearu	O
style	O
(	O
one	O
of	O
writing	O
styles	O
in	O
Japanese	O
)	O
.	O

(	O
1	O
)	O
A	O
caption	O
must	O
contain	O
more	O
than	O
15	O
letters	O
.	O

The	O
workers	O
annotated	O
the	O
images	O
based	O
on	O
the	O
following	O
guidelines	O
.	O

By	O
pressing	O
the	O
send	O
(	O
送信	O
)	O
button	O
,	O
a	O
single	O
task	O
is	O
completed	O
and	O
the	O
next	O
task	O
is	O
started	O
.	O

Each	O
annotator	O
looks	O
at	O
the	O
displayed	O
image	O
and	O
writes	O
the	O
corresponding	O
Japanese	O
description	O
in	O
the	O
text	O
box	O
under	O
the	O
image	O
.	O

Figure	O
1	O
shows	O
the	O
example	O
of	O
the	O
annotation	O
screen	O
in	O
the	O
web	O
system	O
.	O

To	O
annotate	O
captions	O
efficiently	O
,	O
we	O
first	O
developed	O
a	O
web	O
system	O
for	O
caption	O
annotation	O
.	O

Therefore	O
,	O
the	O
total	O
number	O
of	O
captions	O
was	O
820,310	O
.	O

For	O
each	O
image	O
,	O
we	O
provided	O
five	O
Japanese	O
captions	O
.	O

Annotation	O
Procedure	O
.	O

In	O
particular	O
,	O
the	O
study	O
of	O
Miyazaki	O
and	O
Shimizu	O
(	O
2016	O
)	O
is	O
closest	O
to	O
the	O
present	O
study	O
.	O

Recently	O
,	O
a	O
few	O
caption	O
datasets	O
in	O
languages	O
other	O
than	O
English	O
have	O
been	O
constructed	O
(	O
Miyazaki	O
and	O
Shimizu	O
,	O
2016;Grubinger	O
et	O
al	O
.	O
,	O
2006;Elliott	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

Representative	O
examples	O
are	O
PASCAL	O
(	O
Rashtchian	O
et	O
al	O
.	O
,	O
2010	O
)	O
,	O
Flickr3k	O
(	O
Rashtchian	O
et	O
al	O
.	O
,	O
2010;Hodosh	O
et	O
al	O
.	O
,	O
2013	O
)	O
,	O
Flickr30k	O
(	O
Young	O
et	O
al	O
.	O
,	O
2014	O
)	O
-an	O
extension	O
of	O
Flickr3k-	O
,	O
and	O
MS	O
-	O
COCO	O
(	O
Microsoft	O
Common	O
Objects	O
in	O
Context	O
)	O
(	O
Lin	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

Some	O
English	O
image	O
caption	O
datasets	O
have	O
been	O
proposed	O
(	O
Krishna	O
et	O
al	O
.	O
,	O
2016;Kuznetsova	O
et	O
al	O
.	O
,	O
2013;Ordonez	O
et	O
al	O
.	O
,	O
2011;Vedantam	O
et	O
al	O
.	O
,	O
2015;Isola	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

Related	O
Work	O
.	O

Therefore	O
,	O
in	O
this	O
study	O
,	O
we	O
construct	O
a	O
Japanese	O
image	O
caption	O
dataset	O
,	O
and	O
for	O
given	O
images	O
,	O
we	O
aim	O
to	O
generate	O
more	O
natural	O
Japanese	O
captions	O
than	O
translating	O
the	O
generated	O
English	O
captions	O
into	O
the	O
Japanese	O
ones	O
.	O

However	O
,	O
the	O
translated	O
captions	O
may	O
be	O
literal	O
and	O
unnatural	O
because	O
image	O
information	O
can	O
not	O
be	O
reflected	O
in	O
the	O
translation	O
.	O

A	O
straightforward	O
solution	O
is	O
to	O
translate	O
English	O
captions	O
into	O
Japanese	O
ones	O
by	O
using	O
machine	O
translation	O
such	O
as	O
Google	O
Translate	O
.	O

Since	O
most	O
available	O
caption	O
datasets	O
have	O
been	O
constructed	O
for	O
English	O
language	O
,	O
there	O
are	O
few	O
datasets	O
for	O
Japanese	O
.	O

In	O
this	O
study	O
,	O
we	O
consider	O
generating	O
image	O
captions	O
in	O
Japanese	O
.	O

Recognizing	O
various	O
images	O
and	O
generating	O
appropriate	O
captions	O
for	O
the	O
images	O
necessitates	O
the	O
compilation	O
of	O
a	O
large	O
number	O
of	O
image	O
and	O
caption	O
pairs	O
.	O

edu.tr/vl2017/	O
visually	O
impaired	O
people	O
by	O
outputting	O
captions	O
as	O
sounds	O
can	O
be	O
made	O
available	O
.	O

The	O
Workshop	O
on	O
Vision	O
and	O
Language	O
held	O
in	O
2011	O
has	O
since	O
become	O
an	O
annual	O
event1	O
.	O

Integrated	O
processing	O
of	O
natural	O
language	O
and	O
images	O
has	O
attracted	O
attention	O
in	O
recent	O
years	O
.	O

Introduction	O
.	O

