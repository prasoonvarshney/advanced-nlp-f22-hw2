" Nice Try , Kiddo " : Investigating Ad Hominems in Dialogue Responses .
Ad hominem attacks are those that target some feature of a person 's character instead of the position the person is maintaining .
These attacks are harmful because they propagate implicit biases and diminish a person 's credibility .
Since dialogue systems respond directly to user input , it is important to study ad hominems in dialogue responses .
To this end , we propose categories of ad hominems , compose an annotated dataset , and build a classifier to analyze human and dialogue system responses to English Twitter posts .
We specifically compare responses to Twitter topics about marginalized communities ( # Black - LivesMatter , # MeToo ) versus other topics ( # Vegan , # WFH ) , because the abusive language of ad hominems could further amplify the skew of power away from marginalized populations .
Furthermore , we propose a constrained decoding technique that uses salient n - gram similarity as a soft constraint for top - k sampling to reduce the amount of ad hominems generated .
Our results indicate that 1 ) responses from both humans and DialoGPT contain more ad hominems for discussions around marginalized communities , 2 ) different quantities of ad hominems in the training data can influence the likelihood of generating ad hominems , and 3 ) we can use constrained decoding techniques to reduce ad hominems in generated dialogue responses .
Post : Many are trying to co - opt and mischaracterize the # blacklivesmatter movement .
We wo n't allow it ! Resp : I hate how much of a victim complex you guys have .
Post : You 're the reason we need the # MeToo movement .
Resp : Nice try , kiddo .
Post : Stop eating them if you do n't want them to go extinct ! # govegan Resp : I do n't like your username .
Introduction .
Ad hominems attack an opponent 's character or identity instead of the points the opponent is making , and can exist in any conversational setting between two or more entities .
From an argumentation perspective , ad hominems are fallacies , and fallacies rely on faulty reasoning to advance a point ( Hansen , 2020 ) .
These ad hominem fallacies are related to abusive language , toxicity , and microaggressions , and can be expressed with both subtle and explicitly offensive language .
Table 1 presents examples of ad hominem responses to Twitter posts .
Undesirable in any response , ad hominems are unproductive in furthering a meaningful discussion and can reinforce falsehoods .
However , these attacks appeal to emotions and implicit biases to argue a point , and are thus often effectively harmful regardless of whether the attacks are true , recognized , or retracted ( Yap , 2013 ) .
Our work is motivated by this fallacy 's potential to amplify the spread of harmful societal biases .
For communities that are already disproportionately harmed by societal power inequalities , ad hominems further amplify the power imbalance .
Tone policing is a type of ad hominem that seeks to regulate the emotions that a person ( usually of a marginalized population ) can use to deliver their points ( e.g. , not too angrily ) , thereby altogether invalidating the style of delivery , the person 's competence , and the points being conveyed .
Besides directly experiencing ad hominem attacks , marginalized groups could also be disproportionately discouraged from using technologies that propagate these attacks , since abusive language from a technology can deter people from using the technology ( Sood et al . , 2012b ) .
The goal of this study is to analyze ad hominems in dialogue system - and human - generated responses for topics that vary in impact to marginalized populations .
Through analysis , we formulate techniques to reduce ad hominem responses and thus the associated harms , which is especially important for dialogue systems since these systems directly interact with users .
We analyze responses from DialoGPT ( Zhang et al . , 2020a ) and humans to English Twitter posts .
Specifically , we compare responses to Twitter topics about marginalized communities ( # Black - LivesMatter , # MeToo ) versus other topics ( # Vegan , # WFH ) .
Through human annotation and trained classifiers , we find that ad hominems exist in both human and DialoGPT responses .
Across response sources , there are more ad hominems in # Black - LivesMatterand # MeToo - related responses , fewer in # Vegan - related responses , and even fewer in # WFH - related responses .
The presence of more ad hominems in responses to social issues that concern marginalized groups has troubling implications about the amplified harms toward these groups .
Given our analysis , we further propose a constrained decoding algorithm to reduce the amount of ad hominems generated by dialogue systems .
By using salient n - gram similarity to apply soft constraints to top - k sampling , our proposed technique is simple , extensible to reducing other harms , and does not require much additional computation .
At each decoding time step , the technique compares the similarity between the current generated output and salient ad hominem versus non - ad hominem n - grams , possibly selecting alternative token candidates to generate .
This technique is effective at reducing the amount of ad hominems generated across topics while maintaining coherence and relevance .
Our main contribution is a novel analysis of ad hominem responses generated by humans and Di - aloGPT across topics varying in impact to marginalized communities .
For this analysis , we propose empirically - derived ad hominem categories that are further verified through annotation .
Furthermore , we build a new dataset of Twitter posts paired with human - and DialoGPT - generated responses , where the responses have ad hominem - related labels .
Finally , we devise a constrained decoding technique that uses salient n - gram similarity to steer top - k sampling away from ad hominem responses .
We release data and code at https://github.com/ ewsheng / ad - hom - in - dialogue .
Related Work .
This work is related to a broad spectrum of topics , including prior definitions of ad hominems and how ad hominems facilitate biases .
Also , analyzing ad hominems in dialogue systems is related to examining offensive language and other harms .
Lastly , we discuss existing constrained decoding methods .
Ad Hominems In the argumentation literature , theoretical ad hominems include the abusive ( attack on the opponent 's character ) , tu quoque ( " he did it first " ) , circumstantial ( accusation of hypocrisy ) , and guilt by association ( associating the opponent with someone with low credibility ) ( Walton , 1998;Woods , 2007 ) .
Wijze ( 2003 ) criticizes that these textbook examples are not realistic in conversation .
For more empirical categories , Habernal et al .
( 2018 ) propose ad hominem types based on analysis of Reddit 's ChangeMyView discussion threads , and Delobelle et al .
( 2019 ) analyze the name - calling and abusive categories .
Moreover , Wulczyn et al .
( 2017 ) use classifiers for a largescale analysis of personal attacks in Wikipedia comments .
We build upon prior works to define and analyze ad hominems in a conversational setting .
Additionally , Yap ( 2013 ) discusses the harmful effects of implicit biases in forming and evaluating ad hominems .
They emphasize that ad hominem attacks can be harmful to a person 's credibility and expertise even if the attack is recognized as fallacious and irrelevant to the argument .
In particular , because societal norms allow biases and stereotypes to detract from a person 's credibility or expertise , the use of ad hominems can further diminish the rhetorical credibility ( Govier , 1993 ) of marginalized groups .
Offensive Language Detection Ad hominems occur in many forms and are related to different types of offensive language , including abusive language ( Yin et al . , 2009;Chen et al . , 2012;Nobata et al . , 2016 ) , hate speech ( Warner and Hirschberg , 2012;Kwok and Wang , 2013;Djuric et al . , 2015 ) , profanity ( Sood et al . , 2012a ) , and the more subtle forms of microaggressions ( Breitfeller et al . , 2019 ) and projecting biases and stereotypes through power differentials in language ( Sap et al . , 2020 ) .
Ranging from outright insults to condescension , ad hominems are a form of offensive language that is difficult to comprehensively and objectively define .
Nonetheless , these responses are important to characterize , since they can irreparably damage a person 's credibility .
It is also generally important to identify these subtle forms of offensive language , since it is unclear if existing offensive language detection techniques are equally effective for these subtle forms .
Harms in Dialogue Systems Conversational systems are known to perpetuate several types of harms .
Ruane et al .
( 2019 ) caution about harms that can result from using conversational systems and propose striving for trust and transparency ; Roller et al .
( 2020 ) suggest techniques for chatbot safety .
For analysis , Sheng et al .
( 2019 ) evaluate societal biases in language generation , Curry and Rieser ( 2018 ) study how conversational systems respond to sexual harassment , and Khatri et al .
( 2018 ) detect offensive content with a semi - supervised approach .
To reduce harms , Sheng et al .
( 2020 ) present a framework for controlling biases in language generation , and Dinan et al .
( 2019 ) show how adversarial attacks can make models more robust to offensive language usage from humans .
Constrained Decoding For constrained decoding , prior works focus on incorporating words or phrases ( as hard or soft constraints ) into the decoded output .
Swanson et al .
( 2014 ) and Balakrishnan et al .
( 2019 ) use parse trees among other techniques to enforce constraints in the generated text .
Hokamp and Liu ( 2017 ) ; Post and Vilar ( 2018 ) propose variants of Grid Beam Search , which generate output that include lexical constraints .
Miao et al .
( 2019 ) ; Zhang et al .
( 2020b ) ; Susanto et al .
( 2020 ) explore insertion - based non - autoregressive decoding algorithms .
To be compatible with an autoregressive model like DialoGPT and effective for open - domain generation , we apply constrained decoding to top - k sampling .
Our method also differs from these prior works in that it imposes soft constraints to not generate phrases that are likely to lead to ad hominems .
Decoding - time techniques that can be used to reduce harmful language generation , e.g. , the Plug and Play Language Model ( PPLM ) ( Dathathri et al . , 2020 ) , are most relevant to our technique .
Dataset and Model Setup .
This section describes the dataset collection process and the dialogue model variations we analyze .
Dataset Collection Our goal is to understand how ad hominem responses differ across discussions that vary in impact and relevance to marginalized groups .
To that end , we extract English [ post , response ] pairs on different topics from Twitter and also use DialoGPT to generate responses for all collected posts .
We refer to this collective dataset as the ADHOMINTWEETS dataset .
Relevant topics are divided into polarizing ( i.e. , upon the work of Habernal et al .
( 2018 ) to devise ad hominem categories that are both empiricallymotivated and can be annotated with high interannotator agreement .
We specifically include categories such as " ignorance " and " condescension " to cover more subtle forms of personal attacks ( e.g. , tone policing , mansplaining ) that could further diminish the credibility of those who are already marginalized .
We also limit the definition of ad hominem to personal attacks towards the author of the post and not a third person .
Human Annotation .
We collect human annotations that can then be used for analysis and training a classifier to automatically label ad hominems .
Although Habernal et al .
( 2018 ) propose a similar typology of ad hominems , there is no existing dataset annotated with their empirically - derived categories .
Moreover , we study ad hominems in casual conversational settings .
For these reasons , we annotate a subset of ADHOMINTWEETS with ad hominem information .
To measure inter - annotator agreement , we calculate the Worker Agreement With Aggregate ( WAWA ) score , following Ning et al .
( 2020 ) .
The WAWA score compares the majority votes against each annotator and micro - averages the resulting precision , recall , and F 1 scores .
5 Heuristics for Ad Hominems Ad hominem responses are relatively rare and range broadly from explicit to more subtle forms .
For more effective annotation , we use heuristics to choose [ post , response ] pairs where the response is likely to be an ad hominem .
In preliminary analyses , we find that responses that contain certain " you"-phrases such as " you are " are more likely to have ad hominems .
We call these responses you - responses .
6 In addition to pairs with you - responses , we also collect random pairs without you - responses for annotation to ensure that the annotated samples are representative of different ad hominems .
Annotation Task We ask annotators on Mechanical Turk to read a post and response and determine whether the response contains any ad hominem(s ) towards the person who made the post .
We divide ad hominems into the following categories : stupidity , ignorance , trolling / lying , bias , condescension , and other ; examples are in Table 3 .
7 Annotation Round 1 The goal for the first round of human annotation is to collect enough data to train an ad hominem classifier .
To balance targeted and random samples , for each topic ( BLM , MeToo , Vegan , WFH ) and response source ( human , Di - aloGPT ) pair , we randomly select 150 [ post , response ] pairs with you - responses and another 150 pairs without you - responses for annotation .
In total , we gather 2,400 [ post , response ] pairs that are then annotated through Mechanical Turk .
Additional Annotations We conduct three more rounds of annotations to retrieve more ad hominem responses .
For the second and third rounds , we use an ad hominem classifier trained on data from all previous rounds ( with the same architecture and hyperparameters as the final classifier in Sec .
4.2 ) to label unseen samples in ADHOMINTWEETS .
We then select a balanced amount of automaticallylabeled ad hominems and non - ad hominems from each [ topic , response source ] pair to annotate .
8 Some topics ( e.g. , WFH and Vegan ) prompt fewer ad hominem responses , so it is difficult to find enough of these responses " in the wild " to train a more accurate classifier .
Our solution is to manually take the responses annotated as ad hominems and pair them with WFH or Vegan posts .
To verify that these new pairs contain ad hominem responses , we run a fourth round of annotation on these pairs and only keep the ones where the majority of annotators label the response as an ad hominem to the post .
We combine majority annotations across all rounds of annotations to train the final ad hominem classifier used for analysis .
Ad Hominem Classifier .
For large - scale analysis of ad hominems in human and dialogue system responses , we rely on classifier annotation .
To simplify the learning problem , we condense the different ad hominem categories into a binary yes / no scheme , where " yes " indicates the presence of any type and quantity of ad hominems in the response given the post .
We build a classifier to automatically label whether a response contains ad hominems for a given post by fine - tuning a BERT ( Devlin et al . , 2019 ) model with the input format " [ CLS ] POST [ SEP ] RESPONSE [ SEP ] " .
We additionally include comparisons to a baseline classifier built on top of DialoGPT to similarly label whether a post and response pair indicates the presence of an ad hominem response .
This baseline classifier allows a comparative evaluation of a bi - directional encoder model versus an auto - regressive decoder model for ad hominem classification and how this difference may affect the quality of control techniques that rely on the latter ( e.g. , PPLM ( Dathathri et al . , 2020 ) , GeDi ( Krause et al . , 2020 ) ) .
Appendix A.2 includes more details of our model implementation and data statistics ( Table 8) .
Ultimately , the goal is to train an ad hominem detection classifier that has high accuracy across sources and topics , so we curate the dev and test datasets to be balanced across topics , response sources , and ad hominem versus non - ad hominem samples ( through downsampling ) .
Because of the natural imbalance of ad hominem responses for different topics , ad hominem responses for topics like WFH are relatively sparse compared to those for topics like BLM .
We automatically augment our training set to combat this sparsity .
First , we accumulate all posts and responses not present in the dev and test sets .
Next , we choose a random post to pair with a random labeled response to form a new sample .
We generate these new data samples to roughly balance the number of samples across topics and across ad hominems versus nonad hominems for each topic .
These new combinations of [ post , response ] pairs help de - emphasize spurious correlations between topics and classifier labels .
Since the automatic augmentation reduces emphasis on the post when predicting the presence of ad hominems in the response , a natural question is if the post is really necessary to gauge whether the response contains ad hominems .
The answer is mixed - for example , the response " you 're a troll " is an ad hominem for any post .
However , the response " those who promote veganism are arrogant fools " is an ad hominem given the post " everyone should follow veganism " , but not an ad hominem given the post " I do n't understand veganism " .
Empirically , by limiting the classifier input to only responses , the classifier performs worse than if it has both the post and response as input .
9 .
Reducing Ad Hominem Responses .
Inspired by the success of n - gram features in detecting abusive language by Nobata et al .
( 2016 ) , we propose a constrained decoding algorithm to discourage the model from generating n - grams that are semantically similar to salient n - grams found in ad hominem responses .
While we motivate this technique within the context of ad hominems , the technique is applicable to other subtle harms ( e.g. , microaggressions ) in language generation .
A naive method to generate fewer ad hominems is to block words that are likely to occur in ad hominems .
However , ad hominems are contextually determined , meaning that phrases are a better indicator than words , thus motivating our use of n - grams .
Additionally , our algorithm uses soft constraints because there are no words or phrases that always indicate the presence of an ad hominem .
In this section , we describe how our technique SALIENSIMTOP - k extends top - k sampling by incorporating n - gram similarity constraints .
Salient n - grams We define salient ad hominem n - grams to be n - grams that appear more frequently in ad hominem responses than in non - ad hominem responses .
Similarly , salient non - ad hominem n- S(u , a ) = count(u , Da ) + λ a ∈A , a = a count(u , D a ) + λ .(1 ) In Eq . ( 1 ) , u denotes an n - gram , D = { ( s 1 , a 1 ) , ... , ( s m , a m ) } is a corpus where each sample is a sentence s i labeled with attribute a i .
D a is therefore the set of sentences in the corpus with the same attribute a.
A is the set of possible attributes ( e.g. , ad hominem or non - ad hominem ) .
We define the n - gram u to be salient for the attribute a if S(u , a ) ≥ ϕ.
We find setting the smoothing parameter λ = 0.5 and threshold ϕ = 5.5 effective for our experiments , and we compute the salience of 3- , 4- , and 5 - grams .
Table 4 shows that the top salient ad hominem n - grams are intuitively those that are likely to lead to ad hominems .
For example , " you 're being a " is used in contexts such as " you 're being a hypocrite " .
A more overt example of a phrase likely to lead to an ad hominem response is " you 're a troll " .
The amount of you - responses in salient ad hominem ngrams verify our intuition that many ad hominem responses occur in the form of you - responses .
Also , we find that there are more salient ad hominem ngrams than non - ad hominem n - grams , and that the former generally have higher salience scores .
These observations and preliminary experiments suggested that it is useful to consider both types of salient n - grams to reduce ad hominems .
Top - k Sampling .
For open domain language generation , top - k sampling ( Fan et al . , 2018 ) and top - p nucleus sampling ( Holtzman et al . , 2019 ) are popular decoding algorithms that have been shown to maintain topic consistency and promote diversity .
We experiment with constrained decoding through top - k sampling , though our technique is also applicable to nucleus sampling .
As top - k sampling is a general decoding algorithm that can be used with .
SALIENSIMTOP - k.
We reduce the amount of generated ad hominems by encouraging the generation of n - grams that are semantically dissimilar to salient ad hominem n - grams and similar to salient non - ad hominem n - grams .
Alg .
1 details constraints we add to top - k sampling .
In the for - loop , we iterate through each candidate token .
If the current generated output meets a " special_condition " ( e.g. , backtracking limit , first r time steps ) , then we select the current candidate token .
Otherwise we retrieve and average DialoGPT 's embeddings over the most recently generated r - gram to calculate c , an e - dimensional vector where e is the size of the token embedding .
We similarly compute representations to form A , a j × e matrix of j salient ad hominem average n - gram embeddings , and B , a k × e matrix of k salient non - ad hominem average n - gram embeddings .
We then calculate the average pairwise similarity sim_a = 1 j j i=1 sim(A i , c ) , where A i is the i - th row of A , and similarly for sim_b .
We select the current token if the difference between the similarities is under a threshold γ , i.e. , the current r - gram is less similar to the ad hominem n - grams and more similar to the non - ad hominem n - grams .
Otherwise , we backtrack to the previous time step if we iterate through all candidates without finding a suitable one .
By limiting the number of times the algorithm can backtrack while gen- erating a sample , this algorithm adds a constant amount of computational resources compared to the original , non - constrained decoding .
Implementation Details In our experiments , we set k = 40 ( commonly used in previous generation tasks ( Radford et al . , 2019 ) ) .
With parameter tuning , we find t = 10 and γ = 0 effective for our setup .
We use r = 5 to compare the averaged embedding of the most recent 5 - gram with those of salient 3- , 4- , and 5 - grams .
Additionally , we use cosine similarity as the similarity metric and our " special_condition " includes either a ) a limit of 5 for backtracking or b ) the first r time steps .
Results .
Identifying Ad Hominems .
Annotation Across all rounds of annotations , the average WAWA scores include a precision of 0.82 , recall of 0.92 , and F 1 of 0.87 , indicating moderately high majority agreement .
Generally , the agreement scores for the human responses are slightly higher than those for the DialoGPT responses - we hypothesize that the former tend to be more coherent and longer , and thus more informative .
Fine - tuned DialoGPT Responses Figure 1 also shows that fine - tuning on datasets that contain more ad hominem responses leads to more generation of ad hominem responses across topics .
11 From these results , we infer that the original DialoGPT ( which was fine - tuned from GPT-2 ) was trained on a dataset that likely contained relatively more rather than fewer ad hominems .
Additionally , finetuning on a carefully chosen dataset can reduce the quantity of generated ad hominems and associated harms .
Ad Hominem Reduction .
Baselines We compare techniques from two classes of harm reduction methods for language generation : data - based and decoding - based .
Gehman et al .
( 2020 ) Table 7 : Average coherence ( C ) and relevance ( R ) of responses across sources and topics , each on a scale of 1 - 5 , where higher scores are better .
Each value is averaged over 25 random samples ( and 3 annotators per sample ) .
The highest score(s ) per column are bolded , and the lowest score(s ) per column are underlined .
Trigger generates slightly more coherent responses , though at the cost of relevance .
PPLM generates responses that are relatively lower in both coherence and relevance .
SS maintains a decent balance of coherence and relevance , and F WFH + SS produces slightly less coherent responses that are mixed in relevance .
hominems .
For SALIENSIMTOP - k , limiting the number of times we backtrack to previous time steps ensures that the algorithm is not significantly slower compared to the original top - k sampling algorithm .
Empirically , we find that using SALIENSIMTOP - k with a backtracking limit of 5 on the original Di - aloGPT results in 13 % of the decoding operations being " non - forward " operations , where the set of decoding operations are : a ) choosing the current token and moving forward to the next timestep , b ) looking for an alternate token at the same timestep , or c ) moving backward to a previous timestep .
When applying constrained decoding to DialoGPT fine - tuned on WFH , 10 % of the operations are nonforward operations .
Since ad hominems are less common than non - ad hominems , the algorithm is able to proceed with the first sampled candidate ken in most time steps .
Additionally , models or topics that are inclined to generate more ad hominems incur more non - forward operations .
Coherence and Relevance Evaluation To ensure that the ad hominem reduction techniques do not affect the quality of the generated responses , we have annotators label the coherence and relevance of a response to a post , both on a scale of 1 to 5 , where a higher score is better .
The trigger method produces samples that are relatively more coherent , although at the cost of lower relevance to the post .
PPLM generates responses that are relatively lower in both coherence and relevance .
SALIENSIMTOP - k manages to maintain a decent balance of generating both coherent and relevant responses .
Combining SALIENSIMTOP - k with finetuning on WFH data results in responses that are slightly less coherent and mixed in relevance for different topics .
13 Spearman 's correlation is moderately high ( 0.46 ) for relevance and a bit lower for coherence ( 0.38 ) , indicating the task subjectivity .
Discussion The collective results indicate that SALIENSIMTOP - k is an effective standalone ad hominem reduction technique that maintains generated text quality ; while it can be combined with other techniques to further reduce ad hominems , one should carefully evaluate the trade - offs between response coherence and relevance .
Additionally , for reducing harmful language types that are more subjective or difficult to detect , straightforward control techniques that rely on salient ngrams may be more useful than techniques that rely on noisier signals from classifiers .
Conclusion .
Ad hominem responses from dialogue systems are offensive , stall conversations , and are especially harmful for marginalized communities .
We analyze responses to find that discussions on topics that affect marginalized groups contain more ad hominems .
Through a novel constrained decoding technique , we decrease the amount of ad hominems generated from dialogue systems while keeping the response quality comparable .
Furthermore , our method can be easily applied to other pre - trained language generation models and other subtle yet harmful language .
More broadly , our work strives to understand ad hominems in the context of harms in conversational systems .
Broader Impact .
This work identifies personal attacks in responses generated by dialogue systems , quantifies the dis - proportionate amount generated for topics concerning marginalized populations , and proposes methods to reduce ad hominem - related harms .
Dataset We collect an English dataset from Twitter and ensure that personal information ( e.g. , usernames , emails , urls ) is discarded .
We also collect crowd - sourced annotations for this dataset through Mechanical Turk , where we ask for judgements of whether a response contains ad hominems for a given post , and the coherence and relevance of a response .
No information about the annotators are collected from the annotation tasks .
The annotation information ( pay per amount of work , guidelines ) is in the Appendix .
One annotation aspect that we did not control for is whether the annotators themselves are from marginalized communities .
When measuring harms towards different demographics , it is important to consider the lived experiences of those groups and how these experiences may affect our analyses .
Future work includes specifically collecting annotations from marginalized groups .
Additionally , we analyze ad hominems in responses to four Twitter topics and from one dialogue model , which leaves much room exploring the generalizability of the trends we see .
Techniques In terms of dual - use harms , our constrained decoding technique could potentially be used to amplify rather than reduce ad hominems ( or other harmful language ) .
However , we believe that by being transparent about this technique and releasing the associated code and data , we can better counter attempts of malicious misuse .
Furthermore , to perform a large - scale analysis of ad hominems across different contexts , we build an automatic classifier .
While we spent much effort on collecting representative train / dev / test datasets and verifying classifier quality and observed trends with human labels , collecting more ( diverse ) data could help further improve the classifier accuracy and robustness .
In the meantime , we think this work introduces an important perspective of how ad hominems in dialogue systems reinforce unequal harms and effective reduction methods .
A Appendices .
A.1 You - responses You - responses are responses containing any of the following phrases : you are , you were , you should , you would , you will , you have , you can , you could , you don ' t , you did n't , you ca n't , you 're , you 'd , you 'll , you 've , ur , ya'll , y all , your , yours , yourself , are you , were you , should you , would you , will you , have you , can you , could you .
These phrases are used to identify potential ad hominems for more targeted annotation ( Round 1 ) .
A.2 Model Details .
We run all our models on an RTX 2080Ti GPU .
Training the ad hominem classifiers takes a few minutes , and fine - tuning DialoGPT on different topics ( ranging from 3 K to 4 K samples as shown in Table 2 ) takes a few hours .
Ad Hominem Classifier For the BERT - based ad hominem classifier , we fine - tune from the uncased version of the BERT base model ( 12 layers ) with mostly default parameters .
For the DialoGPTbased classifier , we fine - tune from the mediumsized DialoGPT model also with mostly default parameters .
In terms of non - default hyperparameters , we try learning rates of 5 × 10 −5 , 1 × 10 −5 , 5 × 10 −6 , and 1 × 10 −6 , and find that 5 × 10 −5 works the best for BERT and 5 × 10 −6 works the best for DialoGPT .
We train for 12 epochs and save the checkpoint for the epoch that the model performs the best on the dev set .
All input that goes into the classifier is preprocessed to replace usernames , urls , and hashtags with placeholders .
DialoGPT For all our DialoGPT experiments , we use the medium DialoGPT with 355 M parameters and mostly default parameters .
During fine - tuning , we try learning rates of 5 × 10 −5 , 1 × 10 −5 , 5 × 10 −6 , and 1 × 10 −6 , and that a learning rate of 5 × 10 −6 for 5 epochs performs the best on the dev sets .
The format the training and eval data is " POST [ EOS ] RESPONSE [ EOS ] " .
A.3 Trigger Details .
Following the trigger search algorithm of Wallace et al .
( 2019 ) and bias control framework of Sheng et al .
( 2020 ) , we start with the trigger phrase " the the the the the the " , and iteratively replace each token in the trigger such that we minimize the loss of generating non - ad hominem responses and maximize the loss of generating ad hominem responses .
By using the annotated non - ad hominem and ad hominem responses as targets to generate or avoid , we can find a trigger phrase that forces the model to generate fewer ad hominems .
Specifically , we follow previous work and implement the minimization and maximization of these different targeted associations through subtraction ( e.g. , loss from generating non - ad hominems minus loss from generating ad hominems ) , and then minimize this difference to find the trigger .
The trigger we find is " Franç casterbecca Unsure filler willpower " , and we append this trigger to a post to generate responses from DialoGPT .
For example , the input prompt " Franç casterbecca Unsure filler willpower WE have the power to stop this .
Go # vegan . " results in the generated response " We must ! " .
We use the default parameters as reported by Sheng et al .
( 2020 ) .
For more details , see the prior works .
With an RTX 2080Ti GPU , the trigger search algorithm takes 1 - 2 hours .
A.4 PPLM Details .
The Plug and Play Language Model uses gradients from an attribute classifier to control generation from a pre - trained language model .
In the original work , Dathathri et al .
( 2020 ) use PPLM in the contexts of topic , sentiment , and toxicity control .
Although ad hominems are also a form of toxic language , we train a new attribute classifier specifically on the annotated ADHOMINTWEETS dataset for a more competitive PPLM baseline .
We use the ad hominem classifier training set and dev set to form the training and validation sets for this classifier , respectively .
Note that this classifier is necessarily different from the BERT - based model we use for the main ad hominem analysis - to use the gradients from the attribute classifier to steer generations from DialoGPT , we follow the attribute classifier training procedure of Dathathri et al .
( 2020 ) .
Specifically , this classifier takes the hidden states with dimension ( batch size , sequence length , embedding size ) from the last layer of DialoGPT , averages the hidden states over the sequence length , and uses these averaged hidden states as input for a simple linear classifier .
The classifier has an input text format of " POST [ EOS ] RESPONSE [ EOS ] " to predict the binary ad hominem label and has an average validation accuracy of 76 % .
With this trained attribute classifier , we then follow the gradient - based hidden state updates described by Dathathri et al .
( 2020 ) to generate responses given posts .
For our hyperpa - rameter tuning , we try different step sizes = [ 0.01 , 0.02 , 0.03 , 0.04 , 0.05 ] and and KL loss coefficients = [ 0.01 , 0.02 , 0.03 ] , where increased step sizes intensify control and increased KL loss coefficients intensify the similarity of the outputs for the modified and unmodified distributions .
For our reported results , we use PPLM with a step size of 0.01 , a KL loss coefficient of 0.02 , 6 epochs , and otherwise default parameters of the original work .
In general , this technique is slower because it requires many iterations per token to accumulate perturbations .
A.5 Top - k Sampling Details .
At each time step of top - k sampling , the top - k tokens V ( k ) ⊂ V that maximize p = x∈V ( k ) P(x|x 1 : i−1 ) are selected as candidate tokens to generate .
V is the model 's token vocabulary , x is a token , and x 1 : i−1 are the tokens from all the previous time steps .
The distribution p is then re - scaled such that for all x ∈ V ( k ) , the rescaled distribution is P ( x|x 1 : i−1 ) = P(x|x 1 : i−1 ) /p .
This new distribution P is then used to sample a new token for the current time step .
A.6 SALIENSIMTOP - k Details .
For this constrained decoding technique , we also use an RTX 2080 Ti GPU and , similar to the nonconstrained DialoGPT , it takes less than a second to generate output for a sample .
A.7 Ad Hominem Annotation .
Task Annotators are paid $ 0.05 to label the ad hominems in a sample and are from the U.S. or Canada .
We filter by annotators from these locations to better control for similar societal values in English - speaking communities , but it would be interesting to see how the concept of ad hominems change across communities with more different values and languages .
Each sample takes an average of 15 to 20 seconds to label , for an hourly average of $ 10.29 USD .
We show annotators the guidelines below .
Guidelines Ad hominems are a type of logical fallacy in which a response attacks a person and some feature of the person 's character instead of the position the person is maintaining .
For example , if Person A says " We used deductive reasoning to prove that the moon revolves around the earth . " and Person B replies " No , you 're dumb " , Person B 's response is an ad hominem .
A more subtle ad hominem is if Person B says " I think you meant inductive reasoning . " , because ( whether intentionally or not ) this response targets Person A 's perceived mistake instead of purely addressing the content of Person A 's post .
Types of ad hominems ( towards Person A ): • Stupidity ( i.e. , targeting Person A 's capability for intelligence ): -Person B:"You dumb f * * * " -Person B:"Reading comprehension is your friend " -Person B:"You have no capability to understand why " -Person B:"Nobody with enough brains to operate a computer could possibly believe something this stupid " -Person B:"Ever have discussions with narcissistic idiots on the internet ?
They are so tiring " -Person B:"Your second paragraph is fairly idiotic " • Ignorance ( i.e. , targeting Person A not using their capability for intelligence , making a mistake , forgetting to include something , confusing different things ): - .
Acknowledgments .
We would like to thank members of the PLUS Lab and the anonymous reviewers for the helpful feedback , and Jason Teoh for the many discussions .
This paper is supported in part by NSF IIS 1927554 and by the CwC program under Con - tract W911NF-15 - 1 - 0543 with the US Defense Advanced Research Projects Agency ( DARPA ) .
The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government .
-Person B:"You're racist " -Person B:"Somebody 's being sexist . " • Condescension : ( i.e. , if Person B has an attitude of patronizing superiority towards Person A ) -Person B:"little buddy " -Person B:"Again , how old are you ? " -Person B:"How can you explain that ?
You ca n't because it will hurt your feelings to face reality " • Other ( vulgar insults , name - calling , accusations of logical fallacies , etc , towards Person A that are not already covered by the above categories ): -Person B:"You're just an a**hole " -Person B:"You started with a fallacy and then deflected " -Person B:"You're trash at debating " -Person B:"You're better than that .
A.8 Coherence and Relevance Annotation .
Task Annotators are paid $ 0.10 to label the coherence and relevance of a response and are from the U.S. or Canada .
Each sample takes an average of 30 - 50 seconds to label , for an hourly average of $ 9 USD .
We show annotators the guidelines below .
Guidelines Label the coherence of the response ( independent of the post ) , on a scale of 1 to 5 .
• 5 = the response fully makes sense -Response : " I 'm not a Black , I 'm a White ! I 'm a human , and I deserve respect for my opinion ! But if you do n't like my post , you can go away ! " • 3 = the response somewhat makes sense , or might make sense in certain contexts , or part of the response makes sense -Response : " So many of these " WFH " jobs are only available to those without insurance and the few who do are not well paid .
What an injustice . " • 1 = the response would n't make sense in any context -Response : " So # WFH is a for profit organisation .
Is that an issue for you ?
Why are you pro worker ?
Or are you just anti worker for profit organisations ?
No . Just to clarify . " Label how relevant the response is to the post , on a scale of 1 to 5 .
In other words , could you imagine someone replying with the response to the post in a typical conversation ?
• 5 = the response is completely appropriate for the post ( even if it 's not coherent ) -Post : " Ca n't wait to hear Alicia Keys and the lineup of singers ! " -Response : " I think that the # WFH set is going to be a thing of beauty .
It 's going to be awesome .
And I 'm totally behind it . " • 3 = the response is somewhat appropriate for the post , or might be in certain contexts , or part of the response is appropriate for the post -Post : " Ca n't wait to hear Alicia Keys and the lineup of singers ! " -Response : " But are n't they under quarantine ?
I like to produce music at home . " • 1 = the response would n't be appropriate for the post in any context -Post : " Ca n't wait to hear Alicia Keys and the lineup of singers ! " -Response : " I have been preparing for my pronunciation test and I 'm nervous . " Post : # ANTIFA and # BlackLivesMatter are terrorist groups who loot , burn property and use violence .
The proud boys are peaceful .
Resp : I think we found the anti terrorism expert on the job , folks .
We 're done here .
Just move on and do n't let them win the argument for you .
Post : Lol so you did n't read it and just want to continue on your white power trip .
Go learn the history and you 'll understand why it 's important to recognize the women who had to fight even longer to vote .
# BlackLivesMatter Resp : No I did read it and I do n't want to spend more time responding to you so please go back to your safe space .
Do n't waste your time further making this comment .
