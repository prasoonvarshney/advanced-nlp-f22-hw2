import argparse
from curses.ascii import isalnum

if __name__ == '__main__':
    '''
    This script takes your model outputs (in CoNLL format) on
    sentence-level data (generated by paragraph_to_sentence.py), and
    removes the extra newlines to match the paragraph-segmented outputs
    that we expect.
    '''

    parser = argparse.ArgumentParser()
    parser.add_argument('-i', '--infile', default='../data/anlp-sciner-test-sentences.conll',
        help='Your model outputs on the sentence-segmented test set in CoNLL format')
    parser.add_argument('-d', '--dummyfile', default='../data/anlp-sciner-test-empty.conll',
        help='The provided empty CoNLL file, used to determine the original paragraph boundaries')
    parser.add_argument('-o', '--outfile', default='../data/output.conll',
        help='The output CoNLL file that restores paragraph segmentation')
    args = parser.parse_args()

    input_lines = []
    dummy_lines = []
    output_lines = []

    with open(args.infile) as f:
        input_lines = f.readlines()

    with open(args.dummyfile) as f:
        dummy_lines = f.readlines()

    i = 0
    j = 0
    print(len(input_lines))
    while i < len(input_lines) and j < len(dummy_lines):
        # skip whitespace if dummy file doesn't have it

        if input_lines[i] == '\n' and dummy_lines[j] != '\n':
            i += 1

        elif dummy_lines[j] == '\n' and input_lines[i] != '\n':
            output_lines.append('\n')
            j += 1

        else:
            output_lines.append(input_lines[i])
            i += 1
            j += 1

    # make sure all tokens match our dummy reference CoNLL file
    print(len(output_lines), len(dummy_lines))
    assert(len(dummy_lines) == len(output_lines))
    for i in range(len(output_lines)):
        dummy_token = dummy_lines[i].split('\t')[0]
        output_token = output_lines[i].split('\t')[0]
        if dummy_token != output_token:
            print('{} --- dummy: {} output: {}'.format(i, (dummy_token), (output_token)))
        assert(dummy_token == output_token)

    with open(args.outfile, 'w') as f:
        f.write(''.join(output_lines))