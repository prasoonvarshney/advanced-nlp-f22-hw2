We	O
also	O
thank	O
Takeru	O
Miyato	O
,	O
who	O
gave	O
us	O
valuable	O
comments	O
about	O
AdvT	B-MethodName
/	O
VAT	B-MethodName
.	O

We	O
believe	O
that	O
adversarial	B-MethodName
regularization	I-MethodName
can	O
be	O
one	O
of	O
the	O
common	O
and	O
fundamental	O
technologies	O
to	O
further	O
improve	O
the	O
translation	O
quality	O
,	O
such	O
as	O
model	B-MethodName
ensemble	I-MethodName
,	O
byte	B-MethodName
-	I-MethodName
pair	I-MethodName
encoding	I-MethodName
,	O
and	O
back	B-MethodName
-	I-MethodName
translation	I-MethodName
.	O

Additionally	O
,	O
we	O
confirmed	O
that	O
adversarial	B-MethodName
regularization	I-MethodName
techniques	O
effectively	O
worked	O
even	O
if	O
we	O
performed	O
them	O
with	O
the	O
training	O
data	O
increased	O
by	O
a	O
back	B-MethodName
-	I-MethodName
translation	I-MethodName
method	O
.	O

Our	O
experimental	O
results	O
demonstrated	O
that	O
applying	O
VAT	B-MethodName
to	O
both	O
encoder	O
and	O
decoder	O
embeddings	O
consistently	O
outperformed	O
other	O
configurations	O
.	O

This	O
paper	O
discussed	O
the	O
practical	O
usage	O
and	O
benefit	O
of	O
adversarial	B-MethodName
regularization	I-MethodName
based	O
on	O
adversarial	B-MethodName
perturbation	I-MethodName
in	O
the	O
current	O
NMT	B-TaskName
models	O
.	O

We	O
observe	O
that	O
Transformer+VAT	B-MethodName
with	O
using	O
training	O
data	O
increased	O
by	O
the	O
backtranslation	B-MethodName
method	O
seems	O
to	O
generate	O
higher	O
qual	O
-	O
ity	O
translations	O
compared	O
with	O
those	O
of	O
the	O
baseline	B-MethodName
Transformer	I-MethodName
.	O

In	O
addition	O
,	O
the	O
rows	O
+	O
VAT+AdvT	O
show	O
the	O
performance	O
obtained	O
by	O
applying	O
both	O
AdvT	B-MethodName
and	O
VAT	B-MethodName
simultaneously	O
.	O

Therefore	O
,	O
we	O
can	O
expect	O
that	O
VAT	B-MethodName
can	O
improve	O
the	O
translation	O
performance	O
on	O
other	O
datasets	O
and	O
settings	O
with	O
relatively	O
highconfidence	O
.	O

We	O
report	O
that	O
VAT	B-MethodName
did	O
not	O
require	O
us	O
to	O
perform	O
additional	O
heavy	O
hyper	O
-	O
parameter	O
search	O
(	O
excluding	O
the	O
hyper	O
-	O
parameter	O
search	O
in	O
base	O
models	O
)	O
.	O

We	O
observe	O
that	O
Transformer+VAT	B-MethodName
consistently	O
outperformed	O
the	O
baseline	B-MethodName
Transformer	I-MethodName
results	O
in	O
both	O
standard	B-MethodName
(	O
a	O
)	O
and	O
back	B-MethodName
-	I-MethodName
translation	I-MethodName
(	O
b	O
)	O
settings	O
.	O

Furthermore	O
,	O
the	O
row	O
(	O
b	O
)	O
shows	O
the	O
results	O
obtained	O
when	O
we	O
incorporated	O
pseudo	O
-	O
parallel	O
corpora	O
generated	O
using	O
the	O
back	O
-	O
translation	O
method	O
(	O
Sennrich	O
et	O
al	O
.	O
,	O
2016a	O
)	O
generating	O
the	O
pseudo	O
-	O
parallel	O
corpora	O
,	O
we	O
used	O
the	O
WMT14	B-DatasetName
news	I-DatasetName
translation	I-DatasetName
corpus	I-DatasetName
.	O

Results	O
on	O
four	O
language	O
pairs	O
Table	O
3	O
shows	O
the	O
BLEU	B-MetricName
scores	O
of	O
averaged	O
over	O
five	O
models	O
on	O
four	O
different	O
language	O
pairs	O
(	O
directions	O
)	O
,	O
namely	O
German!English	O
,	O
French!English	O
,	O
English!German	O
,	O
and	O
English!French	O
.	O

They	O
referred	O
to	O
this	O
phenomenon	O
of	O
AdvT	B-MethodName
as	O
label	O
leaking	O
.	O

(	O
2017	O
)	O
,	O
AdvT	B-MethodName
generates	O
the	O
adversarial	O
examples	O
from	O
correct	O
examples	O
,	O
and	O
thus	O
,	O
the	O
models	O
trained	O
by	O
AdvT	B-MethodName
tend	O
to	O
overfit	O
to	O
training	O
data	O
rather	O
than	O
those	O
trained	O
by	O
VAT	B-MethodName
.	O

Furthermore	O
,	O
the	O
results	O
of	O
VAT	B-MethodName
was	O
consistently	O
better	O
than	O
those	O
of	O
AdvT.	B-MethodName

Moreover	O
,	O
we	O
achieved	O
better	O
performance	O
when	O
we	O
added	O
perturbation	O
to	O
the	O
encoder	B-HyperparameterValue
-	I-HyperparameterValue
side	I-HyperparameterValue
(	O
encemb	B-HyperparameterValue
)	O
rather	O
than	O
the	O
decoder	B-HyperparameterValue
-	I-HyperparameterValue
side	I-HyperparameterValue
(	O
dec	B-HyperparameterValue
-	I-HyperparameterValue
emb	I-HyperparameterValue
)	O
.	O

Investigation	O
of	O
effective	O
configuration	O
Table	O
2	O
shows	O
the	O
experimental	O
results	O
with	O
configurations	B-HyperparameterName
of	I-HyperparameterName
perturbation	I-HyperparameterName
positions	I-HyperparameterName
(	O
enc	B-HyperparameterValue
-	I-HyperparameterValue
emb	I-HyperparameterValue
,	O
decemb	B-HyperparameterValue
,	O
or	O
enc	B-HyperparameterValue
-	I-HyperparameterValue
dec	I-HyperparameterValue
-	I-HyperparameterValue
emb	I-HyperparameterValue
)	O
and	O
adversarial	B-MethodName
regularization	I-MethodName
techniques	O
(	O
AdvT	B-MethodName
or	O
VAT	B-MethodName
)	O
.	O

Firstly	O
,	O
in	O
terms	O
of	O
the	O
effective	B-HyperparameterName
perturbation	I-HyperparameterName
position	I-HyperparameterName
,	O
enc	B-HyperparameterValue
-	I-HyperparameterValue
dec	I-HyperparameterValue
-	I-HyperparameterValue
emb	I-HyperparameterValue
configurations	O
,	O
which	O
add	O
perturbations	O
to	O
both	O
encoder	O
and	O
decoder	O
embeddings	O
,	O
consistently	O
outperformed	O
other	O
configurations	O
,	O
which	O
used	O
either	O
encoder	O
or	O
decoder	O
only	O
.	O

Note	O
that	O
all	O
reported	O
BLEU	B-MetricName
scores	O
are	O
averaged	O
over	O
five	O
models	O
.	O

As	O
evaluation	O
metrics	O
,	O
we	O
used	O
BLEU	B-MetricName
scores	O
(	O
Papineni	O
et	O
al	O
.	O
,	O
2002	O
)	O
6	O
.	O

We	O
set	O
=	O
1	B-HyperparameterValue
and	O
✏	B-HyperparameterName
=	O
1	B-HyperparameterValue
for	O
all	O
AdvT	B-MethodName
and	O
VAT	B-MethodName
experiments	O
.	O

Hereafter	O
,	O
we	O
refer	O
to	O
the	O
model	O
trained	O
with	O
the	O
adversarial	B-MethodName
regularization	I-MethodName
(	O
`	O
in	O
Eq	O
.	O
7	O
)	O
as	O
AdvT	B-MethodName
,	O
and	O
similarly	O
,	O
with	O
the	O
virtual	B-MethodName
adversarial	I-MethodName
training	I-MethodName
(	O
`	O
K	O
L	O
in	O
Eq	O
.	O
11	O
)	O
as	O
VAT	B-MethodName
.	O

(	O
2015	O
)	O
and	O
self	B-MethodName
-	I-MethodName
attentionbased	I-MethodName
encoder	I-MethodName
-	I-MethodName
decoder	I-MethodName
,	O
the	O
so	O
-	O
called	O
Transformer	B-MethodName
(	O
Vaswani	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

We	O
selected	O
two	O
widely	O
used	O
model	O
architectures	O
,	O
namely	O
,	O
LSTM	B-MethodName
-	I-MethodName
based	I-MethodName
encoder	I-MethodName
-	I-MethodName
decoder	I-MethodName
2	O
https://github.com/moses-smt/	O
mosesdecoder	O
/	O
blob	O
/	O
master	O
/	O
scripts/	O
tokenizer	O
/	O
tokenizer.perl	O
3	O
https://github.com/moses-smt/	O
mosesdecoder	O
/	O
blob	O
/	O
master	O
/	O
scripts/	O
recaser	O
/	O
truecase.perl	O
4	O
https://github.com/rsennrich/	O
subword	O
-	O
nmt	O
used	O
in	O
Luong	O
et	O
al	O
.	O

For	O
preprocessing	O
of	O
our	O
experimental	O
datasets	O
,	O
we	O
used	O
the	O
Moses	B-MethodName
tokenizer	I-MethodName
2	O
and	O
the	O
truecaser	B-MethodName
3	O
.	O

We	O
used	O
the	O
IWSLT	B-DatasetName
2016	I-DatasetName
training	O
set	O
for	O
training	O
models	O
,	O
2012	B-DatasetName
test	O
set	O
(	O
test2012	O
)	O
as	O
the	O
development	O
set	O
,	O
and	O
2013	B-DatasetName
and	O
2014	B-DatasetName
test	O
sets	O
(	O
test2013	O
and	O
test2014	O
)	O
as	O
our	O
test	O
sets	O
.	O

We	O
conducted	O
experiments	O
on	O
the	O
IWSLT	B-DatasetName
evaluation	I-DatasetName
campaign	I-DatasetName
dataset	O
(	O
Cettolo	O
et	O
al	O
.	O
,	O
2012	O
)	O
.	O

Finally	O
,	O
we	O
have	O
three	O
options	O
for	O
applying	O
the	O
perturbation	O
into	O
typical	O
NMT	B-TaskName
models	O
,	O
namely	O
,	O
applying	O
the	O
perturbation	O
into	O
embeddings	O
in	O
the	O
(	O
1	O
)	O
encoder	O
-	O
side	O
only	O
,	O
(	O
2	O
)	O
decoder	O
-	O
side	O
only	O
,	O
and	O
(	O
3	O
)	O
both	O
encoder	O
and	O
decoder	O
sides	O
.	O

For	O
example	O
,	O
let	O
r0	O
j	O
2	O
R	O
D	O
be	O
an	O
adversarial	B-MethodName
perturbation	I-MethodName
vector	O
for	O
the	O
j	O
-	O
th	O
word	O
in	O
output	O
Y	O
.	O

This	O
fact	O
immediately	O
offers	O
us	O
also	O
to	O
consider	O
applying	O
the	O
adversarial	B-MethodName
perturbation	I-MethodName
into	O
the	O
decoder	O
-	O
side	O
embeddings	O
f	O
j	O
.	O

However	O
,	O
NMT	B-TaskName
models	O
generally	O
have	O
another	O
embedding	O
layer	O
in	O
the	O
decoder	O
-	O
side	O
,	O
as	O
we	O
explained	O
in	O
Eq	O
.	O
2	O
.	O

Adversarial	B-MethodName
Regularization	I-MethodName
in	O
NMT	B-TaskName
.	O

It	O
is	O
worth	O
noting	O
here	O
that	O
,	O
in	O
our	O
experiments	O
,	O
we	O
never	O
applied	O
the	O
semi	O
-	O
supervised	O
learning	O
,	O
but	O
used	O
the	O
above	O
equation	O
for	O
calculating	O
perturbation	O
as	O
the	O
replacement	O
of	O
standard	O
adversarial	B-MethodName
regularization	I-MethodName
.	O

`	O
KL	O
(	O
X	O
,	O
r	O
,	O
•	O
,	O
⇥	O
)	O
=	O
KL	O
p(•	O
|X	O
,	O
⇥	O
)	O
||p(•	O
|X	O
,	O
r	O
,	O
⇥	O
)	O
,	O
(	O
11	O
)	O
where	O
KL(•||•	O
)	O
denotes	O
the	O
KL	B-MetricName
divergence	I-MetricName
.	O

This	O
section	O
briefly	O
describes	O
the	O
adversarial	B-MethodName
regularization	I-MethodName
technique	O
applied	O
to	O
the	O
text	B-TaskName
classification	I-TaskName
tasks	O
proposed	O
in	O
Miyato	O
et	O
al	O
.	O

Adversarial	B-MethodName
Regularization	I-MethodName
.	O

We	O
generally	O
use	O
a	O
K	B-MethodName
-	I-MethodName
best	I-MethodName
beam	I-MethodName
search	I-MethodName
to	O
generate	O
an	O
output	O
sentence	O
with	O
the	O
(	O
approximated	O
)	O
K	O
-	O
highest	O
probability	O
given	O
input	O
sentence	O
X	O
in	O
the	O
generation	O
(	O
test	O
)	O
phase	O
.	O

For	O
training	O
,	O
we	O
generally	O
seek	O
the	O
optimal	O
parameters	O
⇥	O
that	O
can	O
minimize	O
the	O
following	O
optimization	O
problem	O
:	O
⇥	O
=	O
argmin	O
⇥	O
J	O
(	O
D	O
,	O
⇥	O
)	O
,	O
(	O
3	O
)	O
J	O
(	O
D	O
,	O
⇥	O
)	O
=	O
1	O
|D|	O
X	O
(	O
X	O
,	O
Y	O
)	O
2D	O
`	O
(	O
X	O
,	O
Y	O
,	O
⇥	O
)	O
,	O
(	O
4	O
)	O
`	O
(	O
X	O
,	O
Y	O
,	O
⇥	O
)	O
=	O
log	O
p(Y	O
|X	O
,	O
⇥	O
)	O
,	O
(	O
5	O
)	O
where	O
⇥	O
represents	O
a	O
set	O
of	O
trainable	O
parameters	O
in	O
the	O
NMT	B-TaskName
model	O
.	O

Thus	O
,	O
the	O
NMT	B-TaskName
model	O
approximates	O
the	O
following	O
conditional	O
probability	O
:	O
p(Y	O
|X	O
)	O
=	O
Y	O
J+1	O
j=1	O
p(y	O
j	O
|y	O
0	O
:	O
j	O
1	O
,	O
X),(1	O
)	O
where	O
y	O
0	O
and	O
y	O
J+1	O
represent	O
one	O
-	O
hot	O
vectors	O
of	O
special	O
beginning	O
-	O
of	O
-	O
sentence	O
(	O
BOS	O
)	O
and	O
end	O
-	O
ofsentence	O
(	O
EOS	O
)	O
tokens	O
,	O
respectively	O
,	O
and	O
X	O
=	O
x	O
1	O
:	O
I	O
and	O
Y	O
=	O
y	O
1	O
:	O
J+1	O
.	O

To	O
explain	O
the	O
NMT	B-TaskName
model	O
concisely	O
,	O
we	O
assume	O
that	O
its	O
input	O
and	O
output	O
are	O
both	O
sequences	O
of	O
one	O
-	O
hot	O
vectors	O
x	O
1	O
:	O
I	O
and	O
y	O
1	O
:	O
J	O
that	O
correspond	O
to	O
input	O
and	O
output	O
sentences	O
whose	O
lengths	O
are	O
I	O
and	O
J	O
,	O
respectively	O
.	O

Model	O
Definition	O
In	O
general	O
,	O
an	O
NMT	B-TaskName
model	O
receives	O
a	O
sentence	O
as	O
input	O
and	O
returns	O
a	O
corresponding	O
(	O
translated	O
)	O
sentence	O
as	O
output	O
.	O

Neural	B-TaskName
Machine	I-TaskName
Translation	I-TaskName
Model	O
.	O

(	O
2018	O
)	O
used	O
virtual	B-MethodName
adversarial	I-MethodName
training	I-MethodName
(	O
VAT	B-MethodName
)	O
,	O
which	O
is	O
a	O
semi	O
-	O
supervised	O
extension	O
of	O
the	O
adversarial	B-MethodName
regularization	I-MethodName
technique	O
originally	O
proposed	O
in	O
Miyato	O
et	O
al	O
.	O

We	O
investigate	O
the	O
effectiveness	O
of	O
the	O
several	O
practical	O
configurations	O
that	O
have	O
not	O
been	O
examined	O
in	O
their	O
paper	O
,	O
such	O
as	O
the	O
combinations	O
with	O
VAT	B-MethodName
and	O
back	B-MethodName
-	I-MethodName
translation	I-MethodName
.	O

They	O
also	O
demonstrated	O
the	O
impacts	O
of	O
the	O
adversarial	B-MethodName
regularization	I-MethodName
technique	O
in	O
NMT	B-TaskName
models	O
.	O

(	O
2019	O
)	O
also	O
investigated	O
the	O
effectiveness	O
of	O
the	O
adversarial	B-MethodName
regularization	I-MethodName
technique	O
in	O
neural	B-TaskName
language	I-TaskName
modeling	I-TaskName
and	O
NMT	B-TaskName
.	O

Namely	O
,	O
they	O
focused	O
on	O
sequential	O
labeling	O
,	O
whereas	O
we	O
discuss	O
NMT	B-TaskName
models	O
.	O

However	O
,	O
the	O
main	O
focus	O
of	O
these	O
methods	O
is	O
the	O
incorporation	O
of	O
adversarial	O
examples	O
in	O
the	O
training	O
phase	O
,	O
which	O
is	O
orthogonal	O
to	O
our	O
attention	O
,	O
adversarial	B-MethodName
regularization	I-MethodName
,	O
as	O
described	O
in	O
Section	O
1	O
.	O

We	O
investigate	O
the	O
effectiveness	O
of	O
several	O
possible	O
configurations	O
that	O
can	O
significantly	O
and	O
consistently	O
improve	O
the	O
performance	O
of	O
typical	O
baseline	O
NMT	B-TaskName
models	O
,	O
such	O
as	O
LSTM	O
-	O
based	O
and	O
Transformer	O
-	O
based	O
models	O
,	O
.	O

Therefore	O
,	O
the	O
goal	O
of	O
this	O
paper	O
is	O
to	O
re	O
-	O
veal	O
the	O
effectiveness	O
of	O
the	O
adversarial	B-MethodName
regularization	I-MethodName
in	O
NMT	B-TaskName
models	O
and	O
encourage	O
researchers	O
/	O
developers	O
to	O
apply	O
the	O
adversarial	B-MethodName
regularization	I-MethodName
as	O
a	O
common	O
technique	O
for	O
further	O
improving	O
the	O
performance	O
of	O
their	O
NMT	B-TaskName
models	O
.	O

Figure	O
1	O
illustrates	O
the	O
model	O
architecture	O
of	O
NMT	B-TaskName
models	O
with	O
adversarial	O
perturbation	O
.	O

Unfortunately	O
,	O
this	O
application	O
is	O
not	O
fully	O
trivial	O
since	O
we	O
potentially	O
have	O
several	O
configurations	O
for	O
applying	O
adversarial	O
perturbations	O
into	O
NMT	B-TaskName
models	O
(	O
see	O
details	O
in	O
Section	O
5	O
)	O
.	O

We	O
aim	O
to	O
further	O
leverage	O
this	O
promising	O
methodology	O
into	O
more	O
sophisticated	O
and	O
critical	O
neural	O
models	O
,	O
i.e.	O
,	O
neural	B-TaskName
machine	I-TaskName
translation	I-TaskName
(	O
NMT	B-TaskName
)	O
models	O
,	O
since	O
NMT	B-TaskName
models	O
recently	O
play	O
one	O
of	O
the	O
central	O
roles	O
in	O
the	O
NLP	O
research	O
community	O
;	O
NMT	B-TaskName
models	O
have	O
been	O
widely	O
utilized	O
for	O
not	O
only	O
NMT	B-TaskName
but	O
also	O
many	O
other	O
NLP	O
tasks	O
,	O
such	O
as	O
text	B-TaskName
summarization	I-TaskName
(	O
Rush	O
et	O
al	O
.	O
,	O
2015;Chopra	O
et	O
al	O
.	O
,	O
2016	O
)	O
,	O
grammatical	B-TaskName
error	I-TaskName
correction	I-TaskName
(	O
Ji	O
et	O
al	O
.	O
,	O
2017	O
)	O
,	O
dialog	B-TaskName
generation	I-TaskName
(	O
Shang	O
et	O
al	O
.	O
,	O
2015	O
)	O
,	O
and	O
parsing	B-TaskName
(	O
Vinyals	O
et	O
al	O
.	O
,	O
2015;Suzuki	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

We	O
refer	O
to	O
this	O
regularization	O
technique	O
as	O
adversarial	B-MethodName
regularization	I-MethodName
.	O

The	O
key	O
idea	O
of	O
their	O
success	O
is	O
to	O
apply	O
adversarial	O
perturbations	O
into	O
the	O
input	O
embedding	O
layer	O
instead	O
of	O
the	O
inputs	O
themselves	O
as	O
used	O
in	O
image	B-TaskName
processing	I-TaskName
tasks	O
.	O

and	O
reported	O
excellent	O
performance	O
improvements	O
on	O
multiple	O
benchmark	O
datasets	O
of	O
text	B-TaskName
classification	I-TaskName
task	O
.	O

(	O
2017	O
)	O
overcame	O
this	O
problem	O
1	O
Our	O
code	O
for	O
replicating	O
the	O
experiments	O
in	O
this	O
paper	O
is	O
available	O
at	O
the	O
following	O
URL	O
:	O
https://github.com/	O
pfnet	O
-	O
research	O
/	O
vat_nmt	O
Encoder	O
Decoder	O
!	O
"	O
#	O
$	O
"	O
!	O
%	O
#	O
$	O
%	O
!	O
&	O
#	O
$	O
&	O
'	O
(	O
#	O
$	O
(	O
)	O
'	O
"	O
#	O
$	O
"	O
)	O
'	O
*	O
#	O
$	O
+	O
)	O
,	O
"	O
,	O
%	O
,	O
+	O
-	O
"	O
Figure	O
1	O
:	O
An	O
intuitive	O
sketch	O
that	O
explains	O
how	O
we	O
add	O
adversarial	O
perturbations	O
to	O
a	O
typical	O
NMT	B-TaskName
model	O
structure	O
for	O
adversarial	B-MethodName
regularization	I-MethodName
.	O

Thus	O
,	O
this	O
paper	O
investigates	O
the	O
effectiveness	O
of	O
several	O
possible	O
configurations	O
of	O
applying	O
the	O
adversarial	O
perturbation	O
and	O
reveals	O
that	O
the	O
adversarial	B-MethodName
regularization	I-MethodName
technique	O
can	O
significantly	O
and	O
consistently	O
improve	O
the	O
performance	O
of	O
widely	O
used	O
NMT	B-TaskName
models	O
,	O
such	O
as	O
LSTMbased	O
and	O
Transformer	O
-	O
based	O
models	O
.	O

We	O
aim	O
to	O
further	O
leverage	O
this	O
promising	O
methodology	O
into	O
more	O
sophisticated	O
and	O
critical	O
neural	O
models	O
in	O
the	O
natural	O
language	O
processing	O
field	O
,	O
i.e.	O
,	O
neural	B-TaskName
machine	I-TaskName
translation	I-TaskName
(	O
NMT	B-TaskName
)	O
models	O
.	O

A	O
regularization	O
technique	O
based	O
on	O
adversarial	B-MethodName
perturbation	I-MethodName
,	O
which	O
was	O
initially	O
developed	O
in	O
the	O
field	O
of	O
image	O
processing	O
,	O
has	O
been	O
successfully	O
applied	O
to	O
text	B-TaskName
classification	I-TaskName
tasks	O
and	O
has	O
yielded	O
attractive	O
improvements	O
.	O

Effective	O
Adversarial	B-MethodName
Regularization	I-MethodName
for	O
Neural	B-TaskName
Machine	I-TaskName
Translation	I-TaskName
.	O

We	O
thank	O
three	O
anonymous	O
reviewers	O
for	O
their	O
helpful	O
comments	O
.	O

Acknowledgments	O
.	O

Conclusion	O
.	O

Actual	O
Translation	O
Examples	O
Table	O
.	O

4	O
shows	O
actual	O
translation	O
examples	O
generated	O
by	O
the	O
models	O
compared	O
in	O
our	O
German!English	O
translation	O
setting	O
.	O

We	O
can	O
further	O
improve	O
the	O
performance	O
in	O
some	O
cases	O
,	O
but	O
the	O
improvement	O
is	O
not	O
consistent	O
among	O
the	O
datasets	O
.	O

As	O
discussed	O
in	O
Kurakin	O
et	O
al	O
.	O

(	O
2016	O
)	O
.	O

This	O
tendency	O
was	O
also	O
observed	O
in	O
the	O
results	O
reported	O
by	O
Miyato	O
et	O
al	O
.	O

Results	O
.	O

We	O
adapted	O
the	O
hyper	O
-	O
parameters	O
based	O
on	O
the	O
several	O
recent	O
previous	O
papers	O
5	O
.	O

Model	O
Configurations	O
.	O

We	O
also	O
applied	O
the	O
byte	O
-	O
pair	O
encoding	O
(	O
BPE	O
)	O
based	O
subword	O
splitting	O
script	O
4	O
with	O
16,000	O
merge	O
operations	O
(	O
Sennrich	O
et	O
al	O
.	O
,	O
2016b	O
)	O
.	O

We	O
removed	O
sentences	O
over	O
50	O
words	O
from	O
the	O
training	O
set	O
.	O

Table	O
1	O
shows	O
the	O
statistics	O
of	O
datasets	O
used	O
in	O
our	O
experiments	O
.	O

Datasets	O
.	O

6	O
Experiments	O
.	O

In	O
addition	O
,	O
we	O
need	O
to	O
slightly	O
modify	O
the	O
definition	O
of	O
r	O
,	O
which	O
is	O
originally	O
the	O
concatenation	O
vector	O
of	O
all	O
r	O
i	O
for	O
all	O
i	O
,	O
to	O
the	O
concatenation	O
vector	O
of	O
all	O
r	O
i	O
and	O
r	O
0	O
j	O
for	O
all	O
i	O
and	O
j.	O

The	O
perturbed	O
embedding	O
f	O
0	O
j	O
2	O
R	O
D	O
is	O
computed	O
for	O
each	O
decoder	O
time	O
-	O
step	O
j	O
as	O
follows	O
:	O
f	O
0	O
j	O
=	O
F	O
y	O
j	O
1	O
+	O
r0	O
j	O
.(12	O
)	O
Then	O
similar	O
to	O
Eq	O
.	O
8	O
,	O
we	O
can	O
calculate	O
r0	O
as	O
:	O
r0	O
j	O
=	O
✏	O
b	O
j	O
||b||	O
2	O
,	O
b	O
j	O
=	O
r	O
f	O
j	O
`	O
(	O
X	O
,	O
Y	O
,	O
⇥	O
)	O
,	O
(	O
13	O
)	O
where	O
b	O
is	O
a	O
concatenated	O
vector	O
of	O
b	O
j	O
for	O
all	O
j.	O

As	O
strictly	O
following	O
the	O
original	O
definition	O
of	O
the	O
conventional	O
adversarial	O
training	O
,	O
the	O
straightforward	O
approach	O
to	O
applying	O
the	O
adversarial	O
perturbation	O
is	O
to	O
add	O
the	O
perturbation	O
into	O
the	O
encoderside	O
embeddings	O
e	O
i	O
as	O
described	O
in	O
Eq	O
.	O
6	O
.	O

This	O
means	O
that	O
the	O
training	O
data	O
is	O
identical	O
in	O
both	O
settings	O
.	O

(	O
9	O
)	O
Finally	O
,	O
we	O
jointly	O
minimize	O
the	O
objective	O
functions	O
J	O
(	O
D	O
,	O
⇥	O
)	O
and	O
A(D	O
,	O
⇥	O
):	O
⇥	O
=	O
argmin	O
⇥	O
n	O
J	O
(	O
D	O
,	O
⇥	O
)	O
+	O
A(D	O
,	O
⇥	O
)	O
o	O
,	O
(	O
10	O
)	O
where	O
is	O
a	O
scalar	O
hyper	O
-	O
parameter	O
that	O
controls	O
the	O
balance	O
of	O
the	O
two	O
loss	O
functions	O
.	O

(	O
8)	O
Thus	O
,	O
based	O
on	O
adversarial	O
perturbation	O
r	O
,	O
the	O
loss	O
function	O
can	O
be	O
defined	O
as	O
:	O
A(D	O
,	O
⇥	O
)	O
=	O
1	O
|D|	O
X	O
(	O
X	O
,	O
Y	O
)	O
2D	O
`	O
(	O
X	O
,	O
r	O
,	O
Y	O
,	O
⇥	O
)	O
.	O

This	O
approximation	O
method	O
induces	O
the	O
following	O
non	O
-	O
iterative	O
solution	O
for	O
calculating	O
ri	O
for	O
all	O
encoder	O
time	O
-	O
step	O
i	O
:	O
ri	O
=	O
✏	O
a	O
i	O
||a||	O
2	O
,	O
a	O
i	O
=	O
r	O
e	O
i	O
`	O
(	O
X	O
,	O
Y	O
,	O
⇥	O
)	O
.	O

(	O
2015	O
)	O
,	O
where	O
`	O
(	O
X	O
,	O
Y	O
,	O
r	O
,	O
⇥	O
)	O
is	O
linearized	O
around	O
X.	O

As	O
a	O
solution	O
,	O
an	O
approximation	O
method	O
was	O
proposed	O
by	O
Goodfellow	O
et	O
al	O
.	O

However	O
,	O
it	O
is	O
generally	O
infeasible	O
to	O
exactly	O
estimate	O
r	O
in	O
Eq	O
.	O
7	O
for	O
deep	O
neural	O
models	O
.	O

Here	O
,	O
`	O
(	O
X	O
,	O
r	O
,	O
Y	O
,	O
⇥	O
)	O
represents	O
an	O
extension	O
of	O
Eq	O
.	O
5	O
,	O
where	O
the	O
perturbation	O
r	O
i	O
in	O
r	O
is	O
applied	O
to	O
the	O
position	O
of	O
ri	O
as	O
described	O
in	O
Eq	O
.	O
6	O
.	O

To	O
obtain	O
the	O
worst	O
case	O
perturbations	O
as	O
an	O
adversarial	O
perturbation	O
in	O
terms	O
of	O
minimizing	O
the	O
log	O
-	O
likelihood	O
of	O
given	O
X	O
,	O
we	O
seek	O
the	O
optimal	O
solution	O
r	O
by	O
maximizing	O
the	O
following	O
equation	O
:	O
r	O
=	O
argmax	O
r,||r||	O
✏	O
n	O
`	O
(	O
X	O
,	O
r	O
,	O
Y	O
,	O
⇥	O
)	O
o	O
,	O
(	O
7	O
)	O
where	O
✏	O
is	O
a	O
scalar	O
hyper	O
-	O
parameter	O
that	O
controls	O
the	O
norm	O
of	O
the	O
perturbation	O
,	O
and	O
r	O
represents	O
a	O
concatenated	O
vector	O
of	O
r	O
i	O
for	O
all	O
i.	O

Adversarial	O
Training	O
(	O
AdvT	O
)	O
.	O

(	O
6	O
)	O
.	O

The	O
perturbed	O
input	O
embedding	O
e	O
0	O
i	O
2	O
R	O
D	O
is	O
computed	O
for	O
each	O
encoder	O
time	O
-	O
step	O
i	O
as	O
follows	O
:	O
e	O
0	O
i	O
=	O
Ex	O
i	O
+	O
ri	O
.	O

Let	O
ri	O
2	O
R	O
D	O
be	O
an	O
adversarial	O
perturbation	O
vector	O
for	O
the	O
i	O
-	O
th	O
word	O
in	O
input	O
X.	O

(	O
2017	O
)	O
.	O

We	O
omit	O
to	O
explain	O
this	O
part	O
in	O
detail	O
as	O
our	O
focus	O
is	O
a	O
regularization	O
technique	O
that	O
is	O
independent	O
of	O
the	O
generation	O
phase	O
.	O

Generation	O
Phase	O
.	O

Training	O
Phase	O
Let	O
D	O
be	O
the	O
training	O
data	O
consisting	O
of	O
a	O
set	O
of	O
pairs	O
of	O
X	O
n	O
and	O
Y	O
n	O
,	O
namely	O
,	O
D	O
=	O
{	O
(	O
X	O
n	O
,	O
Y	O
n	O
)	O
}	O
N	O
n=1	O
,	O
where	O
N	O
represents	O
the	O
amount	O
of	O
training	O
data	O
.	O

Thus	O
,	O
p(y	O
j	O
|y	O
0	O
:	O
j	O
1	O
,	O
X	O
)	O
in	O
Eq	O
.	O
1	O
is	O
calculated	O
as	O
follows	O
:	O
p(y	O
j	O
|y	O
0	O
:	O
j	O
1	O
,	O
X	O
)	O
=	O
AttDec	O
f	O
j	O
,	O
h	O
1	O
:	O
I	O
,	O
h	O
1	O
:	O
I	O
=	O
Enc(e	O
1	O
:	O
I	O
)	O
,	O
f	O
j	O
=	O
F	O
y	O
j	O
1	O
,	O
e	O
i	O
=	O
Ex	O
i	O
,	O
(	O
2	O
)	O
where	O
Enc(•	O
)	O
and	O
AttDec(•	O
)	O
represent	O
functions	O
that	O
abstract	O
the	O
entire	O
encoder	O
and	O
decoder	O
(	O
with	O
an	O
attention	O
mechanism	O
)	O
procedures	O
,	O
respectively	O
.	O

Let	O
E	O
2	O
R	O
D	O
⇥	O
|Vs|	O
and	O
F	O
2	O
R	O
D	O
⇥	O
|Vt|	O
be	O
the	O
encoder	O
and	O
decoder	O
embedding	O
matrices	O
,	O
respectively	O
,	O
where	O
D	O
is	O
the	O
dimension	O
of	O
the	O
embedding	O
vectors	O
.	O

,	O
x	O
j	O
)	O
.	O

Here	O
,	O
we	O
introduce	O
a	O
short	O
notation	O
x	O
i	O
:	O
j	O
for	O
representing	O
a	O
sequence	O
of	O
vectors	O
(	O
x	O
i	O
,	O
.	O

x	O
i	O
and	O
y	O
j	O
denote	O
the	O
one	O
-	O
hot	O
vectors	O
of	O
the	O
i	O
-	O
th	O
and	O
j	O
-	O
th	O
to	O
-	O
kens	O
in	O
input	O
and	O
output	O
sentences	O
,	O
respectively	O
,	O
i.e.	O
x	O
i	O
2	O
{	O
0	O
,	O
1	O
}	O
|Vs|	O
and	O
y	O
j	O
2	O
{	O
0	O
,	O
1	O
}	O
|Vt|	O
.	O

Let	O
V	O
s	O
and	O
V	O
t	O
represent	O
the	O
vocabularies	O
of	O
the	O
input	O
and	O
output	O
sentences	O
,	O
respectively	O
.	O

In	O
parallel	O
to	O
our	O
work	O
,	O
Wang	O
et	O
al	O
.	O

Therefore	O
,	O
the	O
focus	O
of	O
the	O
neural	O
models	O
differs	O
from	O
this	O
paper	O
.	O

(	O
2016	O
)	O
,	O
in	O
their	O
experiments	O
to	O
compare	O
the	O
results	O
with	O
those	O
of	O
their	O
proposed	O
method	O
.	O

Clark	O
et	O
al	O
.	O

They	O
utilized	O
the	O
generated	O
(	O
input	O
)	O
sentences	O
as	O
additional	O
training	O
data	O
.	O

(	O
2017	O
)	O
proposed	O
methods	O
that	O
generate	O
input	O
sentences	O
with	O
random	O
character	O
swaps	O
.	O

For	O
example	O
,	O
Belinkov	O
and	O
Bisk	O
(	O
2018	O
)	O
;	O
Hosseini	O
et	O
al	O
.	O

Several	O
studies	O
have	O
recently	O
applied	O
adversarial	O
training	O
to	O
NLP	O
tasks	O
,	O
e.g.	O
,	O
(	O
Jia	O
and	O
Liang	O
,	O
2017;Belinkov	O
and	O
Bisk	O
,	O
2018;Hosseini	O
et	O
al	O
.	O
,	O
2017;Samanta	O
and	O
Mehta	O
,	O
2017;Miyato	O
et	O
al	O
.	O
,	O
2017;Sato	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

Related	O
Work	O
.	O

An	O
important	O
implication	O
of	O
their	O
study	O
is	O
that	O
their	O
method	O
can	O
be	O
interpreted	O
as	O
a	O
regularization	O
method	O
,	O
and	O
thus	O
,	O
they	O
do	O
not	O
focus	O
on	O
generating	O
adversarial	O
examples	O
.	O

Moreover	O
,	O
those	O
of	O
ri	O
and	O
r0	O
j	O
are	O
in	O
Eq	O
.	O
8	O
and	O
13	O
,	O
respectively	O
.	O

The	O
definitions	O
of	O
e	O
i	O
and	O
f	O
j	O
can	O
be	O
found	O
in	O
Eq	O
.	O
2	O
.	O

Recently	O
,	O
Miyato	O
et	O
al	O
.	O

Since	O
it	O
is	O
unreasonable	O
to	O
add	O
a	O
small	O
perturbation	O
to	O
the	O
symbols	O
,	O
applying	O
the	O
idea	O
of	O
adversarial	O
training	O
to	O
NLP	O
tasks	O
has	O
been	O
recognized	O
as	O
a	O
challenging	O
problem	O
.	O

In	O
the	O
field	O
of	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
,	O
the	O
input	O
is	O
a	O
sequence	O
of	O
discrete	O
symbols	O
,	O
such	O
as	O
words	O
or	O
sentences	O
.	O

This	O
learning	O
framework	O
is	O
referred	O
to	O
as	O
adversarial	O
training	O
.	O

(	O
2015	O
)	O
proposed	O
a	O
learning	O
framework	O
that	O
simultaneously	O
leverages	O
adversarial	O
examples	O
as	O
additional	O
training	O
data	O
for	O
reducing	O
the	O
prediction	O
errors	O
.	O

Subsequently	O
,	O
Goodfellow	O
et	O
al	O
.	O

Such	O
perturbed	O
inputs	O
are	O
often	O
referred	O
to	O
as	O
adversarial	O
examples	O
in	O
the	O
literature	O
.	O

The	O
existence	O
of	O
(	O
small	O
)	O
perturbations	O
that	O
induce	O
a	O
critical	O
prediction	O
error	O
in	O
machine	O
learning	O
models	O
was	O
first	O
discovered	O
and	O
discussed	O
in	O
the	O
field	O
of	O
image	O
processing	O
(	O
Szegedy	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

Introduction	O
.	O

1	O
.	O

However	O
,	O
it	O
is	O
not	O
trivial	O
to	O
apply	O
this	O
methodology	O
to	O
such	O
models	O
.	O

