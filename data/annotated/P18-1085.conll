The	O
authors	O
would	O
like	O
to	O
thank	O
Chuck	O
Rosenberg	O
,	O
Tom	O
Duerig	O
,	O
Neil	O
Alldrin	O
,	O
Zhen	O
Li	O
,	O
Filipe	O
Gonc	O
Â¸alves	O
,	O
Mia	O
Chen	O
,	O
Zhifeng	O
Chen	O
,	O
Samy	O
Bengio	O
,	O
Yu	O
Zhang	O
,	O
Kevin	O
Swersky	O
,	O
Felix	O
Hill	O
and	O
the	O
ACL	O
anonymous	O
reviewers	O
for	O
their	O
valuable	O
advice	O
and	O
feedback	O
.	O

Acknowledgments	O
.	O

We	O
expect	O
that	O
integrating	O
Picturebook	B-MethodName
with	O
these	O
embeddings	O
to	O
lead	O
to	O
further	O
performance	O
improvements	O
as	O
well	O
.	O

Recently	O
,	O
contextualized	O
word	O
representations	O
have	O
shown	O
promising	O
improvements	O
when	O
combined	O
with	O
existing	O
embeddings	O
(	O
Melamud	O
et	O
al	O
.	O
,	O
2016;Peters	O
et	O
al	O
.	O
,	O
2017;McCann	O
et	O
al	O
.	O
,	O
2017;Peters	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

In	O
future	O
work	O
,	O
we	O
would	O
like	O
to	O
explore	O
other	O
aspects	O
of	O
search	O
engines	O
for	O
language	O
grounding	O
as	O
well	O
as	O
the	O
effect	O
these	O
embeddings	O
may	O
have	O
on	O
learning	O
generic	O
sentence	O
representations	O
(	O
Kiros	O
et	O
al	O
.	O
,	O
2015b;Hill	O
et	O
al	O
.	O
,	O
2016;Conneau	O
et	O
al	O
.	O
,	O
2017a;Logeswaran	O
and	O
Lee	O
,	O
2018	O
)	O
.	O

Through	O
the	O
use	O
of	O
multimodal	O
gating	O
,	O
our	O
models	O
lead	O
to	O
interpretable	O
weightings	O
of	O
abstract	O
vs	O
concrete	O
words	O
.	O

In	O
this	O
work	O
we	O
demonstrated	O
that	O
Picturebook	B-MethodName
complements	O
traditional	O
embeddings	O
on	O
a	O
wide	O
variety	O
of	O
tasks	O
.	O

Picturebook	B-MethodName
embeddings	O
offer	O
an	O
alternative	O
approach	O
to	O
constructing	O
word	O
representations	O
grounded	O
in	O
image	O
search	O
engines	O
.	O

Traditionally	O
,	O
word	O
representations	O
have	O
been	O
built	O
on	O
co	O
-	O
occurrences	O
of	O
neighbouring	O
words	O
;	O
and	O
such	O
representations	O
only	O
make	O
use	O
of	O
the	O
statistics	O
of	O
the	O
text	O
distribution	O
.	O

Conclusion	O
.	O

We	O
also	O
observe	O
tags	O
which	O
are	O
exclusively	O
Glove	B-MethodName
oriented	O
,	O
namely	O
adverbs	O
(	O
RB	O
)	O
,	O
prepositions	O
(	O
IN	O
)	O
and	O
determiners	O
(	O
DT	O
)	O
.	O

The	O
highest	O
scoring	O
Picturebook	B-MethodName
words	O
are	O
almost	O
all	O
singular	O
and	O
plural	O
nouns	O
(	O
NN	O
/	O
NNS	O
)	O
.	O

These	O
results	O
are	O
shown	O
in	O
Figure	O
1	O
.	O

MIXER	O
(	O
Ranzato	O
et	O
al	O
.	O
,	O
2016	O
)	O
21.8	O
Beam	O
Search	O
Optimization	O
(	O
Wiseman	O
and	O
Rush	O
,	O
2016	O
)	O
25.5	O
Actor	O
-	O
Critic	O
+	O
Log	O
Likelihood	O
(	O
Bahdanau	O
et	O
al	O
.	O
,	O
2017	O
)	O
28.5	O
Neural	O
Phrase	O
-	O
based	O
Machine	O
Translation	O
(	O
Huang	O
et	O
al	O
.	O
,	O
2018a	O
)	O
29.9	O
Finally	O
we	O
analyze	O
the	O
parts	O
-	O
of	O
-	O
speech	O
(	O
POS	O
)	O
of	O
the	O
highest	O
activated	O
words	O
.	O

Appendix	O
A	O
contains	O
examples	O
of	O
words	O
that	O
most	O
strongly	O
activate	O
Glove	B-MethodName
and	O
Picturebook	B-MethodName
gates	O
.	O

These	O
results	O
provide	O
evidence	O
that	O
our	O
gating	O
mechanism	O
actively	O
prefers	O
Glove	B-MethodName
embeddings	O
for	O
abstract	O
words	O
and	O
Picturebook	B-MethodName
embeddings	O
for	O
concrete	O
words	O
.	O

Moreover	O
,	O
this	O
result	O
holds	O
true	O
across	O
all	O
datasets	O
,	O
even	O
those	O
that	O
are	O
not	O
inherently	O
visual	O
.	O

We	O
observe	O
that	O
gates	O
have	O
high	O
correlations	O
with	O
concreteness	B-MetricName
ratings	I-MetricName
and	O
strong	O
negative	O
correlations	O
with	O
image	B-MetricName
dispersion	I-MetricName
scores	O
.	O

Table	O
10	O
illustrates	O
the	O
result	O
of	O
this	O
analysis	O
.	O

We	O
then	O
compute	O
the	O
Spearman	B-MetricName
correlation	I-MetricName
of	O
mean	O
gate	O
activations	O
with	O
a	O
)	O
concreteness	B-MetricName
ratings	I-MetricName
and	O
b	O
)	O
image	B-MetricName
dispersion	I-MetricName
scores	O
.	O

4	O
For	O
concreteness	O
ratings	O
,	O
we	O
take	O
the	O
intersection	O
of	O
words	O
that	O
have	O
ratings	O
with	O
the	O
dataset	O
vocabulary	O
.	O

For	O
each	O
word	O
,	O
we	O
compute	O
the	O
mean	O
gate	O
activation	O
value	O
for	O
Picturebook	B-MethodName
embeddings	O
.	O

On	O
the	O
other	O
hand	O
,	O
low	O
dispersion	O
ratings	O
were	O
more	O
associated	O
with	O
concrete	O
words	O
.	O

(	O
2014	O
)	O
that	O
abstract	O
words	O
tend	O
to	O
have	O
higher	O
dispersion	O
ratings	O
,	O
due	O
to	O
having	O
much	O
higher	O
variety	O
in	O
the	O
types	O
of	O
images	O
returned	O
from	O
a	O
query	O
.	O

It	O
was	O
shown	O
in	O
Kiela	O
et	O
al	O
.	O

Image	O
dispersion	O
is	O
the	O
average	O
distance	O
between	O
all	O
pairs	O
of	O
images	O
returned	O
from	O
a	O
search	O
query	O
.	O

(	O
2013	O
)	O
which	O
provides	O
ratings	O
for	O
40,000	O
English	O
lemmas	O
.	O

For	O
concreteness	O
ratings	O
,	O
we	O
use	O
the	O
dataset	O
of	O
Brysbaert	O
et	O
al	O
.	O

In	O
our	O
first	O
experiment	O
,	O
we	O
aim	O
to	O
determine	O
how	O
well	O
gate	O
activations	O
correlate	O
to	O
a	O
)	O
human	O
judgments	O
of	O
concreteness	O
and	O
b	O
)	O
image	O
dispersion	O
(	O
Kiela	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

In	O
this	O
section	O
we	O
perform	O
an	O
extensive	O
analysis	O
of	O
the	O
gating	O
mechanism	O
for	O
models	O
trained	O
across	O
datasets	O
used	O
in	O
our	O
experiments	O
.	O

Gate	O
Analysis	O
.	O

This	O
indicates	O
that	O
while	O
our	O
embeddings	O
are	O
useful	O
for	O
smaller	O
MT	B-TaskName
experiments	O
,	O
further	O
research	O
is	O
needed	O
on	O
how	O
to	O
best	O
incorporate	O
grounded	O
representations	O
in	O
larger	O
translation	O
tasks	O
.	O

However	O
,	O
we	O
were	O
not	O
able	O
to	O
improve	O
upon	O
BLEU	B-MetricName
scores	O
from	O
equivalent	O
models	O
that	O
do	O
not	O
use	O
Picturebook	B-MethodName
.	O

For	O
these	O
tasks	O
,	O
we	O
found	O
that	O
models	O
that	O
incorporate	O
Picturebook	B-MethodName
led	O
to	O
faster	O
convergence	O
.	O

We	O
explored	O
the	O
use	O
of	O
Picturebook	B-MethodName
for	O
larger	O
machine	B-TaskName
translation	I-TaskName
tasks	O
,	O
including	O
the	O
popular	O
WMT14	B-DatasetName
benchmarks	O
.	O

Limitations	O
.	O

We	O
also	O
note	O
that	O
our	O
models	O
may	O
not	O
be	O
directly	O
comparable	O
to	O
previously	O
published	O
seq2seq	O
models	O
from	O
(	O
Wiseman	O
and	O
Rush	O
,	O
2016;Bahdanau	O
et	O
al	O
.	O
,	O
2017	O
)	O
since	O
we	O
used	O
a	O
deeper	O
encoder	O
and	O
decoder	O
.	O

We	O
note	O
that	O
the	O
NPMT	O
is	O
not	O
a	O
seq2seq	O
model	O
and	O
can	O
be	O
augmented	O
with	O
our	O
Picturebook	B-MethodName
embeddings	O
.	O

We	O
(	O
Huang	O
et	O
al	O
.	O
,	O
2018a	O
)	O
.	O

We	O
also	O
report	O
results	O
for	O
the	O
IWSLT	B-DatasetName
2014	I-DatasetName
German	I-DatasetName
-	I-DatasetName
English	I-DatasetName
task	O
(	O
Cettolo	O
et	O
al	O
.	O
,	O
2014	O
)	O
in	O
Table	O
9	O
.	O

Compared	O
to	O
our	O
baseline	O
,	O
we	O
report	O
a	O
gain	O
of	O
0.3	B-MetricValue
and	O
1.1	B-MetricValue
BLEU	B-MetricName
for	O
German	O
!	O
English	O
and	O
English	O
!	O
German	O
respectively	O
.	O

On	O
the	O
English	O
!	O
French	O
task	O
,	O
the	O
Picturebook	B-MethodName
models	O
do	O
on	O
average	O
1.2	B-MetricValue
BLEU	B-MetricName
better	O
or	O
1.0	B-MetricValue
METEOR	B-MetricName
over	O
our	O
baseline	O
.	O

We	O
suspect	O
this	O
is	O
due	O
to	O
the	O
fact	O
that	O
we	O
did	O
not	O
use	O
BPE	O
.	O

On	O
the	O
German	O
task	O
,	O
compared	O
to	O
the	O
previously	O
best	O
published	O
results	O
(	O
Caglayan	O
et	O
al	O
.	O
,	O
2017	O
)	O
we	O
do	O
better	O
in	O
BLEU	B-MetricName
but	O
slightly	O
worse	O
in	O
METEOR	B-MetricName
.	O

On	O
the	O
English	O
!	O
German	O
tasks	O
,	O
we	O
find	O
our	O
Picturebook	B-MethodName
model	O
to	O
perform	O
on	O
average	O
0.8	B-MetricValue
BLEU	B-MetricName
or	O
0.7	B-MetricValue
METEOR	B-MetricName
over	O
our	O
baseline	O
.	O

We	O
did	O
not	O
experiment	O
with	O
regularizing	O
the	O
norm	O
of	O
the	O
embeddings	O
.	O

We	O
find	O
the	O
gating	O
mechanism	O
not	O
to	O
help	O
much	O
with	O
the	O
MT	B-TaskName
task	O
since	O
the	O
trainable	O
embeddings	O
are	O
free	O
to	O
change	O
their	O
norm	O
magnitudes	O
.	O

Since	O
seq2seq	O
MT	B-TaskName
models	O
are	O
typically	O
trained	O
without	O
Glove	B-MethodName
embeddings	O
,	O
we	O
also	O
did	O
not	O
use	O
Glove	B-MethodName
embeddings	O
for	O
this	O
task	O
,	O
but	O
rather	O
we	O
combine	O
randomly	O
initialized	O
learnable	O
embeddings	O
with	O
the	O
fixed	O
Picturebook	B-MethodName
embeddings	O
.	O

This	O
is	O
also	O
highlighted	O
where	O
our	O
French	O
models	O
perform	O
better	O
than	O
our	O
German	O
models	O
relatively	O
,	O
due	O
to	O
the	O
compounding	O
nature	O
of	O
German	O
words	O
.	O

We	O
believe	O
this	O
is	O
due	O
to	O
the	O
fact	O
we	O
did	O
not	O
use	O
Byte	O
Pair	O
Encoding	O
(	O
BPE	O
)	O
(	O
Sennrich	O
et	O
al	O
.	O
,	O
2016	O
)	O
,	O
and	O
ME	B-MetricName
-	I-MetricName
TEOR	I-MetricName
captures	O
word	O
stemming	O
(	O
Denkowski	O
and	O
Lavie	O
,	O
2014	O
)	O
.	O

We	O
find	O
our	O
models	O
to	O
perform	O
better	O
in	O
BLEU	B-MetricName
than	O
METEOR	B-MetricName
relatively	O
com	O
-	O
pared	O
to	O
(	O
Caglayan	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

Table	O
7	O
summarizes	O
our	O
English	O
!	O
German	O
results	O
and	O
Table	O
8	O
summarizes	O
our	O
English	O
!	O
French	O
results	O
.	O

We	O
use	O
the	O
standard	O
seq2seq	O
(	O
Sutskever	O
et	O
al	O
.	O
,	O
2015	O
)	O
with	O
content	O
-	O
based	O
attention	O
(	O
Bahdanau	O
et	O
al	O
.	O
,	O
2015	O
)	O
model	O
and	O
we	O
describe	O
our	O
hyperparmeters	O
in	O
Appendix	O
B.	O

(	O
2017	O
)	O
,	O
the	O
winner	O
of	O
the	O
WMT	O
17	O
Multimodal	O
Machine	O
Translation	O
competition	O
(	O
Elliott	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

We	O
compare	O
our	O
Picturebook	B-MethodName
models	O
with	O
other	O
text	O
-	O
only	O
nonensembled	O
models	O
on	O
the	O
Flickr	B-DatasetName
Test2016	I-DatasetName
,	O
Flickr	B-DatasetName
Test2017	I-DatasetName
and	O
MSCOCO	B-DatasetName
test	O
sets	O
from	O
Caglayan	O
et	O
al	O
.	O

We	O
experiment	O
with	O
the	O
Multi30k	B-DatasetName
(	O
Elliott	O
et	O
al	O
.	O
,	O
2016(Elliott	O
et	O
al	O
.	O
,	O
,	O
2017	O
)	O
)	O
dataset	O
for	O
MT	B-TaskName
.	O

Machine	B-TaskName
Translation	I-TaskName
.	O

(	O
2018	O
)	O
,	O
which	O
are	O
more	O
sophisticated	O
methods	O
that	O
incorporate	O
generative	O
modelling	O
,	O
reinforcement	O
learning	O
and	O
attention	O
.	O

(	O
2018b	O
)	O
;	O
Lee	O
et	O
al	O
.	O

(	O
2018	O
)	O
;	O
Huang	O
et	O
al	O
.	O

Our	O
reported	O
results	O
have	O
been	O
recently	O
outperformed	O
by	O
Gu	O
et	O
al	O
.	O

However	O
,	O
using	O
contextual	O
gating	O
results	O
in	O
improvements	O
over	O
the	O
baseline	O
on	O
all	O
metrics	O
except	O
R@1	B-MetricName
for	O
image	O
annotation	O
.	O

Glove+Picturebook	B-MethodName
improves	O
over	O
the	O
Glove	B-MethodName
baseline	O
for	O
image	O
search	O
but	O
falls	O
short	O
on	O
image	O
annotation	O
.	O

(	O
2017	O
)	O
with	O
the	O
exception	O
of	O
Recall@10	B-MetricName
for	O
image	O
annotation	O
,	O
where	O
it	O
performs	O
slightly	O
worse	O
.	O

Our	O
Glove	B-MethodName
baseline	O
was	O
able	O
to	O
match	O
or	O
outperform	O
the	O
reported	O
results	O
in	O
Faghri	O
et	O
al	O
.	O

BoW	O
(	O
Zhang	O
et	O
al	O
.	O
,	O
2015	O
)	O
88.8	O
96.6	O
92.2	O
58.0	O
68.9	O
54.6	O
90.4	O
ngrams	O
(	O
Zhang	O
et	O
al	O
.	O
,	O
2015	O
)	O
92.0	O
98.6	O
95.6	O
56.3	O
68.5	O
54.3	O
92.0	O
ngrams	O
TFIDF	O
(	O
Zhang	O
et	O
al	O
.	O
,	O
2015	O
)	O
92.4	O
98.7	O
95.4	O
54.8	O
68.5	O
52.4	O
91.5	O
fastText	O
(	O
Joulin	O
et	O
al	O
.	O
,	O
2017	O
)	O
91	O
Image	O
Annotation	O
Image	O
Search	O
Model	O
R@1	B-MetricName
R@5	B-MetricName
R@10	B-MetricName
Med	B-MetricName
r	I-MetricName
R@1	B-MetricName
R@5	B-MetricName
R@10	B-MetricName
Med	B-MetricName
r	I-MetricName
VSE++	B-MethodName
(	O
Faghri	O
et	O
Table	O
6	O
:	O
COCO	O
test	O
-	O
set	O
results	O
for	O
image	O
-	O
sentence	O
retrieval	O
experiments	O
.	O

Med	B-MetricName
r	I-MetricName
is	O
the	O
median	B-MetricName
rank	I-MetricName
(	O
low	O
is	O
good	O
)	O
.	O

R@K	B-MetricName
is	O
Recall@K	B-MetricName
(	O
high	O
is	O
good	O
)	O
.	O

Our	O
models	O
use	O
VSE++	B-MethodName
.	O

P.	O

Amz	O
.	O

F.	O

Amz	O
.	O

A.	O

Yah	O
.	O

Yelp	O
F.	O

Model	O
AG	O
DBP	O
Yelp	O
P.	O

Table	O
6	O
displays	O
our	O
results	O
on	O
this	O
task	O
.	O

Full	O
details	O
of	O
the	O
hyperparameters	O
are	O
in	O
Appendix	O
B.	O

As	O
in	O
previous	O
work	O
,	O
we	O
report	O
the	O
mean	B-MetricName
Recall@K	I-MetricName
(	O
R@K	B-MetricName
)	O
and	O
the	O
median	B-MetricName
rank	I-MetricName
over	O
1000	O
images	O
and	O
5000	O
sentences	O
.	O

We	O
re	O
-	O
implement	O
their	O
model	O
with	O
2	O
modifications	O
:	O
1	O
)	O
we	O
replace	O
the	O
unidirectional	O
LSTM	O
encoder	O
with	O
a	O
BiLSTM	O
-	O
Max	O
sentence	O
encoder	O
and	O
2	O
)	O
we	O
use	O
Inception	B-MethodName
-	I-MethodName
V3	I-MethodName
(	O
Szegedy	O
et	O
al	O
.	O
,	O
2016	O
)	O
as	O
our	O
CNN	O
instead	O
of	O
ResNet	O
152	O
(	O
He	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

(	O
2015a	O
)	O
by	O
using	O
hard	O
negatives	O
instead	O
of	O
summing	O
over	O
contrastive	O
examples	O
.	O

VSE++	B-MethodName
improves	O
over	O
the	O
original	O
CNN	O
-	O
LSTM	O
embedding	O
method	O
of	O
Kiros	O
et	O
al	O
.	O

Here	O
,	O
we	O
utilize	O
VSE++	B-MethodName
(	O
Faghri	O
et	O
al	O
.	O
,	O
2017	O
)	O
as	O
our	O
base	O
model	O
and	O
evaluate	O
on	O
the	O
COCO	B-DatasetName
dataset	O
(	O
Lin	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

We	O
next	O
consider	O
experiments	O
that	O
map	O
images	O
and	O
sentences	O
into	O
a	O
common	O
vector	O
space	O
for	O
retrieval	O
.	O

Image	B-TaskName
-	I-TaskName
Sentence	I-TaskName
Ranking	I-TaskName
.	O

We	O
note	O
that	O
the	O
best	O
performing	O
methods	O
on	O
these	O
tasks	O
are	O
based	O
on	O
convolutional	O
neural	O
networks	O
(	O
Conneau	O
et	O
al	O
.	O
,	O
2017b	O
)	O
.	O

This	O
result	O
shows	O
that	O
our	O
embeddings	O
are	O
able	O
to	O
work	O
as	O
a	O
general	O
text	O
embedding	O
,	O
though	O
they	O
typically	O
lag	O
behind	O
Glove	B-MethodName
.	O

Our	O
results	O
show	O
that	O
Picturebook	B-MethodName
embeddings	O
,	O
while	O
minimally	O
aiding	O
in	O
performance	O
,	O
can	O
perform	O
reasonably	O
well	O
on	O
their	O
own	O
-outperforming	O
the	O
n	O
-	O
gram	O
baselines	O
of	O
(	O
Zhang	O
et	O
al	O
.	O
,	O
2015	O
)	O
on	O
5	O
out	O
of	O
7	O
tasks	O
and	O
the	O
unigram	O
fastText	O
baseline	O
on	O
all	O
7	O
tasks	O
.	O

Perhaps	O
unsurprisingly	O
,	O
adding	O
Picturebook	B-MethodName
to	O
Glove	B-MethodName
matches	O
or	O
only	O
slightly	O
improves	O
on	O
5	O
out	O
of	O
7	O
tasks	O
and	O
obtains	O
a	O
lower	O
result	O
on	O
AG	O
News	O
and	O
Yahoo	O
.	O

Our	O
experimental	O
results	O
are	O
provided	O
in	O
Table	O
5	O
.	O

Hyperparameter	O
details	O
are	O
reported	O
in	O
Appendix	O
B.	O

(	O
2015	O
)	O
and	O
compare	O
bag	O
-	O
ofwords	O
models	O
against	O
n	O
-	O
gram	O
baselines	O
provided	O
by	O
the	O
authors	O
as	O
well	O
as	O
fastText	O
(	O
Joulin	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

We	O
experiment	O
with	O
7	O
datasets	O
provided	O
by	O
Zhang	O
et	O
al	O
.	O

Our	O
next	O
set	O
of	O
experiments	O
aims	O
to	O
determine	O
how	O
well	O
Picturebook	B-MethodName
embeddings	O
do	O
on	O
tasks	O
that	O
are	O
primarily	O
non	O
-	O
visual	O
,	O
such	O
as	O
topic	B-TaskName
and	I-TaskName
sentiment	I-TaskName
classification	I-TaskName
.	O

Sentiment	B-TaskName
and	I-TaskName
Topic	I-TaskName
Classification	I-TaskName
.	O

3	O
.	O

(	O
2017a	O
)	O
,	O
from	O
which	O
we	O
improve	O
on	O
their	O
accuracy	B-MetricName
from	O
85.0	B-MetricValue
to	O
86.8	B-MetricValue
on	O
the	O
development	O
set	O
.	O

Finally	O
we	O
note	O
the	O
strength	O
of	O
our	O
own	O
Glove	B-MethodName
baseline	O
over	O
the	O
reported	O
results	O
of	O
Conneau	O
et	O
al	O
.	O

Adding	O
contextual	O
gating	O
was	O
necessary	O
to	O
improve	O
over	O
the	O
Glove	B-MethodName
baseline	O
on	O
SNLI	B-DatasetName
.	O

While	O
non	O
-	O
contextual	O
gating	O
is	O
sufficient	O
to	O
improve	O
bag	O
-	O
of	O
-	O
words	O
methods	O
,	O
with	O
BiLSTM	O
-	O
Max	O
it	O
slightly	O
hurts	O
performance	O
over	O
the	O
Glove	B-MethodName
baseline	O
.	O

It	O
is	O
worth	O
noting	O
the	O
effect	O
that	O
different	O
encoders	O
have	O
when	O
using	O
our	O
embeddings	O
.	O

(	O
2018	O
)	O
.	O

For	O
BiLSTM	O
-	O
Max	O
,	O
our	O
contextual	O
gating	O
sets	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
SNLI	B-DatasetName
sentence	O
encoding	O
methods	O
(	O
methods	O
without	O
interaction	O
layers	O
)	O
,	O
outperforming	O
the	O
recently	O
proposed	O
methods	O
of	O
I	O
m	O
and	O
Cho	O
(	O
2017	O
)	O
;	O
Shen	O
et	O
al	O
.	O

For	O
BoW	O
models	O
,	O
adding	O
Picturebook	B-MethodName
embeddings	O
to	O
Glove	B-MethodName
results	O
in	O
significant	O
gains	O
across	O
all	O
three	O
tasks	O
.	O

Table	O
4	O
displays	O
our	O
results	O
.	O

The	O
full	O
details	O
of	O
hyperparameters	O
are	O
discussed	O
in	O
Appendix	O
B.	O

Due	O
to	O
the	O
small	O
size	O
of	O
the	O
dataset	O
,	O
we	O
only	O
experiment	O
with	O
BoW	O
on	O
SICK	B-DatasetName
.	O

We	O
explore	O
the	O
use	O
of	O
two	O
types	O
of	O
sentential	O
encoders	O
:	O
Bag	O
-	O
of	O
-	O
Words	O
(	O
BoW	O
)	O
and	O
BiLSTM	O
-	O
Max	O
(	O
Conneau	O
et	O
al	O
.	O
,	O
2017a	O
For	O
SICK	O
,	O
we	O
follow	O
previous	O
work	O
and	O
report	O
average	O
results	O
across	O
5	O
runs	O
(	O
Tai	O
et	O
al	O
.	O
,	O
2015	O
)	O
.	O

The	O
first	O
two	O
are	O
natural	O
language	O
inference	O
tasks	O
and	O
the	O
third	O
is	O
a	O
sentence	O
semantic	O
relatedness	O
task	O
.	O

We	O
next	O
consider	O
experiments	O
on	O
3	O
pairwise	O
prediction	O
datasets	O
:	O
SNLI	B-DatasetName
(	O
Bowman	O
et	O
al	O
.	O
,	O
2015	O
)	O
,	O
MultiNLI	B-DatasetName
(	O
Williams	O
et	O
al	O
.	O
,	O
2017	O
)	O
and	O
SICK	B-DatasetName
(	O
Marelli	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

Sentential	O
Inference	O
and	O
Relatedness	O
.	O

All	O
subsequent	O
experiments	O
use	O
10	O
images	O
with	O
semantic	O
Picturebook	B-MethodName
.	O

(	O
2016	O
)	O
showed	O
that	O
after	O
10	O
-	O
20	O
images	O
,	O
performance	O
tends	O
to	O
saturate	O
.	O

Kiela	O
et	O
al	O
.	O

Finally	O
we	O
note	O
that	O
adding	O
more	O
images	O
nearly	O
consistently	O
improves	O
similarity	O
scores	O
across	O
categories	O
.	O

This	O
indicates	O
the	O
importance	O
of	O
the	O
type	O
of	O
similarity	O
used	O
for	O
training	O
the	O
model	O
.	O

We	O
observe	O
a	O
performance	O
difference	O
between	O
our	O
visual	O
and	O
semantic	O
embeddings	O
:	O
on	O
all	O
categories	O
except	O
verbs	O
,	O
the	O
semantic	O
embeddings	O
outperform	O
visual	O
ones	O
,	O
even	O
on	O
the	O
most	O
concrete	O
categories	O
.	O

We	O
also	O
compare	O
to	O
a	O
convolutional	O
network	O
trained	O
with	O
visual	O
similarity	O
.	O

For	O
the	O
hardest	O
subset	O
of	O
words	O
,	O
Picturebook	B-MethodName
performs	O
slightly	O
better	O
than	O
Glove	B-MethodName
while	O
Glove	B-MethodName
performs	O
better	O
across	O
all	O
pairs	O
.	O

Next	O
we	O
observe	O
that	O
the	O
performance	O
of	O
Picturebook	B-MethodName
gets	O
progressively	O
better	O
across	O
each	O
concreteness	O
quartile	O
rating	O
,	O
with	O
a	O
20	O
point	O
improvement	O
over	O
Glove	B-MethodName
for	O
the	O
most	O
concrete	O
category	O
.	O

This	O
result	O
confirms	O
that	O
Glove	B-MethodName
and	O
Picturebook	B-MethodName
capture	O
very	O
different	O
properties	O
of	O
words	O
.	O

For	O
adjectives	O
and	O
the	O
most	O
abstract	O
category	O
,	O
Glove	B-MethodName
performs	O
significantly	O
better	O
,	O
while	O
for	O
the	O
most	O
concrete	O
category	O
Picturebook	B-MethodName
is	O
significantly	O
better	O
.	O

First	O
,	O
we	O
observe	O
that	O
combining	O
Glove	B-MethodName
and	O
Picturebook	B-MethodName
leads	O
to	O
improved	O
similarity	O
across	O
most	O
categories	O
.	O

Table	O
3	O
displays	O
our	O
results	O
,	O
from	O
which	O
several	O
observations	O
can	O
be	O
made	O
.	O

By	O
default	O
,	O
we	O
use	O
10	O
images	O
for	O
each	O
embedding	O
using	O
the	O
semantic	O
convolutional	O
network	O
.	O

We	O
also	O
report	O
results	O
combining	O
Glove	B-MethodName
and	O
Picturebook	B-MethodName
by	O
summing	O
their	O
two	O
independent	O
similarity	O
scores	O
.	O

Note	O
that	O
this	O
reduces	O
to	O
negative	O
cosine	O
distance	O
when	O
using	O
only	O
1	O
image	O
per	O
word	O
.	O

2	O
That	O
is	O
,	O
the	O
score	O
is	O
minus	O
the	O
smallest	O
cosine	O
distance	O
between	O
all	O
pairs	O
of	O
images	O
of	O
the	O
two	O
words	O
.	O

For	O
computing	O
a	O
score	O
between	O
2	O
word	O
pairs	O
with	O
Picturebook	O
,	O
we	O
set	O
s(w	O
(	O
1	O
)	O
,	O
w	O
(	O
2	O
)	O
)	O
=	O
min	O
i	O
,	O
j	O
d(e	O
i	O
,	O
e	O
(	O
2	O
)	O
j	O
)	O
.	O

are	O
computed	O
via	O
cosine	O
similarity	O
.	O

Some	O
rows	O
are	O
copied	O
across	O
sections	O
for	O
ease	O
of	O
reading	O
.	O

Bracketed	O
numbers	O
signify	O
the	O
number	O
of	O
images	O
used	O
.	O

Best	O
results	O
per	O
section	O
are	O
underlined	O
.	O

Best	O
results	O
overall	O
are	O
bolded	O
.	O

For	O
Glove	O
,	O
scores	O
Table	O
3	O
:	O
SimLex-999	B-DatasetName
results	O
(	O
Spearman	O
's	O
â¢	O
)	O
.	O

This	O
is	O
an	O
interesting	O
category	O
since	O
image	O
-	O
based	O
word	O
embeddings	O
are	O
perhaps	O
less	O
likely	O
to	O
confuse	O
similarity	O
with	O
relatedness	O
than	O
distributional	O
-	O
based	O
methods	O
.	O

The	O
hardest	O
pairs	O
are	O
those	O
for	O
which	O
similarity	O
is	O
difficult	O
to	O
distinguish	O
from	O
relatedness	O
.	O

For	O
the	O
concreteness	O
quartiles	O
,	O
the	O
first	O
quartile	O
corresponds	O
to	O
the	O
most	O
abstract	O
words	O
,	O
while	O
the	O
last	O
corresponds	O
to	O
the	O
most	O
concrete	O
words	O
.	O

We	O
use	O
the	O
SimLex-999	B-DatasetName
dataset	O
(	O
Hill	O
et	O
al	O
.	O
,	O
2015	O
)	O
and	O
report	O
results	O
across	O
9	O
categories	O
:	O
all	O
(	O
the	O
whole	O
evaluation	O
)	O
,	O
adjectives	O
,	O
nouns	O
,	O
verbs	O
,	O
concreteness	O
quartiles	O
and	O
the	O
hardest	O
333	O
pairs	O
.	O

Our	O
first	O
quantitative	O
experiment	O
aims	O
to	O
determine	O
how	O
well	O
Picturebook	B-MethodName
embeddings	O
capture	O
word	O
similarity	O
.	O

We	O
also	O
report	O
nearest	O
neighbour	O
examples	O
across	O
languages	O
in	O
Appendix	O
A.1	O
.	O
Word	B-TaskName
similarity	I-TaskName
.	O

For	O
example	O
,	O
the	O
word	O
'	O
is	O
'	O
returns	O
words	O
related	O
to	O
terrorists	O
and	O
ISIS	O
and	O
'	O
it	O
'	O
returns	O
words	O
related	O
to	O
scary	O
and	O
clowns	O
due	O
to	O
the	O
2017	O
film	O
of	O
the	O
same	O
name	O
.	O

Finally	O
,	O
it	O
's	O
worth	O
highlighting	O
that	O
the	O
most	O
frequent	O
association	O
of	O
a	O
word	O
may	O
not	O
be	O
what	O
is	O
represented	O
in	O
image	O
search	O
results	O
.	O

Words	O
like	O
'	O
sun	O
'	O
also	O
return	O
the	O
corresponding	O
word	O
in	O
different	O
languages	O
,	O
such	O
as	O
'	O
Sol	O
'	O
in	O
Spanish	O
and	O
'	O
Soleil	O
'	O
in	O
French	O
.	O

Searching	O
for	O
cities	O
returns	O
cities	O
which	O
have	O
visually	O
similar	O
characteristics	O
.	O

Some	O
words	O
capture	O
multimodality	O
,	O
such	O
as	O
'	O
deep	O
'	O
referring	O
both	O
to	O
deep	O
sea	O
as	O
well	O
as	O
to	O
AI	O
.	O

Often	O
this	O
captures	O
visual	O
similarity	O
as	O
well	O
.	O

These	O
results	O
can	O
be	O
interpreted	O
as	O
follows	O
:	O
the	O
words	O
that	O
appear	O
as	O
neighbours	O
are	O
those	O
which	O
have	O
semantically	O
similar	O
images	O
to	O
that	O
of	O
the	O
query	O
.	O

In	O
order	O
to	O
get	O
a	O
sense	O
of	O
the	O
representations	O
our	O
model	O
learns	O
,	O
we	O
first	O
compute	O
nearest	O
neighbour	O
results	O
of	O
several	O
words	O
,	O
shown	O
in	O
Table	O
2	O
.	O

Nearest	O
neighbours	O
.	O

In	O
most	O
experiments	O
,	O
we	O
end	O
up	O
with	O
baselines	O
that	O
are	O
stronger	O
than	O
what	O
has	O
previously	O
been	O
reported	O
.	O

Since	O
the	O
use	O
of	O
Picturebook	B-MethodName
embeddings	O
adds	O
extra	O
parameters	O
to	O
our	O
models	O
,	O
we	O
include	O
a	O
baseline	O
for	O
each	O
experiment	O
(	O
either	O
based	O
on	O
Glove	O
or	O
learned	O
embeddings	O
)	O
that	O
we	O
extensively	O
tune	O
.	O

Hyperparameter	O
details	O
of	O
each	O
experiment	O
are	O
included	O
in	O
the	O
appendix	O
.	O

To	O
evaluate	O
the	O
effectiveness	O
of	O
our	O
embeddings	O
,	O
we	O
perform	O
both	O
quantitative	O
and	O
qualitative	O
evaluation	O
across	O
a	O
wide	O
range	O
of	O
natural	O
language	O
processing	O
tasks	O
.	O

Experiments	O
.	O

A	O
similar	O
technique	O
to	O
tie	O
the	O
softmax	O
matrix	O
as	O
the	O
transpose	O
of	O
the	O
embedding	O
matrix	O
can	O
be	O
found	O
in	O
language	O
modelling	O
(	O
Press	O
and	O
Wolf	O
,	O
2017;Inan	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

In	O
practice	O
,	O
we	O
find	O
adding	O
additional	O
parameters	O
helps	O
with	O
learning	O
:	O
p(y	O
i	O
|h	O
)	O
=	O
exp(hh	O
,	O
e	O
i	O
+	O
e	O
0	O
i	O
i	O
+	O
b	O
i	O
)	O
P	O
j	O
exp(hh	O
,	O
e	O
j	O
+	O
e	O
0	O
j	O
i	O
+	O
b	O
j	O
)	O
(	O
7	O
)	O
where	O
e	O
0	O
i	O
is	O
a	O
trainable	O
weight	O
vector	O
per	O
word	O
and	O
b	O
i	O
is	O
a	O
trainable	O
bias	O
per	O
word	O
.	O

This	O
can	O
be	O
easily	O
implemented	O
by	O
setting	O
the	O
output	O
softmax	O
matrix	O
as	O
the	O
transpose	O
of	O
the	O
Picturebook	B-MethodName
embedding	O
matrix	O
E	O
p	O
.	O

Let	O
h	O
be	O
our	O
internal	O
representation	O
of	O
our	O
model	O
(	O
i.e.	O
,	O
seq2seq	O
decoder	O
state	O
)	O
,	O
and	O
e	O
i	O
be	O
the	O
i	O
-	O
th	O
word	O
embedding	O
from	O
our	O
Picturebook	B-MethodName
embedding	O
matrix	O
E	O
p	O
:	O
p(y	O
i	O
|h	O
)	O
=	O
exp(hh	O
,	O
e	O
i	O
i	O
)	O
P	O
j	O
exp(hh	O
,	O
e	O
j	O
i)(6	O
)	O
Given	O
a	O
representation	O
h	O
,	O
Equation	O
6	O
simply	O
finds	O
the	O
most	O
similar	O
word	O
in	O
the	O
embedding	O
space	O
.	O

We	O
introduce	O
a	O
differentiable	O
mechanism	O
which	O
allows	O
us	O
to	O
align	O
words	O
across	O
source	O
and	O
target	O
languages	O
in	O
the	O
Picturebook	B-MethodName
embedding	O
domain	O
.	O

We	O
want	O
to	O
perform	O
this	O
inverse	O
image	O
search	O
operation	O
given	O
its	O
Picturebook	B-MethodName
embedding	O
.	O

For	O
example	O
,	O
given	O
the	O
word	O
'	O
bicycle	O
'	O
in	O
English	O
and	O
its	O
Picturebook	B-MethodName
embedding	O
,	O
we	O
want	O
to	O
find	O
the	O
closest	O
French	O
word	O
that	O
would	O
generate	O
this	O
representation	O
(	O
i.e.	O
,	O
'	O
vÃ©lo	O
'	O
)	O
.	O

Given	O
a	O
Picturebook	B-MethodName
embedding	O
,	O
we	O
want	O
to	O
find	O
the	O
closest	O
word	O
or	O
phrase	O
aligned	O
to	O
the	O
representation	O
.	O

In	O
generative	O
modelling	O
problems	O
(	O
i.e.	O
,	O
MT	B-TaskName
)	O
,	O
we	O
want	O
to	O
perform	O
the	O
opposite	O
operation	O
.	O

Up	O
until	O
now	O
,	O
we	O
have	O
only	O
discussed	O
scenarios	O
where	O
we	O
have	O
a	O
word	O
and	O
we	O
want	O
to	O
perform	O
this	O
implicit	O
search	O
operation	O
.	O

Picturebook	B-MethodName
embeddings	O
can	O
be	O
seen	O
as	O
a	O
form	O
of	O
implicit	O
image	O
search	O
:	O
given	O
a	O
word	O
(	O
or	O
phrase	O
)	O
,	O
image	O
search	O
the	O
word	O
query	O
and	O
concatenate	O
the	O
embeddings	O
of	O
the	O
images	O
produced	O
by	O
a	O
CNN	O
.	O

Inverse	B-MethodName
Picturebook	I-MethodName
.	O

We	O
experiment	O
with	O
contextual	O
gating	O
for	O
all	O
experiments	O
that	O
use	O
a	O
bidirectional	O
-	O
LSTM	O
encoder	O
.	O

For	O
contextual	O
gates	O
,	O
we	O
use	O
the	O
same	O
approach	O
as	O
above	O
except	O
we	O
replace	O
the	O
controller	O
(	O
e	O
g	O
,	O
e	O
p	O
)	O
with	O
inputs	O
that	O
have	O
been	O
fed	O
through	O
a	O
bidirectional	O
-	O
LSTM	O
,	O
e.g.	O
(	O
BiLSTM(e	O
g	O
)	O
,	O
BiLSTM(e	O
p	O
)	O
)	O
.	O

In	O
some	O
cases	O
it	O
may	O
be	O
beneficial	O
to	O
use	O
contextual	O
gates	O
that	O
are	O
aware	O
of	O
the	O
sentence	O
that	O
words	O
appear	O
in	O
to	O
decide	O
how	O
to	O
weight	O
Glove	O
and	O
Picturebook	O
embeddings	O
.	O

The	O
gating	O
described	O
above	O
is	O
non	O
-	O
contextual	O
,	O
in	O
the	O
sense	O
that	O
each	O
embedding	O
computes	O
a	O
gate	O
value	O
independent	O
of	O
the	O
context	O
the	O
words	O
occur	O
in	O
.	O

Contextual	O
Gating	O
.	O

We	O
leave	O
comparison	O
of	O
alternative	O
fusion	O
strategies	O
for	O
future	O
work	O
.	O

We	O
chose	O
this	O
form	O
of	O
fusion	O
over	O
other	O
approaches	O
,	O
such	O
as	O
CCA	O
variants	O
and	O
metric	O
learning	O
methods	O
,	O
to	O
allow	O
for	O
easier	O
interpretability	O
and	O
analysis	O
.	O

On	O
some	O
experiments	O
we	O
found	O
it	O
beneficial	O
to	O
include	O
a	O
skip	O
connection	O
from	O
the	O
hidden	O
layer	O
of	O
.	O

Similar	O
gating	O
mechanisms	O
can	O
be	O
found	O
in	O
LSTMs	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
and	O
other	O
multimodal	O
models	O
(	O
Arevalo	O
et	O
al	O
.	O
,	O
2017;Wang	O
et	O
al	O
.	O
,	O
2018;Kiela	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

The	O
gating	O
DNN	O
allows	O
the	O
model	O
to	O
learn	O
how	O
visual	O
a	O
word	O
is	O
as	O
a	O
function	O
of	O
its	O
input	O
e	O
p	O
and	O
e	O
g	O
.	O

We	O
fuse	O
our	O
embeddings	O
using	O
a	O
multimodal	O
gating	O
mechanism	O
:	O
g	O
=	O
(	O
e	O
g	O
,	O
e	O
p	O
)	O
(	O
4	O
)	O
e	O
=	O
g	O
(	O
e	O
g	O
)	O
+	O
(	O
1	O
g	O
)	O
(	O
e	O
p	O
)	O
(	O
5	O
)	O
where	O
is	O
a	O
1	O
hidden	O
layer	O
DNN	O
with	O
ReLU	O
activations	O
and	O
sigmoid	O
outputs	O
,	O
and	O
are	O
1	O
hidden	O
layer	O
DNNs	O
with	O
ReLU	O
activations	O
and	O
tanh	O
outputs	O
.	O

Let	O
e	O
g	O
=	O
e	O
g	O
(	O
w	O
)	O
be	O
our	O
other	O
embedding	O
(	O
i.e.	O
,	O
Glove	O
)	O
for	O
a	O
word	O
w	O
and	O
e	O
p	O
=	O
e	O
p	O
(	O
w	O
)	O
be	O
our	O
Picturebook	B-MethodName
embedding	O
.	O

Consequently	O
,	O
we	O
would	O
like	O
to	O
fuse	O
our	O
Picturebook	B-MethodName
embeddings	O
with	O
other	O
sources	O
of	O
information	O
,	O
for	O
example	O
Glove	O
embeddings	O
(	O
Pennington	O
et	O
al	O
.	O
,	O
2014	O
)	O
or	O
randomly	O
initialized	O
embeddings	O
that	O
will	O
be	O
trained	O
.	O

Picturebook	B-MethodName
embeddings	O
on	O
their	O
own	O
are	O
likely	O
to	O
be	O
useful	O
for	O
representing	O
concrete	O
words	O
but	O
it	O
is	O
not	O
clear	O
whether	O
they	O
will	O
be	O
of	O
benefit	O
for	O
abstract	O
words	O
.	O

Multimodal	O
Fusion	O
Gating	O
.	O

As	O
we	O
will	O
show	O
in	O
our	O
experiments	O
,	O
the	O
semantic	O
Picturebook	B-MethodName
embeddings	O
result	O
in	O
representations	O
that	O
are	O
more	O
useful	O
for	O
natural	O
language	O
processing	O
tasks	O
than	O
the	O
visual	O
embeddings	O
.	O

In	O
our	O
experiments	O
we	O
consider	O
two	O
types	O
of	O
Picturebook	B-MethodName
embedding	O
:	O
one	O
trained	O
through	O
optimizing	O
for	O
visual	O
similarity	O
and	O
another	O
for	O
semantic	O
similarity	O
.	O

As	O
an	O
example	O
,	O
an	O
image	O
of	O
a	O
blue	O
car	O
would	O
have	O
high	O
visual	O
similarity	O
to	O
other	O
blue	O
cars	O
but	O
would	O
have	O
higher	O
semantic	O
similarity	O
to	O
cars	O
of	O
the	O
same	O
make	O
,	O
independent	O
of	O
color	O
.	O

We	O
consider	O
two	O
types	O
of	O
image	O
similarity	O
:	O
visual	O
and	O
semantic	O
.	O

The	O
training	O
procedure	O
is	O
heavily	O
influenced	O
by	O
the	O
choice	O
of	O
similarity	O
function	O
r	O
i	O
,	O
j	O
.	O

Visual	O
vs	O
Semantic	O
Similarity	O
.	O

To	O
obtain	O
the	O
full	O
collection	O
of	O
embeddings	O
,	O
we	O
run	O
the	O
full	O
Glove	O
vocabulary	O
(	O
2.2	O
M	O
words	O
)	O
through	O
image	O
search	O
to	O
obtain	O
a	O
corresponding	O
Picturebook	B-MethodName
embedding	O
to	O
each	O
word	O
in	O
the	O
Glove	O
vocabulary	O
.	O

Most	O
of	O
our	O
experiments	O
use	O
k	O
=	O
10	O
images	O
resulting	O
in	O
a	O
word	O
embedding	O
size	O
of	O
640	O
.	O

In	O
our	O
model	O
,	O
each	O
embedding	O
results	O
in	O
a	O
64	O
-	O
dimensional	O
vector	O
with	O
the	O
final	O
Picturebook	O
embedding	O
being	O
64	O
â¤	O
k	O
dimensions	O
.	O

;	O
f	O
(	O
p	O
w	O
k	O
)	O
]	O
(	O
3	O
)	O
namely	O
,	O
the	O
concatenation	O
of	O
the	O
feature	O
vectors	O
in	O
ranked	O
order	O
.	O

The	O
Picturebook	B-MethodName
embedding	O
for	O
a	O
word	O
w	O
is	O
then	O
represented	O
as	O
:	O
e	O
p	O
(	O
w	O
)	O
=	O
[	O
f	O
(	O
p	O
w	O
1	O
)	O
;	O
f	O
(	O
p	O
w	O
2	O
)	O
;	O
.	O

,	O
p	O
w	O
k	O
.	O

We	O
first	O
perform	O
an	O
image	O
search	O
with	O
query	O
w	O
to	O
obtain	O
a	O
ranked	O
list	O
of	O
images	O
p	O
w	O
1	O
,	O
.	O

Suppose	O
we	O
would	O
like	O
to	O
obtain	O
a	O
Picturebook	B-MethodName
embedding	O
for	O
a	O
given	O
word	O
w.	O

After	O
the	O
model	O
is	O
trained	O
,	O
we	O
can	O
use	O
the	O
convolutional	O
network	O
as	O
a	O
feature	O
extractor	O
for	O
images	O
by	O
computing	O
an	O
embedding	O
vector	O
f	O
(	O
p	O
)	O
for	O
an	O
image	O
p.	O

(	O
2014	O
)	O
for	O
additional	O
details	O
of	O
training	O
,	O
including	O
the	O
specifics	O
of	O
the	O
architecture	O
used	O
.	O

We	O
refer	O
the	O
reader	O
to	O
Wang	O
et	O
al	O
.	O

The	O
model	O
is	O
trained	O
end	O
-	O
to	O
-	O
end	O
using	O
a	O
proprietary	O
dataset	O
with	O
100	O
+	O
million	O
images	O
.	O

The	O
objective	O
function	O
that	O
is	O
optimized	O
is	O
given	O
by	O
:	O
min	O
X	O
i	O
â 	O
i	O
+	O
kW	O
k	O
2	O
2	O
s.t	O
.	O
:	O
l(p	O
i	O
,	O
p	O
+	O
i	O
,	O
p	O
i	O
)	O
ï£¿	O
â 	O
i	O
8p	O
i	O
,	O
p	O
+	O
i	O
,	O
p	O
i	O
such	O
that	O
r(p	O
i	O
,	O
p	O
+	O
i	O
)	O
>	O
r(p	O
i	O
,	O
p	O
i	O
)	O
(	O
2	O
)	O
where	O
â 	O
i	O
are	O
slack	O
variables	O
and	O
W	O
is	O
a	O
vector	O
of	O
the	O
network	O
's	O
model	O
parameters	O
.	O

Suppose	O
we	O
have	O
available	O
pairwise	O
relevance	O
scores	O
r	O
i	O
,	O
j	O
=	O
r(p	O
i	O
,	O
p	O
j	O
)	O
indicating	O
the	O
similarity	O
of	O
images	O
p	O
i	O
and	O
p	O
j	O
.	O

We	O
define	O
the	O
following	O
hinge	O
loss	O
for	O
a	O
given	O
triplet	O
as	O
follows	O
:	O
l(p	O
i	O
,	O
p	O
+	O
i	O
,	O
p	O
i	O
)	O
=	O
max{0	O
,	O
g	O
+	O
D(f	O
(	O
p	O
i	O
)	O
,	O
f(p	O
+	O
i	O
)	O
)	O
D(f	O
(	O
p	O
i	O
)	O
,	O
f(p	O
i	O
)	O
)	O
}	O
(	O
1	O
)	O
where	O
f	O
(	O
p	O
i	O
)	O
represents	O
the	O
embedding	O
of	O
image	O
p	O
i	O
,	O
D(â¢	O
,	O
â¢	O
)	O
is	O
the	O
Euclidean	O
distance	O
and	O
g	O
is	O
a	O
margin	O
(	O
gap	O
)	O
hyperparameter	O
.	O

Let	O
p	O
i	O
,	O
p	O
+	O
i	O
,	O
p	O
i	O
denote	O
a	O
triplet	O
of	O
query	O
,	O
positive	O
and	O
negative	O
images	O
,	O
respectively	O
.	O

(	O
2014	O
)	O
.	O

The	O
convolutional	O
network	O
used	O
to	O
obtain	O
Picturebook	B-MethodName
embeddings	O
is	O
based	O
off	O
of	O
Wang	O
et	O
al	O
.	O

Inducing	O
Picturebook	B-MethodName
Embeddings	O
.	O

We	O
can	O
perform	O
all	O
of	O
these	O
operations	O
offline	O
to	O
construct	O
a	O
matrix	O
E	O
p	O
representing	O
the	O
Picturebook	B-MethodName
embeddings	O
over	O
a	O
vocabulary	O
.	O

Our	O
Picturebook	B-MethodName
embeddings	O
reflect	O
the	O
search	O
rankings	O
by	O
concatenating	O
the	O
individual	O
embeddings	O
in	O
the	O
order	O
of	O
the	O
search	O
results	O
.	O

We	O
then	O
pass	O
each	O
image	O
through	O
a	O
CNN	O
trained	O
with	O
a	O
semantic	O
ranking	O
objective	O
to	O
extract	O
its	O
embedding	O
.	O

Given	O
a	O
word	O
(	O
or	O
phrase	O
)	O
,	O
we	O
image	O
search	O
for	O
the	O
top	O
-	O
k	O
images	O
and	O
extract	O
the	O
images	O
.	O

Our	O
Picturebook	B-MethodName
embeddings	O
ground	O
language	O
using	O
the	O
'	O
snapshots	O
'	O
returned	O
by	O
an	O
image	O
search	O
engine	O
.	O

Picturebook	B-MethodName
Embeddings	O
.	O

The	O
use	O
of	O
image	O
search	O
allows	O
us	O
to	O
obtain	O
visual	O
embeddings	O
for	O
a	O
virtually	O
unlimited	O
vocabulary	O
without	O
needing	O
a	O
mapping	O
function	O
.	O

However	O
,	O
their	O
analysis	O
is	O
restricted	O
to	O
word	O
similarity	O
tasks	O
and	O
they	O
require	O
text	O
-	O
to	O
-	O
image	O
regression	O
to	O
obtain	O
visual	O
embeddings	O
for	O
unseen	O
words	O
,	O
due	O
to	O
the	O
use	O
of	O
ImageNet	O
.	O

(	O
2018	O
)	O
who	O
also	O
consider	O
fusing	O
Glove	O
embeddings	O
with	O
visual	O
features	O
.	O

The	O
work	O
that	O
most	O
closely	O
matches	O
ours	O
is	O
that	O
of	O
Wang	O
et	O
al	O
.	O

(	O
2018	O
)	O
describe	O
an	O
asymmetric	O
gate	O
that	O
allows	O
one	O
modality	O
to	O
'	O
attend	O
'	O
to	O
the	O
other	O
.	O

(	O
2017	O
)	O
introduce	O
a	O
gating	O
mechanism	O
inspired	O
by	O
the	O
LSTM	O
while	O
Kiela	O
et	O
al	O
.	O

Arevalo	O
et	O
al	O
.	O

More	O
recently	O
,	O
gating	O
-	O
based	O
approaches	O
have	O
been	O
developed	O
for	O
fusing	O
traditional	O
word	O
embeddings	O
with	O
visual	O
representations	O
.	O

also	O
fused	O
text	O
-	O
based	O
representations	O
with	O
imagebased	O
representations	O
(	O
Bruni	O
et	O
al	O
.	O
,	O
2014;Lazaridou	O
et	O
al	O
.	O
,	O
2015;Chrupala	O
et	O
al	O
.	O
,	O
2015;Mao	O
et	O
al	O
.	O
,	O
2016;Silberer	O
et	O
al	O
.	O
,	O
2017;Kiela	O
et	O
al	O
.	O
,	O
2017;Collell	O
et	O
al	O
.	O
,	O
2017;Zablocki	O
et	O
al	O
.	O
,	O
2018	O
)	O
and	O
representations	O
derived	O
from	O
a	O
knowledge	O
-	O
graph	O
(	O
Thoma	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

Various	O
work	O
has	O
Method	O
tasks	O
(	O
Bergsma	O
and	O
Durme	O
,	O
2011	O
)	O
bilingual	O
lexicons	O
(	O
Bergsma	O
and	O
Goebel	O
,	O
2011	O
)	O
lexical	O
preference	O
(	O
Kiela	O
et	O
al	O
.	O
,	O
2014	O
)	O
word	O
similarity	O
(	O
Kiela	O
et	O
al	O
.	O
,	O
2015a	O
)	O
lexical	O
entailment	O
detection	O
(	O
Kiela	O
et	O
al	O
.	O
,	O
2015b	O
)	O
bilingual	O
lexicons	O
(	O
Shutova	O
et	O
al	O
.	O
,	O
2016	O
)	O
metaphor	O
identification	O
(	O
Bulat	O
et	O
al	O
.	O
,	O
2015	O
)	O
predicting	O
property	O
norms	O
(	O
Kiela	O
,	O
2016	O
)	O
toolbox	O
(	O
Vulic	O
et	O
al	O
.	O
,	O
2016	O
)	O
bilingual	O
lexicons	O
(	O
Kiela	O
et	O
al	O
.	O
,	O
2016	O
)	O
word	O
similarity	O
(	O
Anderson	O
et	O
al	O
.	O
,	O
2017	O
)	O
decoding	O
brain	O
activity	O
(	O
Glavas	O
et	O
al	O
.	O
,	O
2017	O
)	O
semantic	O
text	O
similarity	O
(	O
Bhaskar	O
et	O
al	O
.	O
,	O
2017	O
)	O
abstract	O
vs	O
concrete	O
nouns	O
(	O
Hartmann	O
and	O
Sogaard	O
,	O
2017	O
)	O
bilingual	O
lexicons	O
(	O
Bulat	O
et	O
al	O
.	O
,	O
2017	O
)	O
decoding	O
brain	O
activity	O
Table	O
1	O
:	O
Existing	O
methods	O
that	O
use	O
image	O
search	O
for	O
grounding	O
and	O
their	O
corresponding	O
tasks	O
.	O

Our	O
work	O
also	O
relates	O
to	O
existing	O
multimodal	O
models	O
combining	O
different	O
representations	O
of	O
the	O
data	O
(	O
Hill	O
and	O
Korhonen	O
,	O
2014	O
)	O
.	O

Our	O
approach	O
differs	O
from	O
the	O
above	O
methods	O
in	O
three	O
main	O
ways	O
:	O
a	O
)	O
we	O
obtain	O
searchgrounded	O
representations	O
for	O
over	O
2	O
million	O
words	O
as	O
opposed	O
to	O
a	O
few	O
thousand	O
,	O
b	O
)	O
we	O
apply	O
our	O
representations	O
to	O
a	O
higher	O
diversity	O
of	O
tasks	O
than	O
previously	O
considered	O
,	O
and	O
c	O
)	O
we	O
introduce	O
a	O
multimodal	O
gating	O
mechanism	O
that	O
allows	O
for	O
a	O
more	O
flexible	O
integration	O
of	O
features	O
than	O
mere	O
concatenation	O
.	O

There	O
has	O
also	O
been	O
other	O
work	O
using	O
other	O
image	O
sources	O
such	O
as	O
ImageNet	O
(	O
Kiela	O
and	O
Bottou	O
,	O
2014;Collell	O
and	O
Moens	O
,	O
2016	O
)	O
over	O
the	O
WordNet	O
synset	O
vocabulary	O
,	O
and	O
using	O
Flickr	O
photos	O
and	O
captions	O
(	O
Joulin	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

Table	O
1	O
illustrates	O
existing	O
methods	O
that	O
utilize	O
image	O
search	O
and	O
the	O
tasks	O
considered	O
in	O
their	O
work	O
.	O

The	O
use	O
of	O
image	O
search	O
for	O
obtaining	O
word	O
representations	O
is	O
not	O
new	O
.	O

Related	O
Work	O
.	O

In	O
particular	O
,	O
networks	O
trained	O
with	O
semantic	O
labels	O
result	O
in	O
better	O
embeddings	O
than	O
those	O
trained	O
with	O
visual	O
labels	O
,	O
even	O
when	O
evaluating	O
similarity	O
on	O
concrete	O
words	O
.	O

â¢	O
We	O
highlight	O
the	O
importance	O
of	O
the	O
convolutional	O
network	O
used	O
to	O
extract	O
embeddings	O
.	O

We	O
also	O
show	O
that	O
Picturebook	B-MethodName
gate	O
activations	O
are	O
negatively	O
correlated	O
with	O
image	O
dispersion	O
(	O
Kiela	O
et	O
al	O
.	O
,	O
2014	O
)	O
,	O
indicating	O
that	O
our	O
model	O
selectively	O
chooses	O
between	O
word	O
embeddings	O
based	O
on	O
their	O
abstraction	O
level	O
.	O

â¢	O
We	O
perform	O
an	O
extensive	O
analysis	O
of	O
our	O
gating	O
mechanism	O
,	O
showing	O
that	O
the	O
gate	O
activations	O
for	O
Picturebook	B-MethodName
embeddings	O
are	O
highly	O
correlated	O
with	O
human	O
judgments	O
of	O
concreteness	O
.	O

This	O
is	O
useful	O
for	O
generative	O
modelling	O
tasks	O
.	O

Given	O
a	O
Picturebook	B-MethodName
embedding	O
,	O
we	O
find	O
the	O
closest	O
words	O
which	O
would	O
generate	O
the	O
embedding	O
.	O

â¢	O
We	O
introduce	O
Inverse	O
Picturebook	B-MethodName
to	O
perform	O
the	O
inverse	O
lookup	O
operation	O
.	O

We	O
apply	O
our	O
approach	O
to	O
over	O
a	O
dozen	O
datasets	O
and	O
several	O
different	O
tasks	O
:	O
word	B-TaskName
similarity	I-TaskName
,	O
sentence	B-TaskName
relatedness	I-TaskName
,	O
natural	B-TaskName
language	I-TaskName
inference	I-TaskName
,	O
topic	B-TaskName
/	I-TaskName
sentiment	I-TaskName
classification	I-TaskName
,	O
image	B-TaskName
sentence	I-TaskName
ranking	I-TaskName
and	O
Machine	B-TaskName
Translation	I-TaskName
(	O
MT	B-TaskName
)	O
.	O

â¢	O
We	O
introduce	O
a	O
multimodal	O
gating	O
mechanism	O
to	O
selectively	O
choose	O
between	O
Glove	B-MethodName
and	O
Picturebook	B-MethodName
embeddings	O
in	O
a	O
task	O
-	O
dependent	O
way	O
.	O

The	O
main	O
contributions	O
of	O
our	O
work	O
are	O
as	O
follows	O
:	O
â¢	O
We	O
obtain	O
Picturebook	B-MethodName
embeddings	O
for	O
the	O
2.2	O
million	O
words	O
that	O
occur	O
in	O
the	O
Glove	O
vocabulary	O
(	O
Pennington	O
et	O
al	O
.	O
,	O
2014	O
)	O
1	O
,	O
allowing	O
each	O
word	O
to	O
have	O
a	O
Glove	B-MethodName
embedding	O
and	O
a	O
parallel	O
grounded	O
word	O
representation	O
.	O

This	O
collection	O
of	O
word	O
representations	O
that	O
we	O
visually	O
1	O
Common	B-DatasetName
Crawl	I-DatasetName
,	O
840B	O
tokens	O
ground	O
via	O
image	O
search	O
is	O
2	O
-	O
3	O
orders	O
of	O
magnitude	O
larger	O
than	O
prior	O
work	O
.	O

Using	O
Google	O
image	O
search	O
,	O
a	O
Picturebook	B-MethodName
embedding	O
for	O
a	O
word	O
is	O
obtained	O
by	O
concatenating	O
the	O
k	O
-	O
feature	O
vectors	O
of	O
our	O
convolutional	O
network	O
on	O
the	O
top	O
-	O
k	O
retrieved	O
search	O
results	O
.	O

Picturebook	B-MethodName
embeddings	O
are	O
obtained	O
through	O
a	O
convolutional	O
network	O
trained	O
with	O
a	O
semantic	O
ranking	O
objective	O
on	O
a	O
proprietary	O
image	O
dataset	O
with	O
over	O
100	O
+	O
million	O
images	O
(	O
Wang	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

In	O
this	O
paper	O
we	O
introduce	O
Picturebook	B-MethodName
embeddings	O
produced	O
by	O
image	O
search	O
using	O
words	O
as	O
queries	O
.	O

While	O
several	O
authors	O
have	O
considered	O
this	O
approach	O
,	O
it	O
has	O
been	O
largely	O
limited	O
to	O
a	O
few	O
thousand	O
queries	O
and	O
only	O
a	O
small	O
number	O
of	O
tasks	O
.	O

These	O
word	O
embeddings	O
are	O
grounded	O
via	O
the	O
retrieved	O
images	O
.	O

This	O
involves	O
retrieving	O
the	O
top	O
-	O
k	O
images	O
from	O
a	O
search	O
engine	O
,	O
running	O
those	O
through	O
a	O
convolutional	O
network	O
and	O
aggregating	O
the	O
results	O
.	O

A	O
very	O
different	O
way	O
to	O
obtain	O
word	O
embeddings	O
is	O
to	O
aggregate	O
features	O
obtained	O
by	O
using	O
the	O
word	O
as	O
a	O
query	O
for	O
an	O
image	O
search	O
engine	O
.	O

While	O
immensely	O
successful	O
,	O
this	O
lookup	O
operation	O
is	O
typically	O
learned	O
through	O
co	O
-	O
occurrence	O
objectives	O
or	O
a	O
task	O
-	O
dependent	O
reward	O
signal	O
.	O

The	O
dominant	O
approach	O
to	O
learning	O
distributed	O
word	O
representations	O
is	O
through	O
indexing	O
a	O
learned	O
matrix	O
.	O

One	O
place	O
to	O
incorporate	O
grounding	O
is	O
in	O
the	O
lookup	O
table	O
that	O
maps	O
tokens	O
to	O
vectors	O
.	O

embodied	O
cognition	O
,	O
search	O
engines	O
allow	O
us	O
to	O
get	O
a	O
form	O
of	O
quasi	O
-	O
grounding	O
from	O
high	O
-	O
coverage	O
'	O
snapshots	O
'	O
of	O
our	O
physical	O
world	O
provided	O
by	O
the	O
interaction	O
of	O
millions	O
of	O
users	O
.	O

While	O
true	O
natural	O
language	O
understanding	O
may	O
require	O
fully	O
*	O
Both	O
authors	O
contributed	O
equally	O
to	O
this	O
work	O
.	O

Search	O
engines	O
allow	O
us	O
to	O
obtain	O
correspondences	O
between	O
language	O
and	O
images	O
that	O
are	O
far	O
less	O
restricted	O
than	O
existing	O
multimodal	O
datasets	O
which	O
typically	O
have	O
restricted	O
vocabularies	O
.	O

One	O
source	O
of	O
grounding	O
,	O
which	O
has	O
been	O
utilized	O
in	O
existing	O
work	O
,	O
is	O
image	O
search	O
engines	O
.	O

In	O
recent	O
years	O
,	O
a	O
large	O
amount	O
of	O
research	O
has	O
focused	O
on	O
integrating	O
vision	O
and	O
language	O
to	O
obtain	O
visually	O
grounded	O
word	O
and	O
sentence	O
representations	O
.	O

Constructing	B-TaskName
grounded	I-TaskName
representations	I-TaskName
of	I-TaskName
natural	I-TaskName
language	I-TaskName
is	O
a	O
promising	O
step	O
towards	O
achieving	O
human	O
-	O
like	O
language	O
learning	O
.	O

Introduction	O
.	O

We	O
also	O
show	O
that	O
gate	O
activations	O
corresponding	O
to	O
Picturebook	B-MethodName
embeddings	O
are	O
highly	O
correlated	O
to	O
human	O
judgments	O
of	O
concreteness	O
ratings	O
.	O

We	O
experiment	O
and	O
report	O
results	O
across	O
a	O
wide	O
range	O
of	O
tasks	O
:	O
word	B-TaskName
similarity	I-TaskName
,	O
natural	B-TaskName
language	I-TaskName
inference	I-TaskName
,	O
semantic	B-TaskName
relatedness	I-TaskName
,	O
sentiment	B-TaskName
/	I-TaskName
topic	I-TaskName
classification	I-TaskName
,	O
image	B-TaskName
-	I-TaskName
sentence	I-TaskName
ranking	I-TaskName
and	O
machine	B-TaskName
translation	I-TaskName
.	O

We	O
also	O
introduce	O
Inverse	B-MethodName
Picturebook	I-MethodName
,	O
a	O
mechanism	O
to	O
map	O
a	O
Picturebook	B-MethodName
embedding	O
back	O
into	O
words	O
.	O

We	O
introduce	O
a	O
multimodal	O
gating	O
function	O
to	O
fuse	O
our	O
Picturebook	B-MethodName
embeddings	O
with	O
other	O
word	O
representations	O
.	O

For	O
each	O
word	O
in	O
a	O
vocabulary	O
,	O
we	O
extract	O
the	O
top	O
-	O
k	O
images	O
from	O
Google	O
image	O
search	O
and	O
feed	O
the	O
images	O
through	O
a	O
convolutional	O
network	O
to	O
extract	O
a	O
word	O
embedding	O
.	O

We	O
introduce	O
Picturebook	B-MethodName
,	O
a	O
large	O
-	O
scale	O
lookup	O
operation	O
to	O
ground	O
language	O
via	O
'	O
snapshots	O
'	O
of	O
our	O
physical	O
world	O
accessed	O
through	O
image	O
search	O
.	O

Illustrative	B-TaskName
Language	I-TaskName
Understanding	I-TaskName
:	O
Large	O
-	O
Scale	O
Visual	B-TaskName
Grounding	I-TaskName
with	I-TaskName
Image	I-TaskName
Search	I-TaskName
.	O

