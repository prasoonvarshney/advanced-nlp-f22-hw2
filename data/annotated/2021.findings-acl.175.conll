Importantly	O
,	O
when	O
q	O
t	O
is	O
chosen	O
such	O
that	O
q	O
t	O
=	O
x	O
t	O
,	O
we	O
can	O
parallelize	O
the	O
computation	O
of	O
the	O
attention	O
mechanism	O
for	O
all	O
time	O
-	O
steps	O
before	O
running	O
a	O
forward	O
pass	O
through	O
the	O
model	O
.	O

The	O
geo	O
-	O
hash	O
information	O
2	O
associated	O
with	O
each	O
utterance	O
encodes	O
a	O
very	O
rough	O
estimate	O
of	O
the	O
geolocation	O
of	O
a	O
user	O
's	O
device	O
.	O

We	O
learn	O
embeddings	O
to	O
represent	O
both	O
the	O
geohash	O
and	O
the	O
dialogue	O
prompt	O
information	O
.	O

We	O
ingest	O
both	O
types	O
of	O
contexts	O
via	O
the	O
concatenationbased	O
approach	O
,	O
using	O
word	O
-	O
embeddings	O
as	O
the	O
attention	O
queries	O
.	O

While	O
we	O
focus	O
on	O
datetime	O
information	O
,	O
we	O
demonstrate	O
that	O
our	O
approach	O
can	O
be	O
applied	O
to	O
any	O
type	O
of	O
non	O
-	O
linguistic	O
context	O
,	O
such	O
as	O
geolocation	O
and	O
dialogue	O
prompts	O
.	O

Moreover	O
,	O
the	O
attention	O
mechanism	O
we	O
propose	O
can	O
improve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
contextual	O
LM	O
models	O
by	O
over	O
2.8	B-MetricValue
%	I-MetricValue
relative	O
in	O
terms	O
of	O
perplexity	B-MetricName
.	O

We	O
find	O
that	O
incorporating	O
datetime	O
context	O
into	O
a	O
LM	O
can	O
yield	O
a	O
relative	O
reduction	O
in	O
perplexity	B-MetricName
of	O
9.0	B-MetricValue
%	I-MetricValue
over	O
a	O
model	O
that	O
does	O
not	O
incorporate	O
context	O
.	O

The	O
proposed	O
model	O
dynamically	O
builds	O
up	O
a	O
representation	O
of	O
contextual	O
information	O
that	O
can	O
be	O
ingested	O
into	O
a	O
RNN	O
-	O
LM	O
via	O
a	O
concatenation	B-MethodName
-	I-MethodName
based	I-MethodName
or	I-MethodName
factorization	I-MethodName
-	I-MethodName
based	I-MethodName
approach	I-MethodName
.	O

In	O
this	O
paper	O
,	O
we	O
introduce	O
an	O
attention	B-MethodName
-	I-MethodName
based	I-MethodName
mechanism	I-MethodName
to	O
condition	O
neural	O
LMs	O
for	O
ASR	B-TaskName
on	O
non	O
-	O
linguistic	O
contextual	O
information	O
.	O

Conclusion	O
.	O

While	O
attention	O
-	O
based	O
models	O
have	O
been	O
used	O
to	O
condition	O
neural	O
models	O
on	O
particular	O
aspects	O
or	O
traits	O
(	O
Zheng	O
et	O
al	O
.	O
,	O
2019;Tang	O
et	O
al	O
.	O
,	O
2016	O
)	O
,	O
we	O
focus	O
on	O
contextual	O
information	O
that	O
benefits	O
ASR	O
systems	O
.	O

(	O
2015	O
)	O
.	O

The	O
attention	B-MethodName
mechanism	I-MethodName
we	O
propose	O
builds	O
on	O
the	O
global	O
attention	O
model	O
proposed	O
by	O
Luong	O
et	O
al	O
.	O

Our	O
contribution	O
lies	O
first	O
in	O
the	O
application	O
of	O
these	O
models	O
to	O
ASR	B-TaskName
,	O
and	O
secondly	O
their	O
extension	O
with	O
an	O
attention	B-MethodName
mechanism	I-MethodName
.	O

Methods	O
that	O
apply	O
low	O
-	O
rank	O
matrix	O
factorization	O
to	O
RNNs	O
are	O
somewhat	O
newer	O
,	O
and	O
were	O
first	O
explored	O
by	O
Kuchaiev	O
and	O
Ginsburg	O
(	O
2017	O
)	O
.	O

The	O
concatenation	B-MethodName
-	I-MethodName
based	I-MethodName
ap	I-MethodName
-	I-MethodName
proach	I-MethodName
has	O
been	O
adopted	O
as	O
a	O
common	O
method	O
for	O
incorporating	O
non	O
-	O
linguistic	O
context	O
into	O
a	O
neural	O
LM	O
(	O
Yogatama	O
et	O
al	O
.	O
,	O
2017;Wen	O
et	O
al	O
.	O
,	O
2013;Ma	O
et	O
al	O
.	O
,	O
2018;Ghosh	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

Outside	O
of	O
ASR	B-TaskName
,	O
our	O
work	O
directly	O
builds	O
upon	O
the	O
concatenation	O
-	O
based	O
(	O
Mikolov	O
and	O
Zweig	O
,	O
2012	O
)	O
and	O
factorization	O
-	O
based	O
(	O
Jaech	O
and	O
Ostendorf	O
,	O
2018a	O
)	O
approaches	O
to	O
condition	O
RNN	B-MethodName
-	I-MethodName
LMs	I-MethodName
on	O
sentence	O
context	O
.	O

Related	O
Work	O
.	O

Another	O
related	O
line	O
of	O
research	O
has	O
explored	O
learning	O
utterance	O
embeddings	O
for	O
dialogue	O
systems	O
using	O
Gaussian	O
mixture	O
models	O
that	O
are	O
enhanced	O
with	O
utterance	O
-	O
level	O
context	O
,	O
such	O
as	O
intent	O
(	O
Yan	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

Overall	O
,	O
the	O
behavior	O
of	O
the	O
attention	B-MethodName
mechanism	I-MethodName
is	O
consistent	O
with	O
our	O
initial	O
hypothesis	O
that	O
certain	O
types	O
of	O
datetime	O
information	O
can	O
benefit	O
a	O
contextual	O
LM	O
model	O
more	O
than	O
others	O
over	O
the	O
course	O
of	O
an	O
utterance	O
.	O

This	O
shift	O
might	O
indicate	O
that	O
the	O
model	O
has	O
learned	O
to	O
condition	O
the	O
type	O
of	O
music	O
users	O
listen	O
to	O
to	O
the	O
hour	O
of	O
the	O
day	O
.	O

Finally	O
when	O
the	O
word	O
"	O
songs	O
"	O
is	O
ingested	O
,	O
the	O
model	O
substantially	O
reduces	O
the	O
weight	O
placed	O
on	O
month	O
information	O
and	O
in	O
turn	O
increases	O
the	O
weight	O
on	O
hour	O
information	O
.	O

Once	O
the	O
model	O
observes	O
the	O
word	O
"	O
christmas	O
"	O
,	O
it	O
places	O
all	O
of	O
the	O
attention	O
on	O
the	O
month	O
information	O
,	O
indicating	O
the	O
model	O
has	O
successfully	O
learned	O
that	O
"	O
christmas	O
"	O
is	O
a	O
word	O
strongly	O
associated	O
with	O
a	O
particular	O
month	O
(	O
i.e.	O
,	O
December	O
)	O
.	O

This	O
would	O
suggest	O
that	O
conditioning	O
on	O
the	O
fact	O
that	O
the	O
utterance	O
was	O
spoken	O
in	O
December	O
can	O
help	O
the	O
model	O
predict	O
the	O
type	O
of	O
media	O
to	O
play	O
.	O

However	O
as	O
the	O
model	O
processes	O
the	O
subsequent	O
words	O
"	O
play	O
me	O
best	O
"	O
,	O
the	O
attention	O
begins	O
to	O
shift	O
towards	O
using	O
more	O
of	O
the	O
month	O
information	O
(	O
i.e.	O
,	O
that	O
this	O
utterance	O
was	O
spoken	O
in	O
December	O
)	O
,	O
and	O
away	O
from	O
hour	O
and	O
day	O
information	O
.	O

When	O
the	O
model	O
processes	O
the	O
start	O
-	O
of	O
-	O
sentence	O
token	O
,	O
the	O
attention	O
mechanism	O
weights	O
each	O
of	O
the	O
datetime	O
information	O
roughly	O
equally	O
.	O

Figure	O
4	O
shows	O
this	O
analysis	O
.	O

For	O
a	O
given	O
utterance	O
like	O
"	O
play	O
me	O
best	O
christmas	O
songs	O
"	O
spoken	O
in	O
December	O
,	O
we	O
highlight	O
the	O
changing	O
weight	O
placed	O
on	O
each	O
of	O
the	O
datetime	O
information	O
.	O

To	O
do	O
so	O
,	O
we	O
visualize	O
the	O
weights	O
of	O
the	O
attention	O
mechanism	O
as	O
an	O
utterance	O
is	O
processed	O
by	O
the	O
model	O
.	O

We	O
next	O
seek	O
to	O
understand	O
how	O
the	O
attention	O
mechanism	O
constructs	O
a	O
dynamic	O
representations	O
of	O
datetime	O
context	O
.	O

Attention	O
Weights	O
.	O

This	O
analysis	O
further	O
corroborates	O
that	O
the	O
trained	O
contextual	O
LMs	O
successfully	O
condition	O
their	O
predictions	O
on	O
datetime	O
information	O
.	O

The	O
horizontal	O
blue	O
dashed	O
line	O
indicates	O
the	O
conditional	O
probability	O
of	O
the	O
word	O
'	O
snooze	O
'	O
following	O
the	O
start	O
-	O
of	O
-	O
sentence	O
token	O
when	O
evaluated	O
with	O
a	O
LM	O
that	O
does	O
not	O
ingest	O
datetime	O
information	O
.	O

As	O
we	O
would	O
expect	O
,	O
the	O
probability	O
of	O
this	O
"	O
snooze	O
"	O
is	O
highest	O
in	O
the	O
morning	O
(	O
between	O
5	O
and	O
6	O
am	O
)	O
,	O
as	O
users	O
are	O
waking	O
up	O
and	O
snoozing	O
their	O
alarms	O
.	O

In	O
Figure	O
3	O
,	O
we	O
evaluate	O
the	O
conditional	O
probability	O
of	O
the	O
word	O
"	O
snooze	O
"	O
in	O
an	O
utterance	O
following	O
the	O
start	O
-	O
of	O
-	O
sentence	O
token	O
,	O
as	O
we	O
vary	O
the	O
hour	O
of	O
day	O
information	O
associated	O
with	O
this	O
utterance	O
.	O

For	O
a	O
given	O
utterance	O
,	O
we	O
can	O
evaluate	O
the	O
probability	O
of	O
the	O
words	O
in	O
the	O
utterance	O
as	O
we	O
vary	O
the	O
datetime	O
information	O
associated	O
with	O
the	O
utterance	O
.	O

In	O
addition	O
to	O
these	O
results	O
,	O
we	O
visualize	O
how	O
the	O
contextual	O
LMs	O
leverage	O
datetime	O
contexts	O
.	O

Visual	O
Analysis	O
.	O

Method	O
.	O

Recall	O
that	O
when	O
trained	O
on	O
correct	O
datetime	O
information	O
this	O
was	O
our	O
best	O
-	O
performing	O
model	O
overall	O
in	O
terms	O
of	O
both	O
perplexity	B-MetricName
and	O
WER	B-MetricName
,	O
indicating	O
that	O
the	O
performance	O
of	O
this	O
model	O
can	O
be	O
attributed	O
in	O
part	O
to	O
its	O
use	O
of	O
contextual	O
information	O
.	O

We	O
observed	O
the	O
overall	O
largest	O
relative	O
degradation	O
in	O
perplexity	B-MetricName
,	O
when	O
using	O
the	O
concatenationbased	O
model	O
.	O

In	O
general	O
,	O
if	O
a	O
model	O
uses	O
datetime	O
information	O
as	O
an	O
additional	O
signal	O
,	O
we	O
would	O
expect	O
the	O
performance	O
of	O
the	O
model	O
to	O
decrease	O
when	O
the	O
datetime	O
context	O
is	O
shuffled	O
.	O

In	O
Table	O
4	O
,	O
we	O
report	O
the	O
relative	O
degradation	O
(	O
i.e.	O
,	O
a	O
negative	O
reduction	O
)	O
in	O
perplexity	O
resulting	O
from	O
evaluating	O
these	O
models	O
on	O
the	O
shuffled	O
datetime	O
contexts	O
.	O

For	O
each	O
of	O
our	O
best	O
-	O
performing	O
models	O
in	O
a	O
given	O
category	O
of	O
method	O
(	O
prepend	B-MethodName
,	O
concat	B-MethodName
,	O
or	O
factor	B-MethodName
)	O
,	O
we	O
retrained	O
and	O
evaluated	O
those	O
models	O
on	O
the	O
dataset	O
containing	O
shuffled	O
datetime	O
information	O
.	O

terance	O
in	O
our	O
training	O
and	O
test	O
sets	O
.	O

To	O
answer	O
this	O
question	O
,	O
we	O
randomly	O
shuffled	O
the	O
datetime	O
information	O
associated	O
with	O
each	O
ut-2	O
We	O
use	O
a	O
two	O
integer	O
precision	O
geo	O
-	O
hash	O
.	O

The	O
first	O
question	O
we	O
hope	O
to	O
answer	O
is	O
:	O
to	O
what	O
extent	O
can	O
the	O
relative	O
improvements	O
in	O
perplexity	B-MetricName
and	O
WER	B-MetricName
in	O
the	O
models	O
that	O
incorporate	O
datetime	O
context	O
be	O
explained	O
by	O
the	O
additional	O
signal	O
from	O
the	O
context	O
versus	O
the	O
additional	O
parameters	O
that	O
these	O
models	O
contain	O
?	O

Datetime	O
Context	O
Signal	O
.	O

In	O
this	O
section	O
,	O
we	O
focus	O
once	O
again	O
on	O
datetime	O
information	O
to	O
better	O
understand	O
how	O
contextual	O
LMs	O
use	O
datetime	O
signal	O
.	O

Analysis	O
.	O

In	O
general	O
,	O
we	O
find	O
that	O
conditioning	O
neural	O
LMs	O
on	O
each	O
of	O
the	O
different	O
types	O
of	O
context	O
reduces	O
perplexity	B-MetricName
and	O
WER	B-MetricName
.	O

Table	O
3	O
summarizes	O
the	O
results	O
.	O

We	O
evaluate	O
these	O
models	O
on	O
a	O
test	O
set	O
of	O
de	O
-	O
identified	O
utterances	O
representative	O
of	O
user	O
interactions	O
with	O
Alexa	O
.	O

Dialogue	O
prompts	O
indicate	O
whether	O
a	O
transcribed	O
utterance	O
was	O
an	O
initial	O
query	O
to	O
the	O
dialog	O
system	O
or	O
if	O
it	O
was	O
a	O
follow	O
-	O
up	O
turn	O
.	O

Relative	O
PPL	B-MetricName
Reduction	I-MetricName
(	O
%	O
)	O
WERR	B-MetricName
(	O
%	O
)	O
Full	O
Tail	O
Full	O
Tail	O
Default	O
0.0	O
0.0	O
0.0	O
0.0	O
Datetime	O
11.4	O
11.6	O
1.6	O
1.7	O
Geo	O
-	O
hash	O
12.4	O
12.5	O
0.5	O
1.0	O
Dialogue	O
Prompt	O
13.9	O
14.1	O
0.3	O
0.6	O
We	O
train	O
the	O
LMs	O
on	O
a	O
subset	O
of	O
the	O
utterances	O
of	O
the	O
initial	O
dataset	O
which	O
also	O
contain	O
utterancelevel	O
geo	O
-	O
hash	O
information	O
and	O
dialogue	O
prompt	O
information	O
.	O

Context	O
Type	O
.	O

To	O
illustrate	O
this	O
point	O
we	O
train	O
two	O
neural	O
LMs	O
using	O
two	O
other	O
types	O
of	O
context	O
:	O
geolocation	O
information	O
and	O
dialogue	O
prompts	O
.	O

We	O
underscore	O
,	O
however	O
,	O
that	O
the	O
contextual	O
mechanism	O
we	O
introduce	O
can	O
be	O
applied	O
to	O
any	O
type	O
of	O
contextual	O
information	O
that	O
can	O
be	O
represented	O
as	O
embeddings	O
.	O

So	O
far	O
,	O
our	O
experiments	O
have	O
focused	O
exclusively	O
on	O
conditioning	O
neural	O
LMs	O
on	O
datetime	O
context	O
.	O

Other	O
Non	O
-	O
Linguistic	O
Context	O
.	O

Method	O
.	O

As	O
in	O
Table	O
1	O
,	O
the	O
concatenationbased	O
model	O
with	O
attention	O
mechanism	O
achieved	O
the	O
largest	O
reductions	O
in	O
WER	O
.	O

We	O
evaluated	O
the	O
relative	O
WER	B-MetricName
reduction(WERR	I-MetricName
)	O
on	O
a	O
large	O
test	O
set	O
of	O
de	O
-	O
identified	O
,	O
transcribed	O
utterances	O
representative	O
of	O
general	O
user	O
interactions	O
with	O
Alexa	O
,	O
as	O
well	O
as	O
on	O
the	O
tail	O
of	O
this	O
dataset	O
.	O

Table	O
2	O
summarizes	O
the	O
results	O
.	O

As	O
the	O
LM	O
component	O
of	O
this	O
system	O
,	O
we	O
used	O
the	O
bestperforming	O
models	O
within	O
each	O
category	O
of	O
method	O
that	O
we	O
report	O
in	O
Table	O
1	O
.	O

In	O
addition	O
to	O
evaluating	O
the	O
models	O
on	O
relative	O
reductions	O
in	O
perplexity	O
,	O
we	O
also	O
validated	O
the	O
downstream	O
performance	O
of	O
a	O
hybrid	B-MethodName
CTC	I-MethodName
-	I-MethodName
HMM	I-MethodName
(	O
Graves	O
et	O
al	O
.	O
,	O
2006	O
)	O
ASR	O
system	O
that	O
incorporated	O
contextual	O
information	O
in	O
its	O
LM	O
component	O
.	O

In	O
this	O
instance	O
,	O
perplexity	B-MetricName
was	O
reduced	O
by	O
2.8	B-MetricValue
%	I-MetricValue
on	O
the	O
tail	O
of	O
our	O
evaluation	O
set	O
,	O
and	O
by	O
2.1	B-MetricValue
%	I-MetricValue
on	O
the	O
full	O
dataset	O
.	O

The	O
use	O
of	O
attention	O
led	O
to	O
the	O
largest	O
relative	O
improvement	O
in	O
the	O
factorization	O
-	O
based	O
approach	O
when	O
using	O
learned	O
context	O
embeddings	O
.	O

In	O
nearly	O
every	O
experiment	O
we	O
ran	O
,	O
we	O
found	O
that	O
our	O
attention	B-MethodName
mechanism	I-MethodName
further	O
reduced	O
perplexity	O
.	O

Again	O
,	O
we	O
found	O
that	O
on	O
the	O
full	O
dataset	O
the	O
improvement	O
in	O
perplexity	B-MetricName
by	O
using	O
our	O
attention	O
mechanism	O
was	O
statistically	O
significant	O
.	O

In	O
the	O
case	O
of	O
the	O
factorization	O
-	O
based	O
approach	O
,	O
we	O
achieved	O
the	O
lowest	O
perplexity	O
when	O
the	O
attention	B-MethodName
mechanism	I-MethodName
used	O
the	O
hidden	O
state	O
of	O
the	O
RNN	O
model	O
as	O
the	O
query	O
vector	O
and	O
datetime	O
information	O
was	O
represented	O
as	O
a	O
feature	O
-	O
engineered	O
vector	O
.	O

Figure	O
2	O
visualizes	O
the	O
intervals	O
.	O

Confidence	O
intervals	O
were	O
calculated	O
by	O
running	O
the	O
training	O
algorithm	O
10	O
times	O
for	O
each	O
model	O
type	O
.	O

We	O
corroborated	O
these	O
results	O
by	O
computing	O
95	O
%	O
confidence	O
intervals	O
for	O
the	O
best	O
-	O
performing	O
concatenation	O
-	O
based	O
models	O
with	O
and	O
without	O
attention	O
.	O

We	O
obtained	O
the	O
best	O
results	O
when	O
representing	O
datetime	O
information	O
as	O
learned	O
embeddings	O
and	O
using	O
the	O
input	O
embedding	O
at	O
a	O
given	O
time	O
-	O
step	O
as	O
the	O
query	O
vector	O
.	O

For	O
the	O
concatenation	O
-	O
based	O
model	O
,	O
we	O
found	O
that	O
adding	O
our	O
attention	O
mechanism	O
led	O
to	O
further	O
reductions	O
in	O
perplexity	B-MetricName
,	O
regardless	O
of	O
the	O
type	O
of	O
query	O
vector	O
or	O
context	O
representation	O
used	O
.	O

We	O
also	O
differentiate	O
between	O
the	O
two	O
variants	O
of	O
encoding	O
the	O
query	O
vector	O
used	O
by	O
the	O
attention	B-MethodName
mechanism	I-MethodName
:	O
either	O
by	O
using	O
the	O
hidden	O
state	O
vector	O
,	O
or	O
the	O
input	O
embedding	O
at	O
a	O
given	O
time	O
-	O
step	O
.	O

In	O
reporting	O
our	O
results	O
,	O
we	O
distinguish	O
between	O
the	O
two	O
forms	O
of	O
representing	O
contextual	O
information	O
:	O
either	O
as	O
learned	O
embeddings	O
or	O
as	O
a	O
featureengineered	O
representation	O
.	O

We	O
additionally	O
trained	O
a	O
simple	O
baseline	O
,	O
Prepend	B-MethodName
,	O
which	O
was	O
comprised	O
of	O
a	O
standard	O
LSTM	O
model	O
that	O
treated	O
datetime	O
context	O
as	O
input	O
tokens	O
that	O
were	O
prepended	O
to	O
the	O
input	O
texts	O
.	O

In	O
Table	O
1	O
,	O
we	O
report	O
the	O
relative	O
decrease	O
in	O
perplexity	O
of	O
models	O
that	O
leveraged	O
datetime	O
context	O
compared	O
to	O
a	O
baseline	O
LSTM	B-MethodName
model	O
that	O
did	O
not	O
use	O
any	O
contextual	O
information	O
.	O

In	O
practice	O
,	O
these	O
two	O
statistics	O
have	O
been	O
shown	O
to	O
be	O
correlated	O
by	O
a	O
power	O
law	O
relationship	O
(	O
Klakow	O
and	O
Peters	O
,	O
2002	O
)	O
.	O

Word	O
error	B-MetricName
rate	I-MetricName
,	O
on	O
the	O
other	O
hand	O
,	O
measures	O
the	O
Levenshtein	O
(	O
minimum	O
edit	O
)	O
distance	O
between	O
a	O
recognized	O
word	O
sequence	O
and	O
a	O
reference	O
word	O
sequence	O
.	O

Perplexity	B-MetricName
is	O
a	O
common	O
statistic	O
widely	O
used	O
in	O
language	O
modeling	O
and	O
speech	O
recognition	O
to	O
measure	O
how	O
well	O
a	O
language	O
model	O
predicts	O
a	O
sample	O
of	O
text	O
(	O
Jelinek	O
et	O
al	O
.	O
,	O
1977	O
)	O
.	O

We	O
used	O
two	O
metrics	O
for	O
our	O
evaluations	O
:	O
perplexity	B-MetricName
and	O
word	O
error	B-MetricName
rate	I-MetricName
.	O

We	O
also	O
defined	O
the	O
head	O
and	O
tail	O
subsets	O
of	O
our	O
development	O
set	O
,	O
representing	O
,	O
respectively	O
,	O
the	O
top	O
5	O
%	O
most	O
frequently	O
occurring	O
utterances	O
,	O
and	O
utterances	O
occurring	O
only	O
once	O
.	O

The	O
utterances	O
in	O
our	O
training	O
and	O
evaluation	O
set	O
were	O
collected	O
in	O
the	O
same	O
time	O
-	O
range	O
.	O

We	O
evaluated	O
our	O
models	O
on	O
a	O
heldout	O
set	O
of	O
utterances	O
that	O
were	O
randomly	O
sampled	O
from	O
the	O
full	O
dataset	O
.	O

Datetime	O
.	O

6	O
Results	O
.	O

We	O
initialized	O
random	O
weights	O
using	O
Xavier	B-HyperparameterValue
-	I-HyperparameterValue
He	I-HyperparameterValue
weight	B-HyperparameterName
initialization	I-HyperparameterName
(	O
He	O
et	O
al	O
.	O
,	O
2015	O
)	O
.	O

Other	O
hyperparameters	O
,	O
such	O
as	O
the	O
initial	B-HyperparameterName
learning	I-HyperparameterName
rate	I-HyperparameterName
,	O
were	O
selected	O
via	O
random	O
search	O
.	O

In	O
practice	O
,	O
we	O
found	O
that	O
the	O
larger	O
the	O
rank	B-HyperparameterName
size	O
the	O
less	O
stable	O
the	O
training	O
procedure	O
became	O
.	O

We	O
set	O
the	O
rank	B-HyperparameterName
of	O
the	O
basis	O
tensors	O
in	O
the	O
factorization	O
-	O
approach	O
to	O
5	B-HyperparameterValue
,	O
after	O
experimenting	O
with	O
rank	B-HyperparameterName
sizes	O
2	B-HyperparameterValue
,	O
3	B-HyperparameterValue
,	O
10	B-HyperparameterValue
,	O
15	B-HyperparameterValue
,	O
20	B-HyperparameterValue
.	O

We	O
initially	O
experimented	O
with	O
smaller	O
and	O
larger	O
embedding	B-HyperparameterName
sizes	I-HyperparameterName
(	O
50	B-HyperparameterValue
,	O
100	B-HyperparameterValue
,	O
1024	B-HyperparameterValue
)	O
,	O
but	O
found	O
that	O
512	B-HyperparameterValue
generally	O
provided	O
a	O
good	O
tradeoff	O
between	O
model	O
performance	O
and	O
compute	O
resources	O
required	O
to	O
train	O
a	O
model	O
.	O

We	O
used	O
a	O
fixed	O
dimensionality	B-HyperparameterName
of	O
512	B-HyperparameterValue
for	O
word	O
,	O
context	O
and	O
hidden	O
state	O
embeddings	O
.	O

Implementation	O
of	O
the	O
model	O
and	O
training	O
procedure	O
was	O
written	O
in	O
PyTorch	O
and	O
native	O
PyTorch	O
libraries	O
.	O

The	O
training	O
of	O
each	O
model	O
was	O
conducted	O
on	O
a	O
single	O
V100	O
GPU	O
,	O
with	O
16	O
GB	O
of	O
memory	O
on	O
a	O
Linux	O
cluster	O
,	O
and	O
took	O
roughly	O
6	O
hours	O
to	O
train	O
.	O

400,000	B-HyperparameterName
batch	B-HyperparameterValue
update	I-HyperparameterValue
steps	I-HyperparameterValue
,	O
using	O
a	O
batch	B-HyperparameterName
-	I-HyperparameterName
size	I-HyperparameterName
of	O
256	B-HyperparameterValue
.	O

Models	O
were	O
trained	O
using	O
the	O
Adam	B-HyperparameterValue
optimizer	B-HyperparameterName
with	O
an	O
initial	B-HyperparameterName
learning	I-HyperparameterName
rate	I-HyperparameterName
of	O
0.001	B-HyperparameterValue
,	O
and	O
a	O
standard	O
cross	O
entropy	O
loss	O
function	O
.	O

Both	O
the	O
concatenationbased	O
and	O
factorization	O
-	O
based	O
methods	O
can	O
be	O
easily	O
adapted	O
to	O
use	O
an	O
LSTM	O
cell	O
.	O

We	O
used	O
a	O
1	B-HyperparameterValue
-	O
recurrent	B-HyperparameterName
-	I-HyperparameterName
layer	I-HyperparameterName
LSTM	O
model	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
as	O
the	O
base	O
model	O
in	O
all	O
of	O
our	O
experiments	O
.	O

Experimental	O
Setup	O
.	O

For	O
an	O
utterance	O
like	O
"	O
turn	O
alarm	O
off	O
"	O
,	O
we	O
showcase	O
how	O
the	O
model	O
builds	O
a	O
dynamic	O
representation	O
of	O
the	O
datetime	O
context	O
,	O
at	O
a	O
given	O
time	O
-	O
step	O
t	O
(	O
t	O
=	O
2	O
in	O
the	O
figure	O
)	O
.	O

In	O
Figure	O
1	O
we	O
illustrate	O
how	O
the	O
attention	O
mechanism	O
augments	O
the	O
concatenation	O
-	O
based	O
approach	O
.	O

m	O
t	O
=	O
|M	O
|	O
i=1	O
α	O
i	O
,	O
t	O
m	O
i	O
We	O
can	O
now	O
use	O
this	O
constructed	O
context	O
,	O
as	O
the	O
context	O
input	O
to	O
either	O
the	O
concatenation	O
-	O
based	O
or	O
the	O
factorization	O
-	O
based	O
approach	O
.	O

We	O
then	O
define	O
the	O
alignment	O
score	O
as	O
α	O
i	O
,	O
t	O
=	O
sof	O
tmax(score(m	O
i	O
,	O
q	O
t	O
)	O
)	O
The	O
alignment	O
scores	O
are	O
finally	O
used	O
to	O
build	O
up	O
a	O
dynamic	O
representation	O
of	O
the	O
context	O
,	O
m	O
t	O
,	O
for	O
a	O
given	O
time	O
-	O
step	O
.	O

For	O
a	O
given	O
m	O
i	O
∈	O
M	O
and	O
q	O
t	O
,	O
we	O
calculate	O
a	O
score	O
as	O
score(m	O
i	O
,	O
q	O
t	O
)	O
=	O
m	O
T	O
i	O
W	O
a	O
q	O
t	O
.	O

The	O
size	O
of	O
W	O
a	O
is	O
R	O
f	O
×e	O
if	O
q	O
t	O
=	O
x	O
t	O
,	O
or	O
R	O
f	O
×d	O
if	O
q	O
t	O
=	O
h	O
t	O
.	O

To	O
compute	O
this	O
score	O
,	O
we	O
learn	O
a	O
weight	O
matrix	O
W	O
a	O
.	O

Regardless	O
of	O
the	O
choice	O
of	O
q	O
t	O
,	O
the	O
attention	O
mechanism	O
first	O
computes	O
a	O
score	O
for	O
each	O
context	O
embedding	O
m	O
i	O
∈	O
M	O
for	O
a	O
given	O
q	O
t	O
.	O

This	O
can	O
not	O
be	O
done	O
when	O
q	O
t	O
=	O
h	O
t	O
,	O
as	O
the	O
attention	O
mechanism	O
can	O
only	O
be	O
computed	O
sequentially	O
for	O
each	O
hidden	O
state	O
of	O
the	O
model	O
.	O

Let	O
q	O
t	O
=	O
h	O
t	O
,	O
where	O
h	O
t	O
is	O
the	O
hidden	O
state	O
of	O
the	O
RNN	O
model	O
at	O
time	O
-	O
step	O
t.	O

2	O
.	O

Let	O
q	O
t	O
=	O
x	O
t	O
,	O
where	O
x	O
t	O
is	O
the	O
embedding	O
for	O
the	O
input	O
word	O
at	O
time	O
-	O
step	O
t.	O

We	O
propose	O
two	O
methods	O
for	O
defining	O
this	O
query	O
vector	O
1	O
.	O

In	O
addition	O
to	O
the	O
set	O
M	O
,	O
the	O
attention	O
mechanism	O
takes	O
in	O
a	O
query	O
vector	O
,	O
q	O
t	O
,	O
for	O
each	O
time	O
-	O
step	O
t.	O

We	O
also	O
experiment	O
with	O
adding	O
a	O
similar	O
vector	O
of	O
all	O
0s	O
in	O
the	O
case	O
where	O
context	O
embeddings	O
are	O
learned	O
,	O
but	O
find	O
no	O
improvement	O
.	O

Thus	O
,	O
the	O
attention	B-MethodName
mechanism	I-MethodName
can	O
act	O
as	O
a	O
learnable	O
gate	O
to	O
limit	O
the	O
non	O
-	O
linguistic	O
context	O
passed	O
into	O
the	O
model	O
.	O

We	O
do	O
so	O
because	O
our	O
attention	O
mechanism	O
builds	O
a	O
dynamic	O
representation	O
of	O
the	O
context	O
by	O
interpolating	O
over	O
multiple	O
context	O
embeddings	O
.	O

However	O
,	O
in	O
the	O
case	O
where	O
datetime	O
information	O
is	O
represented	O
as	O
a	O
feature	O
-	O
engineered	O
vector	O
,	O
we	O
augment	O
M	O
to	O
include	O
an	O
8	O
-	O
dimensional	O
vector	O
of	O
all	O
0s	O
:	O
M	O
=	O
{	O
m	O
,	O
0	O
}	O
.	O

We	O
assume	O
as	O
input	O
to	O
the	O
attention	O
mechanism	O
the	O
same	O
set	O
M	O
of	O
context	O
representations	O
.	O

Using	O
an	O
attention	O
mechanism	O
enables	O
us	O
to	O
dynamically	O
weight	O
the	O
importance	O
that	O
the	O
model	O
places	O
on	O
particular	O
datetime	O
context	O
as	O
the	O
model	O
processes	O
an	O
utterance	O
.	O

By	O
the	O
time	O
the	O
model	O
has	O
observed	O
the	O
words	O
"	O
what	O
temperature	O
will	O
"	O
,	O
we	O
would	O
expect	O
the	O
model	O
to	O
condition	O
the	O
predictions	O
of	O
the	O
remaining	O
words	O
primarily	O
on	O
the	O
hour	O
and	O
day	O
information	O
.	O

For	O
instance	O
,	O
assume	O
a	O
LM	O
is	O
given	O
the	O
phrase	O
"	O
what	O
temperature	O
will	O
it	O
be	O
on	O
friday	O
"	O
.	O

We	O
hypothesize	O
that	O
at	O
certain	O
time	O
-	O
steps	O
within	O
an	O
utterance	O
,	O
attending	O
to	O
particular	O
datetime	O
information	O
will	O
facilitate	O
the	O
model	O
's	O
predictions	O
more	O
than	O
other	O
information	O
.	O

We	O
apply	O
this	O
mechanism	O
to	O
the	O
context	O
embeddings	O
at	O
each	O
time	O
-	O
step	O
of	O
the	O
RNN	O
model	O
,	O
in	O
order	O
to	O
adapt	O
the	O
context	O
representation	O
dynamically	O
.	O

We	O
propose	O
an	O
attention	B-MethodName
mechanism	I-MethodName
that	O
augments	O
both	O
the	O
concatenation	O
-	O
based	O
and	O
factorizationbased	O
approaches	O
.	O

Attention	O
Mechanism	O
.	O

A	O
prediction	O
,	O
ŷt	O
,	O
is	O
generated	O
in	O
the	O
same	O
manner	O
as	O
in	O
the	O
concatenation	O
-	O
based	O
model	O
.	O

The	O
resulting	O
matrices	O
W	O
x	O
,	O
W	O
h	O
are	O
now	O
used	O
as	O
the	O
weights	O
in	O
the	O
RNN	O
cell	O
.	O

We	O
can	O
now	O
use	O
the	O
contextual	O
representation	O
to	O
interpolate	O
the	O
two	O
sets	O
of	O
basis	O
tensors	O
to	O
produce	O
two	O
new	O
weight	O
matrices	O
,	O
W	O
x	O
and	O
W	O
h	O
,	O
where	O
W	O
x	O
=	O
W	O
x	O
+	O
(	O
W	O
(	O
L	O
)	O
x	O
T	O
m	O
)	O
T	O
(	O
W	O
(	O
R	O
)	O
x	O
T	O
m	O
)	O
W	O
h	O
=	O
W	O
h	O
+	O
(	O
W	O
(	O
L	O
)	O
h	O
T	O
m	O
)	O
T	O
(	O
W	O
(	O
R	O
)	O
h	O
T	O
m	O
)	O
.	O

The	O
right	O
adaptation	O
tensors	O
,	O
W	O
(	O
R	O
)	O
x	O
,	O
W	O
(	O
R	O
)	O
h	O
are	O
both	O
of	O
dimensionality	O
R	O
r×d×f	O
.	O

The	O
left	O
adaptation	O
tensors	O
,	O
W	O
(	O
L	O
)	O
x	O
,	O
W	O
(	O
L	O
)	O
h	O
,	O
are	O
of	O
dimensionality	O
R	O
f	O
×e×r	O
,	O
and	O
R	O
f	O
×d×r	O
,	O
respectively	O
.	O

These	O
basis	O
tensors	O
are	O
of	O
fixed	O
rank	B-HyperparameterName
r	B-HyperparameterName
,	O
where	O
r	B-HyperparameterName
is	O
a	O
tuned	O
hyperparameter	O
.	O

The	O
adaption	O
process	O
involves	O
learning	O
basis	O
tensors	O
W	O
(	O
L	O
)	O
x	O
,	O
W	O
(	O
R	O
)	O
x	O
and	O
W	O
(	O
L	O
)	O
h	O
,	O
W	O
(	O
R	O
)	O
h	O
.	O

Compared	O
to	O
the	O
concatenation	O
-	O
based	O
architecture	O
,	O
this	O
approach	O
adapts	O
a	O
larger	O
fraction	O
of	O
the	O
RNN	O
model	O
's	O
parameters	O
.	O

Unlike	O
the	O
concatenation	O
-	O
based	O
approach	O
,	O
which	O
directly	O
inserts	O
contextual	O
information	O
into	O
the	O
RNN	O
cell	O
,	O
the	O
factorization	O
-	O
based	O
method	O
adapts	O
the	O
weight	O
matrices	O
W	O
x	O
,	O
W	O
h	O
of	O
the	O
RNN	O
model	O
.	O

Factorization	O
-	O
based	O
LM	O
Adaptation	O
.	O

To	O
generate	O
a	O
prediction	O
,	O
ŷt	O
for	O
a	O
word	O
at	O
time	O
-	O
step	O
t	O
,	O
h	O
t	O
is	O
passed	O
through	O
a	O
projection	O
layer	O
,	O
W	O
v	O
∈	O
R	O
d×|V	O
|	O
to	O
match	O
the	O
dimension	O
of	O
the	O
vocabulary	O
size	O
|V	O
|	O
,	O
before	O
applying	O
a	O
softmax	O
layer	O
ŷt	O
=	O
sof	O
tmax(W	O
v	O
h	O
t	O
)	O
.	O

Notice	O
that	O
the	O
expression	O
above	O
can	O
be	O
equivalently	O
calculated	O
by	O
concatenating	O
the	O
matrices	O
W	O
m	O
and	O
W	O
x	O
,	O
as	O
well	O
as	O
the	O
vectors	O
m	O
and	O
x	O
t	O
h	O
t	O
=	O
σ([W	O
x	O
;	O
W	O
m	O
]	O
[	O
x	O
t	O
;	O
m	O
]	O
+	O
W	O
h	O
h	O
t−1	O
+	O
b	O
)	O
.	O

In	O
the	O
concatenation	O
-	O
based	O
approach	O
,	O
this	O
hidden	O
-	O
state	O
is	O
adapted	O
in	O
the	O
following	O
manner	O
h	O
t	O
=	O
σ(W	O
m	O
m	O
+	O
W	O
x	O
x	O
t	O
+	O
W	O
h	O
h	O
t−1	O
+	O
b	O
)	O
.	O

A	O
standard	O
RNN	O
model	O
without	O
contextual	O
information	O
keeps	O
track	O
of	O
a	O
hidden	O
-	O
state	O
at	O
time	O
-	O
step	O
t	O
,	O
h	O
t	O
,	O
that	O
is	O
calculated	O
as	O
h	O
t	O
=	O
σ(W	O
x	O
x	O
t	O
+	O
W	O
h	O
h	O
t−1	O
+	O
b	O
)	O
,	O
where	O
x	O
t	O
represents	O
the	O
word	O
embedding	O
at	O
timestep	O
t	O
,	O
b	O
is	O
a	O
bias	O
vector	O
,	O
W	O
x	O
∈	O
R	O
e×d	O
,	O
and	O
W	O
h	O
∈	O
R	O
d×d	O
.	O

In	O
this	O
case	O
,	O
f	B-HyperparameterName
is	O
four	O
-	O
times	O
the	O
size	O
of	O
each	O
individual	O
context	O
embedding	O
.	O

When	O
representing	O
contextual	O
information	O
as	O
learned	O
embeddings	O
,	O
recall	O
that	O
we	O
first	O
concatenate	O
the	O
embeddings	O
together	O
before	O
passing	O
these	O
into	O
the	O
model	O
.	O

In	O
practice	O
,	O
f	B-HyperparameterName
is	O
either	O
a	O
hyperparameter	O
when	O
datetime	O
context	O
is	O
represented	O
as	O
learned	O
embeddings	O
,	O
or	O
f	B-HyperparameterName
=	O
8	B-HyperparameterValue
when	O
this	O
context	O
is	O
represented	O
as	O
a	O
feature	O
-	O
engineered	O
vector	O
.	O

The	O
concatenation	O
-	O
based	O
approach	O
learns	O
a	O
weight	O
matrix	O
W	O
m	O
of	O
dimensionality	O
R	O
f	O
×d	O
,	O
where	O
f	B-HyperparameterName
represents	O
the	O
size	B-HyperparameterName
of	I-HyperparameterName
the	I-HyperparameterName
context	I-HyperparameterName
representation	I-HyperparameterName
and	O
d	B-HyperparameterName
represents	O
the	O
hidden	B-HyperparameterName
-	I-HyperparameterName
dimensionality	I-HyperparameterName
of	O
the	O
RNN	O
model	O
.	O

Concatenation	O
-	O
based	O
LM	O
Adaptation	O
.	O

The	O
methods	O
we	O
discuss	O
,	O
however	O
,	O
can	O
be	O
applied	O
to	O
each	O
layer	O
of	O
a	O
multi	O
-	O
layer	O
RNN	O
model	O
.	O

The	O
notation	O
we	O
use	O
to	O
describe	O
architectures	O
assumes	O
a	O
1	B-HyperparameterValue
-	O
layer	B-HyperparameterName
RNN	O
model	O
.	O

We	O
then	O
introduce	O
our	O
attentionmechanism	B-MethodName
that	O
can	O
be	O
used	O
to	O
augment	O
both	O
of	O
these	O
approaches	O
.	O

In	O
this	O
section	O
,	O
we	O
first	O
describe	O
the	O
architecture	O
of	O
the	O
concatenation	O
-	O
based	O
and	O
factorization	O
-	O
based	O
approaches	O
.	O

Model	O
.	O

A	O
set	O
,	O
M	O
,	O
containing	O
a	O
single	O
embedding	O
M	O
=	O
{	O
m	O
}	O
,	O
where	O
m	O
represents	O
an	O
8dimensional	O
feature	O
-	O
engineered	O
contextual	O
datetime	O
representation	O
,	O
as	O
described	O
in	O
the	O
previous	O
section	O
.	O

2	O
.	O

When	O
using	O
the	O
concatenationbased	O
or	O
factorization	O
-	O
based	O
approaches	O
without	O
attention	O
,	O
we	O
first	O
concatenate	O
the	O
embeddings	O
together	O
,	O
m	O
=	O
[	O
m	O
1	O
;	O
m	O
2	O
;	O
m	O
3	O
;	O
m	O
4	O
]	O
,	O
and	O
use	O
the	O
resulting	O
vector	O
as	O
input	O
to	O
the	O
model	O
.	O

A	O
set	O
,	O
M	O
,	O
of	O
four	O
learned	O
context	O
embeddings	O
M	O
=	O
{	O
m	O
1	O
,	O
m	O
2	O
,	O
m	O
3	O
,	O
m	O
4	O
}	O
,	O
where	O
m	O
1	O
is	O
an	O
encoding	O
of	O
the	O
month	O
information	O
,	O
m	O
2	O
is	O
an	O
encoding	O
of	O
the	O
week	O
information	O
,	O
m	O
3	O
is	O
an	O
encoding	O
of	O
the	O
day	O
of	O
the	O
week	O
information	O
,	O
and	O
m	O
4	O
is	O
an	O
encoding	O
of	O
the	O
hour	O
of	O
the	O
day	O
information	O
.	O

We	O
additionally	O
represent	O
the	O
contextual	O
information	O
as	O
either	O
:	O
1	O
.	O

,	O
n	O
}	O
,	O
where	O
n	O
is	O
the	O
length	O
of	O
the	O
input	O
sequence	O
and	O
e	B-HyperparameterName
is	O
the	O
dimensionality	B-HyperparameterName
of	I-HyperparameterName
the	I-HyperparameterName
word	I-HyperparameterName
embeddings	I-HyperparameterName
.	O

,	O
n	O
}	O
,	O
that	O
are	O
converted	O
by	O
an	O
embedding	O
layer	O
into	O
embeddings	O
x	O
i	O
∈	O
R	O
e	O
for	O
i	O
∈	O
{	O
1	O
,	O
.	O

We	O
assume	O
as	O
input	O
to	O
a	O
model	O
a	O
sequence	O
of	O
either	O
word	O
or	O
subword	O
tokens	O
,	O
w	O
i	O
for	O
i	O
∈	O
{	O
1	O
,	O
.	O

Input	O
Representation	O
.	O

Feature	O
-	O
engineered	O
representation	O
:	O
Additionally	O
,	O
we	O
consider	O
transforming	O
the	O
datetime	O
information	O
into	O
a	O
single	O
8	O
-	O
dimensional	O
feature	O
-	O
engineered	O
vector	O
,	O
where	O
the	O
dimensions	O
of	O
the	O
vector	O
are	O
defined	O
as	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
sin	O
(	O
2π•hour	O
24	O
)	O
cos	O
(	O
2π•hour	O
24	O
)	O
sin	O
(	O
2π•day	O
7	O
)	O
cos	O
(	O
2π•day	O
7	O
)	O
sin	O
(	O
2π•week	O
53	O
)	O
cos	O
(	O
2π•week	O
)	O
sin	O
(	O
2π•month	O
12	O
)	O
cos	O
(	O
2π•month	O
12	O
)	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
Since	O
the	O
datetime	O
context	O
is	O
continuous	O
and	O
cyclical	O
,	O
this	O
approach	O
explicitly	O
encodes	O
tem	O
-	O
poral	O
proximity	O
in	O
the	O
date	O
and	O
time	O
information	O
.	O

2	O
.	O

We	O
experiment	O
with	O
different	O
ways	O
of	O
parsing	O
the	O
information	O
,	O
such	O
as	O
encoding	O
weekday	O
versus	O
weekend	O
,	O
or	O
morning	O
versus	O
evening	O
,	O
but	O
find	O
this	O
information	O
is	O
largely	O
entailed	O
within	O
our	O
method	O
for	O
processing	O
datetime	O
information	O
.	O

These	O
embeddings	O
are	O
initialized	O
as	O
random	O
vectors	O
,	O
and	O
trained	O
along	O
with	O
the	O
rest	O
of	O
the	O
model	O
.	O

These	O
tokens	O
are	O
subsequently	O
used	O
as	O
input	O
to	O
the	O
model	O
,	O
where	O
they	O
are	O
passed	O
through	O
an	O
embedding	O
layer	O
to	O
generate	O
context	O
embeddings	O
.	O

In	O
the	O
example	O
above	O
,	O
we	O
would	O
transform	O
the	O
datetime	O
information	O
into	O
tokens	O
representing	O
:	O
month-12	O
,	O
week-52	O
,	O
wednesday	O
,	O
7	O
am	O
.	O

Learned	O
embeddings	O
:	O
We	O
first	O
consider	O
creating	O
tokens	O
for	O
the	O
month	O
number	O
,	O
week	O
number	O
,	O
day	O
of	O
the	O
week	O
and	O
hour	O
that	O
an	O
utterance	O
was	O
spoken	O
.	O

In	O
order	O
to	O
condition	O
a	O
LM	O
on	O
this	O
datetime	O
information	O
,	O
we	O
consider	O
two	O
methods	O
for	O
transforming	O
the	O
contextual	O
information	O
into	O
a	O
continuous	O
vector	O
representation	O
:	O
1	O
.	O

In	O
the	O
example	O
above	O
,	O
we	O
can	O
infer	O
that	O
the	O
utterance	O
"	O
play	O
christmas	O
music	O
"	O
was	O
spoken	O
on	O
December	O
23	O
,	O
2020	O
at	O
7	O
in	O
the	O
morning	O
local	O
time	O
.	O

A	O
typical	O
utterance	O
in	O
our	O
dataset	O
might	O
look	O
like	O
this	O
:	O
2020	O
-	O
12	O
-	O
23	O
07:00	O
play	O
christmas	O
music	O
.	O

Context	O
Representation	O
.	O

We	O
randomly	O
split	O
our	O
dataset	O
into	O
a	O
training	O
set	O
,	O
development	O
set	O
and	O
test	O
set	O
,	O
using	O
a	O
partition	B-HyperparameterName
ratio	I-HyperparameterName
of	O
90/5/5	B-HyperparameterValue
and	O
we	O
ensure	O
that	O
each	O
partition	O
contains	O
more	O
than	O
500	O
hours	O
worth	O
of	O
data	O
.	O

Any	O
information	O
about	O
the	O
device	O
or	O
the	O
speaker	O
from	O
which	O
an	O
utterance	O
originates	O
has	O
been	O
removed	O
.	O

The	O
datetime	O
information	O
is	O
reported	O
according	O
to	O
the	O
local	O
time	O
zone	O
of	O
each	O
given	O
user	O
.	O

Each	O
utterance	O
also	O
contains	O
associated	O
information	O
about	O
the	O
year	O
,	O
month	O
,	O
day	O
,	O
and	O
hour	O
that	O
the	O
utterance	O
was	O
spoken	O
.	O

We	O
use	O
a	O
corpus	O
of	O
over	O
5,000	O
hours	O
of	O
deidentified	O
,	O
transcribed	O
English	O
utterances	O
,	O
collected	O
over	O
several	O
years	O
.	O

Data	O
.	O

Moreover	O
,	O
our	O
attention	O
mechanism	O
can	O
improve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
for	O
conditional	O
LMs	O
by	O
over	O
2.8	B-MetricValue
%	I-MetricValue
relative	O
in	O
terms	O
of	O
perplexity	B-MetricName
.	O

Compared	O
to	O
a	O
standard	O
model	O
that	O
does	O
not	O
include	O
contextual	O
information	O
,	O
using	O
our	O
method	O
to	O
contextualize	O
a	O
neural	O
LM	O
on	O
datetime	O
information	O
achieves	O
a	O
relative	O
reduction	O
in	O
perplexity	B-MetricName
of	O
7.0	B-MetricValue
%	I-MetricValue
,	O
and	O
a	O
relative	O
reduction	O
in	O
perplexity	B-MetricName
of	O
9.0	B-MetricValue
%	I-MetricValue
when	O
evaluated	O
on	O
the	O
tail	O
of	O
this	O
dataset	O
.	O

We	O
evaluate	O
our	O
method	O
on	O
a	O
large	O
de	O
-	O
identified	O
dataset	O
of	O
transcribed	O
utterances	O
.	O

To	O
underscore	O
this	O
point	O
we	O
also	O
provide	O
results	O
for	O
conditioning	O
LMs	O
on	O
geolocation	O
information	O
and	O
dialogue	O
prompts	O
that	O
are	O
commonly	O
available	O
in	O
ASR	O
systems	O
.	O

Our	O
approach	O
,	O
however	O
,	O
can	O
generalized	O
to	O
any	O
type	O
of	O
context	O
.	O

We	O
concentrate	O
on	O
datetime	O
information	O
because	O
of	O
its	O
widespread	O
availability	O
in	O
many	O
ASR	O
systems	O
.	O

Our	O
experiments	O
focus	O
primarily	O
on	O
conditioning	O
neural	O
LMs	O
on	O
datetime	O
context	O
.	O

The	O
resulting	O
embedding	O
can	O
be	O
used	O
as	O
an	O
additional	O
input	O
to	O
either	O
the	O
concatenation	O
-	O
based	O
or	O
factorization	O
-	O
based	O
model	O
.	O

The	O
attention	B-MethodName
mechanism	I-MethodName
that	O
we	O
propose	O
builds	O
up	O
a	O
dynamic	O
context	O
representation	O
over	O
the	O
course	O
of	O
processing	O
an	O
utterance	O
.	O

We	O
introduce	O
an	O
attention	O
mechanism	O
that	O
augments	O
both	O
the	O
concatenation	O
-	O
based	O
and	O
factorization	O
-	O
based	O
approaches	O
to	O
condition	O
a	O
neural	O
LM	O
on	O
context	O
.	O

This	O
factorization	O
-	O
based	O
approach	O
has	O
proven	O
effective	O
in	O
generating	O
automatic	O
completions	O
of	O
sentences	O
that	O
are	O
personalized	O
for	O
particular	O
users	O
(	O
Jaech	O
and	O
Ostendorf	O
,	O
2018b	O
)	O
.	O

Factorizing	O
the	O
weight	O
-	O
matrix	O
enables	O
a	O
larger	O
fraction	O
of	O
a	O
model	O
's	O
parameters	O
to	O
adjust	O
to	O
a	O
given	O
contextual	O
signal	O
.	O

The	O
authors	O
propose	O
decomposing	O
the	O
weight	O
-	O
matrix	O
into	O
a	O
set	O
of	O
left	O
and	O
right	O
basis	O
tensors	O
which	O
are	O
then	O
multiplied	O
by	O
a	O
learned	O
context	O
embedding	O
to	O
produce	O
a	O
new	O
weight	O
-	O
matrix	O
.	O

In	O
contrast	O
to	O
these	O
methods	O
,	O
Jaech	O
and	O
Ostendorf	O
(	O
2018a	O
)	O
adapt	O
the	O
weight	O
-	O
matrix	O
used	O
in	O
an	O
RNN	O
model	O
to	O
a	O
given	O
contextual	O
input	O
.	O

(	O
2019	O
)	O
use	O
an	O
attention	B-MethodName
mechanism	I-MethodName
over	O
learned	O
personality	O
trait	O
embeddings	O
,	O
in	O
order	O
to	O
generate	O
personalized	O
dialogue	O
responses	O
.	O

The	O
aforementioned	O
approaches	O
learn	O
a	O
representation	O
of	O
the	O
context	O
that	O
is	O
directly	O
used	O
as	O
input	O
to	O
a	O
neural	O
LM	O
.	O

Similarly	O
,	O
Zheng	O
et	O
al	O
.	O

(	O
2016	O
)	O
use	O
an	O
attention	O
module	O
that	O
attends	O
to	O
wordlocation	O
information	O
to	O
predict	O
the	O
polarity	O
of	O
a	O
sentence	O
.	O

Tang	O
et	O
al	O
.	O

Recently	O
,	O
attention	O
mechanisms	O
,	O
initially	O
developed	O
for	O
machine	O
translation	O
(	O
Bahdanau	O
et	O
al	O
.	O
,	O
2014;Luong	O
et	O
al	O
.	O
,	O
2015	O
)	O
,	O
have	O
been	O
used	O
by	O
neural	O
LMs	O
to	O
adaptively	O
condition	O
their	O
predictions	O
on	O
certain	O
non	O
-	O
linguistic	O
contexts	O
.	O

This	O
concatenation	O
-	O
based	O
approach	O
has	O
been	O
used	O
in	O
a	O
variety	O
of	O
domains	O
including	O
text	O
classification	O
(	O
Yogatama	O
et	O
al	O
.	O
,	O
2017	O
)	O
,	O
personalized	O
conversational	O
agents	O
(	O
Wen	O
et	O
al	O
.	O
,	O
2013	O
)	O
,	O
and	O
voice	O
search	O
queries	O
(	O
Ma	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

The	O
most	O
common	O
method	O
for	O
incorporating	O
nonlinguistic	O
information	O
into	O
a	O
RNN	O
-	O
LM	O
is	O
to	O
learn	O
a	O
representation	O
of	O
the	O
context	O
that	O
is	O
concatenated	O
with	O
word	O
embeddings	O
as	O
input	O
to	O
the	O
model	O
.	O

While	O
,	O
outside	O
of	O
ASR	O
,	O
transformer	O
-	O
based	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O
2017	O
)	O
language	O
models	O
have	O
largely	O
replaced	O
RNN	B-MethodName
-	I-MethodName
LMs	I-MethodName
,	O
RNNs	O
remain	O
dominant	O
in	O
ASR	O
architectures	O
such	O
as	O
connectionist	O
temporal	O
classification	O
(	O
Graves	O
et	O
al	O
.	O
,	O
2006	O
)	O
,	O
and	O
RNN	B-MethodName
-	I-MethodName
T	I-MethodName
(	O
Graves	O
,	O
2012;He	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

In	O
this	O
paper	O
,	O
we	O
focus	O
on	O
adapting	O
recurrent	B-MethodName
neural	I-MethodName
network	I-MethodName
language	I-MethodName
models	I-MethodName
(	O
RNN	B-MethodName
-	I-MethodName
LMs	I-MethodName
)	O
to	O
use	O
both	O
text	O
and	O
non	O
-	O
linguistic	O
contextual	O
data	O
for	O
speech	O
recognition	O
in	O
general	O
.	O

These	O
past	O
efforts	O
,	O
however	O
,	O
have	O
largely	O
focused	O
on	O
improving	O
a	O
particular	O
skill	O
of	O
an	O
ASR	O
system	O
,	O
and	O
not	O
the	O
system	O
's	O
speech	O
recognition	O
in	O
general	O
.	O

To	O
-	O
date	O
,	O
some	O
voice	O
assistants	O
have	O
leveraged	O
coarse	O
geographic	O
information	O
for	O
improving	O
location	O
search	O
queries	O
(	O
Bocchieri	O
and	O
Caseiro	O
,	O
2010;Lloyd	O
and	O
Kristjansson	O
,	O
2012	O
)	O
.	O

As	O
an	O
example	O
,	O
knowing	O
that	O
an	O
utterance	O
was	O
spoken	O
on	O
December	O
25th	O
,	O
a	O
LM	O
should	O
learn	O
that	O
the	O
word	O
"	O
christmas	O
"	O
rather	O
than	O
"	O
easter	O
"	O
is	O
more	O
likely	O
to	O
follow	O
the	O
phrase	O
"	O
lookup	O
cookie	O
recipes	O
for	O
"	O
.	O

We	O
hypothesize	O
that	O
these	O
additional	O
data	O
,	O
such	O
as	O
the	O
time	O
at	O
which	O
an	O
utterance	O
was	O
spoken	O
,	O
provide	O
a	O
useful	O
input	O
signal	O
for	O
a	O
LM	O
.	O

They	O
often	O
collect	O
utterances	O
spoken	O
by	O
users	O
,	O
along	O
with	O
associated	O
de	O
-	O
identified	O
contextual	O
information	O
.	O

Voice	O
assistants	O
have	O
become	O
ubiquitous	O
and	O
crucially	O
rely	O
on	O
ASR	O
systems	O
to	O
convert	O
user	O
inputs	O
to	O
text	O
.	O

The	O
LM	O
component	O
is	O
trained	O
separately	O
,	O
typically	O
on	O
large	O
amounts	O
of	O
transcribed	O
utterances	O
that	O
have	O
been	O
collected	O
by	O
an	O
existing	O
speech	O
recognition	O
system	O
.	O

Conventional	O
automatic	O
speech	O
recognition	O
(	O
ASR	O
)	O
systems	O
include	O
a	O
language	O
model	O
(	O
LM	O
)	O
and	O
an	O
acoustic	O
model	O
(	O
AM	O
)	O
.	O

Introduction	O
.	O

When	O
evaluated	O
on	O
utterances	O
extracted	O
from	O
the	O
long	O
tail	O
of	O
the	O
dataset	O
,	O
our	O
method	O
improves	O
perplexity	B-MetricName
by	O
9.0	B-MetricValue
%	I-MetricValue
relative	O
over	O
a	O
standard	O
LM	O
and	O
by	O
over	O
2.8	B-MetricValue
%	I-MetricValue
relative	O
when	O
compared	O
to	O
a	O
state	O
-	O
of	O
-	O
theart	O
model	O
for	O
contextual	O
LM	O
.	O

When	O
applied	O
to	O
a	O
large	O
de	O
-	O
identified	O
dataset	O
of	O
utterances	O
collected	O
by	O
a	O
popular	O
voice	O
assistant	O
platform	O
,	O
our	O
method	O
reduces	O
perplexity	O
by	O
7.0	B-MetricValue
%	I-MetricValue
relative	O
over	O
a	O
standard	O
LM	O
that	O
does	O
not	O
incorporate	O
contextual	O
information	O
.	O

We	O
introduce	O
an	O
attention	B-MethodName
mechanism	I-MethodName
for	O
training	O
neural	O
speech	O
recognition	O
language	O
models	O
on	O
both	O
text	O
and	O
nonlinguistic	O
contextual	O
data	O
1	O
.	O

For	O
some	O
domains	O
like	O
voice	O
assistants	O
,	O
however	O
,	O
additional	O
context	O
,	O
such	O
as	O
time	O
at	O
which	O
an	O
utterance	O
was	O
spoken	O
,	O
provides	O
a	O
rich	O
input	O
signal	O
.	O

Language	O
modeling	O
(	O
LM	O
)	O
for	O
automatic	B-TaskName
speech	I-TaskName
recognition	I-TaskName
(	O
ASR	B-TaskName
)	O
does	O
not	O
usually	O
incorporate	O
utterance	O
level	O
contextual	O
information	O
.	O

Attention	B-MethodName
-	I-MethodName
based	I-MethodName
Contextual	I-MethodName
Language	I-MethodName
Model	I-MethodName
Adaptation	I-MethodName
for	O
Speech	B-TaskName
Recognition	I-TaskName
.	O

As	O
we	O
move	O
away	O
from	O
the	O
morning	O
hours	O
,	O
the	O
conditional	O
probability	O
of	O
the	O
word	O
"	O
snooze	O
"	O
decreases	O
substantially	O
,	O
reaching	O
a	O
low	O
-	O
point	O
by	O
the	O
afternoon	O
and	O
evening	O
.	O

