a	O
black	O
dog	O
plays	O
in	O
the	O
water	O
.	O

a	O
black	O
dog	O
plays	O
around	O
in	O
water	O
looking	O
for	O
fish	O
.	O

Fact	O
→	O
Hum	O
a	O
black	O
dog	O
plays	O
around	O
in	O
water	O
.	O

three	O
kids	O
play	O
on	O
a	O
wall	O
with	O
a	O
green	O
ball	O
fighting	O
for	O
supremacy	O
.	O

three	O
kids	O
on	O
a	O
bar	O
on	O
a	O
field	O
of	O
a	O
date	O
.	O

Fact	O
→	O
Hum	O
three	O
kids	O
play	O
on	O
a	O
wall	O
with	O
a	O
green	O
ball	O
.	O

two	O
dogs	O
play	O
with	O
a	O
tennis	O
ball	O
in	O
the	O
snow	O
celebrating	O
their	O
friendship	O
.	O

two	O
dogs	O
play	O
with	O
a	O
tennis	O
ball	O
in	O
the	O
snow	O
.	O

Fact	O
→	O
Rom	O
two	O
dogs	O
play	O
with	O
a	O
tennis	O
ball	O
in	O
the	O
snow	O
.	O

-i	O
think	O
this	O
would	O
be	O
a	O
good	O
idea	O
if	O
you	O
could	O
not	O
be	O
a	O
statement	O
that	O
harry	O
's	O
signed	O
in	O
one	O
of	O
the	O
schedule	O
.	O

Non	O
-	O
polite	O
Input	O
DRG	O
Our	O
Model	O
jon	O
--please	O
use	O
this	O
resignation	O
letter	O
in	O
lieu	O
of	O
the	O
one	O
sent	O
on	O
friday	O
.	O

We	O
would	O
like	O
to	O
thank	O
Antonis	O
Anastasopoulos	O
,	O
Ritam	O
Dutt	O
,	O
Sopan	O
Khosla	O
,	O
and	O
,	O
Xinyi	O
Wang	O
for	O
the	O
helpful	O
discussions	O
.	O

We	O
would	O
also	O
like	O
to	O
acknowledge	O
NVIDIA	O
's	O
GPU	O
support	O
.	O

This	O
work	O
was	O
also	O
supported	O
in	O
part	O
by	O
ONR	O
Grant	O
N000141812861	O
,	O
NSF	O
IIS1763562	O
,	O
and	O
Apple	O
.	O

The	O
views	O
and	O
conclusions	O
contained	O
herein	O
are	O
those	O
of	O
the	O
authors	O
and	O
should	O
not	O
be	O
interpreted	O
as	O
necessarily	O
representing	O
the	O
official	O
policies	O
or	O
endorsements	O
,	O
either	O
expressed	O
or	O
implied	O
,	O
of	O
the	O
Air	O
Force	O
Research	O
Laboratory	O
or	O
the	O
U.S.	O
Government	O
.	O

The	O
U.S.	O
Government	O
is	O
authorized	O
to	O
reproduce	O
and	O
distribute	O
reprints	O
for	O
Governmental	O
purposes	O
notwithstanding	O
any	O
copyright	O
notation	O
thereon	O
.	O

This	O
material	O
is	O
based	O
on	O
research	O
sponsored	O
in	O
part	O
by	O
the	O
Air	O
Force	O
Research	O
Laboratory	O
under	O
agreement	O
number	O
FA8750	O
-	O
19	O
-	O
2	O
-	O
0200	O
.	O

Acknowledgments	O
.	O

Automatic	O
and	O
human	O
evaluation	O
shows	O
that	O
our	O
approach	O
outperforms	O
other	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
on	O
content	B-MetricName
preservation	I-MetricName
metrics	O
while	O
retaining	O
(	O
or	O
in	O
some	O
cases	O
improving	O
)	O
the	O
transfer	O
accuracies	B-MetricName
.	O

We	O
believe	O
our	O
approach	O
is	O
the	O
first	O
to	O
be	O
robust	O
in	O
cases	O
when	O
the	O
source	O
is	O
style	O
neutral	O
,	O
like	O
the	O
"	O
non	O
-	O
polite	O
"	O
class	O
in	O
the	O
case	O
of	O
politeness	B-TaskName
transfer	I-TaskName
.	O

We	O
extend	O
prior	O
works	O
(	O
Li	O
et	O
al	O
.	O
,	O
2018;Sudhakar	O
et	O
al	O
.	O
,	O
2019	O
)	O
on	O
attribute	B-TaskName
transfer	I-TaskName
by	O
introducing	O
a	O
simple	O
pipeline	O
-tag	B-MethodName
&	I-MethodName
generate	I-MethodName
which	O
is	O
an	O
interpretable	O
two	O
-	O
staged	O
approach	O
for	O
content	O
preserving	O
style	B-TaskName
transfer	I-TaskName
.	O

We	O
introduce	O
the	O
task	O
of	O
politeness	B-TaskName
transfer	I-TaskName
for	O
which	O
we	O
provide	O
a	O
dataset	O
comprised	O
of	O
sentences	O
curated	O
from	O
email	O
exchanges	O
present	O
in	O
the	O
Enron	O
corpus	O
.	O

We	O
conclude	O
that	O
the	O
choice	O
of	O
the	O
tagger	O
variant	O
is	O
dependent	O
on	O
the	O
characterstics	O
of	O
the	O
underlying	O
transfer	B-TaskName
task	I-TaskName
.	O

While	O
the	O
combined	O
tagger	O
learns	O
to	O
use	O
the	O
optimal	O
tagging	O
operation	O
to	O
some	O
extent	O
,	O
a	O
deeper	O
understanding	O
of	O
this	O
phenomenon	O
is	O
an	O
interesting	O
future	O
topic	O
for	O
research	O
.	O

In	O
contrast	O
,	O
on	O
the	O
CAPTIONS	B-DatasetName
dataset	O
,	O
it	O
performs	O
50	O
%	O
more	O
add	O
operations	O
.	O

We	O
find	O
that	O
for	O
Yelp	B-DatasetName
(	O
a	O
polar	O
dataset	O
)	O
the	O
combined	O
tagger	O
performs	O
20	O
%	O
more	O
replace	O
operations	O
(	O
as	O
compared	O
to	O
add	O
operations	O
)	O
.	O

To	O
check	O
if	O
the	O
combined	O
tagger	O
is	O
learning	O
to	O
perform	O
the	O
operation	O
that	O
is	O
more	O
suitable	O
for	O
a	O
dataset	O
,	O
we	O
calculate	O
the	O
fraction	O
of	O
times	O
the	O
combined	O
tagger	O
performs	O
add	O
/	O
replace	O
operations	O
on	O
the	O
Yelp	B-DatasetName
and	O
Captions	B-DatasetName
datasets	O
.	O

Thus	O
,	O
we	O
can	O
use	O
the	O
add	O
-	O
tagger	O
variant	O
for	O
transfer	O
from	O
a	O
polarized	O
class	O
to	O
a	O
neutral	O
class	O
as	O
well	O
.	O

Interestingly	O
,	O
the	O
accuracy	B-MetricName
of	O
the	O
add	O
-	O
tagger	O
is	O
≈	O
50	O
%	O
in	O
the	O
case	O
of	O
Yelp	O
,	O
since	O
adding	O
negative	O
words	O
to	O
a	O
positive	O
sentence	O
or	O
vice	O
-	O
versa	O
neutralizes	O
the	O
classifier	O
scores	O
.	O

On	O
the	O
contrary	O
,	O
for	O
Yelp	B-DatasetName
,	O
where	O
both	O
polarities	O
are	O
clearly	O
defined	O
,	O
the	O
replace	O
-	O
tagger	O
gives	O
the	O
best	O
performance	O
.	O

8	O
We	O
train	O
these	O
tagger	O
variants	O
on	O
the	O
Yelp	B-DatasetName
and	O
Captions	B-DatasetName
datasets	O
and	O
present	O
the	O
results	O
in	O
tagger	O
provides	O
the	O
best	O
accuracy	B-MetricName
with	O
a	O
relatively	O
negligible	O
drop	O
in	O
BLEU	B-MetricName
scores	O
.	O

We	O
also	O
train	O
and	O
compare	O
them	O
with	O
a	O
combined	O
variant	O
.	O

7	O
Ablations	O
We	O
provide	O
a	O
comparison	O
of	O
the	O
two	O
variants	O
of	O
the	O
tagger	O
,	O
namely	O
the	O
replace	O
-	O
tagger	O
and	O
add	O
-	O
tagger	O
on	O
two	O
datasets	O
.	O

Our	O
model	O
follows	O
the	O
strategies	O
prescribed	O
in	O
(	O
Danescu	O
-	O
Niculescu	O
-	O
Mizil	O
et	O
al	O
.	O
,	O
2013	O
)	O
while	O
generating	O
polite	O
sentences	O
.	O

The	O
fourth	O
sentence	O
projects	O
the	O
case	O
where	O
our	O
model	O
uses	O
"	O
thanks	O
"	O
at	O
the	O
end	O
to	O
express	O
gratitude	O
and	O
in	O
turn	O
,	O
makes	O
the	O
sentence	O
more	O
polite	O
.	O

According	O
to	O
the	O
Please	O
Start	O
strategy	O
,	O
it	O
is	O
more	O
direct	O
and	O
insincere	O
to	O
start	O
a	O
sentence	O
with	O
"	O
Please	O
"	O
.	O

The	O
third	O
sentence	O
highlights	O
the	O
ability	O
of	O
the	O
model	O
to	O
add	O
Apologizing	O
words	O
like	O
"	O
Sorry	O
"	O
which	O
helps	O
in	O
deflecting	O
the	O
social	O
threat	O
of	O
the	O
request	O
by	O
attuning	O
to	O
the	O
imposition	O
.	O

The	O
second	O
sentence	O
highlights	O
another	O
subtle	O
concept	O
of	O
politeness	O
of	O
1st	O
Person	O
Plural	O
where	O
adding	O
"	O
we	O
"	O
helps	O
being	O
indirect	O
and	O
creates	O
the	O
sense	O
that	O
the	O
burden	O
of	O
the	O
request	O
is	O
shared	O
between	O
speaker	O
and	O
addressee	O
.	O

The	O
first	O
sentence	O
presents	O
a	O
simple	O
example	O
of	O
the	O
counterfactual	O
modal	O
strategy	O
inducing	O
"	O
Could	O
you	O
please	O
"	O
to	O
make	O
the	O
sentence	O
polite	O
.	O

Our	O
analysis	O
is	O
based	O
on	O
the	O
linguistic	O
strategies	O
for	O
politeness	O
as	O
described	O
in	O
(	O
Danescu	O
-	O
Niculescu	O
-	O
Mizil	O
et	O
al	O
.	O
,	O
2013	O
)	O
.	O

Qualitative	O
Analysis	O
We	O
compare	O
the	O
results	O
of	O
our	O
model	O
with	O
the	O
DRG	B-MethodName
model	O
qualitatively	O
as	O
shown	O
in	O
Table	O
4	O
.	O

Alongside	O
,	O
we	O
also	O
observe	O
consistent	O
improvements	O
of	O
our	O
model	O
on	O
target	B-MetricName
attribute	I-MetricName
matching	I-MetricName
and	O
grammatical	B-MetricName
correctness	I-MetricName
.	O

We	O
observe	O
a	O
significant	O
improvement	O
in	O
content	B-MetricName
preservation	I-MetricName
scores	O
across	O
various	O
datasets	O
(	O
specifically	O
in	O
Politeness	B-DatasetName
domain	O
)	O
highlighting	O
the	O
ability	O
of	O
our	O
model	O
to	O
retain	O
content	O
better	O
than	O
DRG	B-MethodName
.	O

Table	O
3	O
shows	O
the	O
results	O
of	O
human	O
evaluations	O
.	O

Overall	O
,	O
we	O
evaluate	O
both	O
systems	O
on	O
a	O
total	O
of	O
200	O
samples	O
for	O
Politeness	B-DatasetName
and	O
100	O
samples	O
each	O
for	O
Yelp	B-DatasetName
,	O
Gender	B-DatasetName
and	O
Political	B-DatasetName
.	O

(	O
2018	O
)	O
for	O
our	O
human	O
study	O
.	O

We	O
've	O
used	O
the	O
same	O
instructions	O
from	O
Li	O
et	O
al	O
.	O

Instead	O
we	O
rely	O
on	O
the	O
classifier	O
scores	O
for	O
the	O
transfer	O
.	O

Since	O
the	O
judgement	O
of	O
signals	O
that	O
indicate	O
gender	O
and	O
political	O
inclination	O
are	O
prone	O
to	O
personal	O
biases	O
,	O
we	O
do	O
n't	O
annotate	O
these	O
tasks	O
for	O
target	O
attribute	O
match	O
metric	O
.	O

For	O
each	O
of	O
these	O
metrics	O
,	O
the	O
reviewers	O
give	O
a	O
score	O
between	O
1	O
-	O
5	O
to	O
each	O
of	O
the	O
outputs	O
,	O
where	O
1	O
reflects	O
a	O
poor	O
performance	O
on	O
the	O
task	O
and	O
5	O
means	O
a	O
perfect	O
output	O
.	O

(	O
2018	O
)	O
,	O
we	O
select	O
10	O
unbiased	O
human	O
judges	O
to	O
rate	O
the	O
output	O
of	O
our	O
model	O
and	O
DRG	B-MethodName
on	O
three	O
aspects	O
:	O
(	O
1	O
)	O
content	B-MetricName
preservation	I-MetricName
(	O
Con	O
)	O
(	O
2	O
)	O
grammaticality	B-MetricName
of	O
the	O
generated	O
content	O
(	O
Gra	O
)	O
(	O
3	O
)	O
target	B-MetricName
attribute	I-MetricName
match	I-MetricName
of	O
the	O
generations	O
(	O
Att	O
)	O
.	O

Human	O
Evaluation	O
Following	O
Li	O
et	O
al	O
.	O

In	O
some	O
of	O
these	O
cases	O
,	O
we	O
noticed	O
that	O
changing	O
non	O
-	O
tagged	O
words	O
helped	O
in	O
producing	O
outputs	O
that	O
were	O
more	O
natural	O
and	O
fluent	O
.	O

We	O
found	O
that	O
the	O
non	O
-	O
tagged	O
words	O
were	O
changed	O
for	O
only	O
6.9	O
%	O
of	O
the	O
sentences	O
.	O

In	O
order	O
to	O
quantify	O
this	O
,	O
we	O
calculate	O
the	O
fraction	O
of	O
non	O
-	O
tagged	O
words	O
being	O
changed	O
across	O
the	O
datasets	O
.	O

Clearly	O
,	O
replacing	O
content	O
words	O
is	O
not	O
desired	O
since	O
it	O
may	O
drastically	O
change	O
the	O
meaning	O
.	O

Changing	O
Content	O
Words	O
Given	O
that	O
our	O
model	O
is	O
explicitly	O
trained	O
to	O
generate	O
new	O
content	O
only	O
in	O
place	O
of	O
the	O
TAG	O
token	O
,	O
it	O
is	O
expected	O
that	O
a	O
welltrained	O
system	O
will	O
retain	O
most	O
of	O
the	O
non	O
-	O
tagged	O
(	O
content	O
)	O
words	O
.	O

In	O
summary	O
,	O
evaluation	O
via	O
automatic	O
metrics	O
might	O
not	O
truly	O
correlate	O
with	O
task	O
success	O
.	O

Despite	O
high	O
evaluation	O
scores	O
,	O
it	O
does	O
not	O
reflect	O
a	O
high	O
rate	O
of	O
success	O
on	O
the	O
task	O
.	O

This	O
baseline	O
achieves	O
an	O
average	O
accuracy	B-MetricName
score	O
of	O
91.3	B-MetricValue
%	I-MetricValue
and	O
a	O
BLEU	B-MetricName
score	O
of	O
61.44	B-MetricValue
on	O
the	O
Yelp	B-DatasetName
dataset	O
.	O

Similarly	O
,	O
it	O
appends	O
"	O
but	O
overall	O
it	O
was	O
perfect	O
"	O
for	O
transfer	O
into	O
a	O
positive	O
sentiment	O
.	O

This	O
baseline	O
adds	O
"	O
but	O
overall	O
it	O
sucked	O
"	O
at	O
the	O
end	O
of	O
the	O
sentence	O
to	O
transfer	O
it	O
to	O
negative	O
sentiment	O
.	O

We	O
test	O
this	O
hypothesis	O
on	O
the	O
sentiment	B-TaskName
transfer	I-TaskName
task	O
by	O
a	O
Naive	B-MethodName
Baseline	I-MethodName
.	O

BLEU	B-MetricName
relies	O
heavily	O
on	O
n	O
-	O
gram	O
overlap	O
and	O
classifiers	O
can	O
be	O
fooled	O
by	O
certain	O
polarizing	O
keywords	O
.	O

While	O
popular	O
,	O
the	O
metrics	O
of	O
transfer	O
accuracy	B-MetricName
and	O
BLEU	B-MetricName
have	O
significant	O
shortcomings	O
making	O
them	O
susceptible	O
to	O
simple	O
adversaries	O
.	O

Since	O
we	O
do	O
n't	O
make	O
any	O
such	O
assumptions	O
,	O
we	O
perform	O
significantly	O
better	O
on	O
this	O
dataset	O
.	O

Hence	O
,	O
the	O
performance	O
of	O
their	O
model	O
is	O
worse	O
in	O
this	O
case	O
.	O

(	O
2018	O
)	O
,	O
one	O
of	O
the	O
unique	O
aspects	O
of	O
the	O
Amazon	B-DatasetName
dataset	O
is	O
the	O
absence	O
of	O
similar	O
content	O
in	O
both	O
the	O
sentiment	O
polarities	O
.	O

As	O
noted	O
by	O
Li	O
et	O
al	O
.	O

Additionally	O
,	O
we	O
improve	O
the	O
transfer	O
accuracy	B-MetricName
for	O
Amazon	O
by	O
14.2	O
%	O
while	O
achieving	O
accuracies	O
similar	O
to	O
DRG	B-MethodName
on	O
Yelp	B-DatasetName
and	O
Captions	B-DatasetName
.	O

We	O
observe	O
an	O
increase	O
in	O
the	O
BLEU	B-MetricName
-	I-MetricName
reference	I-MetricName
scores	O
by	O
5.25	B-MetricValue
,	O
4.95	B-MetricValue
and	O
3.64	B-MetricValue
on	O
the	O
Yelp	B-DatasetName
,	O
Amazon	B-DatasetName
,	O
and	O
Captions	B-DatasetName
test	O
sets	O
respectively	O
.	O

(	O
2018	O
)	O
.	O

For	O
each	O
of	O
the	O
datasets	O
our	O
test	O
set	O
comprises	O
500	O
samples	O
(	O
with	O
human	O
references	O
)	O
curated	O
by	O
Li	O
et	O
al	O
.	O

In	O
Table	O
2	O
,	O
we	O
compare	O
our	O
model	O
against	O
CAE	B-MethodName
and	O
DRG	B-MethodName
on	O
the	O
Yelp	B-DatasetName
,	O
Amazon	B-DatasetName
,	O
and	O
Captions	B-DatasetName
datasets	O
.	O

The	O
classifier	O
accuracy	B-MetricName
on	O
the	O
generations	O
of	O
our	O
model	O
are	O
comparable	O
(	O
within	O
1	B-MetricValue
%	I-MetricValue
)	O
with	O
that	O
of	O
DRG	B-MethodName
for	O
the	O
Politeness	B-DatasetName
dataset	O
.	O

In	O
general	O
,	O
CAE	B-MethodName
and	O
BST	B-MethodName
achieve	O
high	O
classifier	O
accuracies	B-MetricName
but	O
they	O
fail	O
to	O
retain	O
the	O
original	O
content	O
.	O

The	O
BLEU	B-MetricName
score	O
on	O
the	O
Politeness	B-TaskName
task	I-TaskName
is	O
greater	O
by	O
58.61	B-MetricValue
points	O
with	O
respect	O
to	O
DRG	B-MethodName
.	O

Table	O
1	O
shows	O
that	O
our	O
model	O
achieves	O
significantly	O
higher	O
scores	O
on	O
BLEU	B-MetricName
,	O
ROUGE	B-MetricName
and	O
METEOR	B-MetricName
as	O
compared	O
to	O
the	O
baselines	O
DRG	B-MethodName
,	O
CAE	B-MethodName
and	O
BST	B-MethodName
on	O
the	O
Politeness	B-DatasetName
,	O
Gender	B-DatasetName
and	O
Political	B-DatasetName
datasets	O
.	O

In	O
particular	O
,	O
METEOR	B-MetricName
also	O
uses	O
synonyms	O
and	O
stemmed	O
forms	O
of	O
the	O
words	O
in	O
candidate	O
and	O
reference	O
sentences	O
,	O
and	O
thus	O
may	O
be	O
better	O
at	O
quantifying	O
semantic	O
similarities	O
.	O

We	O
also	O
report	O
ROUGE	B-MetricName
(	O
ROU	B-MetricName
)	O
(	O
Lin	O
,	O
2004	O
)	O
and	O
METEOR	B-MetricName
(	O
MET	B-MetricName
)	O
(	O
Denkowski	O
and	O
Lavie	O
,	O
2011	O
)	O
scores	O
.	O

Additionally	O
,	O
we	O
report	O
the	O
BLEU	B-MetricName
-	I-MetricName
reference	I-MetricName
(	O
BL	B-DatasetName
-	I-DatasetName
r	I-DatasetName
)	O
scores	O
using	O
the	O
human	O
reference	O
sentences	O
on	O
the	O
Yelp	B-DatasetName
,	O
Amazon	B-DatasetName
and	O
Captions	B-DatasetName
datasets	O
(	O
Li	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

The	O
standard	O
metric	O
for	O
measuring	O
content	O
preservation	O
is	O
BLEU	B-MetricName
-	I-MetricName
self	I-MetricName
(	O
BL	B-MetricName
-	I-MetricName
s	I-MetricName
)	O
(	O
Papineni	O
et	O
al	O
.	O
,	O
2002	O
)	O
which	O
is	O
computed	O
with	O
respect	O
to	O
the	O
original	O
sentences	O
.	O

6	O
The	O
metric	O
of	O
transfer	O
accuracy	B-MetricName
(	O
Acc	O
)	O
is	O
defined	O
as	O
the	O
percentage	O
of	O
generated	O
sentences	O
classified	O
to	O
be	O
in	O
the	O
target	O
domain	O
by	O
the	O
classifier	O
.	O

5	O
For	O
politeness	B-TaskName
,	O
we	O
use	O
the	O
classifier	O
trained	O
by	O
(	O
Niu	O
and	O
Bansal	O
,	O
2018	O
)	O
.	O

We	O
use	O
the	O
implementation	O
provided	O
by	O
fastai	O
.	O

The	O
architecture	O
of	O
the	O
classifier	O
is	O
based	O
on	O
AWD	B-MethodName
-	I-MethodName
LSTM	I-MethodName
(	O
Merity	O
et	O
al	O
.	O
,	O
2017	O
)	O
and	O
a	O
softmax	B-HyperparameterValue
layer	B-HyperparameterName
trained	O
via	O
cross	O
-	O
entropy	O
loss	O
.	O

To	O
capture	O
accuracy	B-MetricName
,	O
we	O
use	O
a	O
classifier	O
trained	O
on	O
the	O
nonparallel	O
style	O
corpora	O
for	O
the	O
respective	O
datasets	O
(	O
barring	O
politeness	O
)	O
.	O

Automated	O
Evaluation	O
Following	O
prior	O
work	O
(	O
Li	O
et	O
al	O
.	O
,	O
2018;Shen	O
et	O
al	O
.	O
,	O
2017	O
)	O
,	O
we	O
use	O
automatic	O
metrics	O
for	O
evaluation	O
of	O
the	O
models	O
along	O
two	O
major	O
dimensions	O
:	O
(	O
1	O
)	O
style	O
transfer	O
accuracy	B-MetricName
and	O
(	O
2	O
)	O
content	B-MetricName
preservation	I-MetricName
.	O

For	O
the	O
tagger	O
,	O
we	O
re	O
-	O
rank	O
the	O
final	O
beam	O
search	O
outputs	O
based	O
on	O
the	O
number	O
of	O
[	O
TAG	O
]	O
tokens	O
in	O
the	O
output	O
sequence	O
(	O
favoring	O
more	O
[	O
TAG	O
]	O
tokens	O
)	O
.	O

During	O
inference	O
we	O
use	O
beam	O
search	O
(	O
beam	B-HyperparameterName
size=5	I-HyperparameterName
)	O
to	O
decode	O
tagged	O
sentences	O
and	O
targeted	O
generations	O
for	O
tagger	O
&	O
generator	O
respectively	O
.	O

For	O
Yelp	O
k	B-HyperparameterName
is	O
set	O
to	O
0.97	B-HyperparameterValue
.	O

For	O
all	O
datasets	O
except	O
Yelp	B-DatasetName
we	O
use	O
phrases	O
with	O
p	O
2	O
1	O
(	O
w	O
)	O
≥	O
k	O
=	O
0.9	O
to	O
construct	O
Γ	O
2	O
,	O
Γ	O
1	O
(	O
§	O
4.1	O
)	O
.	O

The	O
value	O
of	O
the	O
smoothing	B-HyperparameterName
parameter	I-HyperparameterName
γ	B-HyperparameterName
in	O
Eq	O
.	O
2	O
is	O
set	O
to	O
0.75	B-HyperparameterValue
.	O

Both	O
modules	O
were	O
also	O
trained	O
with	O
the	O
BPE	O
tokenization	O
(	O
Sennrich	O
et	O
al	O
.	O
,	O
2015	O
)	O
using	O
a	O
vocabulary	O
of	O
size	O
16000	O
for	O
all	O
the	O
datasets	O
except	O
for	O
Captions	B-DatasetName
,	O
which	O
was	O
trained	O
using	O
4000	O
BPE	O
tokens	O
.	O

We	O
empirically	O
observed	O
that	O
these	O
techniques	O
provide	O
an	O
improvement	O
in	O
the	O
fluency	O
and	O
diversity	O
of	O
the	O
generations	O
.	O

For	O
the	O
politeness	O
dataset	O
the	O
generator	O
module	O
is	O
trained	O
with	O
data	O
augmentation	O
techniques	O
like	O
random	O
word	O
shuffle	O
,	O
word	O
drops	O
/	O
replacements	O
as	O
proposed	O
by	O
(	O
I	O
m	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

Dropout	B-HyperparameterName
(	O
Srivastava	O
et	O
al	O
.	O
,	O
2014	O
)	O
with	O
p	B-HyperparameterName
-	I-HyperparameterName
value	I-HyperparameterName
0.3	B-HyperparameterValue
is	O
added	O
for	O
each	O
layer	O
in	O
the	O
transformer	O
.	O

Each	O
transformer	O
has	O
4	B-HyperparameterValue
attention	B-HyperparameterName
heads	I-HyperparameterName
with	O
a	O
512	B-HyperparameterValue
dimensional	I-HyperparameterValue
embedding	B-HyperparameterName
layer	I-HyperparameterName
and	O
hidden	B-HyperparameterName
state	I-HyperparameterName
size	I-HyperparameterName
.	O

Implementation	O
Details	O
We	O
use	O
4	B-HyperparameterValue
-	O
layered	B-HyperparameterName
transformers	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O
2017	O
)	O
to	O
train	O
both	O
tagger	O
and	O
generator	O
modules	O
.	O

DRG	B-MethodName
(	O
Li	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
Style	B-MethodName
Transfer	I-MethodName
Through	I-MethodName
Back	I-MethodName
-	I-MethodName
translation	I-MethodName
(	O
BST	B-MethodName
)	O
(	O
Prabhumoye	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
and	O
Style	B-MethodName
transfer	I-MethodName
from	I-MethodName
nonparallel	I-MethodName
text	I-MethodName
by	I-MethodName
cross	I-MethodName
alignment	I-MethodName
(	O
Shen	O
et	O
al	O
.	O
,	O
2017	O
)	O
(	O
CAE	B-MethodName
)	O
.	O

Baselines	O
We	O
compare	O
our	O
systems	O
against	O
three	O
previous	O
methods	O
.	O

Experiments	O
and	O
Results	O
.	O

For	O
example	O
,	O
in	O
the	O
case	O
of	O
politeness	B-TaskName
transfer	I-TaskName
,	O
the	O
tags	O
added	O
at	O
the	O
beginning	O
(	O
t	O
=	O
0	O
)	O
will	O
almost	O
always	O
be	O
used	O
to	O
generate	O
a	O
token	O
like	O
"	O
Would	O
it	O
be	O
possible	O
...	O
"	O
whereas	O
for	O
a	O
higher	O
t	O
,	O
[	O
TAG	O
]	O
t	O
may	O
be	O
replaced	O
with	O
a	O
token	O
like	O
"	O
thanks	O
"	O
or	O
"	O
sorry	O
.	O
"	O
x	O
(	O
v	O
)	O
i	O
∈	O
X	O
v	O
,	O
v	O
∈	O
{	O
1	O
,	O
.	O

By	O
training	O
both	O
tagger	O
and	O
generator	O
with	O
these	O
positional	O
[	O
TAG	O
]	O
t	O
tokens	O
we	O
enable	O
them	O
to	O
easily	O
realize	O
different	O
distributions	O
of	O
style	O
attributes	O
for	O
different	O
positions	O
in	O
a	O
sentence	O
.	O

T	O
}	O
for	O
a	O
sentence	O
of	O
length	O
T	O
.	O

Hence	O
,	O
instead	O
of	O
using	O
a	O
single	O
[	O
TAG	O
]	O
token	O
,	O
we	O
use	O
a	O
set	O
of	O
positional	O
tokens	O
[	O
TAG	O
]	O
t	O
where	O
t	O
∈	O
{	O
0	O
,	O
1	O
,	O
.	O

L(θ	O
g	O
)	O
=	O
−	O
|Xv|	O
i=1	O
log	O
P	O
θg	O
(	O
x	O
(	O
v	O
)	O
i	O
|z(x	O
i	O
)	O
;	O
θ	O
g	O
)	O
(	O
5	O
)	O
The	O
training	O
data	O
for	O
transfer	O
into	O
style	O
S	O
v	O
comprises	O
of	O
pairs	O
where	O
the	O
input	O
is	O
given	O
by	O
{	O
z(x	O
i	O
)	O
:	O
Finally	O
,	O
we	O
note	O
that	O
the	O
location	O
at	O
which	O
the	O
tags	O
are	O
generated	O
has	O
a	O
significant	O
impact	O
on	O
the	O
distribution	O
over	O
style	O
attributes	O
(	O
in	O
Γ	O
2	O
)	O
that	O
are	O
used	O
to	O
fill	O
the	O
[	O
TAG	O
]	O
token	O
at	O
a	O
particular	O
position	O
.	O

The	O
training	O
for	O
the	O
generator	O
model	O
is	O
complimentary	O
to	O
that	O
of	O
the	O
tagger	O
,	O
in	O
the	O
sense	O
that	O
the	O
generator	O
takes	O
as	O
input	O
the	O
tagged	O
output	O
z(x	O
i	O
)	O
inferred	O
from	O
the	O
source	O
style	O
and	O
modifies	O
the	O
[	O
TAG	O
]	O
tokens	O
to	O
generate	O
the	O
desired	O
sentence	O
x(v	O
)	O
i	O
in	O
the	O
target	O
style	O
S	O
v	O
.	O

Style	O
Targeted	O
Generation	O
.	O

L	O
a	O
(	O
θ	O
t	O
)	O
=	O
−	O
|X	O
1	O
|	O
i=1	O
log	O
P	O
θt	O
(	O
z(x	O
i	O
)	O
|x	O
(	O
2	O
)	O
i	O
\a(x	O
(	O
2	O
)	O
i	O
)	O
;	O
θ	O
t	O
)	O
(	O
4	O
)	O
.	O

The	O
loss	O
objective	O
(	O
L	O
a	O
)	O
given	O
by	O
Eq	O
.	O
4	O
is	O
crucial	O
for	O
tasks	O
like	O
politeness	B-TaskName
transfer	I-TaskName
where	O
one	O
of	O
the	O
styles	O
is	O
poorly	O
defined	O
.	O

In	O
effect	O
,	O
by	O
training	O
in	O
this	O
fashion	O
,	O
the	O
tagger	O
learns	O
to	O
add	O
[	O
TAG	O
]	O
tokens	O
at	O
appropriate	O
locations	O
in	O
a	O
style	O
neutral	O
sentence	O
.	O

For	O
example	O
,	O
in	O
the	O
case	O
of	O
politeness	B-TaskName
transfer	I-TaskName
,	O
we	O
only	O
use	O
the	O
sentences	O
labeled	O
as	O
"	O
polite	O
"	O
for	O
training	O
.	O

Note	O
that	O
we	O
only	O
use	O
samples	O
from	O
X	O
2	O
for	O
training	O
the	O
add	O
-	O
tagger	O
;	O
samples	O
from	O
the	O
style	O
neutral	O
X	O
1	O
are	O
not	O
involved	O
in	O
the	O
training	O
process	O
at	O
all	O
.	O

4	O
,	O
we	O
remove	O
the	O
style	O
phrases	O
"	O
you	O
would	O
like	O
to	O
"	O
and	O
"	O
please	O
"	O
and	O
replace	O
them	O
with	O
[	O
TAG	O
]	O
in	O
the	O
output	O
.	O

As	O
indicated	O
in	O
Fig	O
.	O

For	O
the	O
output	O
we	O
replace	O
the	O
same	O
phrases	O
a	O
(	O
x(2	O
)	O
i	O
)	O
with	O
[	O
TAG	O
]	O
tokens	O
.	O

Essentially	O
,	O
for	O
the	O
input	O
we	O
take	O
samples	O
x	O
(	O
2	O
)	O
i	O
in	O
the	O
target	O
style	O
S	O
2	O
and	O
explicitly	O
remove	O
style	O
phrases	O
a	O
(	O
x(2	O
)	O
i	O
)	O
from	O
it	O
.	O

4	O
)	O
for	O
the	O
add	O
-	O
tagger	O
is	O
given	O
by	O
pairs	O
where	O
the	O
input	O
is	O
{	O
x	O
(	O
2	O
)	O
i	O
\a(x	O
(	O
2	O
)	O
i	O
)	O
:	O
x	O
(	O
2	O
)	O
i	O
∈	O
X	O
2	O
}	O
and	O
the	O
output	O
is	O
{	O
z(x	O
i	O
)	O
:	O
x	O
(	O
2	O
)	O
i	O
∈	O
X	O
2	O
}	O
.	O

The	O
training	O
data	O
(	O
Fig	O
.	O

In	O
such	O
cases	O
,	O
since	O
the	O
source	O
sentences	O
have	O
no	O
attribute	O
markers	O
to	O
remove	O
,	O
the	O
tagger	O
learns	O
to	O
add	O
[	O
TAG	O
]	O
tokens	O
at	O
specific	O
locations	O
suitable	O
for	O
emanating	O
style	O
words	O
in	O
the	O
target	O
style	O
.	O

(	O
2018	O
)	O
)	O
.	O

Examples	O
of	O
such	O
a	O
task	O
include	O
the	O
tasks	O
of	O
politeness	B-TaskName
transfer	I-TaskName
(	O
introduced	O
in	O
this	O
paper	O
)	O
and	O
caption	B-TaskName
style	I-TaskName
transfer	I-TaskName
(	O
used	O
by	O
Li	O
et	O
al	O
.	O

That	O
is	O
,	O
X	O
1	O
consists	O
of	O
style	O
neutral	O
sentences	O
whereas	O
X	O
2	O
consists	O
of	O
sentences	O
in	O
the	O
target	O
style	O
.	O

L	O
r	O
(	O
θ	O
t	O
)	O
=	O
−	O
|X	O
1	O
|	O
i=1	O
log	O
P	O
θt	O
(	O
z(x	O
i	O
)	O
|x	O
(	O
1	O
)	O
i	O
;	O
θ	O
t	O
)	O
(	O
3	O
)	O
The	O
second	O
variant	O
,	O
add	O
-	O
tagger	O
,	O
is	O
designed	O
for	O
cases	O
where	O
the	O
transfer	O
needs	O
to	O
happen	O
from	O
style	O
neutral	O
sentences	O
to	O
the	O
target	O
style	O
.	O

The	O
loss	O
objective	O
for	O
replace	O
-	O
tagger	O
is	O
given	O
by	O
L	O
r	O
(	O
θ	O
t	O
)	O
in	O
Eq	O
.	O
3	O
.	O

In	O
this	O
case	O
the	O
training	O
data	O
comprises	O
of	O
pairs	O
where	O
the	O
input	O
is	O
X	O
1	O
and	O
the	O
output	O
is	O
{	O
z(x	O
i	O
)	O
:	O
x	O
(	O
1	O
)	O
i	O
∈	O
X	O
1	O
}	O
.	O

The	O
first	O
variant	O
,	O
replace	O
-	O
tagger	O
,	O
is	O
suited	O
for	O
a	O
task	O
like	O
sentiment	O
transfer	O
where	O
almost	O
every	O
sentence	O
has	O
some	O
attribute	O
markers	O
a(x	O
(	O
1	O
)	O
i	O
)	O
present	O
in	O
it	O
.	O

Finally	O
,	O
we	O
use	O
the	O
distribution	O
p	O
2	O
1	O
(	O
w)/p	O
1	O
2	O
(	O
w	O
)	O
over	O
Γ	O
2	O
/Γ	O
1	O
(	O
§	O
4.1	O
)	O
to	O
draw	O
samples	O
of	O
attribute	O
-	O
markers	O
that	O
would	O
be	O
replaced	O
with	O
the	O
[	O
TAG	O
]	O
token	O
during	O
the	O
creation	O
of	O
training	O
data	O
.	O

In	O
both	O
the	O
cases	O
,	O
the	O
[	O
TAG	O
]	O
tokens	O
indicate	O
positions	O
where	O
the	O
generator	O
can	O
insert	O
phrases	O
from	O
the	O
target	O
style	O
S	O
2	O
.	O

Depending	O
on	O
the	O
style	B-TaskName
transfer	I-TaskName
task	O
,	O
the	O
tagger	O
is	O
trained	O
to	O
either	O
(	O
1	O
)	O
identify	O
and	O
replace	O
style	O
attributes	O
a	O
(	O
x(1	O
)	O
i	O
)	O
with	O
the	O
token	O
tag	O
[	O
TAG	O
]	O
(	O
replace	O
-	O
tagger	O
)	O
or	O
(	O
2	O
)	O
add	O
the	O
[	O
TAG	O
]	O
token	O
at	O
specific	O
locations	O
in	O
x	O
(	O
1	O
)	O
i	O
(	O
add	O
-	O
tagger	O
)	O
.	O

The	O
tagger	O
model	O
(	O
with	O
parameters	O
θ	O
t	O
)	O
takes	O
as	O
input	O
the	O
sentences	O
in	O
X	O
1	O
and	O
outputs	O
{	O
z(x	O
i	O
)	O
:	O
x	O
i	O
∈	O
X	O
1	O
}	O
.	O

Style	O
Invariant	O
Tagged	O
Sentence	O
.	O

Γ	O
1	O
is	O
computed	O
similarly	O
where	O
we	O
use	O
p	O
1	O
2	O
(	O
w	O
)	O
,	O
η	O
1	O
2	O
(	O
w	O
)	O
.	O

Finally	O
,	O
we	O
estimate	O
Γ	O
2	O
by	O
Γ	O
2	O
=	O
{	O
w	O
:	O
p	O
2	O
1	O
(	O
w	O
)	O
≥	O
k	O
}	O
In	O
other	O
words	O
,	O
Γ	O
2	O
consists	O
of	O
the	O
set	O
of	O
phrases	O
in	O
X	O
2	O
above	O
a	O
given	O
style	O
impact	O
k.	O

We	O
further	O
smooth	O
and	O
normalize	O
η	O
2	O
1	O
(	O
w	O
)	O
to	O
get	O
p	O
2	O
1	O
(	O
w	O
)	O
.	O

Words	O
with	O
higher	O
values	O
for	O
η	O
2	O
1	O
(	O
w	O
)	O
have	O
a	O
higher	O
mean	O
tf	O
-	O
idf	O
in	O
X	O
2	O
vs	O
X	O
1	O
,	O
and	O
thus	O
are	O
more	O
characteristic	O
of	O
S	O
2	O
.	O

η	O
2	O
1	O
(	O
w	O
)	O
=	O
1	O
m	O
m	O
i=1	O
tf	O
-	O
idf(w	O
,	O
x(2	O
)	O
i	O
)	O
1	O
n	O
n	O
j=1	O
tf	O
-	O
idf(w	O
,	O
x(1	O
)	O
j	O
)	O
(	O
1	O
)	O
p	O
2	O
1	O
(	O
w	O
)	O
=	O
η	O
2	O
1	O
(	O
w	O
)	O
γ	O
w	O
η	O
2	O
1	O
(	O
w	O
)	O
γ	O
(	O
2	O
)	O
where	O
,	O
η	O
2	O
1	O
(	O
w	O
)	O
is	O
the	O
ratio	O
of	O
the	O
mean	O
tf	O
-	O
idfs	O
for	O
a	O
given	O
n	O
-	O
gram	O
w	O
present	O
in	O
both	O
X	O
1	O
,	O
X	O
2	O
with	O
|X	O
1	O
|	O
=	O
n	O
and	O
|X	O
2	O
|	O
=	O
m.	O

This	O
is	O
how	O
we	O
define	O
the	O
impactful	O
style	O
markers	O
for	O
style	O
S	O
2	O
.	O

Intuitively	O
,	O
p	O
2	O
1	O
(	O
w	O
)	O
is	O
proportional	O
to	O
the	O
probability	O
of	O
sampling	O
an	O
n	O
-	O
gram	O
present	O
in	O
both	O
X	O
1	O
,	O
X	O
2	O
but	O
having	O
a	O
much	O
higher	O
tf	O
-	O
idf	O
value	O
in	O
X	O
2	O
relative	O
to	O
X	O
1	O
.	O

For	O
a	O
given	O
corpus	O
pair	O
X	O
1	O
,	O
X	O
2	O
in	O
styles	O
S	O
1	O
,	O
S	O
2	O
respectively	O
we	O
first	O
compute	O
a	O
probability	O
distribution	O
p	O
2	O
1	O
(	O
w	O
)	O
over	O
the	O
n	O
-	O
grams	O
w	O
present	O
in	O
both	O
the	O
corpora	O
(	O
Eq	O
.	O
2	O
)	O
.	O

(	O
2018	O
)	O
,	O
we	O
propose	O
a	O
simple	O
approach	O
based	O
on	O
n	O
-	O
gram	O
tf	O
-	O
idfs	O
to	O
estimate	O
the	O
set	O
Γ	O
v	O
,	O
which	O
represents	O
the	O
style	O
markers	O
for	O
style	O
v.	O

Drawing	O
from	O
Li	O
et	O
al	O
.	O

Estimating	O
Style	O
Phrases	O
.	O

In	O
the	O
following	O
sections	O
we	O
discuss	O
in	O
detail	O
the	O
methodologies	O
involved	O
in	O
(	O
1	O
)	O
estimating	O
the	O
relevant	O
attribute	O
markers	O
for	O
a	O
given	O
style	O
,	O
(	O
2	O
)	O
tagger	O
,	O
and	O
(	O
3	O
)	O
generator	O
modules	O
of	O
our	O
approach	O
.	O

The	O
structural	O
bias	O
induced	O
by	O
this	O
two	O
staged	O
approach	O
is	O
helpful	O
in	O
realizing	O
an	O
interpretable	O
style	O
free	O
tagged	O
sentence	O
that	O
explicitly	O
encodes	O
the	O
content	O
.	O

We	O
can	O
also	O
see	O
that	O
the	O
inferred	O
sentence	O
in	O
both	O
the	O
cases	O
is	O
free	O
of	O
the	O
original	O
and	O
target	O
styles	O
.	O

On	O
the	O
contrary	O
,	O
in	O
the	O
second	O
example	O
,	O
the	O
terms	O
"	O
ok	O
"	O
and	O
"	O
bland	O
"	O
are	O
markers	O
of	O
negative	O
sentiment	O
and	O
hence	O
the	O
tagger	O
has	O
replaced	O
them	O
with	O
[	O
TAG	O
]	O
tokens	O
in	O
z(x	O
2	O
)	O
.	O

In	O
the	O
first	O
example	O
x	O
(	O
1	O
)	O
1	O
,	O
where	O
there	O
is	O
no	O
clear	O
style	O
attribute	O
present	O
,	O
our	O
model	O
adds	O
the	O
[	O
TAG	O
]	O
token	O
in	O
z(x	O
1	O
)	O
,	O
indicating	O
that	O
a	O
target	O
style	O
marker	O
should	O
be	O
generated	O
in	O
this	O
position	O
.	O

3	O
shows	O
the	O
overall	O
pipeline	O
of	O
the	O
proposed	O
approach	O
.	O

Training	O
data	O
creation	O
details	O
are	O
given	O
in	O
sections	O
§	O
4.2	O
,	O
§	O
4.3	O
.	O
Fig	O
.	O

tokens	O
.	O

The	O
generator	O
transforms	O
x	O
(	O
1	O
)	O
i	O
into	O
x(2	O
)	O
i	O
which	O
is	O
in	O
target	O
style	O
S	O
2	O
.	O

To	O
create	O
parallel	O
training	O
data	O
,	O
we	O
first	O
estimate	O
the	O
style	O
markers	O
Γ	O
v	O
for	O
a	O
given	O
style	O
S	O
v	O
&	O
then	O
use	O
these	O
to	O
curate	O
style	O
free	O
sentences	O
with	O
[	O
TAG	O
]	O
for	O
an	O
input	O
x	O
(	O
1	O
)	O
i	O
in	O
source	O
style	O
S	O
1	O
.	O

Even	O
though	O
we	O
have	O
non	O
-	O
parallel	O
corpora	O
,	O
both	O
systems	O
are	O
trained	O
in	O
a	O
supervised	O
fashion	O
as	O
sequence	O
-	O
to	O
-	O
sequence	O
models	O
with	O
their	O
own	O
distinct	O
pairs	O
of	O
inputs	O
&	O
outputs	O
.	O

The	O
generator	O
is	O
trained	O
to	O
generate	O
sentences	O
x(2	O
)	O
i	O
in	O
the	O
target	O
style	O
by	O
replacing	O
these	O
[	O
TAG	O
]	O
tokens	O
with	O
stylistically	O
relevant	O
words	O
inferred	O
from	O
target	O
style	O
S	O
2	O
.	O

This	O
is	O
because	O
in	O
such	O
cases	O
we	O
may	O
need	O
to	O
add	O
new	O
phrases	O
to	O
the	O
sentence	O
rather	O
than	O
simply	O
replace	O
existing	O
ones	O
.	O

It	O
is	O
especially	O
critical	O
for	O
tasks	O
like	O
politeness	B-TaskName
transfer	I-TaskName
where	O
the	O
transfer	O
takes	O
place	O
from	O
a	O
non	O
-	O
polite	O
sentence	O
.	O

This	O
is	O
one	O
of	O
the	O
major	O
differences	O
from	O
prior	O
works	O
which	O
mainly	O
focus	O
on	O
removing	O
source	O
style	O
attributes	O
and	O
then	O
replacing	O
them	O
with	O
the	O
target	O
style	O
attributes	O
.	O

This	O
particular	O
capability	O
of	O
the	O
model	O
enables	O
us	O
to	O
generate	O
these	O
tags	O
in	O
an	O
input	O
that	O
is	O
devoid	O
of	O
any	O
attribute	O
marker	O
(	O
i.e.	O
a(x	O
(	O
1	O
)	O
i	O
)	O
=	O
{	O
}	O
)	O
.	O

The	O
former	O
identifies	O
the	O
style	O
attribute	O
markers	O
a(x	O
(	O
1	O
)	O
i	O
)	O
from	O
source	O
style	O
S	O
1	O
and	O
either	O
replaces	O
them	O
with	O
a	O
positional	O
token	O
called	O
[	O
TAG	O
]	O
or	O
merely	O
adds	O
these	O
positional	O
tokens	O
without	O
removing	O
any	O
phrase	O
from	O
the	O
input	O
x	O
(	O
1	O
)	O
i	O
.	O

We	O
train	O
two	O
independent	O
systems	O
for	O
the	O
tagger	O
&	O
generator	O
which	O
have	O
complimentary	O
objectives	O
.	O

The	O
ability	O
of	O
our	O
pipeline	O
to	O
generate	O
observable	O
intermediate	O
outputs	O
z(x	O
i	O
)	O
makes	O
it	O
somewhat	O
more	O
interpretable	O
than	O
those	O
other	O
methods	O
.	O

In	O
these	O
cases	O
z(x	O
i	O
)	O
encodes	O
the	O
input	O
sentence	O
in	O
a	O
continuous	O
latent	O
space	O
whereas	O
for	O
us	O
z(x	O
i	O
)	O
manifests	O
in	O
the	O
surface	O
form	O
.	O

(	O
2017	O
i	O
while	O
being	O
agnostic	O
to	O
style	O
S	O
v	O
.	O

Shen	O
et	O
al	O
.	O

The	O
intermediate	O
variable	O
z(x	O
i	O
)	O
is	O
also	O
seen	O
in	O
other	O
style	O
-	O
transfer	O
methods	O
.	O

Conditioned	O
on	O
z(x	O
i	O
)	O
,	O
we	O
then	O
generate	O
the	O
transferred	O
sentence	O
x(2	O
)	O
i	O
in	O
the	O
target	O
style	O
S	O
2	O
using	O
another	O
model	O
,	O
the	O
generator	O
.	O

The	O
goal	O
of	O
the	O
tagger	O
is	O
to	O
ensure	O
that	O
the	O
sentence	O
z(x	O
i	O
)	O
is	O
agnostic	O
to	O
the	O
original	O
style	O
(	O
S	O
1	O
)	O
of	O
the	O
input	O
sentence	O
.	O

We	O
propose	O
a	O
two	O
staged	O
approach	O
where	O
we	O
first	O
infer	O
a	O
sentence	O
z(x	O
i	O
)	O
from	O
x	O
(	O
1	O
)	O
i	O
using	O
a	O
model	O
,	O
the	O
tagger	O
.	O

For	O
example	O
,	O
phrases	O
like	O
"	O
pretty	O
good	O
"	O
and	O
"	O
worth	O
every	O
penny	O
"	O
are	O
characteristic	O
of	O
the	O
"	O
positive	O
"	O
style	O
in	O
the	O
case	O
of	O
sentiment	O
transfer	O
task	O
.	O

The	O
presence	O
of	O
phrases	O
from	O
Γ	O
v	O
in	O
a	O
sentence	O
x	O
i	O
would	O
asso	O
-	O
ciate	O
the	O
sentence	O
with	O
the	O
style	O
S	O
v	O
.	O

For	O
a	O
style	O
S	O
v	O
where	O
v	O
∈	O
{	O
1	O
,	O
2	O
}	O
,	O
we	O
begin	O
by	O
learning	O
a	O
set	O
of	O
phrases	O
(	O
Γ	O
v	O
)	O
which	O
characterize	O
the	O
style	O
S	O
v	O
.	O

x(2	O
)	O
n	O
}	O
in	O
the	O
target	O
style	O
S	O
2	O
,	O
conditioned	O
on	O
samples	O
in	O
X	O
1	O
.	O

The	O
objective	O
of	O
the	O
task	O
is	O
to	O
efficiently	O
generate	O
samples	O
X1	O
=	O
{	O
x	O
(	O
2	O
)	O
1	O
.	O

x	O
(	O
2	O
)	O
m	O
}	O
from	O
styles	O
S	O
1	O
and	O
S	O
2	O
respectively	O
.	O

x	O
(	O
1	O
)	O
n	O
}	O
and	O
X	O
2	O
=	O
{	O
x	O
(	O
2	O
)	O
1	O
.	O

We	O
are	O
given	O
non	O
-	O
parallel	O
samples	O
of	O
sentences	O
X	O
1	O
=	O
{	O
x	O
(	O
1	O
)	O
1	O
.	O

Methodology	O
.	O

We	O
also	O
use	O
the	O
Amazon	B-DatasetName
dataset	I-DatasetName
of	I-DatasetName
product	I-DatasetName
reviews	I-DatasetName
(	O
He	O
and	O
McAuley	O
,	O
2016	O
)	O
.	O

(	O
2018	O
)	O
.	O

For	O
sentiment	B-DatasetName
transfer	I-DatasetName
,	O
we	O
use	O
the	O
Yelp	B-DatasetName
restaurant	I-DatasetName
review	I-DatasetName
dataset	O
(	O
Shen	O
et	O
al	O
.	O
,	O
2017	O
)	O
to	O
train	O
,	O
and	O
evaluate	O
on	O
a	O
test	O
set	O
of	O
1000	O
sentences	O
released	O
by	O
Li	O
et	O
al	O
.	O

This	O
task	O
parallels	O
the	O
task	O
of	O
politeness	B-TaskName
transfer	I-TaskName
because	O
much	O
like	O
in	O
the	O
case	O
of	O
politeness	B-TaskName
transfer	I-TaskName
,	O
the	O
captions	O
task	O
also	O
involves	O
going	O
from	O
a	O
style	O
neutral	O
(	O
factual	O
)	O
to	O
a	O
style	O
rich	O
(	O
humorous	O
or	O
romantic	O
)	O
parlance	O
.	O

We	O
use	O
this	O
dataset	O
to	O
perform	O
transfer	O
between	O
these	O
styles	O
.	O

The	O
Captions	B-DatasetName
dataset	O
(	O
Gan	O
et	O
al	O
.	O
,	O
2017	O
)	O
has	O
image	O
captions	O
labeled	O
as	O
being	O
factual	O
,	O
romantic	O
or	O
humorous	O
.	O

Other	O
Tasks	O
.	O

This	O
substantiates	O
our	O
claim	O
that	O
the	O
task	O
of	O
politeness	O
transfer	O
is	O
fundamentally	O
different	O
from	O
other	O
attribute	O
transfer	O
tasks	O
like	O
sentiment	O
where	O
both	O
the	O
polarities	O
are	O
clearly	O
defined	O
.	O

We	O
clearly	O
notice	O
that	O
words	O
in	O
the	O
P	O
9	O
bucket	O
are	O
closely	O
linked	O
to	O
polite	O
style	O
,	O
while	O
words	O
in	O
the	O
P	O
0	O
bucket	O
are	O
mostly	O
content	O
words	O
.	O

10	O
of	O
the	O
top	O
30	O
words	O
occurring	O
in	O
each	O
bucket	O
.	O

4	O
The	O
score	O
was	O
calculated	O
for	O
3	O
annotators	O
on	O
a	O
sample	O
set	O
of	O
50	O
sentences	O
.	O

3	O
We	O
used	O
AWD	B-MethodName
-	I-MethodName
LSTM	I-MethodName
based	I-MethodName
classifier	I-MethodName
for	O
classification	O
of	O
action	O
-	O
directive	O
.	O

2	O
We	O
prune	O
the	O
corpus	O
by	O
removing	O
the	O
sentences	O
that	O
1	O
)	O
were	O
less	O
than	O
3	O
words	O
long	O
,	O
2	O
)	O
had	O
more	O
than	O
80	O
%	O
numerical	O
tokens	O
,	O
3	O
)	O
contained	O
email	O
addresses	O
,	O
or	O
4	O
)	O
had	O
repeated	O
occurrences	O
of	O
spurious	O
characters	O
.	O

2	O
,	O
we	O
examine	O
the	O
two	O
extreme	O
buckets	O
with	O
politeness	B-MetricValue
scores	I-MetricValue
of	O
<	O
10	B-MetricValue
%	I-MetricValue
(	O
P	O
0	O
bucket	O
)	O
and	O
>	O
90	B-MetricValue
%	I-MetricValue
(	O
P	O
9	O
bucket	O
)	O
from	O
our	O
corpus	O
by	O
plotting	O
1	O
Pre	O
-	O
processing	O
also	O
involved	O
steps	O
for	O
tokenization	O
(	O
done	O
using	O
spacy	O
(	O
Honnibal	O
and	O
Montani	O
,	O
2017	O
)	O
)	O
and	O
conversion	O
to	O
lower	O
case	O
.	O

In	O
Fig	O
.	O

The	O
annotators	O
had	O
a	O
Fleiss	B-MetricName
's	I-MetricName
Kappa	I-MetricName
score	I-MetricName
(	O
κ	B-MetricValue
)	O
of	O
0.77	B-MetricValue
4	I-MetricValue
and	O
curated	O
a	O
final	O
test	O
set	O
of	O
800	O
sentences	O
.	O

3	O
Further	O
,	O
we	O
use	O
human	O
annotators	O
to	O
manually	O
select	O
the	O
test	O
sentences	O
.	O

We	O
first	O
train	O
a	O
classifier	O
on	O
the	O
switchboard	B-DatasetName
corpus	O
(	O
Jurafsky	O
et	O
al	O
.	O
,	O
1997	O
)	O
to	O
get	O
dialog	O
state	O
tags	O
and	O
filter	O
sentences	O
that	O
have	O
been	O
labeled	O
as	O
either	O
action	O
-	O
directive	O
or	O
quotation	O
.	O

Since	O
the	O
goal	O
of	O
the	O
task	O
is	O
making	O
action	O
directives	O
more	O
polite	O
,	O
we	O
manually	O
curate	O
a	O
test	O
set	O
comprising	O
of	O
such	O
sentences	O
from	O
test	O
splits	O
across	O
the	O
buckets	O
.	O

We	O
use	O
the	O
train	O
-	O
split	O
of	O
the	O
P	O
9	O
bucket	O
of	O
over	O
270	O
K	O
polite	O
sentences	O
as	O
the	O
training	O
data	O
for	O
the	O
politeness	B-TaskName
transfer	I-TaskName
task	I-TaskName
.	O

1	O
)	O
.	O

For	O
our	O
experiments	O
,	O
we	O
assumed	O
all	O
the	O
sentences	O
with	O
a	O
politeness	O
score	O
of	O
over	O
90	B-MetricValue
%	I-MetricValue
by	O
the	O
classifier	O
to	O
be	O
polite	O
,	O
also	O
referred	O
as	O
the	O
P	O
9	O
bucket	O
(	O
marked	O
in	O
green	O
in	O
Fig	O
.	O

All	O
the	O
buckets	O
are	O
further	O
divided	O
into	O
train	O
,	O
test	O
,	O
and	O
dev	O
splits	O
(	O
in	O
a	O
80:10:10	O
ratio	O
)	O
.	O

1	O
)	O
.	O

Finally	O
,	O
we	O
use	O
a	O
politeness	O
classifier	O
(	O
Niu	O
and	O
Bansal	O
,	O
2018	O
)	O
to	O
assign	O
politeness	O
scores	O
to	O
these	O
sentences	O
and	O
filter	O
them	O
into	O
ten	O
buckets	O
based	O
on	O
the	O
score	O
(	O
P	O
0	O
-P	O
9	O
;	O
Fig	O
.	O

Further	O
pruning	O
2	O
led	O
to	O
a	O
cleaned	O
corpus	O
of	O
over	O
1.39	O
million	O
sentences	O
.	O

The	O
first	O
set	O
of	O
pre	O
-	O
processing	O
1	O
steps	O
and	O
de	O
-	O
duplication	O
yielded	O
a	O
corpus	O
of	O
roughly	O
2.5	O
million	O
sentences	O
.	O

We	O
begin	O
by	O
pre	O
-	O
processing	O
the	O
raw	O
Enron	B-TaskName
corpus	O
following	O
Shetty	O
and	O
Adibi	O
(	O
2004	O
)	O
.	O

Emails	O
serve	O
as	O
a	O
medium	O
for	O
exchange	O
of	O
requests	O
,	O
serving	O
as	O
an	O
ideal	O
application	O
for	O
politeness	B-TaskName
transfer	I-TaskName
.	O

The	O
Enron	B-DatasetName
corpus	O
(	O
Klimt	O
and	O
Yang	O
,	O
2004	O
)	O
consists	O
of	O
a	O
large	O
set	O
of	O
email	O
conversations	O
exchanged	O
by	O
the	O
employees	O
of	O
the	O
Enron	B-DatasetName
corporation	O
.	O

Data	O
Preparation	O
.	O

While	O
there	O
can	O
be	O
more	O
than	O
one	O
way	O
of	O
making	O
a	O
sentence	O
polite	O
,	O
for	O
the	O
above	O
examples	O
,	O
adding	O
gratitude	O
(	O
"	O
Thanks	O
and	O
let	O
's	O
stay	O
in	O
touch	O
"	O
)	O
or	O
counterfactuals	O
(	O
"	O
Could	O
you	O
please	O
call	O
me	O
when	O
you	O
get	O
back	O
?	O
"	O
)	O
would	O
make	O
them	O
polite	O
(	O
Danescu	O
-	O
Niculescu	O
-	O
Mizil	O
et	O
al	O
.	O
,	O
2013	O
)	O
.	O

The	O
goal	O
of	O
this	O
task	O
is	O
to	O
convert	O
action	O
-	O
directives	O
to	O
polite	O
requests	O
.	O

(	O
1997	O
)	O
,	O
we	O
use	O
the	O
umbrella	O
term	O
"	O
action	O
-	O
directives	O
"	O
for	O
such	O
sentences	O
.	O

Following	O
Jurafsky	O
et	O
al	O
.	O

Common	O
examples	O
include	O
imperatives	O
"	O
Let	O
's	O
stay	O
in	O
touch	O
"	O
and	O
questions	O
that	O
express	O
a	O
proposal	O
"	O
Can	O
you	O
call	O
me	O
when	O
you	O
get	O
back	O
?	O
"	O
.	O

For	O
the	O
politeness	B-TaskName
transfer	I-TaskName
task	I-TaskName
,	O
we	O
focus	O
on	O
sentences	O
in	O
which	O
the	O
speaker	O
communicates	O
a	O
requirement	O
that	O
the	O
listener	O
needs	O
to	O
fulfill	O
.	O

Politeness	B-TaskName
Transfer	I-TaskName
Task	I-TaskName
.	O

3	O
Tasks	O
and	O
Datasets	O
.	O

Further	O
,	O
our	O
work	O
is	O
more	O
generalizable	O
and	O
we	O
show	O
results	O
on	O
five	O
other	O
style	O
transfer	O
tasks	O
.	O

In	O
contrast	O
,	O
we	O
are	O
capable	O
of	O
generating	O
the	O
entire	O
sentence	O
in	O
the	O
target	O
style	O
.	O

It	O
focuses	O
only	O
on	O
sentiment	O
modification	O
,	O
treating	O
it	O
as	O
a	O
cloze	O
form	O
task	O
of	O
filling	O
in	O
the	O
appropriate	O
words	O
in	O
the	O
target	O
sentiment	O
.	O

(	O
2019	O
)	O
treats	O
style	O
transfer	O
as	O
a	O
conditional	O
language	O
modelling	O
task	O
.	O

Wu	O
et	O
al	O
.	O

This	O
also	O
makes	O
our	O
pipeline	O
faster	O
in	O
addition	O
to	O
being	O
robust	O
to	O
noise	O
.	O

Our	O
methodology	O
differs	O
from	O
these	O
works	O
as	O
it	O
does	O
not	O
require	O
the	O
retrieve	O
stage	O
and	O
makes	O
no	O
assumptions	O
on	O
the	O
existence	O
of	O
similar	O
content	O
phrases	O
in	O
both	O
the	O
styles	O
.	O

However	O
,	O
DRG	B-MethodName
has	O
several	O
limitations	O
:	O
(	O
1	O
)	O
the	O
delete	O
module	O
often	O
marks	O
content	O
words	O
as	O
stylistic	O
markers	O
and	O
deletes	O
them	O
,	O
(	O
2	O
)	O
the	O
retrieve	O
step	O
relies	O
on	O
the	O
presence	O
of	O
similar	O
content	O
in	O
both	O
the	O
source	O
and	O
target	O
styles	O
,	O
(	O
3	O
)	O
the	O
retrieve	O
step	O
is	O
time	O
consuming	O
for	O
large	O
datasets	O
,	O
(	O
4	O
)	O
the	O
pipeline	O
makes	O
the	O
assumption	O
that	O
style	O
can	O
be	O
transferred	O
by	O
deleting	O
stylistic	O
markers	O
and	O
replacing	O
them	O
with	O
target	O
style	O
phrases	O
,	O
(	O
5	O
)	O
the	O
method	O
relies	O
on	O
a	O
fixed	O
corpus	O
of	O
style	O
attribute	O
markers	O
,	O
and	O
is	O
thus	O
limited	O
in	O
its	O
ability	O
to	O
generalize	O
to	O
unseen	O
data	O
during	O
test	O
time	O
.	O

Compared	O
to	O
prior	O
work	O
,	O
"	O
Delete	B-MethodName
,	I-MethodName
Retrieve	I-MethodName
and	I-MethodName
Generate	I-MethodName
"	O
(	O
Li	O
et	O
al	O
.	O
,	O
2018	O
)	O
(	O
referred	O
to	O
as	O
DRG	O
henceforth	O
)	O
and	O
its	O
extension	O
(	O
Sudhakar	O
et	O
al	O
.	O
,	O
2019	O
)	O
are	O
effective	O
methods	O
to	O
generate	O
out	O
-	O
puts	O
in	O
the	O
target	O
style	O
while	O
having	O
a	O
relatively	O
high	O
rate	O
of	O
source	O
content	O
preservation	O
.	O

Current	O
style	B-TaskName
transfer	I-TaskName
techniques	O
(	O
Shen	O
et	O
al	O
.	O
,	O
2017;Hu	O
et	O
al	O
.	O
,	O
2017;Fu	O
et	O
al	O
.	O
,	O
2018;Yang	O
et	O
al	O
.	O
,	O
2018;John	O
et	O
al	O
.	O
,	O
2019	O
)	O
try	O
to	O
disentangle	O
source	O
style	O
from	O
content	O
and	O
then	O
combine	O
the	O
content	O
with	O
the	O
target	O
style	O
to	O
generate	O
the	O
sentence	O
in	O
the	O
target	O
style	O
.	O

We	O
focus	O
our	O
efforts	O
on	O
carving	O
out	O
a	O
task	O
for	O
politeness	B-TaskName
transfer	I-TaskName
and	O
creating	O
a	O
dataset	O
for	O
such	O
a	O
task	O
.	O

Note	O
that	O
formality	O
and	O
politeness	O
are	O
loosely	O
connected	O
but	O
independent	O
styles	O
(	O
Kang	O
and	O
Hovy	O
,	O
2019	O
)	O
.	O

Prior	O
work	O
on	O
style	O
transfer	O
has	O
largely	O
focused	O
on	O
tasks	O
of	O
sentiment	O
modification	O
(	O
Hu	O
et	O
al	O
.	O
,	O
2017;Shen	O
et	O
al	O
.	O
,	O
2017;Li	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
caption	O
transfer	O
(	O
Li	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
persona	O
transfer	O
(	O
Chandu	O
et	O
al	O
.	O
,	O
2019;Zhang	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
gender	O
and	O
political	O
slant	O
transfer	O
(	O
Reddy	O
and	O
Knight	O
,	O
2016;Prabhumoye	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
and	O
formality	O
transfer	O
(	O
Rao	O
and	O
Tetreault	O
,	O
2018;Xu	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

We	O
build	O
upon	O
this	O
body	O
of	O
work	O
by	O
using	O
this	O
corpus	O
as	O
a	O
source	O
for	O
the	O
style	B-TaskName
transfer	I-TaskName
task	I-TaskName
.	O

Prior	O
work	O
on	O
Enron	B-DatasetName
corpus	O
(	O
Yeh	O
and	O
Harnly	O
,	O
2006	O
)	O
has	O
been	O
mostly	O
from	O
a	O
socio	O
-	O
linguistic	O
perspective	O
to	O
observe	O
social	O
power	O
dynamics	O
(	O
Bramsen	O
et	O
al	O
.	O
,	O
2011;McCallum	O
et	O
al	O
.	O
,	O
2007	O
)	O
,	O
formality	O
(	O
Peterson	O
et	O
al	O
.	O
,	O
2011	O
)	O
and	O
politeness	O
(	O
Prabhakaran	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

Their	O
work	O
focuses	O
on	O
contextual	O
dialogue	O
response	O
generation	O
as	O
opposed	O
to	O
content	O
preserving	O
style	O
transfer	O
,	O
while	O
the	O
latter	O
is	O
the	O
central	O
theme	O
of	O
our	O
work	O
.	O

Niu	O
and	O
Bansal	O
(	O
2018	O
)	O
uses	O
this	O
corpus	O
to	O
generate	O
polite	O
dialogues	O
.	O

Recent	O
work	O
(	O
Danescu	O
-	O
Niculescu	O
-	O
Mizil	O
et	O
al	O
.	O
,	O
2013	O
)	O
in	O
computational	O
linguistics	O
has	O
provided	O
a	O
corpus	O
of	O
requests	O
annotated	O
for	O
politeness	O
curated	O
from	O
Wikipedia	B-DatasetName
and	O
StackExchange	B-DatasetName
.	O

Politeness	B-TaskName
and	O
its	O
close	O
relation	O
with	O
power	O
dynamics	O
and	O
social	O
interactions	O
has	O
been	O
well	O
documented	O
(	O
Brown	O
et	O
al	O
.	O
,	O
1987	O
)	O
.	O

Related	O
Work	O
.	O

Finally	O
,	O
we	O
design	O
a	O
"	O
tag	B-MethodName
and	I-MethodName
generate	I-MethodName
"	O
pipeline	O
that	O
is	O
particularly	O
well	O
suited	O
for	O
tasks	O
like	O
politeness	B-TaskName
,	O
while	O
being	O
general	O
enough	O
to	O
match	O
or	O
beat	O
the	O
performance	O
of	O
the	O
existing	O
systems	O
on	O
popular	O
style	O
transfer	O
tasks	O
.	O

In	O
the	O
process	O
,	O
we	O
highlight	O
an	O
important	O
class	O
of	O
problems	O
wherein	O
the	O
transfer	O
involves	O
going	O
from	O
a	O
neutral	O
style	O
to	O
the	O
target	O
style	O
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
we	O
are	O
the	O
first	O
to	O
undertake	O
politeness	B-TaskName
as	O
a	O
style	O
transfer	O
task	O
.	O

Additionally	O
,	O
we	O
hand	O
curate	O
a	O
test	O
set	O
of	O
800	O
samples	O
(	O
from	O
Enron	O
emails	O
)	O
which	O
are	O
annotated	O
as	O
requests	O
.	O

To	O
this	O
end	O
,	O
we	O
provide	O
a	O
large	O
dataset	O
of	O
nearly	O
1.39	O
million	O
sentences	O
labeled	O
for	O
politeness	B-TaskName
(	O
https://github.com/tag-and-generate/	O
politeness	O
-	O
dataset	O
)	O
.	O

Our	O
main	O
contribution	O
is	O
the	O
design	O
of	O
politeness	B-TaskName
transfer	I-TaskName
task	O
.	O

(	O
2018	O
)	O
and	O
improves	O
upon	O
several	O
of	O
its	O
limitations	O
as	O
described	O
in	O
(	O
§	O
2	O
)	O
.	O

Our	O
methodology	O
is	O
inspired	O
by	O
Li	O
et	O
al	O
.	O

The	O
results	O
show	O
that	O
our	O
technique	O
is	O
effective	O
across	O
a	O
broad	O
spectrum	O
of	O
style	O
transfer	O
tasks	O
.	O

Both	O
automatic	O
and	O
human	O
evaluations	O
show	O
that	O
our	O
model	O
beats	O
the	O
stateof	O
-	O
the	O
-	O
art	O
methods	O
in	O
content	O
preservation	O
,	O
while	O
either	O
matching	O
or	O
improving	O
the	O
transfer	O
accuracy	B-MetricName
across	O
six	O
different	O
style	O
transfer	O
tasks	O
(	O
§	O
5	O
)	O
.	O

We	O
evaluate	O
our	O
model	O
on	O
politeness	B-TaskName
transfer	I-TaskName
as	O
well	O
as	O
5	O
additional	O
tasks	O
described	O
in	O
prior	O
work	O
(	O
Shen	O
et	O
al	O
.	O
,	O
2017;Prabhumoye	O
et	O
al	O
.	O
,	O
2018;Li	O
et	O
al	O
.	O
,	O
2018	O
)	O
on	O
content	O
preservation	O
,	O
fluency	O
and	O
style	O
transfer	O
accuracy	O
.	O

Finally	O
,	O
if	O
the	O
input	O
sentence	O
is	O
already	O
in	O
the	O
target	O
style	O
,	O
our	O
model	O
wo	O
n't	O
add	O
any	O
stylistic	O
markers	O
and	O
thus	O
would	O
allow	O
the	O
input	O
to	O
flow	O
as	O
is	O
.	O

Additionally	O
,	O
unlike	O
previous	O
systems	O
,	O
the	O
outputs	O
of	O
the	O
intermediate	O
steps	O
in	O
our	O
system	O
are	O
fully	O
realized	O
,	O
making	O
the	O
whole	O
pipeline	O
interpretable	O
.	O

The	O
generator	O
takes	O
as	O
input	O
the	O
output	O
of	O
the	O
tagger	O
and	O
generates	O
a	O
sentence	O
in	O
the	O
target	O
style	O
.	O

If	O
the	O
sentence	O
has	O
no	O
style	O
attributes	O
,	O
as	O
in	O
the	O
case	O
for	O
politeness	B-TaskName
transfer	I-TaskName
,	O
the	O
tagger	O
adds	O
the	O
tag	O
token	O
in	O
positions	O
where	O
phrases	O
in	O
the	O
target	O
style	O
can	O
be	O
inserted	O
.	O

The	O
tagger	O
identifies	O
the	O
words	O
or	O
phrases	O
which	O
belong	O
to	O
the	O
original	O
style	O
and	O
replaces	O
them	O
with	O
a	O
tag	O
token	O
.	O

We	O
propose	O
a	O
tag	B-MethodName
and	I-MethodName
generate	I-MethodName
pipeline	I-MethodName
to	O
overcome	O
these	O
challenges	O
.	O

Note	O
that	O
this	O
is	O
in	O
stark	O
contrast	O
with	O
the	O
standard	O
style	O
transfer	O
tasks	O
,	O
which	O
involve	O
transferring	O
a	O
sentence	O
from	O
a	O
well	O
-	O
defined	O
style	O
polarity	O
to	O
the	O
other	O
(	O
like	O
positive	O
to	O
negative	O
sentiment	O
)	O
.	O

For	O
our	O
study	O
,	O
we	O
focus	O
on	O
the	O
task	O
of	O
transferring	O
the	O
non	O
-	O
polite	O
sentences	O
to	O
polite	O
sentences	O
,	O
where	O
we	O
simply	O
define	O
non	O
-	O
politeness	O
to	O
be	O
the	O
absence	O
of	O
both	O
politeness	O
and	O
impoliteness	O
.	O

While	O
interesting	O
,	O
such	O
cases	O
can	O
typically	O
be	O
neutralized	O
using	O
lexicons	O
.	O

Further	O
,	O
the	O
other	O
extreme	O
of	O
politeness	B-TaskName
,	O
impolite	O
sentences	O
,	O
are	O
typically	O
riddled	O
with	O
curse	O
words	O
and	O
insulting	O
phrases	O
.	O

However	O
,	O
cues	O
that	O
signal	O
the	O
absence	O
of	O
politeness	B-TaskName
,	O
like	O
direct	O
questions	O
,	O
statements	O
and	O
factuality	O
(	O
Danescu	O
-	O
Niculescu	O
-	O
Mizil	O
et	O
al	O
.	O
,	O
2013	O
)	O
,	O
do	O
not	O
explicitly	O
appear	O
in	O
a	O
sentence	O
,	O
and	O
are	O
thus	O
hard	O
to	O
objectify	O
.	O

It	O
is	O
easy	O
to	O
pinpoint	O
the	O
signals	O
for	O
politeness	B-TaskName
.	O

This	O
example	O
brings	O
out	O
a	O
distinct	O
characteristic	O
of	O
politeness	O
.	O

While	O
the	O
sentence	O
is	O
not	O
impolite	O
,	O
a	O
rephrasing	O
"	O
could	O
you	O
please	O
send	O
me	O
the	O
data	O
"	O
would	O
largely	O
be	O
accepted	O
as	O
a	O
more	O
polite	O
way	O
of	O
phrasing	O
the	O
same	O
statement	O
(	O
Danescu	O
-	O
Niculescu	O
-	O
Mizil	O
et	O
al	O
.	O
,	O
2013	O
)	O
.	O

Consider	O
a	O
common	O
directive	O
in	O
formal	O
communication	O
,	O
"	O
send	O
me	O
the	O
data	O
"	O
.	O

Even	O
after	O
framing	O
politeness	B-TaskName
transfer	I-TaskName
as	O
a	O
task	O
,	O
there	O
are	O
additional	O
challenges	O
involved	O
that	O
differentiate	O
politeness	B-TaskName
from	O
other	O
styles	O
.	O

Thus	O
,	O
we	O
restrict	O
our	O
attention	O
to	O
the	O
notion	O
of	O
politeness	O
as	O
widely	O
accepted	O
by	O
the	O
speakers	O
of	O
North	O
American	O
English	O
in	O
a	O
formal	O
setting	O
.	O

Second	O
,	O
we	O
base	O
our	O
experiments	O
on	O
a	O
dataset	O
derived	O
from	O
the	O
Enron	O
corpus	O
(	O
Klimt	O
and	O
Yang	O
,	O
2004	O
)	O
which	O
consists	O
of	O
email	O
exchanges	O
in	O
an	O
American	O
corporation	O
.	O

We	O
circumscribe	O
the	O
scope	O
of	O
politeness	O
for	O
the	O
purpose	O
of	O
this	O
study	O
as	O
follows	O
:	O
First	O
,	O
we	O
adopt	O
the	O
data	O
driven	O
definition	O
of	O
politeness	O
proposed	O
by	O
(	O
Danescu	O
-	O
Niculescu	O
-	O
Mizil	O
et	O
al	O
.	O
,	O
2013	O
)	O
.	O

For	O
instance	O
,	O
while	O
using	O
"	O
please	O
"	O
in	O
requests	O
made	O
to	O
the	O
closest	O
friends	O
is	O
common	O
amongst	O
the	O
native	O
speakers	O
of	O
North	O
American	O
English	O
,	O
such	O
an	O
act	O
would	O
be	O
considered	O
awkward	O
,	O
if	O
not	O
rude	O
,	O
in	O
the	O
Arab	O
culture	O
(	O
Kádár	O
and	O
Mills	O
,	O
2011	O
)	O
.	O

Second	O
,	O
politeness	O
of	O
a	O
sentence	O
depends	O
on	O
the	O
culture	O
,	O
language	O
,	O
and	O
social	O
structure	O
of	O
both	O
the	O
speaker	O
and	O
the	O
addressed	O
person	O
.	O

First	O
,	O
as	O
noted	O
by	O
(	O
Brown	O
et	O
al	O
.	O
,	O
1987	O
)	O
,	O
the	O
phenomenon	O
of	O
politeness	O
is	O
rich	O
and	O
multifaceted	O
.	O

There	O
are	O
primarily	O
two	O
reasons	O
for	O
this	O
complexity	O
.	O

While	O
native	O
speakers	O
of	O
a	O
language	O
and	O
cohabitants	O
of	O
a	O
region	O
have	O
a	O
good	O
working	O
understanding	O
of	O
the	O
phenomenon	O
of	O
politeness	O
for	O
everyday	O
conversation	O
,	O
pinning	O
it	O
down	O
as	O
a	O
definition	O
is	O
non	O
-	O
trivial	O
(	O
Meier	O
,	O
1995	O
)	O
.	O

It	O
is	O
also	O
imperative	O
to	O
use	O
the	O
appropriate	O
level	O
of	O
politeness	O
for	O
smooth	O
communication	O
in	O
conversations	O
(	O
Coppock	O
,	O
2005	O
)	O
,	O
organizational	O
settings	O
like	O
emails	O
(	O
Peterson	O
et	O
al	O
.	O
,	O
2011	O
)	O
,	O
memos	O
,	O
official	O
documents	O
,	O
and	O
many	O
other	O
settings	O
.	O

The	O
data	O
and	O
code	O
is	O
located	O
at	O
https://	O
github.com/tag-and-generate/Politeness	O
plays	O
a	O
crucial	O
role	O
in	O
social	O
interaction	O
,	O
and	O
is	O
closely	O
tied	O
with	O
power	O
dynamics	O
,	O
social	O
distance	O
between	O
the	O
participants	O
of	O
a	O
conversation	O
,	O
and	O
gender	O
(	O
Brown	O
et	O
al	O
.	O
,	O
1987;Danescu	O
-	O
Niculescu	O
-	O
Mizil	O
et	O
al	O
.	O
,	O
2013	O
)	O
.	O

Additionally	O
,	O
our	O
model	O
surpasses	O
existing	O
methods	O
on	O
human	O
evaluations	O
for	O
grammaticality	O
,	O
meaning	O
preservation	O
and	O
transfer	O
accuracy	O
across	O
all	O
the	O
six	O
style	B-TaskName
transfer	I-TaskName
tasks	O
.	O

For	O
politeness	B-TaskName
as	O
well	O
as	O
five	O
other	O
transfer	O
tasks	O
,	O
our	O
model	O
outperforms	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
on	O
automatic	O
metrics	O
for	O
content	O
preservation	O
,	O
with	O
a	O
comparable	O
or	O
better	O
performance	O
on	O
style	O
transfer	O
accuracy	B-MetricName
.	O

We	O
design	O
a	O
tag	B-MethodName
and	I-MethodName
generate	I-MethodName
pipeline	I-MethodName
that	O
identifies	O
stylistic	O
attributes	O
and	O
subsequently	O
generates	O
a	O
sentence	O
in	O
the	O
target	O
style	O
while	O
preserving	O
most	O
of	O
the	O
source	O
content	O
.	O

We	O
also	O
provide	O
a	O
dataset	O
of	O
more	O
than	O
1.39	O
million	O
instances	O
automatically	O
labeled	O
for	O
politeness	B-TaskName
to	O
encourage	O
benchmark	O
evaluations	O
on	O
this	O
new	O
task	O
.	O

Rao	O
and	O
Tetreault	O
,	O
2018	O
;	O
Xu	O
et	O
al	O
.	O
,	O
2012	O
;	O
Jhamtani	O
et	O
al	O
.	O
,	O
2017	O
)	O
has	O
not	O
focused	O
on	O
politeness	B-TaskName
as	O
a	O
style	B-TaskName
transfer	I-TaskName
task	O
,	O
and	O
we	O
argue	O
that	O
defining	O
it	O
is	O
cumbersome	O
.	O

Motivated	O
by	O
its	O
central	O
importance	O
,	O
in	O
this	O
paper	O
we	O
study	O
the	O
task	O
of	O
converting	O
non	O
-	O
polite	O
sentences	O
to	O
polite	O
sentences	O
while	O
preserving	O
the	O
meaning	O
.	O

Notably	O
,	O
politeness	O
has	O
also	O
been	O
identified	O
as	O
an	O
interpersonal	O
style	O
which	O
can	O
be	O
decoupled	O
from	O
content	O
(	O
Kang	O
and	O
Hovy	O
,	O
2019	O
)	O
.	O

Politeness	B-TaskName
Transfer	I-TaskName
:	O
A	O
Tag	B-MethodName
and	I-MethodName
Generate	I-MethodName
Approach	I-MethodName
.	O

This	O
paper	O
introduces	O
a	O
new	O
task	O
of	O
politeness	B-TaskName
transfer	I-TaskName
which	O
involves	O
converting	O
non	O
-	O
polite	O
sentences	O
to	O
polite	O
sentences	O
while	O
preserving	O
the	O
meaning	O
.	O

Conclusion	O
.	O

