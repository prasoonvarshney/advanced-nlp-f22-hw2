TC	O
and	O
TB	O
were	O
supported	O
by	O
the	O
Australian	O
Research	O
Council	O
.	O

Overall	O
,	O
we	O
conclude	O
that	O
the	O
DIFFVEC	B-MethodName
approach	O
has	O
impressive	O
utility	O
over	O
a	O
broad	O
range	O
of	O
lexical	O
relations	O
,	O
especially	O
under	O
supervised	B-TaskName
classification	I-TaskName
.	O

Negative	B-MethodName
sampling	I-MethodName
also	O
improves	O
classification	B-TaskName
when	O
the	O
training	O
and	O
test	O
vocabulary	O
are	O
split	O
to	O
minimise	O
lexical	O
memorisation	O
.	O

Classification	B-TaskName
performs	O
less	O
well	O
over	O
open	O
data	O
,	O
although	O
with	O
the	O
introduction	O
of	O
automatically	O
-	O
generated	O
negative	O
samples	O
,	O
the	O
results	O
improve	O
substantially	O
.	O

In	O
contrast	O
,	O
classification	B-TaskName
over	O
the	O
DIFFVECs	B-MethodName
works	O
extremely	O
well	O
in	O
a	O
closed	O
-	O
world	O
setting	O
,	O
showing	O
that	O
dimensions	O
of	O
DIFFVECs	B-MethodName
encode	O
lexical	O
relations	O
.	O

Using	O
clustering	B-TaskName
we	O
showed	O
that	O
many	O
types	O
of	O
morphosyntactic	O
and	O
morphosemantic	O
differences	O
are	O
captured	O
by	O
DIFFVECs	B-MethodName
,	O
but	O
that	O
lexical	O
semantic	O
relations	O
are	O
captured	O
less	O
well	O
,	O
a	O
finding	O
which	O
is	O
consistent	O
with	O
previous	O
work	O
(	O
Köper	O
et	O
al	O
.	O
,	O
2015	O
)	O
.	O

At	O
the	O
maximum	O
level	O
of	O
random	O
word	O
pairs	O
in	O
the	O
test	O
data	O
,	O
the	O
F	B-MetricName
-	I-MetricName
score	I-MetricName
for	O
the	O
negative	O
sampling	O
classifier	O
is	O
higher	O
than	O
for	O
the	O
standard	O
classifier	O
.	O

This	O
benefit	O
comes	O
at	O
the	O
expense	O
of	O
recall	B-MetricName
,	O
which	O
is	O
much	O
lower	O
when	O
negative	O
sampling	O
is	O
used	O
(	O
note	O
that	O
recall	O
stays	O
relatively	O
constant	O
as	O
random	O
word	O
pairs	O
are	O
added	O
,	O
as	O
the	O
vast	O
majority	O
of	O
them	O
do	O
not	O
correspond	O
to	O
any	O
relation	O
)	O
.	O

In	O
comparison	O
,	O
the	O
precision	B-MetricName
when	O
negative	O
sampling	O
is	O
used	O
shows	O
only	O
a	O
small	O
drop	O
-	O
off	O
,	O
indicating	O
that	O
negative	O
sampling	O
is	O
effective	O
at	O
maintaining	O
precision	B-MetricName
in	O
an	O
OPEN	O
-	O
WORLD	O
setting	O
even	O
when	O
the	O
training	O
and	O
test	O
vocabulary	O
are	O
disjoint	O
.	O

Observe	O
that	O
the	O
precision	B-MetricName
for	O
the	O
standard	O
clas	O
-	O
sifier	O
decreases	O
rapidly	O
as	O
more	O
random	O
word	O
pairs	O
are	O
added	O
to	O
the	O
test	O
data	O
.	O

(	O
2015b	O
)	O
recently	O
showed	O
that	O
supervised	O
methods	O
using	O
DIFF	B-MethodName
-	I-MethodName
VECs	I-MethodName
achieve	O
artificially	O
high	O
results	O
as	O
a	O
result	O
of	O
"	O
lexical	O
memorisation	O
"	O
over	O
frequent	O
words	O
asso-	O
ciated	O
with	O
the	O
hypernym	O
relation	O
.	O

The	O
most	O
striking	O
difference	O
in	O
performance	O
was	O
for	O
LEXSEM	O
Mero	O
,	O
where	O
the	O
standard	O
classifier	O
generated	O
many	O
false	O
positive	O
noun	O
pairs	O
(	O
e.g.	O
(	O
series	O
,	O
radio	O
)	O
)	O
,	O
but	O
the	O
false	B-MetricName
positive	I-MetricName
rate	I-MetricName
was	O
considerably	O
reduced	O
with	O
negative	O
sampling	O
.	O

Overall	O
this	O
leads	O
to	O
higher	O
F	B-MetricName
-	I-MetricName
scores	I-MetricName
,	O
as	O
shown	O
in	O
Figure	O
3	O
,	O
other	O
than	O
for	O
hypernyms	O
(	O
LEXSEM	O
Hyper	O
)	O
and	O
prefixes	O
(	O
PREFIX	O
)	O
.	O

This	O
follows	O
from	O
the	O
adversarial	O
training	O
scenario	O
:	O
using	O
negative	O
distractors	O
results	O
in	O
a	O
more	O
conservative	O
classifier	O
,	O
that	O
correctly	O
classifies	O
the	O
vast	O
majority	O
of	O
the	O
random	O
word	O
pairs	O
as	O
not	O
corresponding	O
to	O
a	O
given	O
relation	O
,	O
resulting	O
in	O
higher	O
precision	B-MetricName
at	O
the	O
expense	O
of	O
a	O
small	B-MetricName
drop	O
in	O
recall	O
.	O

The	O
results	O
are	O
much	O
lower	O
than	O
for	O
the	O
closed	O
-	O
word	O
setting	O
(	O
Table	O
4	O
)	O
,	O
most	O
notably	O
in	O
terms	O
of	O
precision	B-MetricName
(	O
P	O
)	O
.	O

We	O
train	O
9	O
binary	B-MethodName
RBF	I-MethodName
-	I-MethodName
kernel	I-MethodName
SVM	I-MethodName
classifiers	I-MethodName
on	O
the	O
training	O
partition	O
,	O
and	O
evaluate	O
on	O
our	O
randomly	O
augmented	O
test	O
set	O
.	O

The	O
test	O
data	O
is	O
augmented	O
with	O
an	O
equal	O
quantity	O
of	O
random	O
pairs	O
,	O
generated	O
as	O
follows	O
:	O
(	O
1	O
)	O
sample	O
a	O
seed	O
lexicon	O
by	O
drawing	O
words	O
proportional	O
to	O
their	O
frequency	O
in	O
Wikipedia	O
;	O
11	O
Table	O
5	O
:	O
Precision	O
(	O
P	O
)	O
and	O
recall	O
(	O
R	O
)	O
for	O
OPEN	O
-	O
WORLD	O
classification	O
,	O
using	O
the	O
binary	O
classifier	O
without	O
(	O
"	O
Orig	O
"	O
)	O
and	O
with	O
(	O
"	O
+	O
neg	O
"	O
)	O
negative	O
samples	O
.	O

This	O
setting	O
aims	O
to	O
illustrate	O
whether	O
a	O
DIFF	B-MethodName
-	I-MethodName
VEC	I-MethodName
-	O
based	O
classifier	O
is	O
capable	O
of	O
differentiating	O
related	O
word	O
pairs	O
from	O
noise	O
,	O
and	O
can	O
be	O
applied	O
to	O
open	O
data	O
to	O
learn	O
new	O
related	O
word	O
pairs	O
.	O

We	O
observe	O
no	O
real	O
difference	O
between	O
w2v	B-MethodName
wiki	I-MethodName
and	O
SVD	B-MethodName
wiki	I-MethodName
,	O
supporting	O
the	O
hypothesis	O
of	O
Levy	O
et	O
al	O
.	O

Somewhat	O
surprisingly	O
,	O
given	O
the	O
small	O
dimensionality	O
of	O
the	O
input	O
(	O
vectors	O
of	O
size	O
300	O
for	O
all	O
three	O
methods	O
)	O
,	O
we	O
found	O
that	O
the	O
linear	B-MethodName
SVM	I-MethodName
slightly	O
outperformed	O
a	O
non	B-MethodName
-	I-MethodName
linear	I-MethodName
SVM	I-MethodName
using	O
an	O
RBF	B-MethodName
kernel	I-MethodName
.	O

The	O
PREFIX	O
relation	O
achieved	O
markedly	O
lower	O
recall	O
,	O
resulting	O
in	O
a	O
lower	O
F	B-MetricName
-	I-MetricName
score	I-MetricName
,	O
due	O
to	O
large	O
differences	O
in	O
the	O
predominant	O
usages	O
associated	O
with	O
the	O
respective	O
words	O
(	O
e.g.	O
,	O
(	O
union	O
,	O
reunion	O
)	O
,	O
where	O
the	O
vector	O
for	O
union	O
is	O
heavily	O
biased	O
by	O
contexts	O
associated	O
with	O
trade	O
unions	O
,	O
but	O
reunion	O
is	O
heavily	O
biased	O
by	O
contexts	O
relating	O
to	O
social	O
get	O
-	O
togethers	O
;	O
and	O
(	O
entry	O
,	O
reentry	O
)	O
,	O
where	O
entry	O
is	O
associated	O
with	O
competitions	O
and	O
entrance	O
to	O
schools	O
,	O
while	O
reentry	O
is	O
associated	O
with	O
space	O
travel	O
)	O
.	O

That	O
is	O
,	O
with	O
a	O
simple	O
linear	O
transformation	O
of	O
the	O
embedding	O
dimensions	O
,	O
we	O
are	O
able	O
to	O
achieve	O
near	O
-	O
perfect	O
results	O
.	O

Most	O
of	O
the	O
relationseven	O
the	O
most	O
difficult	O
ones	O
from	O
our	O
clustering	B-TaskName
experiment	O
-are	O
classified	O
with	O
very	O
high	O
Fscore	B-MetricName
.	O

The	O
SVM	B-MethodName
achieves	O
a	O
higher	O
F	B-MetricName
-	I-MetricName
score	I-MetricName
than	O
the	O
baseline	O
on	O
almost	O
every	O
relation	O
,	O
particularly	O
on	O
LEXSEM	O
Hyper	O
,	O
and	O
the	O
lower	O
-	O
frequency	O
NOUN	O
SP	O
,	O
NOUN	O
Coll	O
,	O
and	O
PREFIX	O
.	O

We	O
use	O
an	O
SVM	B-MethodName
with	O
a	O
linear	O
kernel	O
,	O
and	O
report	O
results	O
from	O
10	B-HyperparameterValue
-	O
fold	B-HyperparameterName
cross	O
-	O
validation	O
in	O
Table	O
4	O
.	O

As	O
a	O
baseline	O
,	O
we	O
cluster	O
the	O
data	O
as	O
described	O
in	O
§	O
4	O
,	O
running	O
the	O
clusterer	O
several	O
times	O
over	O
the	O
9	O
-	O
relation	O
data	O
to	O
select	O
the	O
optimal	O
V	B-MetricName
-	I-MetricName
Measure	I-MetricName
value	O
based	O
on	O
the	O
development	O
data	O
,	O
resulting	O
in	O
50	O
clusters	O
.	O

For	O
the	O
CLOSED	O
-	O
WORLD	O
setting	O
,	O
we	O
train	O
and	O
test	O
a	O
multiclass	O
classifier	O
on	O
datasets	O
comprising	O
DIFFVEC	B-MethodName
,	O
r	O
pairs	O
,	O
where	O
r	O
is	O
one	O
of	O
our	O
nine	O
relation	O
types	O
,	O
and	O
DIFFVEC	B-MethodName
is	O
based	O
on	O
one	O
of	O
w2v	B-MethodName
,	O
w2v	B-MethodName
wiki	I-MethodName
and	O
SVD	B-MethodName
.	O

A	O
natural	O
question	O
is	O
whether	O
we	O
can	O
accurately	O
characterise	B-TaskName
lexical	I-TaskName
relations	I-TaskName
through	O
supervised	O
learning	O
over	O
the	O
DIFFVECs	B-MethodName
.	O
For	O
these	O
experiments	O
we	O
use	O
the	O
w2v	B-MethodName
,	O
w2v	B-MethodName
wiki	I-MethodName
,	O
and	O
SVD	B-MethodName
wiki	I-MethodName
embeddings	O
exclusively	O
(	O
based	O
on	O
their	O
superior	O
performance	O
in	O
the	O
clustering	O
experiment	O
)	O
,	O
and	O
a	O
subset	O
of	O
the	O
relations	O
which	O
is	O
both	O
representative	O
of	O
the	O
breadth	O
of	O
the	O
full	O
relation	O
set	O
,	O
and	O
for	O
which	O
we	O
have	O
sufficient	O
data	O
for	O
supervised	O
training	O
and	O
evaluation	O
,	O
namely	O
:	O
NOUN	O
Coll	O
,	O
LEXSEM	O
Event	O
,	O
LEXSEM	O
Hyper	O
,	O
LEXSEM	O
Mero	O
,	O
NOUN	O
SP	O
,	O
PREFIX	O
,	O
VERB	O
3	O
,	O
VERB	O
3Past	O
,	O
and	O
VERB	O
Past	O
(	O
see	O
Table	O
2	O
)	O
.	O

Classification	B-TaskName
.	O

Given	O
the	O
encouraging	O
results	O
from	O
our	O
clustering	B-TaskName
experiment	O
,	O
we	O
next	O
evaluate	O
DIFFVECs	B-MethodName
in	O
a	O
supervised	B-TaskName
relation	I-TaskName
classification	I-TaskName
setting	O
.	O

This	O
is	O
interesting	O
from	O
a	O
DIFFVEC	B-MethodName
point	O
of	O
view	O
,	O
since	O
it	O
shows	O
that	O
the	O
lexical	O
semantics	O
of	O
one	O
word	O
in	O
the	O
pair	O
can	O
overwhelm	O
the	O
semantic	O
content	O
of	O
the	O
DIFFVEC	B-MethodName
(	O
something	O
that	O
we	O
return	O
to	O
investigate	O
in	O
§	O
5.4	O
)	O
.	O

This	O
polysemy	O
results	O
in	O
the	O
distance	O
represented	O
in	O
the	O
DIFFVEC	B-MethodName
for	O
such	O
pairs	O
being	O
above	O
average	O
for	O
VERB	O
3	O
,	O
and	O
consequently	O
clustered	O
with	O
other	O
cross	O
-	O
POS	O
relations	O
.	O

Considering	O
w2v	B-MethodName
embeddings	O
,	O
for	O
VERB	O
3	O
there	O
was	O
a	O
single	O
cluster	O
consisting	O
of	O
around	O
90	O
%	O
of	O
VERB	O
3	O
word	O
pairs	O
.	O

Looking	O
across	O
the	O
different	O
lexical	O
relation	O
types	O
,	O
the	O
morphosyntactic	O
paradigm	O
relations	O
(	O
NOUN	O
SP	O
and	O
the	O
three	O
VERB	O
relations	O
)	O
are	O
by	O
far	O
the	O
easiest	O
to	O
capture	O
.	O

The	O
results	O
are	O
in	O
Table	O
3	O
,	O
with	O
the	O
lowest	O
entropy	B-MetricName
(	O
purest	O
clustering	O
)	O
for	O
each	O
relation	O
indicated	O
in	O
bold	O
.	O

Since	O
the	O
samples	O
are	O
distributed	O
nonuniformly	O
,	O
we	O
normalise	O
entropy	B-MetricName
results	O
for	O
each	O
method	O
by	O
log(n	O
)	O
where	O
n	B-HyperparameterName
is	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
samples	I-HyperparameterName
in	O
a	O
particular	O
relation	O
.	O

For	O
each	O
embedding	O
method	O
,	O
we	O
present	O
the	O
entropy	B-MetricName
for	O
the	O
cluster	O
size	O
where	O
V	B-MetricName
-	I-MetricName
measure	I-MetricName
was	O
maximised	O
over	O
the	O
development	O
data	O
.	O

We	O
additionally	O
calculated	O
the	O
entropy	B-MetricName
for	O
each	O
lexical	O
relation	O
,	O
based	O
on	O
the	O
distribution	O
of	O
instances	O
belonging	O
to	O
a	O
given	O
relation	O
across	O
the	O
different	O
clusters	O
(	O
and	O
simple	O
MLE	O
)	O
.	O

However	O
,	O
both	O
methods	O
still	O
perform	O
well	O
above	O
SENNA	B-MethodName
and	O
HLBL	B-MethodName
,	O
and	O
w2v	B-MethodName
has	O
a	O
clear	O
empirical	O
advantage	O
over	O
GloVe	B-MethodName
.	O
We	O
note	O
that	O
SVD	B-MethodName
wiki	I-MethodName
performs	O
almost	O
as	O
well	O
as	O
w2v	B-MethodName
wiki	I-MethodName
,	O
consistent	O
with	O
the	O
results	O
of	O
Levy	O
et	O
al	O
.	O

HLBL	B-MethodName
and	O
SENNA	B-MethodName
performed	O
very	O
The	O
lower	O
V	B-MetricName
-	I-MetricName
measure	I-MetricName
for	O
w2v	B-MethodName
wiki	I-MethodName
and	O
GloVe	B-MethodName
wiki	I-MethodName
(	O
as	O
compared	O
to	O
w2v	B-MethodName
and	O
GloVe	B-MethodName
,	O
respectively	O
)	O
indicates	O
that	O
the	O
volume	O
of	O
training	O
data	O
plays	O
a	O
role	O
in	O
the	O
clustering	O
results	O
.	O

GloVe	B-MethodName
and	O
SVD	B-MethodName
mirror	O
this	O
result	O
,	O
but	O
are	O
consistently	O
below	O
w2v	B-MethodName
at	O
a	O
V	B-MetricName
-	I-MetricName
Measure	I-MetricName
of	O
around	O
0.31	B-MetricValue
.	O

8	O
Observe	O
that	O
w2v	B-MethodName
achieves	O
the	O
best	O
results	O
,	O
with	O
a	O
V	B-MetricName
-	I-MetricName
Measure	I-MetricName
value	O
of	O
around	O
0.36	B-MetricValue
,	O
9	O
which	O
is	O
relatively	O
constant	O
over	O
varying	O
numbers	O
of	O
clusters	O
.	O

We	O
show	O
results	O
for	O
different	O
numbers	B-HyperparameterName
of	I-HyperparameterName
clusters	I-HyperparameterName
,	O
from	O
N	B-HyperparameterName
=	O
10	B-HyperparameterValue
in	O
steps	O
of	O
10	O
,	O
up	O
to	O
N	B-HyperparameterName
=	O
80	B-HyperparameterValue
(	O
beyond	O
which	O
the	O
clustering	O
quality	O
diminishes	O
)	O
.	O

Figure	O
2	O
presents	O
V	B-MetricName
-	I-MetricName
Measure	I-MetricName
values	O
over	O
the	O
test	O
data	O
for	O
each	O
of	O
the	O
four	O
word	O
embedding	O
models	O
.	O

Spectral	B-MethodName
clustering	I-MethodName
has	O
two	O
hyperparameters	O
:	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
clusters	I-HyperparameterName
,	O
and	O
the	O
pairwise	B-HyperparameterName
similarity	I-HyperparameterName
measure	I-HyperparameterName
for	O
comparing	O
DIFF	B-MethodName
-	I-MethodName
VECs	I-MethodName
.	O
We	O
tune	O
the	O
hyperparameters	O
over	O
development	O
data	O
,	O
in	O
the	O
form	O
of	O
15	O
%	O
of	O
the	O
data	O
obtained	O
by	O
random	O
sampling	O
,	O
selecting	O
the	O
configuration	O
that	O
maximises	O
the	O
V	B-MetricName
-	I-MetricName
Measure	I-MetricName
(	O
Rosenberg	O
and	O
Hirschberg	O
,	O
2007	O
)	O
.	O

We	O
cluster	O
the	O
DIFFVECs	B-MethodName
between	O
all	O
word	O
pairs	O
in	O
our	O
dataset	O
using	O
spectral	B-MethodName
clustering	I-MethodName
(	O
Von	O
Luxburg	O
,	O
2007	O
)	O
.	O

In	O
order	O
to	O
test	O
these	O
assumptions	O
,	O
we	O
cluster	O
our	O
15	O
-	O
relation	O
closed	O
-	O
world	O
dataset	O
in	O
the	O
first	O
instance	O
,	O
and	O
evaluate	O
against	O
the	O
lexical	O
resources	O
in	O
§	O
3.2	O
.	O
As	O
further	O
motivation	O
,	O
we	O
projected	O
the	O
DIFF	B-MethodName
-	I-MethodName
VEC	I-MethodName
space	O
for	O
a	O
small	O
number	O
of	O
samples	O
of	O
each	O
class	O
using	O
t	O
-	O
SNE	O
(	O
Van	O
der	O
Maaten	O
and	O
Hinton	O
,	O
2008	O
)	O
,	O
and	O
found	O
that	O
many	O
of	O
the	O
morphosyntactic	O
relations	O
(	O
VERB	O
3	O
,	O
VERB	O
Past	O
,	O
VERB	O
3Past	O
,	O
NOUN	O
SP	O
)	O
form	O
tight	O
clusters	O
(	O
Figure	O
1	O
)	O
.	O

Assuming	O
DIFFVECs	B-MethodName
are	O
capable	O
of	O
capturing	O
all	O
lexical	O
relations	O
equally	O
,	O
we	O
would	O
expect	O
clustering	O
to	O
be	O
able	O
to	O
identify	O
sets	O
of	O
word	O
pairs	O
with	O
high	O
relational	O
similarity	O
,	O
or	O
equivalently	O
clusters	O
of	O
similar	O
offset	O
vectors	O
.	O

Clustering	B-TaskName
.	O

(	O
2006a	O
)	O
,	O
Princeton	B-DatasetName
Word	I-DatasetName
-	I-DatasetName
Net	I-DatasetName
(	O
Fellbaum	O
,	O
1998	O
)	O
,	O
Wiktionary	B-DatasetName
,	O
5	O
and	O
a	O
web	B-DatasetName
lexicon	I-DatasetName
of	I-DatasetName
collective	I-DatasetName
nouns	I-DatasetName
,	O
6	O
as	O
listed	O
in	O
Table	O
2	O
.	O

The	O
final	O
dataset	O
consists	O
of	O
12,458	O
triples	O
relation	O
,	O
word	O
1	O
,	O
word	O
2	O
,	O
comprising	O
15	O
relation	O
types	O
,	O
extracted	O
from	O
SemEval'12	B-DatasetName
(	O
Jurgens	O
et	O
al	O
.	O
,	O
2012	O
)	O
,	O
BLESS	B-DatasetName
(	O
Baroni	O
and	O
Lenci	O
,	O
2011	O
)	O
,	O
the	O
MSR	B-DatasetName
analogy	I-DatasetName
dataset	O
(	O
Mikolov	O
et	O
al	O
.	O
,	O
2013c	O
)	O
,	O
the	O
light	B-DatasetName
verb	I-DatasetName
dataset	O
of	O
Tan	O
et	O
al	O
.	O

In	O
order	O
to	O
evaluate	O
the	O
applicability	O
of	O
the	O
DIFF	B-MethodName
-	I-MethodName
VEC	I-MethodName
approach	O
to	O
relations	O
of	O
different	O
types	O
,	O
we	O
assembled	O
a	O
set	O
of	O
lexical	O
relations	O
in	O
three	O
broad	O
categories	O
:	O
lexical	O
semantic	O
relations	O
,	O
morphosyntactic	O
paradigm	O
relations	O
,	O
and	O
morphosemantic	O
relations	O
.	O

For	O
the	O
other	O
models	O
we	O
used	O
the	O
following	O
parameter	O
values	O
:	O
for	O
w2v	B-MethodName
,	O
context	B-HyperparameterName
window	I-HyperparameterName
=	O
8	B-HyperparameterValue
,	O
negative	B-HyperparameterName
samples	I-HyperparameterName
=	O
25	B-HyperparameterValue
,	O
hs	B-HyperparameterName
=	O
0	B-HyperparameterValue
,	O
sample	B-HyperparameterName
=	O
1e-4	B-HyperparameterValue
,	O
and	O
iterations	B-HyperparameterName
=	O
15	B-HyperparameterValue
;	O
and	O
for	O
GloVe	B-MethodName
,	O
context	B-HyperparameterName
window	I-HyperparameterName
=	O
15	B-HyperparameterValue
,	O
x	B-HyperparameterName
max	I-HyperparameterName
=	O
10	B-HyperparameterValue
,	O
and	O
iterations	B-HyperparameterName
=	O
15	B-HyperparameterValue
.	O

(	O
2015a	O
)	O
in	O
setting	O
the	O
context	B-HyperparameterName
window	I-HyperparameterName
size	O
to	O
2	B-HyperparameterValue
,	O
negative	B-HyperparameterName
sampling	I-HyperparameterName
parameter	I-HyperparameterName
to	O
1	B-HyperparameterValue
,	O
eigenvalue	B-HyperparameterName
weighting	I-HyperparameterName
to	O
0.5	B-HyperparameterValue
,	O
and	O
context	B-HyperparameterName
distribution	I-HyperparameterName
smoothing	I-HyperparameterName
to	O
0.75	B-HyperparameterValue
;	O
other	O
parameters	O
were	O
assigned	O
their	O
default	O
values	O
.	O

For	O
the	O
SVD	B-MethodName
model	I-MethodName
,	O
we	O
followed	O
the	O
recommendations	O
of	O
Levy	O
et	O
al	O
.	O

For	O
w2v	B-MethodName
wiki	I-MethodName
,	O
GloVe	B-MethodName
wiki	I-MethodName
and	O
SVD	B-MethodName
wiki	I-MethodName
we	O
used	O
English	B-DatasetName
Wikipedia	I-DatasetName
.	O

(	O
2010	O
)	O
,	O
trained	O
on	O
the	O
Reuters	B-DatasetName
English	I-DatasetName
newswire	I-DatasetName
corpus	O
.	O

For	O
HLBL	B-MethodName
and	O
SENNA	B-MethodName
,	O
we	O
use	O
the	O
pre	O
-	O
trained	O
embeddings	O
from	O
Turian	O
et	O
al	O
.	O

The	O
final	O
model	O
,	O
SENNA	B-MethodName
(	O
Collobert	O
and	O
Weston	O
,	O
2008	O
)	O
,	O
was	O
initially	O
proposed	O
for	O
multi	O
-	O
task	O
training	O
of	O
several	O
language	O
processing	O
tasks	O
,	O
from	O
language	O
modelling	O
through	O
to	O
semantic	O
role	O
labelling	O
.	O

HLBL	B-MethodName
(	O
Mnih	O
and	O
Hinton	O
,	O
2009	O
)	O
is	O
a	O
log	O
-	O
bilinear	O
formulation	O
of	O
an	O
n	O
-	O
gram	O
language	O
model	O
,	O
which	O
predicts	O
the	O
ith	O
word	O
based	O
on	O
context	O
words	O
(	O
i	O
−	O
n	O
,	O
.	O

The	O
SVD	B-MethodName
model	I-MethodName
(	O
Levy	O
et	O
al	O
.	O
,	O
2015a	O
)	O
uses	O
positive	O
pointwise	O
mutual	O
information	O
(	O
PMI	O
)	O
matrix	O
defined	O
as	O
:	O
PPMI(w	O
,	O
c	O
)	O
=	O
max(log	O
P	O
(	O
w	O
,	O
c	O
)	O
P	O
(	O
w	O
)	O
P	O
(	O
c	O
)	O
,	O
0	O
)	O
,	O
where	O
P	O
(	O
w	O
,	O
c	O
)	O
is	O
the	O
joint	O
probability	O
of	O
word	O
w	O
and	O
context	O
c	O
,	O
and	O
P	O
(	O
w	O
)	O
and	O
P	O
(	O
c	O
)	O
are	O
their	O
marginal	O
probabilities	O
.	O

The	O
model	O
was	O
trained	O
on	O
English	B-DatasetName
Wikipedia	I-DatasetName
and	O
the	O
English	B-DatasetName
Gigaword	I-DatasetName
corpus	O
version	O
5	O
.	O

The	O
GloVe	B-MethodName
model	O
(	O
Pennington	O
et	O
al	O
.	O
,	O
2014	O
)	O
is	O
based	O
on	O
a	O
similar	O
bilinear	O
formulation	O
,	O
framed	O
as	O
a	O
low	O
-	O
rank	O
decomposition	O
of	O
the	O
matrix	O
of	O
corpus	O
co	O
-	O
occurrence	O
frequencies	O
:	O
J	O
=	O
1	O
2	O
V	O
i	O
,	O
j=1	O
f	O
(	O
P	O
ij	O
)	O
(	O
w	O
i	O
wj	O
−	O
log	O
P	O
ij	O
)	O
2	O
,	O
where	O
w	O
i	O
is	O
a	O
vector	O
for	O
the	O
left	O
context	O
,	O
w	O
j	O
is	O
a	O
vector	O
for	O
the	O
right	O
context	O
,	O
P	O
ij	O
is	O
the	O
relative	O
frequency	O
of	O
word	O
j	O
in	O
the	O
context	O
of	O
word	O
i	O
,	O
and	O
f	O
is	O
a	O
heuristic	O
weighting	O
function	O
to	O
balance	O
the	O
influence	O
of	O
high	O
versus	O
low	O
term	O
frequencies	O
.	O

2	O
Google	B-DatasetName
News	I-DatasetName
data	O
was	O
used	O
to	O
train	O
the	O
model	O
.	O

1	O
w2v	B-MethodName
CBOW	I-MethodName
(	O
Continuous	O
Bag	O
-	O
Of	O
-	O
Words	O
;	O
Mikolov	O
et	O
al	O
.	O

We	O
additionally	O
normalise	O
the	O
w2v	B-MethodName
wiki	I-MethodName
and	O
SVD	B-MethodName
wiki	I-MethodName
vectors	O
to	O
unit	O
length	O
;	O
GloVe	B-MethodName
wiki	I-MethodName
is	O
natively	O
normalised	O
by	O
column	O
.	O

For	O
consistency	O
of	O
comparison	O
,	O
we	O
train	O
SVD	B-MethodName
as	O
well	O
as	O
a	O
version	O
of	O
w2v	B-MethodName
and	O
GloVe	B-MethodName
(	O
which	O
we	O
call	O
w2v	B-MethodName
wiki	I-MethodName
and	O
GloVe	B-MethodName
wiki	I-MethodName
,	O
respectively	O
)	O
on	O
the	O
English	B-TaskName
Wikipedia	I-TaskName
corpus	O
(	O
comparable	O
in	O
size	O
to	O
the	O
training	O
data	O
of	O
SENNA	B-MethodName
and	O
HLBL	B-MethodName
)	O
,	O
and	O
apply	O
the	O
preprocessing	O
of	O
Levy	O
et	O
al	O
.	O

We	O
also	O
include	O
SVD	B-MethodName
(	O
Levy	O
et	O
al	O
.	O
,	O
2015a	O
)	O
,	O
a	O
count	O
-	O
based	O
model	O
which	O
factorises	O
a	O
positive	O
PMI	O
(	O
PPMI	O
)	O
matrix	O
.	O

We	O
consider	O
four	O
highly	O
successful	O
word	O
embedding	O
models	O
in	O
our	O
experiments	O
:	O
w2v	B-MethodName
(	O
Mikolov	O
et	O
al	O
.	O
,	O
2013a;Mikolov	O
et	O
al	O
.	O
,	O
2013b	O
)	O
,	O
GloVe	B-MethodName
(	O
Pennington	O
et	O
al	O
.	O
,	O
2014	O
)	O
,	O
SENNA	B-MethodName
(	O
Collobert	O
and	O
Weston	O
,	O
2008	O
)	O
,	O
and	O
HLBL	B-MethodName
(	O
Mnih	O
and	O
Hinton	O
,	O
2009	O
)	O
,	O
as	O
detailed	O
below	O
.	O

(	O
2015a	O
)	O
,	O
to	O
test	O
the	O
generalisability	O
of	O
DIFFVECs	B-MethodName
to	O
count	O
-	O
based	O
word	O
embeddings	O
.	O

As	O
the	O
focus	O
of	O
this	O
paper	O
is	O
not	O
the	O
word	O
embedding	O
pre	O
-	O
training	O
approaches	O
so	O
much	O
as	O
the	O
utility	O
of	O
the	O
DIFFVECs	B-MethodName
for	O
lexical	B-TaskName
relation	I-TaskName
learning	I-TaskName
,	O
we	O
take	O
a	O
selection	O
of	O
four	O
pre	O
-	O
trained	O
word	O
embeddings	O
with	O
strong	O
currency	O
in	O
the	O
literature	O
,	O
as	O
detailed	O
in	O
§	O
3.1	O
.	O
We	O
also	O
include	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
count	O
-	O
based	O
approach	O
of	O
Levy	O
et	O
al	O
.	O

Such	O
dimensions	O
could	O
be	O
identified	O
and	O
exploited	O
as	O
part	O
of	O
a	O
clustering	B-TaskName
or	O
classification	B-TaskName
method	O
,	O
in	O
the	O
context	O
of	O
identifying	O
relations	O
between	O
word	O
pairs	O
or	O
classes	O
of	O
DIFFVECs	B-MethodName
.	O
In	O
order	O
to	O
test	O
the	O
generalisability	O
of	O
the	O
DIFF	B-MethodName
-	I-MethodName
VEC	I-MethodName
method	O
,	O
we	O
require	O
:	O
(	O
1	O
)	O
word	O
embeddings	O
,	O
and	O
(	O
2	O
)	O
a	O
set	O
of	O
lexical	O
relations	O
to	O
evaluate	O
against	O
.	O

A	O
second	O
assumption	O
is	O
that	O
there	O
exist	O
dimensions	O
,	O
or	O
directions	O
,	O
in	O
the	O
embedding	O
vector	O
spaces	O
responsible	O
for	O
a	O
particular	O
lexical	O
relation	O
.	O

While	O
a	O
range	O
of	O
methods	O
have	O
been	O
proposed	O
for	O
composing	O
word	O
vectors	O
(	O
Baroni	O
et	O
al	O
.	O
,	O
2012;Weeds	O
et	O
al	O
.	O
,	O
2014;Roller	O
et	O
al	O
.	O
,	O
2014	O
)	O
,	O
in	O
this	O
research	O
we	O
focus	O
exclusively	O
on	O
DIFFVEC	B-MethodName
(	O
i.e.	O
w	O
2	O
−	O
w	O
1	O
)	O
.	O

Our	O
starting	O
point	O
for	O
lexical	B-TaskName
relation	I-TaskName
learning	I-TaskName
is	O
the	O
assumption	O
that	O
important	O
information	O
about	O
various	O
types	O
of	O
relations	O
is	O
implicitly	O
embedded	O
in	O
the	O
offset	O
vectors	O
.	O

We	O
define	O
the	O
task	O
of	O
lexical	B-TaskName
relation	I-TaskName
learning	I-TaskName
to	O
take	O
a	O
set	O
of	O
(	O
ordered	O
)	O
word	O
pairs	O
{	O
(	O
w	O
i	O
,	O
w	O
j	O
)	O
}	O
and	O
a	O
set	O
of	O
binary	O
lexical	O
relations	O
R	O
=	O
{	O
r	O
k	O
}	O
,	O
and	O
map	O
each	O
word	O
pair	O
(	O
w	O
i	O
,	O
w	O
j	O
)	O
as	O
follows	O
:	O
(	O
a	O
)	O
(	O
w	O
i	O
,	O
w	O
j	O
)	O
→	O
r	O
k	O
∈	O
R	O
,	O
i.e.	O
the	O
"	O
closed	O
-	O
world	O
"	O
setting	O
,	O
where	O
we	O
assume	O
that	O
all	O
word	O
pairs	O
can	O
be	O
uniquely	O
classified	O
according	O
to	O
a	O
relation	O
in	O
R	O
;	O
or	O
(	O
b	O
)	O
(	O
w	O
i	O
,	O
w	O
j	O
)	O
→	O
r	O
k	O
∈	O
R	O
∪	O
{	O
φ	O
}	O
where	O
φ	O
signifies	O
the	O
fact	O
that	O
none	O
of	O
the	O
relations	O
in	O
R	O
apply	O
to	O
the	O
word	O
pair	O
in	O
question	O
,	O
i.e.	O
the	O
"	O
open	O
-	O
world	O
"	O
setting	O
.	O

However	O
,	O
their	O
evaluation	O
is	O
performed	O
in	O
the	O
context	O
of	O
relational	O
similarity	O
,	O
and	O
they	O
do	O
not	O
perform	O
clustering	B-TaskName
or	O
classification	B-TaskName
on	O
the	O
DIFFVECs	B-MetricName
.	O
General	O
Approach	O
and	O
Resources	O
.	O

(	O
2013	O
)	O
divide	O
antonym	O
pairs	O
into	O
semantic	O
classes	O
such	O
as	O
quality	O
,	O
time	O
,	O
gender	O
,	O
and	O
distance	O
,	O
finding	O
that	O
for	O
about	O
two	O
-	O
thirds	O
of	O
antonym	O
classes	O
,	O
DIFFVECs	B-MethodName
are	O
significantly	O
more	O
correlated	O
than	O
random	O
.	O

Neural	O
networks	O
have	O
also	O
been	O
developed	O
for	O
joint	O
learning	O
of	O
lexical	O
and	O
relational	O
similarity	O
,	O
making	O
use	O
of	O
the	O
WordNet	O
relation	O
hierarchy	O
(	O
Bordes	O
et	O
al	O
.	O
,	O
2013;Socher	O
et	O
al	O
.	O
,	O
2013;Xu	O
et	O
al	O
.	O
,	O
2014;Yu	O
and	O
Dredze	O
,	O
2014;Faruqui	O
et	O
al	O
.	O
,	O
2015;Fried	O
and	O
Duh	O
,	O
2015	O
)	O
.	O

(	O
2014	O
)	O
similarly	O
use	O
embeddings	O
to	O
predict	O
hypernym	O
relations	O
,	O
in	O
this	O
case	O
clustering	O
words	O
by	O
topic	O
to	O
show	O
that	O
hypernym	O
DIFFVECs	B-MethodName
can	O
be	O
broken	O
down	O
into	O
more	O
fine	O
-	O
grained	O
relations	O
.	O

This	O
has	O
given	O
rise	O
to	O
a	O
series	O
of	O
papers	O
exploring	O
the	O
DIFFVEC	B-MethodName
idea	O
in	O
different	O
contexts	O
.	O

Recently	O
,	O
attention	O
has	O
turned	O
to	O
using	O
vector	O
space	O
models	O
of	O
words	O
for	O
relation	B-TaskName
classification	I-TaskName
and	O
relational	B-TaskName
similarity	I-TaskName
prediction	I-TaskName
.	O

Relation	B-TaskName
learning	I-TaskName
is	O
an	O
important	O
and	O
long	O
-	O
standing	O
task	O
in	O
NLP	O
and	O
has	O
been	O
the	O
focus	O
of	O
a	O
number	O
of	O
shared	O
tasks	O
(	O
Girju	O
et	O
al	O
.	O
,	O
2007;Hendrickx	O
et	O
al	O
.	O
,	O
2010;Jurgens	O
et	O
al	O
.	O
,	O
2012	O
)	O
.	O

On	O
the	O
other	O
hand	O
,	O
relational	B-TaskName
similarity	I-TaskName
prediction	I-TaskName
involves	O
assessing	O
the	O
degree	O
to	O
which	O
a	O
word	O
pair	O
(	O
A	O
,	O
B	O
)	O
stands	O
in	O
the	O
same	O
relation	O
as	O
another	O
pair	O
(	O
C	O
,	O
D	O
)	O
,	O
or	O
to	O
complete	O
an	O
analogy	O
A	O
:	O
B	O
:	O
:	O
C	O
:	O
-?-	O
.	O

In	O
the	O
Open	B-TaskName
Information	I-TaskName
Extraction	I-TaskName
paradigm	O
(	O
Banko	O
et	O
al	O
.	O
,	O
2007;Weikum	O
and	O
Theobald	O
,	O
2010	O
)	O
,	O
also	O
known	O
as	O
unsupervised	B-TaskName
relation	I-TaskName
extraction	I-TaskName
,	O
the	O
relations	O
themselves	O
are	O
also	O
learned	O
from	O
the	O
text	O
(	O
e.g.	O
in	O
the	O
form	O
of	O
text	O
labels	O
)	O
.	O

Given	O
a	O
word	O
pair	O
,	O
the	O
relation	B-TaskName
classification	I-TaskName
task	O
involves	O
assigning	O
a	O
word	O
pair	O
to	O
the	O
correct	O
relation	O
from	O
a	O
pre	O
-	O
defined	O
set	O
.	O

In	O
relation	B-TaskName
extraction	I-TaskName
,	O
related	O
word	O
pairs	O
in	O
a	O
corpus	O
and	O
the	O
relevant	O
relation	O
are	O
identified	O
.	O

Relation	B-TaskName
learning	I-TaskName
in	O
NLP	O
includes	O
relation	B-TaskName
extraction	I-TaskName
,	O
relation	B-TaskName
classification	I-TaskName
,	O
and	O
relational	B-TaskName
similarity	I-TaskName
prediction	I-TaskName
.	O

We	O
also	O
find	O
that	O
hyper	B-MethodName
-	I-MethodName
parameter	I-MethodName
optimised	I-MethodName
count	I-MethodName
-	I-MethodName
based	I-MethodName
methods	I-MethodName
are	O
competitive	O
with	O
predictbased	O
methods	O
under	O
both	O
clustering	O
and	O
supervised	O
relation	O
classification	O
,	O
in	O
line	O
with	O
the	O
findings	O
of	O
Levy	O
et	O
al	O
.	O

Second	O
,	O
we	O
perform	O
classification	B-TaskName
over	O
the	O
DIFF	B-MethodName
-	I-MethodName
VECs	I-MethodName
and	O
obtain	O
remarkably	O
high	O
accuracy	B-MetricName
in	O
a	O
closed	O
-	O
world	O
setting	O
(	O
over	O
a	O
predefined	O
set	O
of	O
word	O
pairs	O
,	O
each	O
of	O
which	O
corresponds	O
to	O
a	O
lexical	O
relation	O
in	O
the	O
training	O
data	O
)	O
.	O

First	O
,	O
we	O
cluster	O
the	O
DIFFVECs	B-MethodName
to	O
test	O
whether	O
the	O
clusters	O
map	O
onto	O
true	O
lexical	O
relations	O
.	O

We	O
then	O
apply	O
DIFFVECs	B-MethodName
to	O
two	O
new	O
tasks	O
:	O
unsupervised	B-TaskName
and	O
supervised	B-TaskName
relation	I-TaskName
extraction	I-TaskName
.	O

Moreover	O
,	O
the	O
task	O
does	O
not	O
explore	O
the	O
full	O
implications	O
of	O
DIFFVECs	B-MethodName
as	O
meaningful	O
vector	O
space	O
objects	O
in	O
their	O
own	O
right	O
,	O
because	O
it	O
only	O
looks	O
for	O
a	O
one	O
-	O
best	O
answer	O
to	O
the	O
particular	O
lexical	O
analogies	O
in	O
the	O
test	O
set	O
.	O

The	O
success	O
of	O
the	O
simple	O
offset	O
method	O
on	O
analogy	O
completion	O
suggests	O
that	O
the	O
difference	O
vectors	O
(	O
"	O
DIFFVEC	B-MethodName
"	O
hereafter	O
)	O
must	O
themselves	O
be	O
meaningful	O
:	O
their	O
direction	O
and/or	O
magnitude	O
encodes	O
a	O
lexical	O
relation	O
.	O

Recently	O
,	O
attention	O
has	O
been	O
focused	O
on	O
identifying	B-TaskName
lexical	I-TaskName
relations	I-TaskName
using	O
word	O
embeddings	O
,	O
which	O
are	O
dense	O
,	O
low	O
-	O
dimensional	O
vectors	O
obtained	O
either	O
from	O
a	O
"	O
predict	O
-	O
based	O
"	O
neural	O
network	O
trained	O
to	O
predict	O
word	O
contexts	O
,	O
or	O
a	O
"	O
countbased	O
"	O
traditional	O
distributional	O
similarity	O
method	O
combined	O
with	O
dimensionality	O
reduction	O
.	O

Learning	O
to	O
identify	B-TaskName
lexical	I-TaskName
relations	I-TaskName
is	O
a	O
fundamental	O
task	O
in	O
natural	O
language	O
processing	O
(	O
"	O
NLP	O
"	O
)	O
,	O
and	O
can	O
contribute	O
to	O
many	O
NLP	O
applications	O
including	O
paraphrasing	O
and	O
generation	O
,	O
machine	O
translation	O
,	O
and	O
ontology	O
building	O
(	O
Banko	O
et	O
al	O
.	O
,	O
2007;Hendrickx	O
et	O
al	O
.	O
,	O
2010	O
)	O
.	O

Take	O
and	O
Took	O
,	O
Gaggle	O
and	O
Goose	O
,	O
Book	O
and	O
Read	O
:	O
Evaluating	O
the	O
Utility	O
of	O
Vector	O
Differences	O
for	O
Lexical	B-TaskName
Relation	I-TaskName
Learning	I-TaskName
.	O

LR	O
was	O
supported	O
by	O
EPSRC	O
grant	O
EP	O
/	O
I037512/1	O
and	O
ERC	O
Starting	O
Grant	O
DisCoTex	O
(	O
306920	O
)	O
.	O

Acknowledgments	O
.	O

This	O
paper	O
is	O
the	O
first	O
to	O
test	O
the	O
generalisability	O
of	O
the	O
vector	O
difference	O
approach	O
across	O
a	O
broad	O
range	O
of	O
lexical	O
relations	O
(	O
in	O
raw	O
number	O
and	O
also	O
variety	O
)	O
.	O

Conclusions	O
.	O

The	O
results	O
are	O
shown	O
in	O
Figure	O
4	O
.	O

We	O
then	O
train	O
classifiers	O
with	O
and	O
without	O
negative	O
sampling	O
(	O
§	O
5.3	O
)	O
,	O
incrementally	O
adding	O
the	O
random	O
word	O
pairs	O
from	O
§	O
5.2	O
to	O
the	O
test	O
data	O
(	O
from	O
no	O
random	O
word	O
pairs	O
to	O
five	O
times	O
the	O
original	O
size	O
of	O
the	O
test	O
data	O
)	O
to	O
investigate	O
the	O
interaction	O
of	O
negative	O
sampling	O
with	O
greater	O
diversity	O
in	O
the	O
test	O
set	O
when	O
there	O
is	O
a	O
split	O
vocabulary	O
.	O

(	O
2015b	O
)	O
in	O
splitting	O
our	O
vocabulary	O
into	O
training	O
and	O
test	O
partitions	O
,	O
to	O
ensure	O
there	O
is	O
no	O
overlap	O
between	O
training	O
and	O
test	O
vocabulary	O
.	O

To	O
address	O
this	O
effect	O
,	O
we	O
follow	O
Levy	O
et	O
al	O
.	O

Weeds	O
.	O

Lexical	O
Memorisation	O
.	O

For	O
example	O
,	O
(	O
animal	O
,	O
cat	O
)	O
,	O
(	O
animal	O
,	O
dog	O
)	O
,	O
and	O
(	O
animal	O
,	O
pig	O
)	O
all	O
share	O
the	O
superclass	O
animal	O
,	O
and	O
the	O
model	O
thus	O
learns	O
to	O
classify	O
as	O
positive	O
any	O
word	O
pair	O
with	O
animal	O
as	O
the	O
first	O
word	O
.	O

(	O
2014	O
)	O
and	O
Levy	O
et	O
al	O
.	O

et	O
al	O
.	O

The	O
classifier	O
was	O
able	O
to	O
capture	O
(	O
herd	O
,	O
horses	O
)	O
but	O
not	O
(	O
run	O
,	O
salmon	O
)	O
,	O
(	O
party	O
,	O
jays	O
)	O
or	O
(	O
singular	O
,	O
boar	O
)	O
as	O
instances	O
of	O
NOUN	O
Coll	O
,	O
possibly	O
because	O
of	O
polysemy	O
.	O

For	O
example	O
,	O
the	O
standard	O
classifier	O
for	O
NOUN	O
Coll	O
learned	O
to	O
match	O
word	O
pairs	O
including	O
an	O
animal	O
name	O
(	O
e.g.	O
,	O
(	O
plague	O
,	O
rats	O
)	O
)	O
,	O
while	O
training	O
with	O
negative	O
samples	O
resulted	O
in	O
much	O
more	O
conservative	O
predictions	O
and	O
consequently	O
much	O
lower	O
recall	O
.	O

Observe	O
that	O
the	O
precision	O
is	O
much	O
higher	O
and	O
recall	O
somewhat	O
lower	O
compared	O
to	O
the	O
classifier	O
trained	O
with	O
only	O
positive	O
samples	O
.	O

12	O
The	O
results	O
are	O
shown	O
in	O
the	O
right	O
half	O
of	O
Table	O
5	O
(	O
as	O
"	O
+	O
neg	O
"	O
)	O
.	O

After	O
training	O
our	O
classifier	O
,	O
we	O
evaluate	O
its	O
predictions	O
in	O
the	O
same	O
way	O
as	O
in	O
§	O
5.2	O
,	O
using	O
the	O
same	O
test	O
set	O
combining	O
related	O
and	O
random	O
word	O
pairs	O
.	O

Both	O
types	O
of	O
distractors	O
are	O
added	O
to	O
the	O
training	O
set	O
,	O
such	O
that	O
there	O
are	O
equal	O
numbers	O
of	O
valid	O
relations	O
,	O
opposite	O
pairs	O
and	O
shuffled	O
pairs	O
.	O

This	O
is	O
targeted	O
at	O
relations	O
that	O
take	O
specific	O
word	O
classes	O
in	O
particular	O
positions	O
,	O
e.g.	O
,	O
(	O
VB	O
,	O
VBD	O
)	O
word	O
pairs	O
,	O
so	O
that	O
the	O
model	O
learns	O
to	O
encode	O
the	O
relation	O
rather	O
than	O
simply	O
learning	O
the	O
properties	O
of	O
the	O
word	O
classes	O
.	O

shuffled	O
pairs	O
:	O
generated	O
by	O
replacing	O
w	O
2	O
with	O
a	O
random	O
word	O
w	O
2	O
from	O
the	O
same	O
relation	O
,	O
Shuff	O
w1	O
,	O
w2	O
=	O
word	O
2	O
−	O
word	O
1	O
.	O

This	O
ensures	O
the	O
classifier	O
adequately	O
captures	O
the	O
asymmetry	O
in	O
the	O
relations	O
.	O

distractors	O
:	O
opposite	O
pairs	O
:	O
generated	O
by	O
switching	O
the	O
order	O
of	O
word	O
pairs	O
,	O
Oppos	O
w1	O
,	O
w2	O
=	O
word	O
1	O
−	O
word	O
2	O
.	O

To	O
this	O
end	O
,	O
we	O
automatically	O
generated	O
two	O
types	O
of	O
negative	O
dings	O
.	O

The	O
basic	O
intuition	O
behind	O
this	O
approach	O
is	O
to	O
construct	O
samples	O
which	O
will	O
force	O
the	O
model	O
to	O
learn	O
decision	O
boundaries	O
that	O
more	O
tightly	O
capture	O
the	O
true	O
scope	O
of	O
a	O
given	O
relation	O
.	O

To	O
address	O
the	O
problem	O
of	O
incorrectly	O
classifying	O
random	O
word	O
pairs	O
as	O
valid	O
relations	O
,	O
we	O
retrain	O
the	O
classifier	O
on	O
a	O
dataset	O
comprising	O
both	O
valid	O
and	O
automatically	O
-	O
generated	O
negative	O
distractor	O
samples	O
.	O

OPEN	O
-	O
WORLD	O
Training	O
with	O
Negative	O
Sampling	O
.	O

That	O
is	O
,	O
the	O
model	O
captures	O
syntax	O
,	O
but	O
lacks	O
the	O
ability	O
to	O
capture	O
lexical	O
paradigms	O
,	O
and	O
tends	O
to	O
overgenerate	O
.	O

For	O
instance	O
,	O
the	O
random	O
pairs	O
(	O
have	O
,	O
works	O
)	O
,	O
(	O
turn	O
,	O
took	O
)	O
,	O
and	O
(	O
works	O
,	O
started	O
)	O
were	O
incorrectly	O
classified	O
as	O
VERB	O
3	O
,	O
VERB	O
Past	O
and	O
VERB	O
3Past	O
,	O
respectively	O
.	O

The	O
results	O
of	O
our	O
experiments	O
are	O
presented	O
in	O
the	O
left	O
half	O
of	O
Table	O
5	O
,	O
in	O
which	O
we	O
report	O
on	O
results	O
over	O
the	O
combination	O
of	O
the	O
original	O
test	O
data	O
from	O
§	O
5.1	O
and	O
the	O
random	O
word	O
pairs	O
,	O
noting	O
that	O
recall	O
(	O
R	O
)	O
for	O
OPEN	O
-	O
WORLD	O
takes	O
the	O
form	O
of	O
relative	O
recall	O
(	O
Pantel	O
et	O
al	O
.	O
,	O
2004	O
)	O
over	O
the	O
positively	O
-	O
classified	O
word	O
pairs	O
.	O

Fully	O
annotating	O
our	O
random	O
word	O
pairs	O
is	O
prohibitively	O
expensive	O
,	O
so	O
instead	O
,	O
we	O
manually	O
annotated	O
only	O
the	O
word	O
pairs	O
which	O
were	O
positively	O
classified	O
by	O
one	O
of	O
our	O
models	O
.	O

This	O
procedure	O
generates	O
word	O
pairs	O
that	O
are	O
representative	O
of	O
the	O
frequency	O
profile	O
of	O
our	O
corpus	O
.	O

(	O
2	O
)	O
take	O
the	O
Cartesian	O
product	O
over	O
pairs	O
of	O
words	O
from	O
the	O
seed	O
lexicon	O
;	O
(	O
3	O
)	O
sample	O
word	O
pairs	O
uniformly	O
from	O
this	O
set	O
.	O

10	O
For	O
these	O
experiments	O
,	O
we	O
train	O
a	O
binary	O
classifier	O
for	O
each	O
relation	O
type	O
,	O
using	O
2	O
3	O
of	O
our	O
relation	O
data	O
for	O
training	O
and	O
1	O
3	O
for	O
testing	O
.	O

We	O
now	O
turn	O
to	O
a	O
more	O
challenging	O
evaluation	O
setting	O
:	O
a	O
test	O
set	O
including	O
word	O
pairs	O
drawn	O
at	O
random	O
.	O

OPEN	O
-	O
WORLD	O
Classification	O
.	O

The	O
impact	O
of	O
the	O
training	O
data	O
volume	O
for	O
pre	O
-	O
training	O
of	O
the	O
embeddings	O
is	O
also	O
less	O
pronounced	O
than	O
in	O
the	O
case	O
of	O
our	O
clustering	O
experiment	O
.	O

(	O
2015a	O
)	O
that	O
under	O
appropriate	O
parameter	O
settings	O
,	O
count	O
-	O
based	O
methods	O
achieve	O
high	O
results	O
.	O

We	O
label	O
each	O
cluster	O
with	O
the	O
majority	O
class	O
based	O
on	O
the	O
training	O
instances	O
,	O
and	O
evaluate	O
the	O
resultant	O
labelling	O
for	O
the	O
test	O
instances	O
.	O

CLOSED	O
-	O
WORLD	O
Classification	O
.	O

(	O
2015b	O
)	O
for	O
hypernyms	O
,	O
by	O
experimenting	O
with	O
disjoint	O
training	O
and	O
test	O
vocabulary	O
.	O

(	O
2014	O
)	O
and	O
Levy	O
et	O
al	O
.	O

For	O
both	O
settings	O
,	O
we	O
further	O
investigate	O
whether	O
there	O
is	O
a	O
lexical	O
memorisation	O
effect	O
for	O
a	O
broad	O
range	O
of	O
relation	O
types	O
of	O
the	O
sort	O
identified	O
by	O
Weeds	O
et	O
al	O
.	O

We	O
consider	O
two	O
applications	O
:	O
(	O
1	O
)	O
a	O
CLOSED	O
-	O
WORLD	O
setting	O
similar	O
to	O
the	O
unsupervised	O
evaluation	O
,	O
in	O
which	O
the	O
classifier	O
only	O
encounters	O
word	O
pairs	O
which	O
correspond	O
to	O
one	O
of	O
the	O
nine	O
relations	O
;	O
and	O
(	O
2	O
)	O
a	O
more	O
challenging	O
OPEN	O
-	O
WORLD	O
setting	O
where	O
random	O
word	O
pairs	O
-which	O
may	O
or	O
may	O
not	O
correspond	O
to	O
one	O
of	O
our	O
relations	O
-are	O
included	O
in	O
the	O
evaluation	O
.	O

LEXSEM	O
Mero	O
was	O
also	O
split	O
into	O
multiple	O
clusters	O
along	O
topical	O
lines	O
,	O
with	O
separate	O
clusters	O
for	O
weapons	O
,	O
dwellings	O
,	O
vehicles	O
,	O
etc	O
.	O

A	O
related	O
phenomenon	O
was	O
observed	O
for	O
NOUN	O
Coll	O
,	O
where	O
the	O
instances	O
were	O
assigned	O
to	O
a	O
large	O
mixed	O
cluster	O
containing	O
word	O
pairs	O
where	O
the	O
second	O
word	O
referred	O
to	O
an	O
animal	O
,	O
reflecting	O
the	O
fact	O
that	O
most	O
of	O
the	O
collective	O
nouns	O
in	O
our	O
dataset	O
relate	O
to	O
animals	O
,	O
e.g.	O
(	O
stand	O
,	O
horse	O
)	O
,	O
(	O
ambush	O
,	O
tigers	O
)	O
,	O
(	O
antibiotics	O
,	O
bacteria	O
)	O
.	O

Here	O
,	O
the	O
noun	O
saw	O
is	O
ambiguous	O
with	O
a	O
high	O
-	O
frequency	O
past	O
-	O
tense	O
verb	O
;	O
hurt	O
and	O
wipe	O
also	O
have	O
ambigous	O
POS	O
.	O

For	O
VERB	O
Past	O
,	O
a	O
single	O
relatively	O
pure	O
cluster	O
was	O
generated	O
,	O
with	O
minor	O
contamination	O
due	O
to	O
pairs	O
such	O
as	O
(	O
hurt	O
,	O
saw	O
)	O
,	O
(	O
utensil	O
,	O
saw	O
)	O
,	O
and	O
(	O
wipe	O
,	O
saw	O
)	O
.	O

Example	O
VERB	O
3	O
pairs	O
incorrectly	O
clustered	O
are	O
:	O
(	O
study	O
,	O
studies	O
)	O
,	O
(	O
run	O
,	O
runs	O
)	O
,	O
and	O
(	O
like	O
,	O
likes	O
)	O
.	O

Most	O
errors	O
resulted	O
from	O
POS	O
ambiguity	O
,	O
leading	O
to	O
confusion	O
with	O
VERB	O
-	O
NOUN	O
in	O
particular	O
.	O

The	O
lexical	O
semantic	O
relations	O
,	O
on	O
the	O
other	O
hand	O
,	O
are	O
the	O
hardest	O
to	O
capture	O
for	O
all	O
embeddings	O
.	O

(	O
2015a	O
)	O
.	O

Under	O
the	O
additional	O
assumption	O
that	O
a	O
given	O
word	O
pair	O
corresponds	O
to	O
a	O
unique	O
lexical	O
relation	O
(	O
in	O
line	O
with	O
our	O
definition	O
of	O
the	O
lexical	O
relation	O
learning	O
task	O
in	O
§	O
3	O
)	O
,	O
a	O
hard	O
clustering	O
approach	O
is	O
appropriate	O
.	O

7	O
.	O

We	O
manually	O
filtered	O
the	O
data	O
to	O
remove	O
duplicates	O
(	O
e.g.	O
,	O
as	O
part	O
of	O
merging	O
the	O
two	O
sources	O
of	O
LEXSEM	O
Hyper	O
intances	O
)	O
,	O
and	O
normalise	O
directionality	O
.	O

(	O
2013c	O
)	O
,	O
but	O
we	O
include	O
a	O
much	O
wider	O
range	O
of	O
lexical	O
semantic	O
relations	O
,	O
especially	O
those	O
standardly	O
evaluated	O
in	O
the	O
relation	O
classification	O
literature	O
.	O

There	O
is	O
some	O
overlap	O
between	O
our	O
relations	O
and	O
those	O
included	O
in	O
the	O
analogy	O
task	O
of	O
Mikolov	O
et	O
al	O
.	O

We	O
additionally	O
constrained	O
the	O
dataset	O
to	O
the	O
words	O
occurring	O
in	O
all	O
embedding	O
sets	O
.	O

4	O
Consequently	O
we	O
excluded	O
symmetric	O
lexical	O
relations	O
such	O
as	O
synonymy	O
.	O

We	O
constrained	O
the	O
relations	O
to	O
be	O
binary	O
and	O
to	O
have	O
fixed	O
directionality	O
.	O

Lexical	O
Relations	O
.	O

During	O
the	O
training	O
phase	O
,	O
for	O
each	O
model	O
we	O
set	O
a	O
word	O
frequency	O
threshold	O
of	O
5	O
.	O

(	O
2015a	O
)	O
,	O
3	O
i.e.	O
,	O
lower	O
-	O
cased	O
all	O
words	O
and	O
removed	O
non	O
-	O
textual	O
elements	O
.	O

We	O
followed	O
the	O
same	O
preprocessing	O
procedure	O
described	O
in	O
Levy	O
et	O
al	O
.	O

In	O
both	O
cases	O
,	O
the	O
embeddings	O
were	O
scaled	O
by	O
the	O
global	O
standard	O
deviation	O
over	O
the	O
word	O
-	O
embedding	O
matrix	O
,	O
W	O
scaled	O
=	O
0.1	O
×	O
W	O
σ(W	O
)	O
.	O

,	O
w	O
i−1	O
,	O
w	O
k	O
)	O
,	O
where	O
the	O
last	O
c	O
−	O
1	O
words	O
are	O
used	O
as	O
context	O
,	O
and	O
f	O
(	O
x	O
)	O
is	O
a	O
non	O
-	O
linear	O
function	O
of	O
the	O
input	O
,	O
defined	O
as	O
a	O
multi	O
-	O
layer	O
perceptron	O
.	O

,	O
w	O
i−1	O
,	O
w	O
i	O
)	O
+	O
f	O
(	O
w	O
i−c	O
,	O
.	O

Here	O
we	O
focus	O
on	O
the	O
statistical	O
language	O
modelling	O
component	O
,	O
which	O
has	O
a	O
pairwise	O
ranking	O
objective	O
to	O
maximise	O
the	O
relative	O
score	O
of	O
each	O
word	O
in	O
its	O
local	O
context	O
:	O
J	O
=	O
1	O
T	O
T	O
i=1	O
V	O
k=1	O
max	O
0	O
,	O
1	O
−	O
f	O
(	O
w	O
i−c	O
,	O
.	O

where	O
wi	O
=	O
n−1	O
j=1	O
C	O
j	O
w	O
i−j	O
is	O
the	O
context	O
embedding	O
,	O
C	O
j	O
is	O
a	O
scaling	O
matrix	O
,	O
and	O
b	O
*	O
is	O
a	O
bias	O
term	O
.	O

This	O
leads	O
to	O
the	O
following	O
training	O
objective	O
:	O
J	O
=	O
1	O
T	O
T	O
i=1	O
exp	O
(	O
w	O
i	O
w	O
i	O
+	O
b	O
i	O
)	O
V	O
k=1	O
exp	O
(	O
w	O
i	O
w	O
k	O
+	O
b	O
k	O
)	O
,	O
duty	O
,	O
denoting	O
either	O
the	O
embedding	O
for	O
the	O
ith	O
token	O
,	O
wi	O
,	O
or	O
kth	O
word	O
type	O
,	O
w	O
k	O
.	O

,	O
i	O
−	O
2	O
,	O
i	O
−	O
1	O
)	O
.	O

The	O
matrix	O
is	O
factorised	O
by	O
singular	O
value	O
decomposition	O
.	O

We	O
use	O
the	O
focus	O
word	O
vectors	O
,	O
W	O
=	O
{	O
w	O
k	O
}	O
V	O
k=1	O
,	O
normalised	O
such	O
that	O
each	O
w	O
k	O
=	O
1	O
.	O

(	O
2013a	O
)	O
)	O
predicts	O
a	O
word	O
from	O
its	O
context	O
using	O
a	O
model	O
with	O
the	O
objective	O
:	O
J	O
=	O
1	O
T	O
T	O
i=1	O
log	O
exp	O
w	O
i	O
j∈[−c,+c],j	O
=	O
0	O
wi+j	O
V	O
k=1	O
exp	O
w	O
k	O
j∈[−c,+c],j	O
=	O
0	O
wi+j	O
where	O
w	O
i	O
and	O
wi	O
are	O
the	O
vector	O
representations	O
for	O
the	O
ith	O
word	O
(	O
as	O
a	O
focus	O
or	O
context	O
word	O
,	O
respectively	O
)	O
,	O
V	O
is	O
the	O
vocabulary	O
size	O
,	O
T	O
is	O
the	O
number	O
of	O
tokens	O
in	O
the	O
corpus	O
,	O
and	O
c	O
is	O
the	O
context	O
window	O
size	O
.	O

(	O
2015a	O
)	O
.	O

Word	O
Embeddings	O
.	O

Dimensions	O
.	O

Name	O
.	O

(	O
2013c	O
)	O
)	O
,	O
but	O
also	O
including	O
morphosyntactic	O
and	O
morphosemantic	O
relations	O
(	O
see	O
§	O
3.2	O
)	O
.	O

To	O
this	O
end	O
,	O
we	O
construct	O
a	O
dataset	O
from	O
a	O
variety	O
of	O
sources	O
,	O
focusing	O
on	O
lexical	O
semantic	O
relations	O
(	O
which	O
are	O
less	O
well	O
represented	O
in	O
the	O
analogy	O
dataset	O
of	O
Mikolov	O
et	O
al	O
.	O

For	O
the	O
lexical	O
relations	O
,	O
we	O
want	O
a	O
range	O
of	O
relations	O
that	O
is	O
representative	O
of	O
the	O
types	O
of	O
relational	O
learning	O
tasks	O
targeted	O
in	O
the	O
literature	O
,	O
and	O
where	O
there	O
is	O
availability	O
of	O
annotated	O
data	O
.	O

(	O
2015	O
)	O
is	O
somewhat	O
more	O
constrained	O
than	O
the	O
set	O
we	O
use	O
,	O
there	O
is	O
a	O
good	O
deal	O
of	O
overlap	O
.	O

Although	O
the	O
set	O
of	O
relations	O
tested	O
by	O
Köper	O
et	O
al	O
.	O

They	O
test	O
a	O
variety	O
of	O
relations	O
including	O
word	O
similarity	O
,	O
antonyms	O
,	O
synonyms	O
,	O
hypernyms	O
,	O
and	O
meronyms	O
,	O
in	O
a	O
novel	O
analogy	O
task	O
.	O

(	O
2015	O
)	O
undertake	O
a	O
systematic	O
study	O
of	O
morphosyntactic	O
and	O
semantic	O
relations	O
on	O
word	O
embeddings	O
produced	O
with	O
word2vec	O
(	O
"	O
w2v	O
"	O
hereafter	O
;	O
see	O
§	O
3.1	O
)	O
for	O
English	O
and	O
German	O
.	O

Köper	O
et	O
al	O
.	O

Roller	O
and	O
Erk	O
(	O
2016	O
)	O
analyse	O
the	O
performance	O
of	O
vector	O
concatenation	O
and	O
difference	O
on	O
the	O
task	O
of	O
predicting	O
lexical	O
entailment	O
and	O
show	O
that	O
vector	O
concatenation	O
overwhelmingly	O
learns	O
to	O
detect	O
Hearst	O
patterns	O
(	O
e.g.	O
,	O
including	O
,	O
such	O
as	O
)	O
.	O

(	O
2015	O
)	O
train	O
a	O
classifier	O
on	O
word	O
pairs	O
,	O
using	O
word	O
embeddings	O
to	O
predict	O
coordinates	O
,	O
hypernyms	O
,	O
and	O
meronyms	O
.	O

Necs	O
¸ulescu	O
et	O
al	O
.	O

Makrai	O
et	O
al	O
.	O

However	O
,	O
there	O
has	O
been	O
no	O
systematic	O
investigation	O
of	O
the	O
range	O
of	O
relations	O
for	O
which	O
the	O
vector	O
difference	O
method	O
is	O
most	O
effective	O
,	O
although	O
there	O
have	O
been	O
some	O
smallerscale	O
investigations	O
in	O
this	O
direction	O
.	O

Another	O
strand	O
of	O
work	O
responding	O
to	O
the	O
vector	O
difference	O
approach	O
has	O
analysed	O
the	O
structure	O
of	O
predict	O
-	O
based	O
embedding	O
models	O
in	O
order	O
to	O
help	O
explain	O
their	O
success	O
on	O
the	O
analogy	O
and	O
other	O
tasks	O
(	O
Levy	O
and	O
Goldberg	O
,	O
2014a;Levy	O
and	O
Goldberg	O
,	O
2014b;Arora	O
et	O
al	O
.	O
,	O
2015	O
)	O
.	O

Fu	O
et	O
al	O
.	O

Kim	O
and	O
de	O
Marneffe	O
(	O
2013	O
)	O
use	O
word	O
embeddings	O
to	O
derive	O
representations	O
of	O
adjective	O
scales	O
,	O
e.g.	O
hot	O
-	O
warm	O
-	O
coolcold	O
.	O

(	O
2013	O
)	O
,	O
who	O
combine	O
a	O
neural	O
language	O
model	O
with	O
a	O
pattern	O
-	O
based	O
classifier	O
.	O

The	O
original	O
analogy	O
dataset	O
has	O
been	O
used	O
to	O
evaluate	O
predict	O
-	O
based	O
language	O
models	O
by	O
Mnih	O
and	O
Kavukcuoglu	O
(	O
2013	O
)	O
and	O
also	O
Zhila	O
et	O
al	O
.	O

An	O
exciting	O
development	O
,	O
and	O
the	O
inspiration	O
for	O
this	O
paper	O
,	O
has	O
been	O
the	O
demonstration	O
that	O
vector	O
difference	O
over	O
word	O
embeddings	O
(	O
Mikolov	O
et	O
al	O
.	O
,	O
2013c	O
)	O
can	O
be	O
used	O
to	O
model	O
word	O
analogy	O
tasks	O
.	O

Distributional	O
word	O
vectors	O
have	O
been	O
used	O
for	O
detection	O
of	O
relations	O
such	O
as	O
hypernymy	O
(	O
Geffet	O
and	O
Dagan	O
,	O
2005;Kotlerman	O
et	O
al	O
.	O
,	O
2010;Lenci	O
and	O
Benotto	O
,	O
2012;Weeds	O
et	O
al	O
.	O
,	O
2014;Rimell	O
,	O
2014;Santus	O
et	O
al	O
.	O
,	O
2014	O
)	O
and	O
qualia	O
structure	O
(	O
Yamada	O
et	O
al	O
.	O
,	O
2009	O
)	O
.	O

A	O
lexical	O
relation	O
is	O
a	O
binary	O
relation	O
r	O
holding	O
between	O
a	O
word	O
pair	O
(	O
w	O
i	O
,	O
w	O
j	O
)	O
;	O
for	O
example	O
,	O
the	O
pair	O
(	O
cart	O
,	O
wheel	O
)	O
stands	O
in	O
the	O
WHOLE	O
-	O
PART	O
relation	O
.	O

Background	O
and	O
Related	O
Work	O
.	O

(	O
2015a	O
)	O
.	O

We	O
find	O
that	O
this	O
improves	O
the	O
model	O
performance	O
substantially	O
.	O

We	O
then	O
investigate	O
methods	O
for	O
better	O
attuning	O
the	O
learned	O
class	O
representation	O
to	O
the	O
lexical	O
relations	O
,	O
focusing	O
on	O
methods	O
for	O
automatically	O
synthesising	O
negative	O
instances	O
.	O

When	O
we	O
move	O
to	O
an	O
open	O
-	O
world	O
setting	O
including	O
random	O
word	O
pairs	O
-many	O
of	O
which	O
do	O
not	O
correspond	O
to	O
any	O
lexical	O
relation	O
in	O
the	O
training	O
data	O
-the	O
results	O
are	O
poor	O
.	O

We	O
find	O
that	O
the	O
clustering	O
works	O
remarkably	O
well	O
,	O
although	O
syntactic	O
relations	O
are	O
captured	O
better	O
than	O
semantic	O
ones	O
.	O

In	O
this	O
paper	O
,	O
we	O
introduce	O
a	O
new	O
,	O
larger	O
dataset	O
covering	O
many	O
well	O
-	O
known	O
lexical	O
relation	O
types	O
from	O
the	O
linguistics	O
and	O
cognitive	O
science	O
literature	O
.	O

Previous	O
analogy	O
completion	O
tasks	O
used	O
with	O
word	O
embeddings	O
have	O
limited	O
coverage	O
of	O
lexical	O
relation	O
types	O
.	O

For	O
example	O
,	O
the	O
paris	O
−	O
france	O
vector	O
appears	O
to	O
encode	O
CAPITAL	O
-	O
OF	O
,	O
presumably	O
by	O
cancelling	O
out	O
the	O
features	O
of	O
paris	O
that	O
are	O
France	O
-	O
specific	O
,	O
and	O
retaining	O
the	O
features	O
that	O
distinguish	O
a	O
capital	O
city	O
(	O
Levy	O
and	O
Goldberg	O
,	O
2014a	O
)	O
.	O

The	O
key	O
operation	O
in	O
these	O
models	O
is	O
vector	O
difference	O
,	O
or	O
vector	O
offset	O
.	O

Remarkably	O
,	O
since	O
the	O
model	O
is	O
not	O
trained	O
for	O
this	O
task	O
,	O
the	O
relational	O
structure	O
of	O
the	O
vector	O
space	O
appears	O
to	O
be	O
an	O
emergent	O
property	O
.	O

The	O
results	O
extend	O
to	O
several	O
semantic	O
relations	O
such	O
as	O
CAPITAL	O
-	O
OF	O
(	O
paris−france+poland	O
≈	O
warsaw	O
)	O
and	O
morphosyntactic	O
relations	O
such	O
as	O
PLURALISATION	O
(	O
cars	O
−	O
car	O
+	O
apple	O
≈	O
apples	O
)	O
.	O

A	O
well	O
-	O
known	O
example	O
involves	O
predicting	O
the	O
vector	O
queen	O
from	O
the	O
vector	O
combination	O
king	O
−	O
man	O
+	O
woman	O
,	O
where	O
linear	O
operations	O
on	O
word	O
vectors	O
appear	O
to	O
capture	O
the	O
lexical	O
relation	O
governing	O
the	O
analogy	O
,	O
in	O
this	O
case	O
OPPOSITE	O
-	O
GENDER	O
.	O

(	O
2013a	O
)	O
and	O
other	O
similar	O
language	O
models	O
have	O
been	O
shown	O
to	O
perform	O
well	O
on	O
an	O
analogy	O
completion	O
task	O
(	O
Mikolov	O
et	O
al	O
.	O
,	O
2013b;Mikolov	O
et	O
al	O
.	O
,	O
2013c;Levy	O
and	O
Goldberg	O
,	O
2014a	O
)	O
,	O
in	O
the	O
space	O
of	O
relational	O
sim	O
-	O
ilarity	O
prediction	O
(	O
Turney	O
,	O
2006	O
)	O
,	O
where	O
the	O
task	O
is	O
to	O
predict	O
the	O
missing	O
word	O
in	O
analogies	O
such	O
as	O
A	O
:	O
B	O
:	O
:	O
C	O
:	O
-?-	O
.	O

The	O
skipgram	O
model	O
of	O
Mikolov	O
et	O
al	O
.	O

Introduction	O
.	O

We	O
find	O
that	O
word	O
embeddings	O
capture	O
a	O
surprising	O
amount	O
of	O
information	O
,	O
and	O
that	O
,	O
under	O
suitable	O
supervised	O
training	O
,	O
vector	O
subtraction	O
generalises	O
well	O
to	O
a	O
broad	O
range	O
of	O
relations	O
,	O
including	O
over	O
unseen	O
lexical	O
items	O
.	O

In	O
this	O
paper	O
,	O
we	O
carry	O
out	O
such	O
an	O
evaluation	O
in	O
two	O
learning	O
settings	O
:	O
(	O
1	O
)	O
spectral	O
clustering	O
to	O
induce	O
word	O
relations	O
,	O
and	O
(	O
2	O
)	O
supervised	O
learning	O
to	O
classify	O
vector	O
differences	O
into	O
relation	O
types	O
.	O

Prior	O
work	O
has	O
evaluated	O
this	O
intriguing	O
result	O
using	O
a	O
word	O
analogy	O
prediction	O
formulation	O
and	O
hand	O
-	O
selected	O
relations	O
,	O
but	O
the	O
generality	O
of	O
the	O
finding	O
over	O
a	O
broader	O
range	O
of	O
lexical	O
relation	O
types	O
and	O
different	O
learning	O
settings	O
has	O
not	O
been	O
evaluated	O
.	O

Recent	O
work	O
has	O
shown	O
that	O
simple	O
vector	O
subtraction	O
over	O
word	O
embeddings	O
is	O
surprisingly	O
effective	O
at	O
capturing	O
different	O
lexical	O
relations	O
,	O
despite	O
lacking	O
explicit	O
supervision	O
.	O

