In	O
summary	O
,	O
we	O
can	O
report	O
that	O
ensemble	B-MethodName
methods	I-MethodName
turned	O
out	O
to	O
be	O
fruitful	O
when	O
applied	O
to	O
CompLex	B-DatasetName
.	O

Our	O
systems	O
ware	O
ranked	B-MetricName
15/54	B-MetricValue
and	O
19/37	B-MetricValue
during	O
shared	O
task	O
evaluations	O
according	O
to	O
Pearson	B-MetricName
's	I-MetricName
correlation	I-MetricName
coefficient	I-MetricName
.	O

We	O
presented	O
both	O
our	O
systems	O
submitted	O
to	O
SemEval-2021	O
Task	O
1	O
combining	O
a	O
heterogeneous	B-MethodName
feature	I-MethodName
set	I-MethodName
with	I-MethodName
gradient	I-MethodName
boosting	I-MethodName
as	O
regression	O
algorithm	O
.	O

In	O
order	O
to	O
compare	O
the	O
predicted	O
values	O
of	O
our	O
models	O
to	O
the	O
ground	O
truth	O
data	O
,	O
we	O
scatterplotted	O
the	O
relationship	O
between	O
ground	O
truth	O
labels	O
and	O
the	O
scores	O
predicted	O
by	O
our	O
systems	O
(	O
see	O
Figures	O
1	O
and	O
2	O
)	O
using	O
the	O
CompLex	B-DatasetName
evaluation	O
set	O
.	O

This	O
could	O
be	O
explained	O
by	O
the	O
fact	O
that	O
most	O
multiword	O
expressions	O
within	O
the	O
CompLex	B-DatasetName
corpus	O
follow	O
the	O
structure	O
of	O
a	O
semantic	O
head	O
in	O
combination	O
with	O
a	O
modifier	O
as	O
most	O
of	O
them	O
are	O
either	O
multi	O
token	O
compounds	O
or	O
single	O
token	O
nouns	O
modified	O
by	O
adjectives	O
.	O

In	O
the	O
case	O
of	O
our	O
model	O
dealing	O
with	O
multiword	O
expressions	O
,	O
the	O
ten	O
most	O
important	O
features	O
all	O
stem	O
from	O
the	O
flair	B-MethodName
-	I-MethodName
mix	I-MethodName
-	I-MethodName
backward	I-MethodName
embedding	O
of	O
the	O
second	O
word	O
.	O

A	O
few	O
single	O
dimension	O
from	O
the	O
embeddings	O
provided	O
by	O
flair	B-MethodName
-	I-MethodName
mix	I-MethodName
-	I-MethodName
backward	I-MethodName
seem	O
to	O
play	O
the	O
major	O
role	O
here	O
.	O

Within	O
this	O
category	O
,	O
the	O
most	O
dominant	O
features	O
for	O
both	O
models	O
came	O
from	O
the	O
flair	B-MethodName
-	I-MethodName
mix	I-MethodName
-	I-MethodName
backward	I-MethodName
and	O
flair	B-MethodName
-	I-MethodName
mix	I-MethodName
-	I-MethodName
forward	I-MethodName
models	O
(	O
see	O
Tables	O
3	O
and	O
4	O
)	O
.	O

Inspecting	O
the	O
results	O
of	O
these	O
calculations	O
,	O
we	O
noticed	O
that	O
our	O
systems	O
did	O
not	O
use	O
the	O
character	O
bigram	O
frequencies	O
derived	O
from	O
the	O
Google	B-DatasetName
Books	I-DatasetName
Corpus	I-DatasetName
,	O
nor	O
the	O
frequencies	O
from	O
EFLLex	B-DatasetName
or	O
the	O
word	O
list	O
inclusion	O
features	O
.	O

Feature	O
importance	O
was	O
calculated	O
using	O
the	O
evaluation	O
set	O
of	O
CompLex	B-DatasetName
.	O

To	O
determine	O
which	O
features	O
were	O
used	O
by	O
our	O
models	O
to	O
predict	O
lexical	O
complexity	O
,	O
we	O
rely	O
on	O
the	O
functionality	O
provided	O
by	O
CatBoost	B-MethodName
which	O
scores	O
each	O
feature	O
for	O
its	O
influence	O
on	O
a	O
given	O
final	O
prediction	O
.	O

According	O
to	O
this	O
,	O
our	O
systems	O
achieved	O
the	O
15th	B-MetricValue
and	O
19th	B-MetricValue
rank	B-MetricName
respectively	O
.	O

Throughout	O
the	O
shared	O
task	O
,	O
the	O
systems	O
were	O
evaluated	O
with	O
regard	O
to	O
Pearson	B-MetricName
's	I-MetricName
correlation	I-MetricName
coefficient	I-MetricName
,	O
Spearman	B-MetricName
's	I-MetricName
rank	I-MetricName
correlation	I-MetricName
coefficient	I-MetricName
,	O
mean	B-MetricName
average	I-MetricName
error	I-MetricName
,	O
mean	B-MetricName
squared	I-MetricName
error	I-MetricName
and	O
R2	B-MetricName
with	O
Pearson	B-MetricName
's	I-MetricName
correlation	I-MetricName
coefficient	I-MetricName
determining	O
the	O
main	O
ranking	O
.	O

The	O
other	O
one	O
was	O
the	O
Academic	B-DatasetName
Word	I-DatasetName
List	I-DatasetName
as	O
described	O
in	O
Coxhead	B-DatasetName
(	O
2011	O
)	O
,	O
a	O
structured	O
lexicon	O
of	O
terms	O
used	O
primarily	O
in	O
academic	O
discourse	O
which	O
we	O
believed	O
to	O
contain	O
more	O
complex	O
words	O
.	O

Here	O
,	O
our	O
idea	O
was	O
that	O
this	O
could	O
help	O
to	O
identify	O
simple	O
words	O
within	O
CompLex	B-DatasetName
.	O

The	O
first	O
one	O
is	O
Ogden	B-DatasetName
's	I-DatasetName
Basic	I-DatasetName
English	I-DatasetName
Vocabulary	I-DatasetName
10	O
,	O
a	O
list	O
of	O
simple	O
words	O
used	O
for	O
writing	O
simple	O
English	O
as	O
described	O
in	O
Ogden	O
(	O
1932	O
)	O
.	O

We	O
included	O
this	O
set	O
as	O
we	O
deemed	O
that	O
CEFR	B-MethodName
as	O
a	O
framework	O
for	O
rating	O
language	O
competence	O
could	O
also	O
function	O
as	O
an	O
according	O
proxy	O
.	O

The	O
third	O
set	O
we	O
used	O
was	O
EFLLex	B-DatasetName
(	O
Dürlich	O
and	O
Franc	O
¸ois	O
,	O
2018	O
)	O
which	O
lists	O
the	O
frequencies	O
of	O
words	O
within	O
several	O
pieces	O
of	O
English	O
literature	O
appropriate	O
for	O
different	O
CEFR	O
9	O
levels	O
.	O

Besides	O
SUBTLEXus	B-DatasetName
,	O
we	O
utilised	O
the	O
character	O
bigram	O
frequencies	O
from	O
Norvig	B-DatasetName
(	O
2013	O
)	O
which	O
were	O
extracted	O
from	O
the	O
Google	B-DatasetName
Books	I-DatasetName
Corpus	I-DatasetName
.	O

The	O
first	O
of	O
these	O
data	O
sets	O
was	O
the	O
frequency	O
list	O
extracted	O
from	O
the	O
SUB	B-DatasetName
-	I-DatasetName
TLEXus	I-DatasetName
corpus	I-DatasetName
(	O
Brysbaert	O
and	O
New	O
,	O
2009	O
)	O
consisting	O
of	O
various	O
movie	O
subtitles	O
from	O
which	O
we	O
used	O
the	O
log	O
-	O
normalised	O
term	O
frequency	O
and	O
the	O
log	O
-	O
normalised	O
document	O
frequency	O
as	O
features	O
.	O

As	O
we	O
switched	O
to	O
using	O
gradient	B-MethodName
boosting	I-MethodName
for	O
our	O
final	O
systems	O
,	O
we	O
decided	O
to	O
use	O
the	O
fine	O
-	O
tuned	O
variants	O
of	O
the	O
transformer	O
embedding	O
models	O
as	O
using	O
them	O
led	O
to	O
small	O
improvements	O
when	O
testing	O
our	O
models	O
on	O
the	O
shared	O
task	O
trial	O
sets	O
compared	O
to	O
using	O
the	O
non	O
-	O
fine	O
-	O
tuned	O
variants	O
.	O

While	O
we	O
deemed	O
this	O
an	O
okay	O
result	O
,	O
we	O
decided	O
to	O
stick	O
with	O
gradient	B-MethodName
boosting	I-MethodName
for	O
our	O
final	O
systems	O
as	O
early	O
experiments	O
with	O
this	O
algorithm	O
yielded	O
results	O
superior	O
to	O
the	O
purely	O
neural	O
approach	O
when	O
evaluated	O
on	O
the	O
same	O
set	O
.	O

This	O
model	O
achieved	O
a	O
Pearson	B-MetricName
's	I-MetricName
correlation	I-MetricName
coefficient	I-MetricName
score	O
of	O
0.7103	B-MetricValue
when	O
evaluated	O
on	O
the	O
trial	O
set	O
.	O

This	O
network	O
was	O
then	O
trained	O
for	O
5	B-HyperparameterValue
epochs	B-HyperparameterName
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.000001	B-HyperparameterValue
,	O
mean	B-HyperparameterValue
squared	I-HyperparameterValue
error	I-HyperparameterValue
as	O
loss	B-HyperparameterName
function	I-HyperparameterName
and	O
Adam	B-HyperparameterValue
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
as	O
optimizer	B-HyperparameterName
on	O
the	O
training	O
set	O
part	O
of	O
CompLex	B-DatasetName
.	O

This	O
collection	O
of	O
embeddings	O
was	O
derived	O
from	O
previous	O
experiments	O
on	O
the	O
CompLex	B-DatasetName
corpus	O
where	O
we	O
tried	O
to	O
fine	O
-	O
tune	O
a	O
purely	O
neural	O
model	O
using	O
the	O
approach	O
of	O
stacking	O
different	O
embedding	O
models	O
in	O
combination	O
with	O
an	O
attached	O
prediction	O
head	O
central	O
to	O
flairNLP	O
8	O
(	O
Akbik	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

This	O
includes	O
the	O
transformer	O
-	O
based	O
(	O
Devlin	O
et	O
al	O
.	O
,	O
2019	O
)	O
BiomedNLP	B-MethodName
-	I-MethodName
PubMedBERTbase	I-MethodName
-	I-MethodName
uncased	I-MethodName
-	I-MethodName
abstract	I-MethodName
(	O
Gu	O
et	O
al	O
.	O
,	O
2020	O
)	O
,	O
distilgpt2	B-MethodName
4	O
(	O
Radford	O
et	O
al	O
.	O
,	O
2018	O
)	O
and	O
distilbert	B-MethodName
-	I-MethodName
base	I-MethodName
-	I-MethodName
uncased	I-MethodName
(	O
Sanh	O
et	O
al	O
.	O
,	O
2019	O
)	O
,	O
the	O
contextual	O
string	O
embed	O
-	O
ding	O
models	O
mix	B-MethodName
-	I-MethodName
forward	I-MethodName
and	O
mix	B-MethodName
-	I-MethodName
backward	I-MethodName
5	O
(	O
Akbik	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
and	O
the	O
static	O
GloVe	B-MethodName
6	O
(	O
Pennington	O
et	O
al	O
.	O
,	O
2014	O
)	O
and	O
English	O
fastText	B-MethodName
7	O
(	O
Bojanowski	O
et	O
al	O
.	O
,	O
2017	O
)	O
embeddings	O
.	O

All	O
of	O
these	O
features	O
are	O
encoded	O
as	O
one-	O
,	O
respectively	O
n	O
-	O
hot	O
vectors	O
using	O
the	O
LabelBinarizer	B-MethodName
and	O
Mul	B-MethodName
-	I-MethodName
tiLabelBinarizer	I-MethodName
classes	O
provided	O
by	O
Scikit	O
-	O
learn	O
(	O
Pedregosa	O
et	O
al	O
.	O
,	O
2011	O
)	O
.	O

Syntactic	O
features	O
:	O
This	O
category	O
of	O
features	O
includes	O
XPOS-	O
,	O
UPOS-	O
,	O
dependencyand	O
named	O
entity	O
tags	O
as	O
well	O
as	O
universal	O
features	O
2	O
inferred	O
using	O
the	O
English	B-MethodName
Stanza	I-MethodName
3	I-MethodName
(	O
Qi	O
et	O
al	O
.	O
,	O
2020	O
)	O
model	O
fit	O
to	O
the	O
version	O
of	O
the	O
English	O
Web	O
Treebank	O
following	O
the	O
Universal	O
Dependencies	O
formalism	O
(	O
Silveira	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

While	O
the	O
problem	O
presented	O
in	O
their	O
paper	O
is	O
formulated	O
as	O
a	O
binary	O
classification	O
task	O
using	O
different	O
data	O
sets	O
,	O
we	O
wanted	O
to	O
test	O
if	O
their	O
findings	O
would	O
still	O
translate	O
to	O
a	O
regression	O
task	O
on	O
CompLex	B-DatasetName
.	O

(	O
2018	O
)	O
that	O
ensemble	B-MethodName
-	I-MethodName
based	I-MethodName
learners	I-MethodName
perform	O
best	O
for	O
complex	O
word	O
identification	O
contributed	O
to	O
this	O
decision	O
,	O
as	O
well	O
.	O

Additionally	O
,	O
we	O
set	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
maximum	I-HyperparameterName
iterations	I-HyperparameterName
to	O
5000	B-HyperparameterValue
and	O
then	O
used	O
the	O
trial	O
set	O
to	O
perform	O
early	O
stopping	O
during	O
training	O
in	O
order	O
to	O
determine	O
the	O
exact	O
number	O
of	O
required	O
iterations	O
.	O

We	O
set	O
the	O
growing	B-HyperparameterName
policy	I-HyperparameterName
to	O
lossguide	B-HyperparameterValue
,	O
the	O
L2	B-HyperparameterName
leaf	I-HyperparameterName
regularisation	I-HyperparameterName
to	O
15	B-HyperparameterValue
,	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
to	I-HyperparameterName
0.01	B-HyperparameterValue
,	O
tree	B-HyperparameterName
depth	I-HyperparameterName
to	O
6	B-HyperparameterValue
and	O
the	O
maximum	B-HyperparameterName
number	I-HyperparameterName
of	I-HyperparameterName
leaves	I-HyperparameterName
to	O
15	B-HyperparameterValue
.	O

Our	O
models	O
are	O
based	O
on	O
the	O
implementation	O
of	O
gradient	O
boosting	O
provided	O
by	O
CatBoost	B-MethodName
1	O
(	O
Dorogush	O
et	O
al	O
.	O
,	O
2018;Prokhorenkova	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

We	O
trained	O
one	O
model	O
to	O
predict	B-TaskName
single	I-TaskName
word	I-TaskName
lexical	I-TaskName
complexity	I-TaskName
scores	O
and	O
another	O
one	O
to	O
predict	B-TaskName
bigram	I-TaskName
multiword	I-TaskName
expression	I-TaskName
complexity	I-TaskName
scores	O
.	O

Our	O
systems	O
rely	O
on	O
gradient	B-MethodName
-	I-MethodName
boosted	I-MethodName
regression	I-MethodName
tree	I-MethodName
ensembles	I-MethodName
(	O
Mason	O
et	O
al	O
.	O
,	O
1999	O
)	O
for	O
predicting	B-TaskName
lexical	I-TaskName
complexity	I-TaskName
scores	O
.	O

This	O
shared	O
task	O
focused	O
on	O
predicting	B-TaskName
lexical	I-TaskName
complexity	I-TaskName
for	O
English	O
,	O
German	O
,	O
Spanish	O
and	O
a	O
multi	O
-	O
lingual	O
data	O
set	O
with	O
a	O
French	O
test	O
set	O
.	O

The	O
most	O
important	O
findings	O
for	O
this	O
shared	O
task	O
were	O
that	O
ensemble	B-MethodName
methods	I-MethodName
performed	O
best	O
in	O
predicting	B-TaskName
lexical	I-TaskName
complexity	I-TaskName
with	O
word	O
frequency	O
being	O
the	O
best	O
indicator	O
.	O

CompLex	B-DatasetName
is	O
divided	O
into	O
two	O
sub	O
-	O
corpora	O
,	O
one	O
dealing	O
with	O
the	O
complexity	O
of	O
single	O
words	O
and	O
the	O
other	O
one	O
with	O
the	O
complexity	O
of	O
bigram	O
multiword	O
expressions	O
.	O

For	O
the	O
shared	O
task	O
,	O
CompLex	B-DatasetName
corpus	I-DatasetName
(	O
Shardlow	O
et	O
al	O
.	O
,	O
2020(Shardlow	O
et	O
al	O
.	O
,	O
,	O
2021b	O
)	O
)	O
was	O
used	O
as	O
data	O
set	O
.	O

The	O
code	O
and	O
our	O
full	O
results	O
can	O
be	O
found	O
at	O
https://github.com/SGombert/	O
tudacclsemeval	O
.	O

Out	O
of	O
all	O
participants	O
,	O
our	O
systems	O
were	O
ranked	B-MetricName
15/54	B-MetricValue
in	O
the	O
single	O
word	O
-	O
and	O
19/37	B-MetricValue
in	O
the	O
multiword	O
category	O
during	O
the	O
official	O
shared	O
task	O
evaluations	O
according	O
to	O
Pearson	B-MetricName
's	I-MetricName
correlation	I-MetricName
coefficient	I-MetricName
.	O

We	O
assumed	O
that	O
lexical	O
complexity	O
could	O
be	O
correlated	O
with	O
a	O
wide	O
range	O
of	O
features	O
,	O
neural	O
ones	O
as	O
much	O
as	O
distributional	O
or	O
psycholinguistic	O
ones	O
,	O
which	O
is	O
why	O
we	O
chose	O
to	O
use	O
an	O
ensemble	B-MethodName
-	I-MethodName
based	I-MethodName
method	O
in	O
the	O
form	O
of	O
gradient	B-MethodName
boosting	I-MethodName
(	O
Mason	O
et	O
al	O
.	O
,	O
1999	O
)	O
for	O
our	O
system	O
as	O
it	O
usually	O
performs	O
best	O
for	O
tasks	O
where	O
such	O
a	O
feature	O
set	O
is	O
needed	O
compared	O
to	O
solely	O
neural	O
models	O
which	O
need	O
dense	O
,	O
homogeneous	O
input	O
data	O
to	O
perform	O
well	O
.	O

Our	O
approach	O
to	O
solve	O
this	O
problem	O
relies	O
on	O
gradient	B-MethodName
-	I-MethodName
boosted	I-MethodName
regression	I-MethodName
tree	I-MethodName
ensembles	I-MethodName
which	O
we	O
fit	O
on	O
a	O
heterogeneous	O
feature	O
set	O
including	O
different	O
word	O
embedding	O
models	O
,	O
linguistic	O
features	O
,	O
WordNet	O
features	O
,	O
psycholinguistic	O
lexica	O
,	O
corpus	O
-	O
based	O
word	O
frequencies	O
and	O
word	O
lists	O
.	O

The	O
term	O
lexical	B-TaskName
complexity	I-TaskName
prediction	I-TaskName
describes	O
the	O
task	O
of	O
assigning	O
a	O
word	O
or	O
multiword	O
expression	O
a	O
continuous	O
or	O
discrete	O
score	O
signifying	O
its	O
likeliness	O
of	O
being	O
understood	O
well	O
within	O
a	O
given	O
context	O
,	O
especially	O
by	O
a	O
non	O
-	O
native	O
speaker	O
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
our	O
contribution	O
to	O
SemEval-2021	O
Shared	O
Task	O
1	O
(	O
Shardlow	O
et	O
al	O
.	O
,	O
2021a	O
)	O
,	O
a	O
shared	O
task	O
focused	O
on	O
the	O
topic	O
of	O
lexical	B-TaskName
complexity	I-TaskName
prediction	I-TaskName
.	O

We	O
can	O
show	O
that	O
especially	O
contextualised	O
string	O
embeddings	O
(	O
Akbik	O
et	O
al	O
.	O
,	O
2018	O
)	O
can	O
help	O
with	O
predicting	B-TaskName
lexical	I-TaskName
complexity	I-TaskName
.	O

Our	O
approach	O
relies	O
on	O
gradient	B-MethodName
boosted	I-MethodName
regression	I-MethodName
tree	I-MethodName
ensembles	I-MethodName
fitted	O
using	O
a	O
heterogeneous	O
feature	O
set	O
combining	O
linguistic	O
features	O
,	O
static	O
and	O
contextualized	O
word	O
embeddings	O
,	O
psycholinguistic	O
norm	O
lexica	O
,	O
WordNet	O
,	O
word	O
-	O
and	O
character	O
bigram	O
frequencies	O
and	O
inclusion	O
in	O
word	O
lists	O
to	O
create	O
a	O
model	O
able	O
to	O
assign	O
a	O
word	O
or	O
multiword	O
expression	O
a	O
context	O
-	O
dependent	O
complexity	B-MetricName
score	O
.	O

The	O
aim	O
of	O
this	O
shared	O
task	O
was	O
to	O
create	O
systems	O
able	O
to	O
predict	B-TaskName
the	I-TaskName
lexical	I-TaskName
complexity	I-TaskName
of	O
word	O
tokens	O
and	O
bigram	O
multiword	O
expressions	O
within	O
a	O
given	O
sentence	O
context	O
,	O
a	O
continuous	O
value	O
indicating	O
the	O
difficulty	O
in	O
understanding	O
a	O
respective	O
utterance	O
.	O

TUDA	O
-	O
CCL	O
at	O
SemEval-2021	O
Task	O
1	O
:	O
Using	O
Gradient	B-MethodName
-	I-MethodName
boosted	I-MethodName
Regression	I-MethodName
Tree	I-MethodName
Ensembles	I-MethodName
Trained	O
on	O
a	O
Heterogeneous	O
Feature	O
Set	O
for	O
Predicting	B-TaskName
Lexical	I-TaskName
Complexity	I-TaskName
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
our	O
systems	O
submitted	O
to	O
SemEval-2021	O
Task	O
1	O
on	O
lexical	B-TaskName
complexity	I-TaskName
prediction	I-TaskName
(	O
Shardlow	O
et	O
al	O
.	O
,	O
2021a	O
)	O
.	O

Moreover	O
,	O
our	O
systems	O
rarely	O
assign	O
scores	O
below	O
0.2	O
.	O
It	O
must	O
be	O
explored	O
further	O
if	O
there	O
are	O
features	O
which	O
could	O
improve	O
our	O
systems	O
in	O
this	O
respect	O
.	O

We	O
attribute	O
this	O
to	O
a	O
relationship	O
between	O
lexical	O
complexity	O
and	O
the	O
distribution	O
of	O
characters	O
throughout	O
words	O
and	O
sentences	O
,	O
but	O
this	O
needs	O
further	O
clarification	O
which	O
could	O
be	O
the	O
objective	O
of	O
future	O
work	O
.	O

The	O
type	O
of	O
feature	O
playing	O
the	O
most	O
important	O
role	O
for	O
our	O
models	O
are	O
contextual	O
string	O
embeddings	O
as	O
they	O
influenced	O
the	O
outcome	O
the	O
most	O
.	O

However	O
,	O
the	O
results	O
achieved	O
by	O
our	O
systems	O
were	O
still	O
close	O
to	O
the	O
best	O
results	O
,	O
especially	O
in	O
the	O
case	O
of	O
the	O
system	O
dealing	O
with	O
single	O
word	O
complexity	O
.	O

Conclusion	O
.	O

This	O
indicates	O
that	O
our	O
feature	O
set	O
does	O
not	O
contain	O
features	O
which	O
could	O
help	O
our	O
models	O
to	O
identify	O
very	O
simple	O
words	O
.	O

The	O
system	O
dealing	O
with	O
multiword	O
expressions	O
does	O
not	O
assign	O
any	O
value	O
below	O
0.2	O
at	O
all	O
and	O
the	O
one	O
dealing	O
with	O
single	O
word	O
complexity	O
rarely	O
does	O
so	O
.	O

It	O
can	O
be	O
observed	O
that	O
both	O
systems	O
,	O
especially	O
the	O
one	O
dealing	O
with	O
single	O
word	O
complexity	O
,	O
show	O
the	O
tendency	O
to	O
assign	O
slightly	O
higher	O
scores	O
than	O
given	O
in	O
the	O
ground	O
truth	O
for	O
simple	O
words	O
and	O
slightly	O
lower	O
scores	O
for	O
complex	O
words	O
.	O

Ground	O
Truth	O
.	O

Predictions	O
vs.	O

However	O
,	O
without	O
further	O
research	O
,	O
this	O
currently	O
remains	O
pure	O
speculation	O
.	O

This	O
links	O
each	O
input	O
dimension	O
also	O
to	O
a	O
larger	O
variety	O
of	O
latently	O
encoded	O
distributional	O
knowledge	O
which	O
could	O
then	O
contain	O
certain	O
regularities	O
strongly	O
correlated	O
with	O
lexical	O
complexity	O
.	O

As	O
a	O
consequence	O
,	O
such	O
models	O
use	O
fewer	O
input	O
dimensions	O
and	O
each	O
of	O
the	O
dimensions	O
present	O
is	O
in	O
turn	O
involved	O
in	O
the	O
encoding	O
of	O
more	O
different	O
words	O
.	O

While	O
the	O
exact	O
reason	O
for	O
the	O
strong	O
influence	O
of	O
the	O
contextualised	O
string	O
embeddings	O
is	O
hard	O
to	O
determine	O
due	O
to	O
the	O
fact	O
that	O
embeddings	O
lack	O
the	O
property	O
of	O
being	O
easily	O
interpretable	O
,	O
we	O
assume	O
that	O
the	O
dominant	O
role	O
they	O
play	O
for	O
the	O
results	O
could	O
be	O
determined	O
by	O
them	O
being	O
calculated	O
on	O
the	O
character	O
level	O
(	O
Akbik	O
et	O
al	O
.	O
,	O
2018	O
)	O
instead	O
of	O
the	O
level	O
of	O
fixed	O
words	O
or	O
subword	O
units	O
such	O
as	O
morphemes	O
.	O

Each	O
entry	O
refers	O
to	O
a	O
single	O
dimension	O
of	O
the	O
feature	O
vector	O
.	O

Table	O
4	O
:	O
The	O
10	O
most	O
important	O
features	O
observed	O
for	O
our	O
system	O
dealing	O
with	O
multiword	O
expression	O
complexity	O
and	O
their	O
categories	O
.	O

It	O
is	O
intuitive	O
from	O
a	O
linguistic	O
point	O
of	O
view	O
that	O
in	O
such	O
cases	O
,	O
the	O
semantic	O
head	O
,	O
which	O
comes	O
as	O
second	O
element	O
,	O
should	O
play	O
the	O
dominant	O
semantic	O
role	O
resulting	O
in	O
it	O
being	O
more	O
influential	O
in	O
the	O
overall	O
results	O
.	O

While	O
features	O
from	O
all	O
other	O
categories	O
were	O
utilised	O
,	O
the	O
most	O
dominant	O
features	O
by	O
far	O
are	O
contained	O
in	O
the	O
word	O
embedding	O
category	O
.	O

The	O
outputs	O
of	O
this	O
method	O
are	O
normalised	O
so	O
that	O
the	O
sum	O
of	O
the	O
importance	O
values	O
of	O
all	O
features	O
equals	O
100	O
.	O

This	O
is	O
achieved	O
by	O
changing	O
a	O
respective	O
feature	O
values	O
and	O
observing	O
the	O
resulting	O
change	O
in	O
the	O
model	O
prediction	O
(	O
see	O
11	O
for	O
further	O
information	O
on	O
the	O
exact	O
method	O
)	O
.	O

Most	O
Important	O
Features	O
.	O

(	O
2021a	O
)	O
.	O

The	O
full	O
results	O
for	O
all	O
submitted	O
systems	O
are	O
presented	O
in	O
Shardlow	O
et	O
al	O
.	O

Further	O
hyperparameter	O
tuning	O
and	O
the	O
addition	O
of	O
more	O
features	O
could	O
likely	O
close	O
this	O
gap	O
.	O

The	O
results	O
show	O
that	O
our	O
systems	O
,	O
while	O
only	O
achieving	O
upper	O
mid	O
-	O
table	O
results	O
on	O
average	O
,	O
come	O
close	O
to	O
the	O
best	O
systems	O
performance	O
-	O
wise	O
which	O
speaks	O
for	O
our	O
approach	O
.	O

Table	O
1	O
shows	O
the	O
results	O
achieved	O
by	O
our	O
system	O
dealing	O
with	O
single	O
words	O
and	O
Table	O
2	O
the	O
results	O
achieved	O
by	O
our	O
system	O
dealing	O
with	O
multiword	O
expressions	O
.	O

Results	O
.	O

Metric	O
.	O

In	O
both	O
cases	O
,	O
we	O
encoded	O
the	O
inclusion	O
of	O
a	O
word	O
within	O
a	O
respective	O
word	O
list	O
binarily	O
.	O

Word	O
Lists	O
:	O
We	O
used	O
two	O
different	O
word	O
lists	O
as	O
features	O
.	O

In	O
the	O
case	O
of	O
both	O
sets	O
,	O
our	O
intuition	O
was	O
that	O
lower	O
frequency	O
would	O
likely	O
function	O
as	O
a	O
proxy	O
for	O
complexity	O
.	O

Here	O
,	O
to	O
represent	O
a	O
word	O
,	O
we	O
calculated	O
the	O
mean	O
of	O
all	O
frequencies	O
of	O
the	O
bigrams	O
consituting	O
the	O
same	O
and	O
used	O
this	O
as	O
feature	O
.	O

Word	O
frequencies	O
:	O
We	O
utilised	O
three	O
resources	O
containing	O
corpus	O
-	O
based	O
word	O
respectively	O
character	O
bigram	O
frequencies	O
.	O

In	O
both	O
cases	O
,	O
the	O
inclusion	O
of	O
these	O
features	O
was	O
mainly	O
motivated	O
by	O
our	O
general	O
intuition	O
that	O
the	O
perceived	O
complexity	O
of	O
words	O
could	O
be	O
linked	O
to	O
different	O
psycholinguistic	O
variables	O
.	O

The	O
ratings	O
within	O
this	O
lexicon	O
were	O
derived	O
algorithmically	O
from	O
smaller	O
lexicons	O
using	O
linear	O
combinations	O
and	O
semantic	O
similarity	O
scores	O
to	O
approximate	O
the	O
ratings	O
for	O
words	O
not	O
included	O
in	O
the	O
source	O
lexica	O
.	O

The	O
second	O
lexicon	O
is	O
described	O
in	O
Malandrakis	O
and	O
Narayanan	O
(	O
2015	O
)	O
and	O
includes	O
ratings	O
for	O
arousal	O
,	O
dominance	O
,	O
valence	O
,	O
pleasantness	O
,	O
concreteness	O
,	O
imagability	O
,	O
age	O
of	O
acquisition	O
,	O
familarity	O
,	O
pronouncability	O
,	O
context	O
availability	O
and	O
gender	O
ladenness	O
.	O

These	O
ratings	O
were	O
acquired	O
from	O
annotators	O
on	O
the	O
Amazon	O
Mechanical	O
Turk	O
platform	O
.	O

(	O
2013	O
)	O
and	O
scores	O
words	O
with	O
empirical	O
ratings	O
for	O
pleasantness	O
,	O
arousal	O
and	O
dominance	O
using	O
the	O
SAM	O
score	O
(	O
Bradley	O
and	O
Lang	O
,	O
1994	O
)	O
.	O

The	O
first	O
one	O
is	O
described	O
in	O
Warriner	O
et	O
al	O
.	O

Psycholinguistic	O
norm	O
lexica	O
:	O
Our	O
feature	O
set	O
includes	O
two	O
psycholinguistic	O
norm	O
lexica	O
.	O

During	O
this	O
training	O
,	O
fine	O
-	O
tuning	O
was	O
active	O
for	O
all	O
transformer	O
-	O
based	O
language	O
models	O
so	O
that	O
their	O
weights	O
were	O
adjusted	O
during	O
the	O
process	O
and	O
scalar	O
mixing	O
(	O
Liu	O
et	O
al	O
.	O
,	O
2019	O
)	O
was	O
used	O
for	O
the	O
transformer	O
-	O
based	O
language	O
models	O
as	O
it	O
was	O
not	O
foreseeable	O
which	O
layers	O
of	O
the	O
transformer	O
models	O
would	O
influence	O
results	O
the	O
most	O
.	O

More	O
precisely	O
,	O
in	O
the	O
setup	O
we	O
chose	O
,	O
the	O
outputs	O
of	O
all	O
language	O
models	O
were	O
fed	O
to	O
a	O
feed	O
-	O
forward	O
layer	O
responsible	O
for	O
calculating	O
the	O
final	O
complexity	O
scores	O
.	O

Word	O
embeddings	O
:	O
We	O
used	O
multiple	O
static	O
and	O
contextual	O
word	O
embedding	O
models	O
for	O
our	O
feature	O
set	O
.	O

The	O
main	O
intuition	O
behind	O
using	O
this	O
resource	O
was	O
that	O
the	O
length	O
of	O
the	O
shortest	O
hypernym	O
path	O
and	O
the	O
count	O
for	O
the	O
different	O
lexico	O
-	O
semantic	O
relations	O
could	O
be	O
a	O
good	O
indicator	O
for	O
lexical	O
complexity	O
.	O

We	O
accessed	O
WordNet	O
using	O
NLTK	O
(	O
Bird	O
et	O
al	O
.	O
,	O
2009	O
)	O
.	O

In	O
cases	O
where	O
multiple	O
synsets	O
were	O
given	O
for	O
a	O
word	O
,	O
we	O
calculated	O
the	O
respective	O
means	O
and	O
in	O
cases	O
where	O
a	O
given	O
word	O
was	O
not	O
included	O
in	O
the	O
resource	O
,	O
we	O
set	O
all	O
respective	O
feature	O
values	O
to	O
0	O
.	O

WordNet	O
features	O
:	O
Here	O
,	O
we	O
included	O
the	O
numbers	O
of	O
hypernyms	O
,	O
root	O
hypernyms	O
,	O
hyponyms	O
,	O
member	O
holonyms	O
,	O
part	O
meronyms	O
and	O
member	O
meronyms	O
of	O
the	O
respective	O
word(s	O
)	O
as	O
well	O
as	O
the	O
number	O
of	O
given	O
examples	O
and	O
the	O
length	O
of	O
the	O
shortest	O
hypernym	O
path	O
from	O
WordNet	O
(	O
Miller	O
,	O
1995	O
)	O
.	O

In	O
addition	O
to	O
the	O
tags	O
assigned	O
to	O
the	O
word(s	O
)	O
whose	O
score	O
was	O
to	O
be	O
predicted	O
,	O
we	O
included	O
the	O
XPOSand	O
UPOS	O
tags	O
of	O
the	O
two	O
neighbouring	O
words	O
to	O
the	O
left	O
and	O
to	O
the	O
right	O
as	O
well	O
as	O
the	O
dependency	O
tags	O
of	O
the	O
siblings	O
,	O
direct	O
children	O
and	O
the	O
parent	O
of	O
the	O
word(s	O
)	O
within	O
the	O
dependency	O
structure	O
of	O
a	O
given	O
sentence	O
.	O

Thus	O
,	O
the	O
exact	O
number	O
of	O
input	O
features	O
was	O
7424	O
for	O
our	O
system	O
dealing	O
with	O
single	O
words	O
and	O
14848	O
for	O
our	O
system	O
dealing	O
with	O
multiword	O
expressions	O
.	O

In	O
case	O
of	O
our	O
system	O
dealing	O
with	O
bigram	O
multiword	O
expressions	O
,	O
we	O
calculated	O
such	O
a	O
vector	O
for	O
each	O
of	O
both	O
words	O
and	O
then	O
concatenated	O
them	O
to	O
acquire	O
the	O
final	O
input	O
vectors	O
.	O

The	O
following	O
paragraphs	O
describe	O
the	O
features	O
we	O
used	O
to	O
create	O
the	O
feature	O
vectors	O
used	O
to	O
represent	O
words	O
.	O

Feature	O
Engineering	O
.	O

Moreover	O
,	O
the	O
reportings	O
of	O
Paetzold	O
and	O
Specia	O
(	O
2016	O
)	O
and	O
Yimam	O
et	O
al	O
.	O

The	O
motivation	O
behind	O
using	O
this	O
algorithm	O
was	O
its	O
general	O
ability	O
to	O
perform	O
well	O
on	O
heterogeneous	O
and	O
sparse	O
feature	O
sets	O
which	O
allowed	O
us	O
to	O
mix	O
regular	O
linguistic	O
features	O
,	O
WordNet	O
features	O
,	O
word	O
embeddings	O
,	O
psycho	O
-	O
linguistic	O
norm	O
lexica	O
,	O
corpusbased	O
word	O
frequencies	O
and	O
selected	O
word	O
lists	O
as	O
all	O
of	O
these	O
were	O
features	O
we	O
assumed	O
to	O
possibly	O
correlate	O
with	O
lexical	O
complexity	O
.	O

System	O
Overview	O
.	O

The	O
findings	O
of	O
this	O
shared	O
task	O
confirmed	O
the	O
finding	O
of	O
the	O
previous	O
one	O
that	O
using	O
ensemble	O
methods	O
yield	O
best	O
results	O
for	O
complex	O
word	O
identification	O
with	O
a	O
system	O
submitted	O
by	O
Gooding	O
and	O
Kochmar	O
(	O
2018	O
)	O
relying	O
on	O
decision	O
tree	O
ensembles	O
.	O

The	O
data	O
for	O
this	O
was	O
acquired	O
by	O
presenting	O
annotators	O
on	O
Amazon	O
Mechanical	O
Turk	O
with	O
paragraphs	O
of	O
text	O
and	O
letting	O
them	O
mark	O
words	O
which	O
according	O
to	O
their	O
perception	O
could	O
hinder	O
the	O
same	O
paragraph	O
from	O
being	O
understood	O
by	O
a	O
less	O
proficient	O
reader	O
.	O

(	O
2018	O
)	O
.	O

In	O
2018	O
,	O
a	O
second	O
shared	O
task	O
was	O
conducted	O
on	O
the	O
same	O
topic	O
as	O
described	O
in	O
Yimam	O
et	O
al	O
.	O

In	O
the	O
first	O
one	O
,	O
a	O
word	O
was	O
considered	O
complex	O
if	O
at	O
least	O
one	O
of	O
the	O
annotators	O
had	O
judged	O
it	O
as	O
such	O
,	O
and	O
in	O
the	O
second	O
one	O
,	O
each	O
word	O
was	O
given	O
20	O
different	O
labels	O
,	O
one	O
per	O
annotator	O
.	O

From	O
these	O
judgements	O
,	O
two	O
different	O
data	O
sets	O
were	O
derived	O
.	O

The	O
data	O
set	O
used	O
for	O
this	O
task	O
was	O
created	O
by	O
presenting	O
20	O
nonnative	O
speakers	O
with	O
sentences	O
and	O
letting	O
them	O
judge	O
whether	O
the	O
words	O
contained	O
within	O
these	O
sentences	O
were	O
rated	O
as	O
complex	O
or	O
not	O
.	O

Here	O
,	O
the	O
problem	O
of	O
determining	O
the	O
complexity	O
of	O
a	O
word	O
was	O
formulated	O
as	O
a	O
classification	O
task	O
designed	O
to	O
determine	O
whether	O
a	O
word	O
could	O
be	O
considered	O
as	O
being	O
complex	O
or	O
not	O
.	O

The	O
first	O
approaches	O
to	O
the	O
systematic	O
prediction	O
of	O
lexical	O
complexity	O
were	O
made	O
during	O
SemEval-2016	O
Task	O
11	O
(	O
Paetzold	O
and	O
Specia	O
,	O
2016	O
)	O
.	O

Related	O
Work	O
.	O

The	O
assigned	O
scores	O
were	O
then	O
projected	O
onto	O
values	O
between	O
0	O
and	O
1	O
and	O
averaged	O
between	O
all	O
annotators	O
to	O
calculate	O
the	O
final	O
scores	O
.	O

The	O
scores	O
given	O
for	O
simple	O
words	O
,	O
respectively	O
multiword	O
expressions	O
,	O
were	O
derived	O
from	O
letting	O
annotators	O
subjectively	O
judge	O
the	O
difficulty	O
of	O
understanding	O
words	O
respectively	O
word	O
bigrams	O
on	O
a	O
Likert	O
scale	O
ranging	O
from	O
1	O
to	O
5	O
with	O
1	O
indicating	O
a	O
very	O
simple	O
and	O
5	O
a	O
very	O
complex	O
word	O
.	O

For	O
the	O
task	O
,	O
both	O
subcorpora	O
were	O
partitioned	O
into	O
training	O
,	O
test	O
and	O
trial	O
sets	O
.	O

Within	O
both	O
CompLex	O
sub	O
-	O
corpora	O
,	O
the	O
sentences	O
are	O
organised	O
into	O
quadruples	O
consisting	O
of	O
a	O
given	O
sentence	O
,	O
a	O
reference	O
to	O
its	O
original	O
corpus	O
,	O
a	O
selected	O
word	O
,	O
respectively	O
a	O
multiword	O
expression	O
from	O
this	O
sentence	O
,	O
and	O
a	O
continuous	O
complexity	O
score	O
denoting	O
the	O
difficulty	O
of	O
this	O
selected	O
word	O
or	O
bigram	O
which	O
is	O
to	O
be	O
predicted	O
by	O
systems	O
submitted	O
to	O
the	O
shared	O
task	O
.	O

Accordingly	O
,	O
the	O
shared	O
task	O
was	O
divided	O
into	O
two	O
sub	O
-	O
tasks	O
,	O
one	O
dedicated	O
to	O
each	O
sub	O
-	O
corpus	O
.	O

This	O
English	O
corpus	O
consists	O
of	O
sentences	O
extracted	O
from	O
the	O
World	O
English	O
Bible	O
of	O
the	O
multilingual	O
corpus	O
consisting	O
of	O
bible	O
translations	O
published	O
by	O
Christodoulopoulos	O
and	O
Steedman	O
(	O
2015	O
)	O
,	O
the	O
English	O
version	O
of	O
Europarl	O
(	O
Koehn	O
,	O
2005	O
)	O
,	O
a	O
corpus	O
containing	O
various	O
texts	O
concerned	O
with	O
European	O
policy	O
,	O
and	O
CRAFT	O
(	O
Bada	O
et	O
al	O
.	O
,	O
2012	O
)	O
,	O
a	O
corpus	O
consisting	O
of	O
biomedical	O
articles	O
.	O

Task	O
Setup	O
.	O

Background	O
.	O

Our	O
key	O
discovery	O
is	O
that	O
while	O
features	O
from	O
nearly	O
all	O
categories	O
provided	O
by	O
us	O
were	O
used	O
by	O
our	O
systems	O
,	O
contextual	O
string	O
embeddings	O
(	O
Akbik	O
et	O
al	O
.	O
,	O
2018	O
)	O
were	O
the	O
by	O
far	O
most	O
important	O
category	O
of	O
features	O
to	O
determine	O
lexical	O
complexity	O
for	O
both	O
systems	O
.	O

Predicting	O
these	O
scores	O
can	O
be	O
formulated	O
as	O
a	O
regression	O
problem	O
.	O

One	O
could	O
imagine	O
using	O
such	O
scores	O
to	O
extract	O
vocabulary	O
lists	O
appropriate	O
for	O
a	O
learner	O
level	O
from	O
corpora	O
and	O
literature	O
(	O
Alfter	O
and	O
Volodina	O
,	O
2018	O
)	O
,	O
to	O
judge	O
if	O
a	O
given	O
piece	O
of	O
literature	O
fits	O
a	O
learner	O
's	O
skill	O
or	O
to	O
assist	O
authors	O
of	O
textbooks	O
in	O
finding	O
a	O
level	O
of	O
textual	O
difficulty	O
appropriate	O
for	O
a	O
target	O
audience	O
.	O

Solving	O
this	O
task	O
could	O
benefit	O
second	O
-	O
language	O
learners	O
and	O
non	O
-	O
native	O
speakers	O
in	O
various	O
ways	O
.	O

Introduction	O
.	O

