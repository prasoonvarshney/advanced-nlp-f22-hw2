This	O
work	O
was	O
supported	O
by	O
EPSRC	O
grant	O
EP	O
/	O
L027623/1	O
.	O

We	O
propose	O
these	O
techniques	O
as	O
practical	O
approaches	O
to	O
including	O
target	O
syntax	O
in	O
NMT	B-TaskName
.	O

We	O
further	O
improve	O
on	O
the	O
individual	O
results	O
via	O
a	O
decoding	O
strategy	O
allowing	O
ensembling	B-MethodName
of	I-MethodName
models	I-MethodName
producing	O
different	O
output	O
representations	O
,	O
such	O
as	O
subword	O
units	O
and	O
syntax	O
.	O

We	O
train	O
these	O
models	O
using	O
a	O
delayed	B-MethodName
SGD	I-MethodName
update	I-MethodName
training	I-MethodName
procedure	I-MethodName
that	O
is	O
especially	O
effective	O
for	O
the	O
long	O
representations	O
that	O
arise	O
from	O
including	O
target	O
language	O
syntactic	O
information	O
in	O
the	O
output	O
.	O

We	O
report	O
strong	O
performance	O
with	O
individual	O
models	O
that	O
meets	O
or	O
improves	O
over	O
the	O
recent	O
best	O
WAT	B-MethodName
Ja	I-MethodName
-	I-MethodName
En	I-MethodName
ensemble	I-MethodName
results	O
.	O

However	O
,	O
we	O
found	O
that	O
this	O
gives	O
little	O
improvement	O
in	O
BLEU	B-MetricName
over	O
unconstrained	O
decoding	O
although	O
it	O
remains	O
an	O
interesting	O
line	O
of	O
research	O
.	O

We	O
find	O
that	O
the	O
syntax	B-MethodName
model	I-MethodName
is	O
often	O
more	O
grammatical	O
,	O
even	O
when	O
the	O
plain	B-MethodName
BPE	I-MethodName
model	O
may	O
share	O
more	O
vocabulary	O
with	O
the	O
reference	O
(	O
Table	O
2	O
)	O
.	O

To	O
highlight	O
these	O
,	O
we	O
examine	O
hypotheses	O
generated	O
by	O
the	O
plain	B-MethodName
BPE	I-MethodName
and	O
linearized	B-MethodName
derivation	I-MethodName
models	O
.	O

However	O
,	O
an	O
ensemble	B-MethodName
of	I-MethodName
models	I-MethodName
producing	O
plain	B-MethodName
BPE	I-MethodName
and	O
linearized	B-MethodName
derivations	I-MethodName
improves	O
by	O
0.5	B-MetricValue
BLEU	B-MetricName
over	O
the	O
plain	B-MethodName
BPE	I-MethodName
baseline	O
.	O

Ensembles	B-MethodName
of	I-MethodName
two	I-MethodName
identical	I-MethodName
models	I-MethodName
trained	O
with	O
different	O
seeds	O
only	O
slightly	O
improve	O
over	O
the	O
single	B-MethodName
model	I-MethodName
(	O
Table	O
5	O
)	O
.	O

It	O
has	O
been	O
suggested	O
that	O
decaying	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
can	O
have	O
a	O
similar	O
effect	O
to	O
large	O
batch	O
training	O
(	O
Smith	O
et	O
al	O
.	O
,	O
2017	O
)	O
,	O
but	O
reducing	O
the	O
initial	O
learning	B-HyperparameterName
rate	I-HyperparameterName
by	O
a	O
factor	B-HyperparameterValue
of	I-HyperparameterValue
8	I-HyperparameterValue
alone	O
did	O
not	O
give	O
the	O
same	O
improvements	O
.	O

Accumulating	O
the	O
gradient	O
over	O
8	B-HyperparameterValue
batches	B-HyperparameterName
of	O
size	O
4096	B-HyperparameterValue
gives	O
a	O
3	B-MetricValue
BLEU	B-MetricName
improvement	O
for	O
the	O
linear	B-MethodName
derivation	I-MethodName
model	O
.	O

English	O
constituency	O
trees	O
are	O
obtained	O
using	O
CKYlark	B-MethodName
(	O
Oda	O
et	O
al	O
.	O
,	O
2015	O
)	O
,	O
with	O
words	O
replaced	O
by	O
BPE	O
subwords	O
.	O

We	O
report	O
all	O
experiments	O
for	O
Japanese	O
-	O
English	O
,	O
using	O
the	O
first	B-HyperparameterValue
1	I-HyperparameterValue
M	I-HyperparameterValue
training	B-HyperparameterName
sentences	I-HyperparameterName
of	O
the	O
Japanese	O
-	O
English	O
ASPEC	B-DatasetName
data	O
(	O
Nakazawa	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

For	O
these	O
models	O
we	O
use	O
embedding	B-HyperparameterName
size	I-HyperparameterName
400	B-HyperparameterValue
,	O
a	O
single	B-HyperparameterValue
BiLSTM	B-HyperparameterName
layer	I-HyperparameterName
of	O
size	O
750	B-HyperparameterValue
,	O
and	O
batch	B-HyperparameterName
size	I-HyperparameterName
80	B-HyperparameterValue
.	O

For	O
comparison	O
with	O
earlier	O
target	O
syntax	O
work	O
,	O
we	O
also	O
train	O
two	O
RNN	B-MethodName
attention	I-MethodName
-	I-MethodName
based	I-MethodName
seq2seq	I-MethodName
models	O
(	O
Bahdanau	O
et	O
al	O
.	O
,	O
2015	O
)	O
with	O
normal	O
SGD	O
to	O
produce	O
plain	O
BPE	O
sequences	O
and	O
linearized	O
derivations	O
.	O

In	O
all	O
cases	O
we	O
decode	O
using	O
SGNMT	B-MethodName
(	O
Stahlberg	O
et	O
al	O
.	O
,	O
2017	O
)	O
with	O
beam	B-HyperparameterName
size	I-HyperparameterName
4	B-HyperparameterValue
,	O
using	O
the	O
average	O
of	O
the	O
final	O
20	B-HyperparameterValue
checkpoints	B-HyperparameterName
.	O

All	O
Transformer	O
architectures	O
are	O
Ten	B-MethodName
-	I-MethodName
sor2Tensor	I-MethodName
's	O
base	O
Transformer	O
model	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O
2018	O
)	O
with	O
a	O
batch	O
size	O
of	O
4096	O
.	O

Each	O
multirepresentation	B-MethodName
ensemble	I-MethodName
consists	O
of	O
the	O
plain	O
BPE	O
model	O
and	O
one	O
other	O
individual	O
model	O
.	O

We	O
decode	O
with	O
individual	O
models	O
and	O
two	B-MethodName
-	I-MethodName
model	I-MethodName
ensembles	I-MethodName
,	O
comparing	O
results	O
for	O
single	O
-	O
representation	O
and	O
multi	B-MethodName
-	I-MethodName
representation	I-MethodName
ensembles	I-MethodName
.	O

To	O
compare	O
target	O
representations	O
we	O
train	O
Transformer	B-MethodName
models	I-MethodName
with	O
target	O
representations	O
(	O
1	O
)	O
,	O
(	O
2	O
)	O
,	O
(	O
4	O
)	O
and	O
(	O
5	O
)	O
shown	O
in	O
Table	O
1	O
,	O
using	O
delayed	B-MethodName
SGD	I-MethodName
updates	I-MethodName
every	O
8	B-HyperparameterValue
batches	B-HyperparameterName
.	O

We	O
first	O
explore	O
the	O
effect	O
of	O
our	O
delayed	B-MethodName
SGD	I-MethodName
update	I-MethodName
training	O
scheme	O
on	O
single	O
models	O
,	O
contrasting	O
updates	O
every	O
batch	O
with	O
accumulated	O
updates	O
every	O
8	B-HyperparameterValue
batches	B-HyperparameterName
.	O

This	O
lets	O
us	O
effectively	O
use	O
very	O
large	O
batch	B-HyperparameterName
sizes	I-HyperparameterName
without	O
requiring	O
multiple	O
GPUs	O
.	O
Ensembling	O
Representations	O
.	O

We	O
accumulate	O
gradients	O
over	O
a	O
fixed	O
number	B-HyperparameterName
of	I-HyperparameterName
batches	I-HyperparameterName
before	O
using	O
the	O
accumulated	O
gradients	O
to	O
update	O
the	O
model	O
1	O
.	O

Our	O
strategy	O
avoids	O
this	O
problem	O
by	O
using	O
delayed	B-MethodName
SGD	I-MethodName
updates	I-MethodName
.	O

During	O
NMT	B-TaskName
training	O
,	O
by	O
default	O
,	O
the	O
gradients	O
used	O
to	O
update	O
model	O
parameters	O
are	O
calculated	O
over	O
individual	O
batches	O
.	O

The	O
Ten	B-MethodName
-	I-MethodName
sor2Tensor	I-MethodName
framework	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O
2018	O
)	O
defines	O
batch	O
size	O
as	O
the	O
number	O
of	O
tokens	O
per	O
batch	O
,	O
so	O
batches	O
will	O
contain	O
fewer	O
sequences	O
if	O
their	O
average	O
length	O
increases	O
.	O

Delayed	B-MethodName
SGD	I-MethodName
Update	I-MethodName
Training	O
for	O
Long	O
Sequences	O
.	O

Garmash	O
and	O
Monz	O
(	O
2016	O
)	O
show	O
translation	O
improvements	O
with	O
multi	B-MethodName
-	I-MethodName
source	I-MethodName
-	I-MethodName
language	I-MethodName
NMT	I-MethodName
ensembles	I-MethodName
.	O

Hokamp	O
(	O
2017	O
)	O
shows	O
improvements	O
in	O
the	O
quality	B-TaskName
estimation	I-TaskName
task	O
using	O
ensembles	O
of	O
NMT	B-TaskName
models	O
with	O
multiple	O
input	O
representations	O
which	O
share	O
an	O
output	O
representation	O
.	O

(	O
2017	O
)	O
combine	O
recurrent	B-MethodName
neural	I-MethodName
network	I-MethodName
grammar	I-MethodName
(	I-MethodName
RNNG	I-MethodName
)	I-MethodName
models	O
(	O
Dyer	O
et	O
al	O
.	O
,	O
2016	O
)	O
with	O
attention	B-MethodName
-	I-MethodName
based	I-MethodName
models	O
to	O
produce	O
well	O
-	O
formed	O
dependency	O
trees	O
.	O

(	O
2017	O
)	O
perform	O
NMT	B-TaskName
with	O
syntax	O
annotation	O
in	O
the	O
form	O
of	O
Combinatory	O
Categorial	O
Grammar	O
(	O
CCG	O
)	O
supertags	O
.	O

Long	O
sequences	O
make	O
training	O
more	O
difficult	O
(	O
Bahdanau	O
et	O
al	O
.	O
,	O
2015	O
)	O
,	O
which	O
we	O
address	O
with	O
an	O
adjusted	O
training	O
procedure	O
for	O
the	O
Transformer	O
architecture	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O
2017	O
)	O
,	O
using	O
delayed	B-MethodName
SGD	I-MethodName
updates	I-MethodName
which	O
accumulate	O
gradients	O
over	O
multiple	O
batches	O
.	O

As	O
part	O
of	O
our	O
investigation	O
we	O
suggest	O
strategies	O
for	O
practical	O
NMT	B-TaskName
with	O
very	O
long	O
target	O
sequences	O
.	O

We	O
hypothesize	O
that	O
an	O
NMT	B-MethodName
ensemble	I-MethodName
would	O
be	O
strengthened	O
if	O
its	O
component	O
models	O
were	O
complementary	O
in	O
this	O
way	O
.	O

However	O
,	O
ensembling	B-MethodName
often	O
requires	O
component	O
models	O
to	O
make	O
predictions	O
relating	O
to	O
the	O
same	O
output	O
sequence	O
position	O
at	O
each	O
time	O
step	O
.	O

Previous	O
work	O
has	O
observed	O
that	O
NMT	B-TaskName
models	O
trained	O
to	O
generate	O
target	O
syntax	O
can	O
exhibit	O
improved	O
sentence	O
structure	O
(	O
Aharoni	O
and	O
Goldberg	O
,	O
2017;Eriguchi	O
et	O
al	O
.	O
,	O
2017	O
)	O
relative	O
to	O
those	O
trained	O
on	O
plain	O
-	O
text	O
,	O
while	O
plain	O
-	O
text	O
models	O
produce	O
shorter	O
sequences	O
and	O
so	O
may	O
encode	O
lexical	O
information	O
more	O
easily	O
(	O
Nadejde	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

Ensembles	O
of	O
multiple	O
NMT	B-TaskName
models	O
consistently	O
and	O
significantly	O
improve	O
over	O
single	O
models	O
(	O
Garmash	O
and	O
Monz	O
,	O
2016	O
)	O
.	O

We	O
formulate	O
beam	O
search	O
over	O
such	O
ensembles	O
using	O
WFSTs	B-MethodName
,	O
and	O
describe	O
a	O
delayed	B-MethodName
SGD	I-MethodName
update	I-MethodName
training	I-MethodName
procedure	I-MethodName
that	O
is	O
especially	O
effective	O
for	O
long	O
representations	O
like	O
linearized	O
syntax	O
.	O

We	O
explore	O
strategies	O
for	O
incorporating	O
target	O
syntax	O
into	O
Neural	B-TaskName
Machine	I-TaskName
Translation	I-TaskName
.	O

Multi	O
-	O
representation	O
Ensembles	O
and	O
Delayed	O
SGD	O
Updates	O
Improve	O
Syntax	B-TaskName
-	I-TaskName
based	I-TaskName
NMT	I-TaskName
.	O

Acknowledgments	O
.	O

Conclusions	O
.	O

It	O
is	O
also	O
possible	O
to	O
constrain	O
decoding	O
of	O
linearized	O
trees	O
and	O
derivations	O
to	O
wellformed	O
outputs	O
.	O

Our	O
solution	O
was	O
to	O
penalise	O
scores	O
of	O
non	O
-	O
terminals	O
under	O
the	O
syntax	O
model	O
by	O
a	O
constant	O
factor	O
.	O

In	O
ensembling	O
plain	O
-	O
text	O
with	O
a	O
syntax	O
external	O
representation	O
we	O
observed	O
that	O
in	O
a	O
small	O
proportion	O
of	O
cases	O
non	O
-	O
terminals	O
were	O
over	O
-	O
generated	O
,	O
due	O
to	O
the	O
mismatch	O
in	O
target	O
sequence	O
lengths	O
.	O

By	O
ensembling	O
syntax	O
and	O
plain	O
-	O
text	O
we	O
hope	O
to	O
benefit	O
from	O
their	O
complementary	O
strengths	O
.	O

Our	O
syntax	O
models	O
achieve	O
similar	O
results	O
despite	O
producing	O
much	O
longer	O
sequences	O
.	O

Our	O
plain	O
BPE	O
baseline	O
(	O
Table	O
4	O
)	O
outperforms	O
the	O
current	O
best	O
system	O
on	O
WAT	O
Ja	O
-	O
En	O
,	O
an	O
8	O
-	O
model	O
ensemble	O
(	O
Morishita	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

Our	O
first	O
results	O
in	O
Table	O
3	O
show	O
that	O
large	O
batch	O
training	O
can	O
significantly	O
improve	O
the	O
performance	O
of	O
single	O
Transformers	O
,	O
particularly	O
when	O
trained	O
to	O
produce	O
longer	O
sequences	O
.	O

Results	O
and	O
Discussion	O
.	O

The	O
linearized	O
derivation	O
uses	O
additional	O
tokens	O
for	O
non	O
-	O
terminals	O
with	O
<	O
/R	O
>	O
.	O

Non	O
-	O
terminals	O
are	O
included	O
as	O
separate	O
tokens	O
.	O

We	O
train	O
separate	O
Japanese	O
(	O
lowercased	O
)	O
and	O
English	O
(	O
cased	O
)	O
BPE	O
vocabularies	O
on	O
the	O
plain	O
-	O
text	O
,	O
with	O
30	O
K	O
merges	O
each	O
.	O

All	O
models	O
use	O
plain	O
BPE	O
Japanese	O
source	O
sentences	O
.	O

Experiments	O
.	O

Symbols	O
in	O
the	O
internal	O
representation	O
are	O
consumed	O
as	O
needed	O
to	O
stay	O
synchronized	O
with	O
the	O
external	O
representation	O
,	O
as	O
illustrated	O
in	O
Figure	O
1	O
;	O
epsilons	O
are	O
consumed	O
with	O
a	O
probability	O
of	O
1	O
.	O

As	O
search	O
proceeds	O
,	O
each	O
model	O
score	O
is	O
updated	O
separately	O
with	O
its	O
appropriate	O
representation	O
.	O

This	O
leads	O
to	O
an	O
outer	O
beam	O
search	O
over	O
external	O
representations	O
with	O
inner	O
beam	O
searches	O
for	O
the	O
best	O
matching	O
internal	O
representations	O
.	O

The	O
ensembled	O
score	O
of	O
h	O
is	O
then	O
:	O
P	O
(	O
h	O
j	O
|h	O
<	O
j	O
)	O
=	O
P	O
o	O
(	O
h	O
j	O
|h	O
<	O
j	O
)	O
×	O
(	O
3	O
)	O
max	O
(	O
x	O
,	O
y)∈M	O
(	O
h	O
)	O
P	O
i	O
(	O
i(y)|i(x	O
)	O
)	O
The	O
max	O
performed	O
for	O
each	O
partial	O
hypothesis	O
h	O
is	O
itself	O
approximated	O
by	O
a	O
beam	O
search	O
.	O

The	O
set	O
of	O
partial	O
paths	O
yielding	O
h	O
are	O
:	O
M	O
(	O
h	O
)	O
=	O
(	O
2	O
)	O
{	O
(	O
x	O
,	O
y)|xyz	O
∈	O
P	O
,	O
o(x	O
)	O
=	O
h	O
<	O
j	O
,	O
o(xy	O
)	O
=	O
h	O
}	O
2	O
See	O
the	O
tokenization	O
wrappers	O
in	O
https://	O
github.com/ucam-smt/sgnmt	O
Here	O
z	O
is	O
the	O
path	O
suffix	O
.	O

h	O
j	O
be	O
a	O
partial	O
hypothesis	O
in	O
the	O
output	O
representation	O
.	O

Let	O
h	O
=	O
h	O
1	O
.	O

In	O
practice	O
,	O
beam	O
decoding	O
is	O
performed	O
in	O
the	O
external	O
representation	O
,	O
i.e.	O
over	O
projections	O
of	O
paths	O
in	O
P	O
2	O
.	O

A	O
path	O
(	O
Sennrich	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

Let	O
P	O
be	O
the	O
paths	O
in	O
T	O
leading	O
from	O
the	O
start	O
state	O
to	O
any	O
final	O
state	O
.	O

Mapping	O
from	O
word	O
to	O
BPE	O
representations	O
is	O
straightforward	O
,	O
and	O
mapping	O
from	O
(	O
linearized	O
)	O
syntax	O
to	O
plain	O
-	O
text	O
simply	O
deletes	O
non	O
-	O
terminals	O
.	O

The	O
complexity	O
of	O
the	O
transduction	O
depends	O
on	O
the	O
representations	O
.	O

To	O
formulate	O
an	O
ensembling	O
decoder	O
over	O
pairs	O
of	O
these	O
representations	O
,	O
we	O
assume	O
we	O
have	O
a	O
transducer	O
T	O
that	O
maps	O
from	O
one	O
representation	O
to	O
the	O
other	O
representation	O
.	O

Table	O
1	O
shows	O
several	O
different	O
representations	O
of	O
the	O
same	O
hypothesis	O
.	O

Training	O
on	O
multiple	O
GPUs	O
is	O
one	O
way	O
to	O
increase	O
the	O
amount	O
of	O
data	O
used	O
to	O
estimate	O
gradients	O
,	O
but	O
it	O
requires	O
significant	O
resources	O
.	O

However	O
,	O
with	O
such	O
large	O
batches	O
the	O
model	O
size	O
may	O
exceed	O
available	O
GPU	O
memory	O
.	O

Previous	O
research	O
has	O
used	O
very	O
large	O
batches	O
to	O
improve	O
training	O
convergence	O
while	O
requiring	O
fewer	O
model	O
updates	O
(	O
Smith	O
et	O
al	O
.	O
,	O
2017;Neishi	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

A	O
possible	O
consequence	O
is	O
that	O
batches	O
containing	O
fewer	O
sequences	O
per	O
update	O
may	O
have	O
'	O
noisier	O
'	O
estimated	O
gradients	O
than	O
batches	O
with	O
more	O
sequences	O
.	O

We	O
suggest	O
a	O
training	O
strategy	O
for	O
the	O
Transformer	O
model	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O
2017	O
)	O
which	O
gives	O
improved	O
performance	O
for	O
long	O
sequences	O
,	O
like	O
syntax	O
representations	O
,	O
without	O
requiring	O
additional	O
GPU	O
memory	O
.	O

We	O
map	O
words	O
to	O
subwords	O
as	O
described	O
in	O
Section	O
3	O
.	O

The	O
original	O
tree	O
can	O
be	O
directly	O
reproduced	O
from	O
the	O
sequence	O
,	O
so	O
that	O
structure	O
information	O
is	O
maintained	O
.	O

Our	O
linearized	O
derivation	O
representation	O
(	O
(	O
4	O
)	O
in	O
Table	O
1	O
)	O
consists	O
of	O
the	O
derivation	O
's	O
right	O
-	O
hand	O
side	O
tokens	O
with	O
an	O
end	O
-	O
of	O
-	O
rule	O
marker	O
,	O
<	O
/R	O
>	O
,	O
marking	O
the	O
last	O
non	O
-	O
terminal	O
in	O
each	O
rule	O
.	O

We	O
therefore	O
propose	O
a	O
derivation	O
-	O
based	O
representation	O
which	O
is	O
much	O
more	O
compact	O
than	O
a	O
linearized	O
parse	O
tree	O
(	O
examples	O
in	O
Table	O
1	O
)	O
.	O

We	O
wish	O
to	O
ensemble	O
using	O
models	O
which	O
generate	O
linearized	O
constituency	O
trees	O
but	O
these	O
representations	O
can	O
be	O
very	O
long	O
and	O
difficult	O
to	O
model	O
.	O

Ensembles	O
of	O
Syntax	O
Models	O
.	O

Previous	O
approaches	O
to	O
ensembling	O
diverse	O
models	O
focus	O
on	O
model	O
inputs	O
.	O

(	O
2017	O
)	O
similarly	O
produce	O
both	O
words	O
and	O
arcstandard	O
algorithm	O
actions	O
(	O
Nivre	O
,	O
2004	O
)	O
.	O

Wu	O
et	O
al	O
.	O

Eriguchi	O
et	O
al	O
.	O

They	O
demonstrate	O
improved	O
target	O
language	O
reordering	O
when	O
producing	O
syntax	O
.	O

Aharoni	O
and	O
Goldberg	O
(	O
2017	O
)	O
translate	O
from	O
source	O
BPE	O
into	O
target	O
linearized	O
parse	O
trees	O
,	O
but	O
omit	O
POS	O
tags	O
to	O
reduce	O
sequence	O
length	O
.	O

Nadejde	O
et	O
al	O
.	O

Related	O
Work	O
.	O

We	O
also	O
suggest	O
a	O
syntax	O
representation	O
which	O
results	O
in	O
much	O
shorter	O
sequences	O
.	O

These	O
long	O
sequences	O
may	O
arise	O
through	O
the	O
use	O
of	O
linearized	O
constituency	O
trees	O
and	O
can	O
be	O
much	O
longer	O
than	O
their	O
plain	O
byte	O
pair	O
encoded	O
(	O
BPE	O
)	O
equivalent	O
representations	O
(	O
Table	O
1	O
)	O
.	O

We	O
propose	O
an	O
approach	O
to	O
decoding	O
ensembles	O
of	O
models	O
generating	O
different	O
representations	O
,	O
focusing	O
on	O
models	O
generating	O
syntax	O
.	O

Models	O
producing	O
different	O
sentence	O
representations	O
are	O
necessarily	O
synchronized	O
to	O
enable	O
this	O
.	O

Introduction	O
.	O

Our	O
approach	O
gives	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
a	O
difficult	O
Japanese	O
-	O
English	O
task	O
.	O

We	O
specifically	O
focus	O
on	O
syntax	O
in	O
ensembles	O
containing	O
multiple	O
sentence	O
representations	O
.	O

