arship	O
scheme	O
and	O
by	O
the	O
ADAPT	O
Centre	O
for	O
Digital	O
Content	O
Technology	O
which	O
is	O
funded	O
under	O
the	O
SFI	O
Research	O
Centres	O
Programme	O
(	O
Grant	O
13	O
/	O
RC/2106	O
)	O
and	O
is	O
co	O
-	O
funded	O
under	O
the	O
European	O
Regional	O
Development	O
Fund	O
.	O

Furthermore	O
,	O
we	O
would	O
like	O
to	O
experiment	O
with	O
larger	O
datasets	O
to	O
verify	O
whether	O
the	O
positive	O
effect	O
of	O
the	O
linguistic	O
features	O
remains	O
.	O

In	O
the	O
next	O
step	O
,	O
we	O
will	O
let	O
the	O
models	O
converge	O
,	O
create	O
the	O
ensemble	O
models	O
for	O
the	O
different	O
systems	O
and	O
compute	O
whether	O
the	O
increase	O
in	O
BLEU	B-MetricName
score	O
is	O
significant	O
.	O

In	O
the	O
future	O
,	O
we	O
would	O
like	O
to	O
perform	O
manual	O
evaluations	O
on	O
the	O
outputs	O
of	O
our	O
systems	O
to	O
see	O
whether	O
they	O
correlate	O
with	O
the	O
BLEU	B-MetricName
scores	O
.	O

Furthermore	O
,	O
the	O
benefit	O
of	O
the	O
additional	O
features	O
is	O
more	O
clear	O
on	O
a	O
dissimilar	O
test	O
set	O
which	O
is	O
in	O
accordance	O
with	O
our	O
original	O
hypothesis	O
stating	O
that	O
semantic	O
and	O
syntactic	O
features	O
(	O
and	O
their	O
combination	O
)	O
can	O
be	O
beneficial	O
for	O
generalization	O
.	O

For	O
both	O
language	O
pairs	O
we	O
observe	O
that	O
adding	O
extra	O
semantic	O
and/or	O
syntactic	O
features	O
leads	O
to	O
faster	O
convergence	O
.	O

In	O
this	O
work	O
we	O
experimented	O
with	O
EN	O
-	O
FR	O
and	O
EN	O
-	O
DE	O
data	O
augmented	O
with	O
semantic	O
and	O
syntactic	O
features	O
.	O

Conclusions	O
and	O
Future	O
Work	O
.	O

1	O
0	O
0	O
0	O
0	O
2	O
0	O
0	O
0	O
0	O
3	O
0	O
0	O
0	O
0	O
4	O
0	O
0	O
0	O
0	O
5	O
0	O
0	O
0	O
0	O
6	O
0	O
0	O
0	O
0	O
7	O
0	O
0	O
0	O
0	O
8	O
0	O
0	O
0	O
0	O
9	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
1	O
1	O
0	O
0	O
0	O
0	O
1	O
2	O
0	O
0	O
0	O
0	O
1	O
3	O
0	O
0	O
0	O
0	O
1	O
4	O
0	O
0	O
0	O
0	O
.	O

However	O
,	O
their	O
combination	O
(	O
SST	B-MethodName
-	I-MethodName
CCG	I-MethodName
)	O
leads	O
to	O
a	O
more	O
robust	O
NMT	O
system	O
with	O
a	O
higher	O
BLEU	B-MetricName
(	O
see	O
Table	O
2	O
)	O
.	O

In	O
the	O
last	O
iterations	O
,	O
see	O
Figure	O
4	O
,	O
we	O
see	O
how	O
the	O
two	O
systems	O
enriched	O
with	O
supersense	O
tags	O
and	O
CCG	O
tags	O
lead	O
to	O
small	O
improvements	O
over	O
the	O
baseline	O
.	O

Figure	O
3	O
compares	O
the	O
BPE	B-MethodName
-	I-MethodName
ed	I-MethodName
baseline	O
system	O
(	O
BPE	O
)	O
with	O
the	O
NMT	O
system	O
enriched	O
with	O
SST	O
and	O
CCG	O
tags	O
(	O
SST	B-MethodName
-	I-MethodName
CCG	I-MethodName
)	O
.	O

The	O
results	O
for	O
the	O
EN	O
-	O
DE	O
system	O
are	O
very	O
similar	O
to	O
the	O
EN	O
-	O
FR	O
system	O
:	O
the	O
model	O
converges	O
faster	O
and	O
we	O
observe	O
the	O
same	O
trends	O
with	O
respect	O
to	O
the	O
BLEU	B-MetricName
scores	O
of	O
the	O
different	O
systems	O
.	O

English	O
-	O
German	O
.	O

This	O
supports	O
our	O
hypothesis	O
that	O
semantic	O
and	O
syntactic	O
features	O
are	O
particularly	O
useful	O
when	O
combined	O
.	O

Moreover	O
,	O
the	O
benefit	O
of	O
syntactic	O
and	O
semantic	O
features	O
seems	O
to	O
be	O
more	O
than	O
cumulative	O
at	O
some	O
points	O
,	O
confirming	O
the	O
idea	O
that	O
providing	O
both	O
information	O
sources	O
can	O
help	O
the	O
NMT	O
system	O
learn	O
semantico	O
-	O
syntactic	O
patterns	O
.	O

The	O
best	O
CCG	B-MethodName
-	I-MethodName
SST	I-MethodName
model	O
(	O
23.21	B-MetricValue
BLEU	B-MetricName
)	O
outperforms	O
the	O
best	O
BPE	B-MethodName
-	I-MethodName
ed	I-MethodName
baseline	O
model	O
(	O
22.54	B-MetricValue
BLEU	B-MetricName
)	O
with	O
0.67	B-MetricValue
BLEU	B-MetricName
(	O
see	O
Table	O
1	O
)	O
.	O

Although	O
Sennrich	O
and	O
Haddow	O
(	O
2016	O
)	O
observe	O
that	O
features	O
are	O
not	O
necessarily	O
cumulative	O
(	O
possibly	O
since	O
the	O
information	O
from	O
the	O
syntactic	O
features	O
partially	O
overlapped	O
)	O
,	O
the	O
system	O
enriched	O
with	O
both	O
semantic	O
and	O
syntactic	O
features	O
outperforms	O
the	O
two	O
separate	O
systems	O
as	O
well	O
as	O
the	O
baseline	O
system	O
.	O

1	O
0	O
0	O
0	O
0	O
2	O
0	O
0	O
0	O
0	O
3	O
0	O
0	O
0	O
0	O
4	O
0	O
0	O
0	O
0	O
5	O
0	O
0	O
0	O
0	O
6	O
0	O
0	O
0	O
0	O
7	O
0	O
0	O
0	O
0	O
8	O
0	O
0	O
0	O
0	O
9	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
1	O
1	O
0	O
0	O
0	O
0	O
1	O
2	O
0	O
0	O
0	O
0	O
1	O
3	O
0	O
0	O
0	O
0	O
1	O
4	O
0	O
0	O
0	O
0	O
1	O
5	O
0	O
0	O
0	O
0	O
1	O
6	O
0	O
0	O
0	O
0	O
To	O
see	O
in	O
more	O
detail	O
how	O
our	O
semantically	O
enriched	O
SST	B-MethodName
system	O
compares	O
to	O
an	O
NMT	O
system	O
with	O
syntactic	O
CCG	O
supertags	O
and	O
how	O
a	O
system	O
that	O
integrates	O
both	O
semantic	O
features	O
and	O
syntactic	O
features	O
(	O
SST	B-MethodName
-	I-MethodName
CCG	I-MethodName
)	O
performs	O
,	O
a	O
more	O
detailed	O
graph	O
is	O
provided	O
in	O
Figure	O
2	O
where	O
we	O
zoom	O
in	O
on	O
later	O
stages	O
of	O
the	O
learning	O
process	O
.	O

From	O
the	O
graph	O
,	O
it	O
can	O
also	O
be	O
observed	O
that	O
the	O
system	O
has	O
a	O
more	O
robust	O
,	O
consistent	O
learning	O
curve	O
.	O

Figure	O
1	O
compares	O
the	O
BPE	O
-	O
ed	O
baseline	O
system	O
(	O
BPE	B-MethodName
)	O
with	O
the	O
supertag	B-MethodName
-	I-MethodName
supersensetag	I-MethodName
system	I-MethodName
(	O
CCG	B-MethodName
-	I-MethodName
SST	I-MethodName
)	O
automatically	O
evaluated	O
on	O
the	O
newstest2013	B-DatasetName
(	O
in	O
terms	O
of	O
BLEU	B-MetricValue
(	O
Papineni	O
et	O
al	O
.	O
,	O
2002	O
)	O
)	O
over	O
all	O
150,000	B-HyperparameterValue
iterations	B-HyperparameterName
.	O

As	O
we	O
hypothesized	O
,	O
the	O
benefits	O
of	O
the	O
features	O
added	O
,	O
was	O
more	O
clear	O
on	O
the	O
newstest2013	B-DatasetName
than	O
on	O
the	O
Europarl	B-DatasetName
test	O
set	O
.	O

For	O
both	O
test	O
sets	O
,	O
the	O
NMT	B-MethodName
system	I-MethodName
with	I-MethodName
supersenses	I-MethodName
(	O
SST	B-MethodName
)	O
converges	O
faster	O
than	O
the	O
baseline	O
(	O
BPE	B-MethodName
)	O
NMT	O
system	O
.	O

English	O
-	O
French	O
.	O

Results	O
.	O

All	O
systems	O
are	O
trained	O
for	O
150,000	B-HyperparameterValue
iterations	B-HyperparameterName
and	O
evaluated	O
after	O
every	O
10,000	B-HyperparameterValue
iterations	B-HyperparameterName
.	O

We	O
trained	O
all	O
our	O
BPE	O
-	O
ed	O
NMT	O
systems	O
with	O
CCG	O
tag	O
features	O
,	O
supersensetags	O
(	O
SST	O
)	O
,	O
POS	O
tags	O
and	O
the	O
combination	O
of	O
syntactic	O
features	O
(	O
POS	O
or	O
CCG	O
)	O
with	O
the	O
semantic	O
ones	O
(	O
SST	O
)	O
.	O

We	O
ran	O
the	O
BPE	O
algorithm	O
with	O
89	B-HyperparameterValue
,	O
500	B-HyperparameterValue
operations	B-HyperparameterName
.	O

In	O
order	O
to	O
by	O
-	O
pass	O
the	O
OOV	O
problem	O
and	O
reduce	O
the	O
number	O
of	O
dictionary	O
entries	O
we	O
use	O
word	O
-	O
segmentation	O
with	O
BPE	O
(	O
Sennrich	O
,	O
2015	O
)	O
.	O

We	O
keep	O
the	O
embedding	B-HyperparameterName
layer	I-HyperparameterName
fixed	O
to	O
700	B-HyperparameterValue
for	O
all	O
models	O
in	O
order	O
to	O
ensure	O
that	O
the	O
improvements	O
are	O
not	O
simply	O
due	O
to	O
an	O
increase	O
of	O
the	O
parameters	O
in	O
the	O
embedding	O
layer	O
.	O

We	O
used	O
the	O
nematus	O
toolkit	O
(	O
Sennrich	O
et	O
al	O
.	O
,	O
2017	O
)	O
to	O
train	O
encoder	O
-	O
decoder	O
NMT	O
models	O
with	O
the	O
following	O
parameters	O
:	O
vocabulary	O
size	O
:	O
35000	O
,	O
maximum	B-HyperparameterName
sentence	I-HyperparameterName
length	I-HyperparameterName
:	O
60	B-HyperparameterValue
,	O
vector	B-HyperparameterName
dimension	I-HyperparameterName
:	O
1024	B-HyperparameterValue
,	O
word	B-HyperparameterName
embedding	I-HyperparameterName
layer	I-HyperparameterName
:	O
700	B-HyperparameterValue
,	O
learning	B-HyperparameterName
optimizer	I-HyperparameterName
:	O
adadelta	B-HyperparameterValue
.	O

Description	O
of	O
the	O
NMT	O
system	O
.	O

Two	O
different	O
test	O
sets	O
are	O
used	O
in	O
order	O
to	O
show	O
how	O
additional	O
semantic	O
and	O
syntactic	O
features	O
can	O
help	O
the	O
NMT	O
system	O
translate	O
different	O
types	O
of	O
test	O
sets	O
and	O
thus	O
evaluate	O
the	O
general	O
effect	O
of	O
our	O
improvement	O
.	O

We	O
test	O
the	O
systems	O
on	O
5	O
K	O
sentences	O
(	O
different	O
from	O
the	O
training	O
data	O
)	O
extracted	O
from	O
Europarl	B-DatasetName
and	O
the	O
newstest2013	B-DatasetName
.	O

An	O
example	O
of	O
a	O
CCGtagged	O
sentence	O
is	O
given	O
in	O
(	O
6	O
):	O
Our	O
NMT	O
systems	O
are	O
trained	O
on	O
1	O
M	O
parallel	O
sentences	O
of	O
the	O
Europarl	B-DatasetName
corpus	O
for	O
EN	O
-	O
FR	O
and	O
EN	O
-	O
DE	O
(	O
Koehn	O
,	O
2005	O
)	O
.	O

This	O
kind	O
of	O
information	O
can	O
help	O
resolve	O
ambiguity	O
in	O
terms	O
of	O
prepositional	O
attachment	O
,	O
among	O
others	O
.	O

CCG	O
tags	O
provide	O
global	O
syntactic	O
information	O
on	O
the	O
lexical	O
level	O
.	O

The	O
POS	O
tags	O
are	O
generated	O
by	O
the	O
Stanford	O
POS	O
-	O
tagger	O
(	O
Toutanova	O
et	O
al	O
.	O
,	O
2003	O
)	O
;	O
for	O
the	O
supertags	O
we	O
used	O
the	O
EasySRL	O
tool	O
(	O
Lewis	O
et	O
al	O
.	O
,	O
2015	O
)	O
which	O
annotates	O
words	O
with	O
CCG	O
tags	O
.	O

To	O
support	O
our	O
hypothesis	O
we	O
also	O
experimented	O
with	O
syntactic	O
features	O
(	O
separately	O
and	O
in	O
combination	O
with	O
the	O
semantic	O
ones	O
):	O
POS	O
tags	O
and	O
CCG	O
supertags	O
.	O

We	O
hypothesize	O
that	O
more	O
general	O
semantic	O
information	O
can	O
be	O
particularly	O
useful	O
for	O
NMT	O
in	O
combination	O
with	O
more	O
detailed	O
syntactic	O
information	O
.	O

Supertags	O
and	O
POS	O
-	O
tags	O
.	O

If	O
the	O
MWE	O
did	O
not	O
receive	O
a	O
particular	O
tag	O
,	O
we	O
added	O
the	O
tag	O
mwe	O
to	O
all	O
its	O
components	O
,	O
as	O
in	O
example	O
(	O
5	O
)	O
(	O
4	O
)	O
Input	O
:	O
"	O
EU	O
citizens	O
"	O
SST	O
:	O
"	O
EU	O
citizens|GROUP	O
"	O
Output	O
:	O
"	O
EU|GROUP	O
citizens|GROUP	O
"	O
(	O
5	O
)	O
Input	O
:	O
"	O
a	O
number	O
of	O
"	O
SST	O
:	O
"	O
a	O
number	O
of	O
"	O
Output	O
:	O
"	O
a|mwe	O
number|mwe	O
of|mwe	O
"	O
.	O

op@@|GROUP	O
ers|GROUP	O
"	O
For	O
the	O
MWEs	O
we	O
decided	O
to	O
copy	O
the	O
supersense	O
tag	O
to	O
all	O
the	O
words	O
of	O
the	O
MWE	O
(	O
if	O
provided	O
by	O
the	O
tagger	O
)	O
,	O
as	O
in	O
(	O
4	O
)	O
.	O

(	O
3	O
)	O
Input	O
:	O
"	O
the	O
stormtroopers	O
"	O
SST	O
:	O
"	O
the	O
stormtroopers|GROUP	O
"	O
BPE	O
:	O
"	O
the	O
stor@@	O
m@@	O
tro@@	O
op@@	O
ers	O
"	O
Output	O
:	O
"	O
the|none	O
stor@@|GROUP	O
...	O

Furthermore	O
,	O
we	O
add	O
a	O
none	O
tag	O
to	O
all	O
words	O
that	O
did	O
not	O
receive	O
a	O
supersense	O
tag	O
.	O

It	O
is	O
split	O
into	O
5	O
subword	O
units	O
so	O
the	O
supersense	O
tag	O
feature	O
is	O
copied	O
to	O
all	O
its	O
five	O
subword	O
units	O
.	O

In	O
(	O
3	O
)	O
,	O
we	O
give	O
an	O
example	O
with	O
the	O
word	O
'	O
stormtroopers	O
'	O
that	O
is	O
tagged	O
with	O
the	O
supersense	O
tag	O
'	O
GROUP	O
'	O
.	O

For	O
each	O
word	O
that	O
is	O
split	O
into	O
subword	O
units	O
,	O
we	O
copy	O
the	O
features	O
of	O
the	O
word	O
in	O
question	O
to	O
its	O
subword	O
units	O
.	O

(	O
2016	O
)	O
using	O
a	O
variant	O
of	O
BPE	O
for	O
word	O
segmentation	O
capable	O
of	O
encoding	O
open	O
vocabularies	O
with	O
a	O
compact	O
symbol	O
vocabulary	O
of	O
variablelength	O
subword	O
units	O
.	O

To	O
reduce	O
the	O
number	O
of	O
out	O
-	O
of	O
-	O
vocabulary	O
(	O
OOV	O
)	O
words	O
,	O
we	O
follow	O
the	O
approach	O
of	O
Sennrich	O
et	O
al	O
.	O

These	O
embedding	O
vectors	O
are	O
then	O
concatenated	O
into	O
one	O
embedding	O
vector	O
and	O
used	O
in	O
the	O
model	O
instead	O
of	O
the	O
simple	O
word	O
embedding	O
one	O
(	O
Sennrich	O
and	O
Haddow	O
,	O
2016	O
)	O
.	O

A	O
separate	O
embedding	O
is	O
learned	O
for	O
every	O
source	O
-	O
side	O
feature	O
provided	O
(	O
the	O
word	O
itself	O
,	O
POS	O
-	O
tag	O
,	O
supersense	O
tag	O
etc	O
.	O
)	O
.	O

(	O
2014	O
)	O
.	O

We	O
add	O
this	O
semantico	O
-	O
syntactic	O
information	O
in	O
the	O
source	O
as	O
an	O
extra	O
feature	O
in	O
the	O
embedding	O
layer	O
following	O
the	O
approach	O
of	O
Sennrich	O
and	O
Haddow	O
(	O
2016	O
)	O
,	O
who	O
extended	O
the	O
model	O
of	O
Bahdanau	O
et	O
al	O
.	O

In	O
Example	O
(	O
2	O
)	O
,	O
the	O
1	O
https://github.com/nschneid/	O
pysupersensetagger	O
2	O
All	O
the	O
examples	O
are	O
extracted	O
from	O
our	O
data	O
used	O
later	O
on	O
to	O
train	O
the	O
NMT	O
systems	O
MWEs	O
in	O
fact	O
,	O
a	O
number	O
of	O
and	O
EU	O
citizens	O
are	O
retrieved	O
by	O
the	O
tagger	O
.	O

(	O
2	O
)	O
Since	O
MWEs	O
and	O
supersenses	O
naturally	O
complement	O
each	O
other	O
,	O
Schneider	O
and	O
Smith	O
(	O
2015	O
)	O
integrated	O
the	O
MWE	O
identification	O
task	O
(	O
Schneider	O
et	O
al	O
.	O
,	O
2014	O
)	O
with	O
the	O
supersense	O
tagging	O
task	O
of	O
Ciaramita	O
and	O
Altun	O
(	O
2006	O
)	O
.	O

Furthermore	O
,	O
there	O
is	O
a	O
separate	O
tag	O
to	O
distinguish	O
auxiliary	O
verbs	O
from	O
main	O
verbs	O
.	O

This	O
way	O
,	O
the	O
supersenses	O
also	O
provide	O
syntactic	O
information	O
useful	O
for	O
disambiguation	O
as	O
in	O
(	O
2	O
)	O
,	O
where	O
the	O
word	O
work	O
is	O
correctly	O
tagged	O
as	O
a	O
noun	O
(	O
with	O
its	O
capitalized	O
supersense	O
tag	O
ACT	O
)	O
in	O
the	O
first	O
part	O
of	O
the	O
sentence	O
and	O
as	O
a	O
verb	O
(	O
with	O
the	O
lowercased	O
supersense	O
tag	O
social	O
)	O
.	O

However	O
,	O
the	O
supersense	O
tags	O
for	O
verbs	O
are	O
always	O
lowercased	O
while	O
the	O
ones	O
for	O
nouns	O
are	O
capitalized	O
.	O

To	O
obtain	O
the	O
supersense	O
tags	O
we	O
used	O
the	O
AMALGrAM	O
(	O
A	O
Machine	O
Analyzer	O
of	O
Lexical	O
Groupings	O
and	O
Meanings	O
)	O
2.0	O
tool	O
1	O
which	O
in	O
addition	O
to	O
the	O
noun	O
and	O
verb	O
supersenses	O
analyzes	O
English	O
input	O
sentences	O
for	O
MWEs	O
.	O
An	O
example	O
of	O
a	O
sentence	O
annotated	O
with	O
the	O
AMALGrAM	O
tool	O
is	O
given	O
in	O
(	O
1	O
):	O
2	O
(	O
1	O
)	O
(	O
a	O
)	O
"	O
He	O
seemed	O
to	O
have	O
little	O
faith	O
in	O
our	O
democratic	O
structures	O
,	O
suggesting	O
that	O
various	O
articles	O
could	O
be	O
misused	O
by	O
governments	O
.	O
"	O
(	O
b	O
)	O
"	O
He	O
seemed|cognition	O
to	O
have|stative	O
little	O
faith|COGNITION	O
in	O
our	O
democratic	O
structures|ARTIFACT	O
,	O
suggesting|communication	O
that	O
various	O
articles|COMMUNICATION	O
could	O
be|'a	O
misused|social	O
by	O
governments|GROUP	O
.	O
"	O
As	O
can	O
be	O
noted	O
in	O
(	O
1	O
)	O
,	O
some	O
supersenses	O
,	O
such	O
as	O
cognition	O
exist	O
for	O
both	O
nouns	O
and	O
verbs	O
.	O

The	O
supersenses	O
cover	O
all	O
nouns	O
and	O
verbs	O
with	O
a	O
total	O
of	O
41	O
supersense	O
categories	O
,	O
26	O
for	O
nouns	O
and	O
15	O
for	O
verbs	O
.	O

Supersenses	O
are	O
a	O
term	O
which	O
refers	O
to	O
the	O
top	O
-	O
level	O
hypernyms	O
in	O
the	O
WordNet	O
(	O
Miller	O
,	O
1995	O
)	O
taxonomy	O
,	O
sometimes	O
also	O
referred	O
to	O
as	O
semantic	O
fields	O
(	O
Schneider	O
and	O
Smith	O
,	O
2015	O
)	O
.	O

The	O
novelty	O
of	O
our	O
work	O
is	O
the	O
integration	O
of	O
explicit	O
semantic	O
features	O
supersenses	O
into	O
an	O
NMT	O
system	O
.	O

Supersense	O
Tags	O
.	O

3	O
Semantics	O
and	O
Syntax	O
in	O
NMT	O
.	O

Similarly	O
to	O
syntactic	O
features	O
,	O
we	O
hypothesize	O
that	O
semantic	O
features	O
in	O
the	O
form	O
of	O
semantic	O
'	O
classes	O
'	O
can	O
be	O
beneficial	O
for	O
NMT	O
providing	O
it	O
with	O
an	O
extra	O
ability	O
to	O
generalize	O
and	O
thus	O
better	O
learn	O
more	O
complex	O
semanticosyntactic	O
patters	O
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
there	O
has	O
not	O
been	O
any	O
work	O
on	O
explicitly	O
integrating	O
semantic	O
information	O
in	O
NMT	O
.	O

(	O
2017	O
)	O
focus	O
on	O
incorporating	O
sourceside	O
long	O
distance	O
dependencies	O
by	O
enriching	O
each	O
source	O
state	O
with	O
global	O
dependency	O
structure	O
.	O

Wu	O
et	O
al	O
.	O

They	O
focus	O
on	O
source	O
-	O
side	O
syntactic	O
information	O
based	O
on	O
Head	O
-	O
Driven	O
Phrase	O
Structure	O
Grammar	O
(	O
Sag	O
et	O
al	O
.	O
,	O
1999	O
)	O
where	O
target	O
words	O
are	O
aligned	O
not	O
only	O
with	O
the	O
corresponding	O
source	O
words	O
but	O
with	O
the	O
entire	O
source	O
phrase	O
.	O

(	O
2016	O
)	O
integrated	O
syntactic	O
information	O
in	O
the	O
form	O
of	O
linearized	O
parse	O
trees	O
by	O
using	O
an	O
encoder	O
that	O
computes	O
vector	O
representations	O
for	O
each	O
phrase	O
in	O
the	O
source	O
tree	O
.	O

Similarly	O
,	O
Eriguchi	O
et	O
al	O
.	O

A	O
similar	O
observation	O
was	O
made	O
by	O
Li	O
et	O
al	O
(	O
2017	O
2017	O
)	O
observe	O
that	O
the	O
mixed	B-MethodName
RNN	I-MethodName
(	O
the	O
simplest	O
RNN	O
encoder	O
)	O
,	O
where	O
words	O
and	O
label	O
annotation	O
vectors	O
are	O
simply	O
stitched	O
together	O
in	O
the	O
input	O
sequences	O
,	O
yields	O
the	O
best	O
performance	O
with	O
a	O
significant	O
improvement	O
(	O
1.4	B-MetricValue
BLEU	B-MetricName
)	O
.	O

Moreover	O
,	O
they	O
experiment	O
with	O
serializing	O
and	O
multitasking	O
and	O
show	O
that	O
tightly	O
coupling	O
the	O
words	O
with	O
their	O
syntactic	O
features	O
leads	O
to	O
improvements	O
in	O
translation	O
quality	O
(	O
measured	O
by	O
BLEU	B-MetricName
)	O
while	O
a	O
multitask	O
approach	O
(	O
where	O
the	O
NMT	O
predicts	O
CCG	O
supertags	O
and	O
words	O
independently	O
)	O
does	O
not	O
perform	O
better	O
than	O
the	O
baseline	O
system	O
.	O

(	O
2017	O
)	O
extend	O
their	O
work	O
by	O
including	O
CCG	O
supertags	O
as	O
explicit	O
features	O
in	O
a	O
factored	O
NMT	O
systems	O
.	O

Nadejde	O
et	O
al	O
.	O

When	O
evaluating	O
the	O
gains	O
from	O
the	O
features	O
individually	O
,	O
it	O
results	O
that	O
the	O
gain	O
from	O
different	O
features	O
is	O
not	O
fully	O
cumulative	O
.	O

Sennrich	O
and	O
Haddow	O
(	O
2016	O
)	O
show	O
that	O
the	O
inclusion	O
of	O
linguistic	O
fea	O
-	O
tures	O
leads	O
to	O
improvements	O
over	O
the	O
NMT	O
baseline	O
for	O
EN	O
-	O
DE	O
(	O
0.6	B-MetricValue
BLEU	B-MetricName
)	O
,	O
DE	O
-	O
EN	O
(	O
1.5	B-MetricValue
BLEU	B-MetricName
)	O
and	O
EN	O
-	O
RO	O
(	O
1.0	B-MetricValue
BLEU	B-MetricName
)	O
.	O

Furthermore	O
,	O
sometimes	O
information	O
is	O
present	O
in	O
the	O
encoding	O
vectors	O
but	O
is	O
lost	O
during	O
the	O
decoding	O
phase	O
(	O
Vanmassenhove	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

(	O
2016	O
)	O
show	O
that	O
although	O
NMT	O
systems	O
are	O
able	O
to	O
partially	O
learn	O
syntactic	O
information	O
,	O
more	O
complex	O
patterns	O
remain	O
problematic	O
.	O

Similarly	O
,	O
on	O
the	O
syntax	O
level	O
,	O
Shi	O
et	O
al	O
.	O

They	O
then	O
investigated	O
whether	O
features	O
such	O
as	O
lemmas	O
,	O
subword	O
tags	O
,	O
morphological	O
features	O
,	O
POS	O
tags	O
and	O
dependency	O
labels	O
could	O
be	O
useful	O
for	O
NMT	O
systems	O
or	O
whether	O
their	O
inclusion	O
is	O
redundant	O
.	O

By	O
representing	O
the	O
encoder	O
input	O
as	O
a	O
combination	O
of	O
features	O
(	O
Alexandrescu	O
and	O
Kirchhoff	O
,	O
2006	O
)	O
,	O
Sennrich	O
and	O
Haddow	O
(	O
2016	O
)	O
generalized	O
the	O
embedding	O
layer	O
in	O
such	O
a	O
way	O
that	O
an	O
arbitrary	O
number	O
of	O
linguistic	O
features	O
can	O
be	O
explicitly	O
integrated	O
.	O

Furthermore	O
,	O
the	O
encoder	O
and	O
attention	O
layers	O
can	O
be	O
shared	O
between	O
features	O
.	O

The	O
integration	O
of	O
linguistic	O
features	O
can	O
be	O
handled	O
in	O
a	O
flexible	O
way	O
without	O
creating	O
sparsity	O
issues	O
or	O
limiting	O
context	O
information	O
(	O
within	O
the	O
same	O
sentence	O
)	O
.	O

One	O
of	O
the	O
main	O
strengths	O
of	O
NMT	O
is	O
its	O
strong	O
ability	O
to	O
generalize	O
.	O

However	O
,	O
adding	O
syntactic	O
features	O
to	O
SMT	O
systems	O
led	O
to	O
improvements	O
with	O
respect	O
to	O
word	O
order	O
and	O
morphological	O
agreement	O
(	O
Williams	O
and	O
Koehn	O
,	O
2012;Sennrich	O
,	O
2015	O
)	O
.	O

Compared	O
to	O
factored	O
NMT	O
models	O
,	O
factored	O
SMT	O
models	O
have	O
some	O
disadvantages	O
:	O
(	O
a	O
)	O
adding	O
factors	O
increases	O
the	O
sparsity	O
of	O
the	O
models	O
,	O
(	O
b	O
)	O
the	O
n	O
-	O
grams	O
limit	O
the	O
size	O
of	O
context	O
that	O
is	O
taken	O
into	O
account	O
,	O
and	O
(	O
c	O
)	O
features	O
are	O
assumed	O
to	O
be	O
independent	O
of	O
each	O
other	O
.	O

In	O
SMT	O
,	O
various	O
linguistic	O
features	O
such	O
as	O
stems	O
(	O
Toutanova	O
et	O
al	O
.	O
,	O
2008	O
)	O
lemmas	O
(	O
Mareček	O
et	O
al	O
.	O
,	O
2011;Fraser	O
et	O
al	O
.	O
,	O
2012	O
)	O
,	O
POStags	O
(	O
Avramidis	O
and	O
Koehn	O
,	O
2008	O
)	O
,	O
dependency	O
labels	O
(	O
Avramidis	O
and	O
Koehn	O
,	O
2008	O
)	O
and	O
supertags	O
(	O
Hassan	O
et	O
al	O
.	O
,	O
2007;Haque	O
et	O
al	O
.	O
,	O
2009	O
)	O
are	O
integrated	O
using	O
pre	O
-	O
or	O
post	O
-	O
processing	O
techniques	O
often	O
involving	O
factored	O
phrase	O
-	O
based	O
models	O
(	O
Koehn	O
and	O
Hoang	O
,	O
2007	O
)	O
.	O

Related	O
Work	O
.	O

Finally	O
,	O
We	O
conclude	O
and	O
present	O
some	O
of	O
the	O
ideas	O
for	O
future	O
work	O
in	O
Section	O
6	O
.	O

The	O
experimental	O
set	O
-	O
up	O
is	O
described	O
in	O
Section	O
4	O
followed	O
by	O
the	O
results	O
in	O
Section	O
5	O
.	O

Next	O
,	O
Section	O
3	O
presents	O
the	O
semantic	O
and	O
syntactic	O
features	O
used	O
.	O

The	O
remainder	O
of	O
this	O
paper	O
is	O
structured	O
as	O
follows	O
:	O
First	O
,	O
in	O
Section	O
2	O
,	O
the	O
related	O
work	O
is	O
discussed	O
.	O

We	O
further	O
experiment	O
with	O
a	O
combination	O
of	O
semantic	O
supersenses	O
and	O
syntactic	O
supertag	O
features	O
(	O
CCG	O
syntactic	O
categories	O
(	O
Steedman	O
,	O
2000	O
)	O
using	O
EasySRL	O
(	O
Lewis	O
et	O
al	O
.	O
,	O
2015	O
)	O
)	O
and	O
less	O
complex	O
features	O
such	O
as	O
POS	O
-	O
tags	O
,	O
assuming	O
that	O
supersense	O
-	O
tags	O
have	O
the	O
potential	O
to	O
be	O
useful	O
especially	O
in	O
combination	O
with	O
syntactic	O
information	O
.	O

(	O
2016	O
)	O
,	O
replicating	O
the	O
tags	O
for	O
every	O
subword	O
unit	O
obtained	O
by	O
byte	O
-	O
pair	O
encoding	O
(	O
BPE	O
)	O
.	O

The	O
features	O
are	O
integrated	O
using	O
the	O
framework	O
of	O
Sennrich	O
et	O
al	O
.	O

To	O
obtain	O
these	O
features	O
,	O
we	O
used	O
the	O
AMALGrAM	O
2.0	O
tool	O
(	O
Schneider	O
et	O
al	O
.	O
,	O
2014;Schneider	O
and	O
Smith	O
,	O
2015	O
)	O
which	O
analyses	O
the	O
input	O
sentence	O
for	O
Multi	O
-	O
Word	O
Expressions	O
as	O
well	O
as	O
noun	O
and	O
verb	O
supersenses	O
.	O

We	O
investigate	O
the	O
effect	O
of	O
integrating	O
supersense	O
features	O
(	O
26	O
for	O
nouns	O
,	O
15	O
for	O
verbs	O
)	O
into	O
an	O
NMT	O
system	O
.	O

Inspired	O
by	O
Named	O
Entity	O
Recognition	O
which	O
provides	O
such	O
abstractions	O
for	O
a	O
limited	O
set	O
of	O
words	O
,	O
supersense	O
-	O
tagging	O
uses	O
an	O
inventory	O
of	O
more	O
general	O
semantic	O
classes	O
for	O
domain	O
-	O
independent	O
settings	O
(	O
Schneider	O
and	O
Smith	O
,	O
2015	O
)	O
.	O

To	O
apply	O
semantic	O
abstractions	O
at	O
the	O
word	O
-	O
level	O
that	O
enable	O
a	O
characterisation	O
beyond	O
that	O
what	O
can	O
be	O
superficially	O
derived	O
,	O
coarse	O
-	O
grained	O
semantic	O
classes	O
can	O
be	O
used	O
.	O

Furthermore	O
,	O
a	O
combination	O
of	O
both	O
syntactic	O
and	O
semantic	O
features	O
would	O
provide	O
the	O
NMT	O
system	O
with	O
a	O
way	O
of	O
learning	O
semantico	O
-	O
syntactic	O
patterns	O
.	O

However	O
,	O
making	O
some	O
level	O
of	O
semantics	O
more	O
explicitly	O
available	O
at	O
the	O
word	O
level	O
can	O
provide	O
the	O
translation	O
system	O
with	O
a	O
higher	O
level	O
of	O
abstraction	O
beneficial	O
to	O
learn	O
more	O
complex	O
constructions	O
.	O

This	O
might	O
be	O
explained	O
by	O
the	O
fact	O
that	O
NMT	O
models	O
already	O
have	O
means	O
of	O
learning	O
semantic	O
similarities	O
through	O
word	O
-	O
embeddings	O
,	O
where	O
words	O
are	O
represented	O
in	O
a	O
common	O
vector	O
space	O
(	O
Mikolov	O
et	O
al	O
.	O
,	O
2013	O
)	O
.	O

Although	O
there	O
has	O
been	O
some	O
work	O
on	O
semantic	O
features	O
for	O
SMT	O
(	O
Banchs	O
and	O
Costa	O
-	O
Jussà	O
,	O
2011	O
)	O
,	O
so	O
far	O
,	O
no	O
work	O
has	O
been	O
done	O
on	O
enriching	O
NMT	O
systems	O
with	O
more	O
general	O
semantic	O
features	O
at	O
the	O
word	O
-	O
level	O
.	O

When	O
integrating	O
linguistic	O
information	O
into	O
an	O
MT	O
system	O
,	O
following	O
the	O
central	O
role	O
assigned	O
to	O
syntax	O
by	O
many	O
linguists	O
,	O
the	O
focus	O
has	O
been	O
mainly	O
on	O
the	O
integration	O
of	O
syntactic	O
features	O
.	O

More	O
recent	O
work	O
showed	O
that	O
explicitly	O
(	O
Sennrich	O
and	O
Haddow	O
,	O
2016;Nadejde	O
et	O
al	O
.	O
,	O
2017;Bastings	O
et	O
al	O
.	O
,	O
2017;Aharoni	O
and	O
Goldberg	O
,	O
2017	O
)	O
or	O
implicitly	O
(	O
Eriguchi	O
et	O
al	O
.	O
,	O
2017	O
)	O
modeling	O
extra	O
syntactic	O
information	O
into	O
an	O
NMT	O
system	O
on	O
the	O
source	O
(	O
and/or	O
target	O
)	O
side	O
could	O
lead	O
to	O
improvements	O
in	O
translation	O
quality	O
.	O

Although	O
NMT	B-TaskName
seems	O
to	O
partially	O
'	O
learn	O
'	O
or	O
generalize	O
some	O
patterns	O
related	O
to	O
syntax	O
from	O
the	O
raw	O
,	O
sentence	O
-	O
aligned	O
parallel	O
data	O
,	O
more	O
complex	O
phenomena	O
(	O
e.g.	O
prepositional	O
-	O
phrase	O
attachment	O
)	O
remain	O
problematic	O
(	O
Bentivogli	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

Compared	O
to	O
Statistical	O
Machine	O
Translation	O
(	O
SMT	O
)	O
,	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
,	O
NMT	B-TaskName
performs	O
particularly	O
well	O
when	O
it	O
comes	O
to	O
word	O
-	O
reorderings	O
and	O
translations	O
involving	O
morphologically	O
rich	O
languages	O
(	O
Bentivogli	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

Neural	O
Machine	O
Translation	O
(	O
NMT	O
)	O
models	O
have	O
recently	O
become	O
the	O
state	O
-	O
of	O
-	O
the	O
art	O
in	O
the	O
field	O
of	O
Machine	B-TaskName
Translation	I-TaskName
(	O
Bahdanau	O
et	O
al	O
.	O
,	O
2014;Cho	O
et	O
al	O
.	O
,	O
2014;Kalchbrenner	O
et	O
al	O
.	O
,	O
2014;Sutskever	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

Introduction	O
.	O

In	O
experiments	O
on	O
various	O
test	O
sets	O
,	O
we	O
observe	O
that	O
such	O
features	O
(	O
and	O
particularly	O
when	O
combined	O
)	O
help	O
the	O
NMT	O
model	O
training	O
to	O
converge	O
faster	O
and	O
improve	O
the	O
model	O
quality	O
according	O
to	O
the	O
BLEU	B-MetricName
scores	O
.	O

In	O
this	O
paper	O
we	O
incorporate	O
semantic	O
supersensetags	O
and	O
syntactic	O
supertag	O
features	O
into	O
EN	O
-	O
FR	O
and	O
EN	O
-	O
DE	O
factored	O
NMT	O
systems	O
.	O

SuperNMT	B-MethodName
:	O
Neural	B-MethodName
Machine	I-MethodName
Translation	I-MethodName
with	I-MethodName
Semantic	I-MethodName
Supersenses	I-MethodName
and	I-MethodName
Syntactic	I-MethodName
Supertags	I-MethodName
.	O

