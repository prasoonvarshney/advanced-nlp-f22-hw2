Unlike	O
recent	O
language	O
representation	O
models	O
(	O
Peters	O
et	O
al	O
.	O
,	O
2018a;Radford	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
BERT	B-MethodName
is	O
designed	O
to	O
pretrain	O
deep	O
bidirectional	O
representations	O
from	O
unlabeled	O
text	O
by	O
jointly	O
conditioning	O
on	O
both	O
left	O
and	O
right	O
context	O
in	O
all	O
layers	O
.	O

As	O
a	O
result	O
,	O
the	O
pre	O
-	O
trained	O
BERT	B-MethodName
model	O
can	O
be	O
finetuned	O
with	O
just	O
one	O
additional	O
output	O
layer	O
to	O
create	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
for	O
a	O
wide	O
range	O
of	O
tasks	O
,	O
such	O
as	O
question	B-TaskName
answering	I-TaskName
and	O
language	B-TaskName
inference	I-TaskName
,	O
without	O
substantial	O
taskspecific	O
architecture	O
modifications	O
.	O

BERT	B-MethodName
is	O
conceptually	O
simple	O
and	O
empirically	O
powerful	O
.	O

Introduction	O
.	O

Language	O
model	O
pre	O
-	O
training	O
has	O
been	O
shown	O
to	O
be	O
effective	O
for	O
improving	O
many	O
natural	O
language	O
processing	O
tasks	O
(	O
Dai	O
and	O
Le	O
,	O
2015	O
;	O
Peters	O
et	O
al	O
.	O
,	O
2018a;Radford	O
et	O
al	O
.	O
,	O
2018;Howard	O
and	O
Ruder	O
,	O
2018	O
)	O
.	O

These	O
include	O
sentence	O
-	O
level	O
tasks	O
such	O
as	O
natural	B-TaskName
language	I-TaskName
inference	I-TaskName
(	O
Bowman	O
et	O
al	O
.	O
,	O
2015;Williams	O
et	O
al	O
.	O
,	O
2018	O
)	O
and	O
paraphrasing	O
(	O
Dolan	O
and	O
Brockett	O
,	O
2005	O
)	O
,	O
which	O
aim	O
to	O
predict	O
the	O
relationships	O
between	O
sentences	O
by	O
analyzing	O
them	O
holistically	O
,	O
as	O
well	O
as	O
token	O
-	O
level	O
tasks	O
such	O
as	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
and	O
question	B-TaskName
answering	I-TaskName
,	O
where	O
models	O
are	O
required	O
to	O
produce	O
fine	O
-	O
grained	O
output	O
at	O
the	O
token	O
level	O
(	O
Tjong	O
Kim	O
Sang	O
and	O
De	O
Meulder	O
,	O
2003;Rajpurkar	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

There	O
are	O
two	O
existing	O
strategies	O
for	O
applying	O
pre	O
-	O
trained	O
language	O
representations	O
to	O
downstream	O
tasks	O
:	O
feature	O
-	O
based	O
and	O
fine	O
-	O
tuning	O
.	O

The	O
feature	O
-	O
based	O
approach	O
,	O
such	O
as	O
ELMo	B-MethodName
(	O
Peters	O
et	O
al	O
.	O
,	O
2018a	O
)	O
,	O
uses	O
task	O
-	O
specific	O
architectures	O
that	O
include	O
the	O
pre	O
-	O
trained	O
representations	O
as	O
additional	O
features	O
.	O

The	O
fine	O
-	O
tuning	O
approach	O
,	O
such	O
as	O
the	O
Generative	B-MethodName
Pre	I-MethodName
-	I-MethodName
trained	I-MethodName
Transformer	I-MethodName
(	O
OpenAI	B-MethodName
GPT	I-MethodName
)	O
(	O
Radford	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
introduces	O
minimal	O
task	O
-	O
specific	O
parameters	O
,	O
and	O
is	O
trained	O
on	O
the	O
downstream	O
tasks	O
by	O
simply	O
fine	O
-	O
tuning	O
all	O
pretrained	O
parameters	O
.	O

The	O
two	O
approaches	O
share	O
the	O
same	O
objective	O
function	O
during	O
pre	O
-	O
training	O
,	O
where	O
they	O
use	O
unidirectional	O
language	O
models	O
to	O
learn	O
general	O
language	O
representations	O
.	O

We	O
argue	O
that	O
current	O
techniques	O
restrict	O
the	O
power	O
of	O
the	O
pre	O
-	O
trained	O
representations	O
,	O
especially	O
for	O
the	O
fine	O
-	O
tuning	O
approaches	O
.	O

The	O
major	O
limitation	O
is	O
that	O
standard	O
language	O
models	O
are	O
unidirectional	O
,	O
and	O
this	O
limits	O
the	O
choice	O
of	O
architectures	O
that	O
can	O
be	O
used	O
during	O
pre	O
-	O
training	O
.	O

Such	O
restrictions	O
are	O
sub	O
-	O
optimal	O
for	O
sentence	O
-	O
level	O
tasks	O
,	O
and	O
could	O
be	O
very	O
harmful	O
when	O
applying	O
finetuning	O
based	O
approaches	O
to	O
token	O
-	O
level	O
tasks	O
such	O
as	O
question	B-TaskName
answering	I-TaskName
,	O
where	O
it	O
is	O
crucial	O
to	O
incorporate	O
context	O
from	O
both	O
directions	O
.	O

In	O
this	O
paper	O
,	O
we	O
improve	O
the	O
fine	O
-	O
tuning	O
based	O
approaches	O
by	O
proposing	O
BERT	B-MethodName
:	O
Bidirectional	B-MethodName
Encoder	I-MethodName
Representations	I-MethodName
from	I-MethodName
Transformers	I-MethodName
.	O

The	O
masked	O
language	O
model	O
randomly	O
masks	O
some	O
of	O
the	O
tokens	O
from	O
the	O
input	O
,	O
and	O
the	O
objective	O
is	O
to	O
predict	O
the	O
original	O
vocabulary	O
i	O
d	O
of	O
the	O
masked	O
word	O
based	O
only	O
on	O
its	O
context	O
.	O

Unlike	O
left	O
-	O
toright	O
language	O
model	O
pre	O
-	O
training	O
,	O
the	O
MLM	O
objective	O
enables	O
the	O
representation	O
to	O
fuse	O
the	O
left	O
and	O
the	O
right	O
context	O
,	O
which	O
allows	O
us	O
to	O
pretrain	O
a	O
deep	O
bidirectional	O
Transformer	O
.	O

Unlike	O
Radford	O
et	O
al	O
.	O

This	O
is	O
also	O
in	O
contrast	O
to	O
Peters	O
et	O
al	O
.	O

(	O
2018a	O
)	O
,	O
which	O
uses	O
a	O
shallow	O
concatenation	O
of	O
independently	O
trained	O
left	O
-	O
to	O
-	O
right	O
and	O
right	O
-	O
to	O
-	O
left	O
LMs	O
.	O
•	O
We	O
show	O
that	O
pre	O
-	O
trained	O
representations	O
reduce	O
the	O
need	O
for	O
many	O
heavily	O
-	O
engineered	O
taskspecific	O
architectures	O
.	O

BERT	B-MethodName
is	O
the	O
first	O
finetuning	O
based	O
representation	O
model	O
that	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
a	O
large	O
suite	O
of	O
sentence	O
-	O
level	O
and	O
token	O
-	O
level	O
tasks	O
,	O
outperforming	O
many	O
task	O
-	O
specific	O
architectures	O
.	O

•	O
BERT	B-MethodName
advances	O
the	O
state	O
of	O
the	O
art	O
for	O
eleven	O
NLP	O
tasks	O
.	O

Related	O
Work	O
.	O

There	O
is	O
a	O
long	O
history	O
of	O
pre	O
-	O
training	O
general	O
language	O
representations	O
,	O
and	O
we	O
briefly	O
review	O
the	O
most	O
widely	O
-	O
used	O
approaches	O
in	O
this	O
section	O
.	O

Unsupervised	O
Feature	O
-	O
based	O
Approaches	O
.	O

Learning	O
widely	O
applicable	O
representations	O
of	O
words	O
has	O
been	O
an	O
active	O
area	O
of	O
research	O
for	O
decades	O
,	O
including	O
non	O
-	O
neural	O
(	O
Brown	O
et	O
al	O
.	O
,	O
1992;Ando	O
and	O
Zhang	O
,	O
2005;Blitzer	O
et	O
al	O
.	O
,	O
2006	O
)	O
and	O
neural	O
(	O
Mikolov	O
et	O
al	O
.	O
,	O
2013;Pennington	O
et	O
al	O
.	O
,	O
2014	O
)	O
methods	O
.	O

Pre	O
-	O
trained	O
word	O
embeddings	O
are	O
an	O
integral	O
part	O
of	O
modern	O
NLP	O
systems	O
,	O
offering	O
significant	O
improvements	O
over	O
embeddings	O
learned	O
from	O
scratch	O
(	O
Turian	O
et	O
al	O
.	O
,	O
2010	O
)	O
.	O

To	O
pretrain	O
word	O
embedding	O
vectors	O
,	O
left	O
-	O
to	O
-	O
right	O
language	O
modeling	O
objectives	O
have	O
been	O
used	O
(	O
Mnih	O
and	O
Hinton	O
,	O
2009	O
)	O
,	O
as	O
well	O
as	O
objectives	O
to	O
discriminate	O
correct	O
from	O
incorrect	O
words	O
in	O
left	O
and	O
right	O
context	O
(	O
Mikolov	O
et	O
al	O
.	O
,	O
2013	O
)	O
.	O

These	O
approaches	O
have	O
been	O
generalized	O
to	O
coarser	O
granularities	O
,	O
such	O
as	O
sentence	O
embeddings	O
(	O
Kiros	O
et	O
al	O
.	O
,	O
2015;Logeswaran	O
and	O
Lee	O
,	O
2018	O
)	O
or	O
paragraph	O
embeddings	O
(	O
Le	O
and	O
Mikolov	O
,	O
2014	O
)	O
.	O

ELMo	B-MethodName
and	O
its	O
predecessor	O
(	O
Peters	O
et	O
al	O
.	O
,	O
2017(Peters	O
et	O
al	O
.	O
,	O
,	O
2018a	O
)	O
)	O
generalize	O
traditional	O
word	O
embedding	O
research	O
along	O
a	O
different	O
dimension	O
.	O

They	O
extract	O
context	O
-	O
sensitive	O
features	O
from	O
a	O
left	O
-	O
to	O
-	O
right	O
and	O
a	O
right	O
-	O
to	O
-	O
left	O
language	O
model	O
.	O

The	O
contextual	O
representation	O
of	O
each	O
token	O
is	O
the	O
concatenation	O
of	O
the	O
left	O
-	O
to	O
-	O
right	O
and	O
right	O
-	O
to	O
-	O
left	O
representations	O
.	O

Melamud	O
et	O
al	O
.	O

We	O
introduce	O
a	O
new	O
language	O
representation	O
model	O
called	O
BERT	B-MethodName
,	O
which	O
stands	O
for	O
Bidirectional	B-MethodName
Encoder	I-MethodName
Representations	I-MethodName
from	I-MethodName
Transformers	I-MethodName
.	O

For	O
example	O
,	O
in	O
OpenAI	B-MethodName
GPT	I-MethodName
,	O
the	O
authors	O
use	O
a	O
left	O
-	O
toright	O
architecture	O
,	O
where	O
every	O
token	O
can	O
only	O
attend	O
to	O
previous	O
tokens	O
in	O
the	O
self	O
-	O
attention	O
layers	O
of	O
the	O
Transformer	B-MethodName
(	O
Vaswani	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

(	O
2018	O
)	O
,	O
which	O
uses	O
unidirectional	O
language	O
models	O
for	O
pre	O
-	O
training	O
,	O
BERT	B-MethodName
uses	O
masked	O
language	O
models	O
to	O
enable	O
pretrained	O
deep	O
bidirectional	O
representations	O
.	O

When	O
integrating	O
contextual	O
word	O
embeddings	O
with	O
existing	O
task	O
-	O
specific	O
architectures	O
,	O
ELMo	B-MethodName
advances	O
the	O
state	O
of	O
the	O
art	O
for	O
several	O
major	O
NLP	O
benchmarks	O
(	O
Peters	O
et	O
al	O
.	O
,	O
2018a	O
)	O
including	O
question	B-TaskName
answering	I-TaskName
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O
2016	O
)	O
,	O
sentiment	B-TaskName
analysis	I-TaskName
(	O
Socher	O
et	O
al	O
.	O
,	O
2013	O
)	O
,	O
and	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
Tjong	O
Kim	O
Sang	O
and	O
De	O
Meulder	O
,	O
2003	O
)	O
.	O

(	O
2016	O
)	O
proposed	O
learning	O
contextual	O
representations	O
through	O
a	O
task	O
to	O
predict	O
a	O
single	O
word	O
from	O
both	O
left	O
and	O
right	O
context	O
using	O
LSTMs	B-MethodName
.	O
Similar	O
to	O
ELMo	B-MethodName
,	O
their	O
model	O
is	O
feature	O
-	O
based	O
and	O
not	O
deeply	O
bidirectional	O
.	O

Fedus	O
et	O
al	O
.	O

Unsupervised	O
Fine	O
-	O
tuning	O
Approaches	O
.	O

As	O
with	O
the	O
feature	O
-	O
based	O
approaches	O
,	O
the	O
first	O
works	O
in	O
this	O
direction	O
only	O
pre	O
-	O
trained	O
word	O
embedding	O
parameters	O
from	O
unlabeled	O
text	O
(	O
Collobert	O
and	O
Weston	O
,	O
2008	O
)	O
.	O

At	O
least	O
partly	O
due	O
to	O
this	O
advantage	O
,	O
OpenAI	B-MethodName
GPT	I-MethodName
(	O
Radford	O
et	O
al	O
.	O
,	O
2018	O
)	O
achieved	O
previously	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
many	O
sentencelevel	O
tasks	O
from	O
the	O
GLUE	B-DatasetName
benchmark	O
(	O
Wang	O
et	O
al	O
.	O
,	O
2018a	O
)	O
.	O

The	O
advantage	O
of	O
these	O
approaches	O
is	O
that	O
few	O
parameters	O
need	O
to	O
be	O
learned	O
from	O
scratch	O
.	O

Left	O
-	O
to	O
-	O
right	O
language	O
model	O
-	O
BERT	B-MethodName
BERT	B-MethodName
E	O
[	O
CLS	O
]	O
E	O
1	O
E	O
[	O
SEP	O
]	O
...	O

E	O
N	O
E	O
1	O
'	O
...	O

E	O
M	O
'	O
C	O
T	O
1	O
T	O
[	O
SEP	O
]	O
...	O

...	O

...	O

E	O
N	O
E	O
1	O
'	O
...	O

E	O
M	O
'	O
C	O
T	O
1	O
T	O
[	O
SEP	O
]	O
...	O

ing	O
and	O
auto	B-MethodName
-	I-MethodName
encoder	I-MethodName
objectives	O
have	O
been	O
used	O
for	O
pre	O
-	O
training	O
such	O
models	O
(	O
Howard	O
and	O
Ruder	O
,	O
2018;Radford	O
et	O
al	O
.	O
,	O
2018;Dai	O
and	O
Le	O
,	O
2015	O
)	O
.	O

Transfer	O
Learning	O
from	O
Supervised	O
Data	O
.	O

There	O
has	O
also	O
been	O
work	O
showing	O
effective	O
transfer	O
from	O
supervised	O
tasks	O
with	O
large	O
datasets	O
,	O
such	O
as	O
natural	B-TaskName
language	I-TaskName
inference	I-TaskName
(	O
Conneau	O
et	O
al	O
.	O
,	O
2017	O
)	O
and	O
machine	B-TaskName
translation	I-TaskName
(	O
McCann	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

BERT	B-MethodName
.	O

Computer	O
vision	O
research	O
has	O
also	O
demonstrated	O
the	O
importance	O
of	O
transfer	O
learning	O
from	O
large	O
pre	O
-	O
trained	O
models	O
,	O
where	O
an	O
effective	O
recipe	O
is	O
to	O
fine	O
-	O
tune	O
models	O
pre	O
-	O
trained	O
with	O
Ima	B-DatasetName
-	I-DatasetName
geNet	I-DatasetName
(	O
Deng	O
et	O
al	O
.	O
,	O
2009;Yosinski	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

We	O
introduce	O
BERT	B-MethodName
and	O
its	O
detailed	O
implementation	O
in	O
this	O
section	O
.	O

There	O
are	O
two	O
steps	O
in	O
our	O
framework	O
:	O
pre	O
-	O
training	O
and	O
fine	O
-	O
tuning	O
.	O

During	O
pre	O
-	O
training	O
,	O
the	O
model	O
is	O
trained	O
on	O
unlabeled	O
data	O
over	O
different	O
pre	O
-	O
training	O
tasks	O
.	O

For	O
finetuning	O
,	O
the	O
BERT	B-MethodName
model	O
is	O
first	O
initialized	O
with	O
the	O
pre	O
-	O
trained	O
parameters	O
,	O
and	O
all	O
of	O
the	O
parameters	O
are	O
fine	O
-	O
tuned	O
using	O
labeled	O
data	O
from	O
the	O
downstream	O
tasks	O
.	O

Each	O
downstream	O
task	O
has	O
separate	O
fine	O
-	O
tuned	O
models	O
,	O
even	O
though	O
they	O
are	O
initialized	O
with	O
the	O
same	O
pre	O
-	O
trained	O
parameters	O
.	O

A	O
distinctive	O
feature	O
of	O
BERT	B-MethodName
is	O
its	O
unified	O
architecture	O
across	O
different	O
tasks	O
.	O

The	O
question	B-TaskName
-	I-TaskName
answering	I-TaskName
example	O
in	O
Figure	O
1	O
will	O
serve	O
as	O
a	O
running	O
example	O
for	O
this	O
section	O
.	O

There	O
is	O
mini	O
-	O
mal	O
difference	O
between	O
the	O
pre	O
-	O
trained	O
architecture	O
and	O
the	O
final	O
downstream	O
architecture	O
.	O

(	O
2017	O
)	O
and	O
released	O
in	O
the	O
tensor2tensor	O
library	O
.	O

(	O
2017	O
)	O
as	O
well	O
as	O
excellent	O
guides	O
such	O
as	O
"	O
The	O
Annotated	O
Transformer	O
.	O
"	O
2	O
In	O
this	O
work	O
,	O
we	O
denote	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
layers	I-HyperparameterName
(	O
i.e.	O
,	O
Transformer	O
blocks	O
)	O
as	O
L	B-HyperparameterName
,	O
the	O
hidden	B-HyperparameterName
size	I-HyperparameterName
as	O
H	B-HyperparameterName
,	O
and	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
self	I-HyperparameterName
-	I-HyperparameterName
attention	I-HyperparameterName
heads	I-HyperparameterName
as	O
A.	B-HyperparameterName

3	O
We	O
primarily	O
report	O
results	O
on	O
two	O
model	O
sizes	O
:	O
BERT	B-MethodName
BASE	I-MethodName
(	O
L=12	B-HyperparameterName
,	O
H=768	B-HyperparameterName
,	O
A=12	B-HyperparameterValue
,	O
Total	B-HyperparameterName
Param	I-HyperparameterName
-	I-HyperparameterName
eters=110	I-HyperparameterName
M	B-HyperparameterValue
)	O
and	O
BERT	B-MethodName
LARGE	I-MethodName
(	O
L=24	B-HyperparameterName
,	O
H=1024	B-HyperparameterName
,	O
A=16	B-HyperparameterName
,	O
Total	B-HyperparameterName
Parameters=340	I-HyperparameterName
M	B-HyperparameterValue
)	O
.	O

BERT	B-MethodName
BASE	I-MethodName
was	O
chosen	O
to	O
have	O
the	O
same	O
model	O
size	O
as	O
OpenAI	B-MethodName
GPT	I-MethodName
for	O
comparison	O
purposes	O
.	O

Critically	O
,	O
however	O
,	O
the	O
BERT	B-MethodName
Transformer	O
uses	O
bidirectional	O
self	O
-	O
attention	O
,	O
while	O
the	O
GPT	B-MethodName
Transformer	O
uses	O
constrained	O
self	O
-	O
attention	O
where	O
every	O
token	O
can	O
only	O
attend	O
to	O
context	O
to	O
its	O
left	O
.	O

4	O
Input	O
/	O
Output	O
Representations	O
To	O
make	O
BERT	B-MethodName
handle	O
a	O
variety	O
of	O
down	O
-	O
stream	O
tasks	O
,	O
our	O
input	O
representation	O
is	O
able	O
to	O
unambiguously	O
represent	O
both	O
a	O
single	O
sentence	O
and	O
a	O
pair	O
of	O
sentences	O
(	O
e.g.	O
,	O
Question	O
,	O
Answer	O
)	O
in	O
one	O
token	O
sequence	O
.	O

Throughout	O
this	O
work	O
,	O
a	O
"	O
sentence	O
"	O
can	O
be	O
an	O
arbitrary	O
span	O
of	O
contiguous	O
text	O
,	O
rather	O
than	O
an	O
actual	O
linguistic	O
sentence	O
.	O

A	O
"	O
sequence	O
"	O
refers	O
to	O
the	O
input	O
token	O
sequence	O
to	O
BERT	B-MethodName
,	O
which	O
may	O
be	O
a	O
single	O
sentence	O
or	O
two	O
sentences	O
packed	O
together	O
.	O

We	O
use	O
WordPiece	O
embeddings	O
(	O
Wu	O
et	O
al	O
.	O
,	O
2016	O
)	O
with	O
a	O
30,000	O
token	O
vocabulary	O
.	O

The	O
first	O
token	O
of	O
every	O
sequence	O
is	O
always	O
a	O
special	O
classification	O
token	O
(	O
[	O
CLS	O
]	O
)	O
.	O

The	O
final	O
hidden	O
state	O
corresponding	O
to	O
this	O
token	O
is	O
used	O
as	O
the	O
aggregate	O
sequence	O
representation	O
for	O
classification	O
tasks	O
.	O

Sentence	O
pairs	O
are	O
packed	O
together	O
into	O
a	O
single	O
sequence	O
.	O

We	O
differentiate	O
the	O
sentences	O
in	O
two	O
ways	O
.	O

First	O
,	O
we	O
separate	O
them	O
with	O
a	O
special	O
token	O
(	O
[	O
SEP	O
]	O
)	O
.	O

Second	O
,	O
we	O
add	O
a	O
learned	O
embedding	O
to	O
every	O
token	O
indicating	O
whether	O
it	O
belongs	O
to	O
sentence	O
A	O
or	O
sentence	O
B.	O

As	O
shown	O
in	O
Figure	O
1	O
,	O
we	O
denote	O
input	O
embedding	O
as	O
E	O
,	O
the	O
final	O
hidden	O
vector	O
of	O
the	O
special	O
[	O
CLS	O
]	O
token	O
as	O
C	O
∈	O
R	O
H	O
,	O
and	O
the	O
final	O
hidden	O
vector	O
for	O
the	O
i	O
th	O
input	O
token	O
as	O
T	O
i	O
∈	O
R	O
H	O
.	O

For	O
a	O
given	O
token	O
,	O
its	O
input	O
representation	O
is	O
constructed	O
by	O
summing	O
the	O
corresponding	O
token	O
,	O
segment	O
,	O
and	O
position	O
embeddings	O
.	O

A	O
visualization	O
of	O
this	O
construction	O
can	O
be	O
seen	O
in	O
Figure	O
2	O
.	O

Pre	O
-	O
training	O
BERT	B-MethodName
.	O

Unlike	O
Peters	O
et	O
al	O
.	O

(	O
2018a	O
)	O
and	O
Radford	O
et	O
al	O
.	O

(	O
2018	O
)	O
,	O
we	O
do	O
not	O
use	O
traditional	O
left	O
-	O
to	O
-	O
right	O
or	O
right	O
-	O
to	O
-	O
left	O
language	O
models	O
to	O
pre	O
-	O
train	O
BERT	B-MethodName
.	O

Instead	O
,	O
we	O
pre	O
-	O
train	O
BERT	B-MethodName
using	O
two	O
unsupervised	O
tasks	O
,	O
described	O
in	O
this	O
section	O
.	O

This	O
step	O
is	O
presented	O
in	O
the	O
left	O
part	O
of	O
Figure	O
1	O
.	O

Unfortunately	O
,	O
standard	O
conditional	O
language	O
models	O
can	O
only	O
be	O
trained	O
left	O
-	O
to	O
-	O
right	O
or	O
right	O
-	O
to	O
-	O
left	O
,	O
since	O
bidirectional	O
conditioning	O
would	O
allow	O
each	O
word	O
to	O
indirectly	O
"	O
see	O
itself	O
"	O
,	O
and	O
the	O
model	O
could	O
trivially	O
predict	O
the	O
target	O
word	O
in	O
a	O
multi	O
-	O
layered	O
context	O
.	O

former	O
is	O
often	O
referred	O
to	O
as	O
a	O
"	O
Transformer	O
encoder	O
"	O
while	O
the	O
left	O
-	O
context	O
-	O
only	O
version	O
is	O
referred	O
to	O
as	O
a	O
"	O
Transformer	O
decoder	O
"	O
since	O
it	O
can	O
be	O
used	O
for	O
text	O
generation	O
.	O

In	O
order	O
to	O
train	O
a	O
deep	O
bidirectional	O
representation	O
,	O
we	O
simply	O
mask	O
some	O
percentage	O
of	O
the	O
input	O
tokens	O
at	O
random	O
,	O
and	O
then	O
predict	O
those	O
masked	O
tokens	O
.	O

In	O
this	O
case	O
,	O
the	O
final	O
hidden	O
vectors	O
corresponding	O
to	O
the	O
mask	O
tokens	O
are	O
fed	O
into	O
an	O
output	O
softmax	O
over	O
the	O
vocabulary	O
,	O
as	O
in	O
a	O
standard	O
LM	O
.	O

In	O
contrast	O
to	O
denoising	B-MethodName
auto	I-MethodName
-	I-MethodName
encoders	I-MethodName
(	O
Vincent	O
et	O
al	O
.	O
,	O
2008	O
)	O
,	O
we	O
only	O
predict	O
the	O
masked	O
words	O
rather	O
than	O
reconstructing	O
the	O
entire	O
input	O
.	O

Although	O
this	O
allows	O
us	O
to	O
obtain	O
a	O
bidirectional	O
pre	O
-	O
trained	O
model	O
,	O
a	O
downside	O
is	O
that	O
we	O
are	O
creating	O
a	O
mismatch	O
between	O
pre	O
-	O
training	O
and	O
fine	O
-	O
tuning	O
,	O
since	O
the	O
[	O
MASK	O
]	O
token	O
does	O
not	O
appear	O
during	O
fine	O
-	O
tuning	O
.	O

To	O
mitigate	O
this	O
,	O
we	O
do	O
not	O
always	O
replace	O
"	O
masked	O
"	O
words	O
with	O
the	O
actual	O
[	O
MASK	O
]	O
token	O
.	O

The	O
training	O
data	O
generator	O
chooses	O
15	O
%	O
of	O
the	O
token	O
positions	O
at	O
random	O
for	O
prediction	O
.	O

If	O
the	O
i	O
-	O
th	O
token	O
is	O
chosen	O
,	O
we	O
replace	O
the	O
i	O
-	O
th	O
token	O
with	O
(	O
1	O
)	O
the	O
[	O
MASK	O
]	O
token	O
80	O
%	O
of	O
the	O
time	O
(	O
2	O
)	O
a	O
random	O
token	O
10	O
%	O
of	O
the	O
time	O
(	O
3	O
)	O
the	O
unchanged	O
i	O
-	O
th	O
token	O
10	O
%	O
of	O
the	O
time	O
.	O

In	O
all	O
of	O
our	O
experiments	O
,	O
we	O
mask	O
15	O
%	O
of	O
all	O
WordPiece	O
tokens	O
in	O
each	O
sequence	O
at	O
random	O
.	O

Then	O
,	O
T	O
i	O
will	O
be	O
used	O
to	O
predict	O
the	O
original	O
token	O
with	O
cross	O
entropy	O
loss	O
.	O

In	O
order	O
to	O
train	O
a	O
model	O
that	O
understands	O
sentence	O
relationships	O
,	O
we	O
pre	O
-	O
train	O
for	O
a	O
binarized	O
next	O
sentence	O
prediction	O
task	O
that	O
can	O
be	O
trivially	O
generated	O
from	O
any	O
monolingual	O
corpus	O
.	O

Specifically	O
,	O
when	O
choosing	O
the	O
sentences	O
A	O
and	O
B	O
for	O
each	O
pretraining	O
example	O
,	O
50	O
%	O
of	O
the	O
time	O
B	O
is	O
the	O
actual	O
next	O
sentence	O
that	O
follows	O
A	O
(	O
labeled	O
as	O
IsNext	O
)	O
,	O
and	O
50	O
%	O
of	O
the	O
time	O
it	O
is	O
a	O
random	O
sentence	O
from	O
the	O
corpus	O
(	O
labeled	O
as	O
NotNext	O
)	O
.	O

5	O
Despite	O
its	O
simplicity	O
,	O
we	O
demonstrate	O
in	O
Section	O
5.1	O
that	O
pre	O
-	O
training	O
towards	O
this	O
task	O
is	O
very	O
beneficial	O
to	O
both	O
QA	B-TaskName
and	O
NLI	B-TaskName
.	O

(	O
2017	O
)	O
and	O
Logeswaran	O
and	O
Lee	O
(	O
2018	O
)	O
.	O

However	O
,	O
in	O
prior	O
work	O
,	O
only	O
sentence	O
embeddings	O
are	O
transferred	O
to	O
down	O
-	O
stream	O
tasks	O
,	O
where	O
BERT	B-MethodName
transfers	O
all	O
parameters	O
to	O
initialize	O
end	O
-	O
task	O
model	O
parameters	O
.	O

Pre	O
-	O
training	O
data	O
.	O

The	O
pre	O
-	O
training	O
procedure	O
largely	O
follows	O
the	O
existing	O
literature	O
on	O
language	O
model	O
pre	O
-	O
training	O
.	O

For	O
the	O
pre	O
-	O
training	O
corpus	O
we	O
use	O
the	O
BooksCorpus	B-DatasetName
(	O
800	O
M	O
words	O
)	O
(	O
Zhu	O
et	O
al	O
.	O
,	O
2015	O
)	O
and	O
English	B-DatasetName
Wikipedia	I-DatasetName
(	O
2,500	O
M	O
words	O
)	O
.	O

For	O
Wikipedia	O
we	O
extract	O
only	O
the	O
text	O
passages	O
and	O
ignore	O
lists	O
,	O
tables	O
,	O
and	O
headers	O
.	O

It	O
is	O
critical	O
to	O
use	O
a	O
document	O
-	O
level	O
corpus	O
rather	O
than	O
a	O
shuffled	O
sentence	O
-	O
level	O
corpus	O
such	O
as	O
the	O
Billion	B-DatasetName
Word	I-DatasetName
Benchmark	I-DatasetName
(	O
Chelba	O
et	O
al	O
.	O
,	O
2013	O
)	O
in	O
order	O
to	O
extract	O
long	O
contiguous	O
sequences	O
.	O

Fine	O
-	O
tuning	O
BERT	O
.	O

Fine	O
-	O
tuning	O
is	O
straightforward	O
since	O
the	O
selfattention	O
mechanism	O
in	O
the	O
Transformer	B-MethodName
allows	O
BERT	B-MethodName
to	O
model	O
many	O
downstream	O
taskswhether	O
they	O
involve	O
single	O
text	O
or	O
text	O
pairs	O
-	O
by	O
swapping	O
out	O
the	O
appropriate	O
inputs	O
and	O
outputs	O
.	O

For	O
applications	O
involving	O
text	O
pairs	O
,	O
a	O
common	O
pattern	O
is	O
to	O
independently	O
encode	O
text	O
pairs	O
before	O
applying	O
bidirectional	O
cross	O
attention	O
,	O
such	O
as	O
Parikh	O
et	O
al	O
.	O

(	O
2016	O
)	O
;	O
Seo	O
et	O
al	O
.	O

(	O
2017	O
)	O
.	O

BERT	B-MethodName
instead	O
uses	O
the	O
self	O
-	O
attention	O
mechanism	O
to	O
unify	O
these	O
two	O
stages	O
,	O
as	O
encoding	O
a	O
concatenated	O
text	O
pair	O
with	O
self	O
-	O
attention	O
effectively	O
includes	O
bidirectional	O
cross	O
attention	O
between	O
two	O
sentences	O
.	O

At	O
the	O
input	O
,	O
sentence	O
A	O
and	O
sentence	O
B	O
from	O
pre	O
-	O
training	O
are	O
analogous	O
to	O
(	O
1	O
)	O
sentence	O
pairs	O
in	O
paraphrasing	O
,	O
(	O
2	O
)	O
hypothesis	O
-	O
premise	O
pairs	O
in	O
entailment	B-TaskName
,	O
(	O
3	O
)	O
question	O
-	O
passage	O
pairs	O
in	O
question	B-TaskName
answering	I-TaskName
,	O
and	O
(	O
4	O
)	O
a	O
degenerate	O
text-∅	O
pair	O
in	O
text	B-TaskName
classification	I-TaskName
or	O
sequence	B-TaskName
tagging	I-TaskName
.	O

For	O
each	O
task	O
,	O
we	O
simply	O
plug	O
in	O
the	O
taskspecific	O
inputs	O
and	O
outputs	O
into	O
BERT	B-MethodName
and	O
finetune	O
all	O
the	O
parameters	O
end	O
-	O
to	O
-	O
end	O
.	O

At	O
the	O
output	O
,	O
the	O
token	O
representations	O
are	O
fed	O
into	O
an	O
output	O
layer	O
for	O
tokenlevel	O
tasks	O
,	O
such	O
as	O
sequence	B-TaskName
tagging	I-TaskName
or	O
question	B-TaskName
answering	I-TaskName
,	O
and	O
the	O
[	O
CLS	O
]	O
representation	O
is	O
fed	O
into	O
an	O
output	O
layer	O
for	O
classification	O
,	O
such	O
as	O
entailment	B-TaskName
or	O
sentiment	B-TaskName
analysis	I-TaskName
.	O

Compared	O
to	O
pre	O
-	O
training	O
,	O
fine	O
-	O
tuning	O
is	O
relatively	O
inexpensive	O
.	O

All	O
of	O
the	O
results	O
in	O
the	O
paper	O
can	O
be	O
replicated	O
in	O
at	O
most	O
1	O
hour	O
on	O
a	O
single	O
Cloud	O
TPU	O
,	O
or	O
a	O
few	O
hours	O
on	O
a	O
GPU	O
,	O
starting	O
from	O
the	O
exact	O
same	O
pre	O
-	O
trained	O
model	O
.	O

7	O
We	O
describe	O
the	O
task	O
-	O
specific	O
details	O
in	O
the	O
corresponding	O
subsections	O
of	O
Section	O
4	O
.	O

More	O
details	O
can	O
be	O
found	O
in	O
Appendix	O
A.5	O
.	O
Experiments	O
.	O

In	O
this	O
section	O
,	O
we	O
present	O
BERT	B-MethodName
fine	O
-	O
tuning	O
results	O
on	O
11	O
NLP	O
tasks	O
.	O

GLUE	B-DatasetName
.	O

Detailed	O
descriptions	O
of	O
GLUE	B-DatasetName
datasets	O
are	O
included	O
in	O
Appendix	O
B.1	O
.	O
To	O
fine	O
-	O
tune	O
on	O
GLUE	B-DatasetName
,	O
we	O
represent	O
the	O
input	O
sequence	O
(	O
for	O
single	O
sentence	O
or	O
sentence	O
pairs	O
)	O
as	O
described	O
in	O
Section	O
3	O
,	O
and	O
use	O
the	O
final	O
hidden	O
vector	O
C	O
∈	O
R	O
H	O
corresponding	O
to	O
the	O
first	O
input	O
token	O
(	O
[	O
CLS	O
]	O
)	O
as	O
the	O
aggregate	O
representation	O
.	O

The	O
only	O
new	O
parameters	O
introduced	O
during	O
fine	O
-	O
tuning	O
are	O
classification	O
layer	O
weights	O
W	O
∈	O
R	O
K×H	O
,	O
where	O
K	O
is	O
the	O
number	O
of	O
labels	O
.	O

We	O
compute	O
a	O
standard	O
classification	O
loss	O
with	O
C	O
and	O
W	O
,	O
i.e.	O
,	O
log(softmax(CW	O
T	O
)	O
)	O
.	O

We	O
use	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
32	B-HyperparameterValue
and	O
fine	O
-	O
tune	O
for	O
3	O
epochs	O
over	O
the	O
data	O
for	O
all	O
GLUE	B-DatasetName
tasks	O
.	O

For	O
each	O
task	O
,	O
we	O
selected	O
the	O
best	O
fine	O
-	O
tuning	O
learning	B-HyperparameterName
rate	I-HyperparameterName
(	O
among	O
5e-5	B-HyperparameterValue
,	O
4e-5	B-HyperparameterValue
,	O
3e-5	B-HyperparameterValue
,	O
and	O
2e-5	B-HyperparameterValue
)	O
on	O
the	O
Dev	O
set	O
.	O

Additionally	O
,	O
for	O
BERT	B-MethodName
LARGE	I-MethodName
we	O
found	O
that	O
finetuning	O
was	O
sometimes	O
unstable	O
on	O
small	O
datasets	O
,	O
so	O
we	O
ran	O
several	O
random	O
restarts	O
and	O
selected	O
the	O
best	O
model	O
on	O
the	O
Dev	O
set	O
.	O

With	O
random	O
restarts	O
,	O
we	O
use	O
the	O
same	O
pre	O
-	O
trained	O
checkpoint	O
but	O
perform	O
different	O
fine	O
-	O
tuning	O
data	O
shuffling	O
and	O
classifier	O
layer	O
initialization	O
.	O

9	O
Results	O
are	O
presented	O
in	O
Table	O
1	O
.	O

The	O
General	B-DatasetName
Language	I-DatasetName
Understanding	I-DatasetName
Evaluation	I-DatasetName
(	O
GLUE	B-DatasetName
)	O
benchmark	O
(	O
Wang	O
et	O
al	O
.	O
,	O
2018a	O
)	O
is	O
a	O
collection	O
of	O
diverse	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
tasks	O
.	O

Both	O
BERT	B-MethodName
BASE	I-MethodName
and	O
BERT	B-MethodName
LARGE	I-MethodName
outperform	O
all	O
systems	O
on	O
all	O
tasks	O
by	O
a	O
substantial	O
margin	O
,	O
obtaining	O
4.5	B-MetricValue
%	I-MetricValue
and	O
7.0	B-MetricValue
%	I-MetricValue
respective	O
average	O
accuracy	B-MethodName
improvement	O
over	O
the	O
prior	O
state	O
of	O
the	O
art	O
.	O

Note	O
that	O
BERT	B-MethodName
BASE	I-MethodName
and	O
OpenAI	B-MethodName
GPT	I-MethodName
are	O
nearly	O
identical	O
in	O
terms	O
of	O
model	O
architecture	O
apart	O
from	O
the	O
attention	O
masking	O
.	O

On	O
the	O
official	O
GLUE	B-DatasetName
leaderboard	O
10	O
,	O
BERT	B-MethodName
LARGE	I-MethodName
obtains	O
a	O
score	B-MetricName
of	O
80.5	B-MetricValue
,	O
compared	O
to	O
OpenAI	B-MethodName
GPT	I-MethodName
,	O
which	O
obtains	O
72.8	B-MetricValue
as	O
of	O
the	O
date	O
of	O
writing	O
.	O

For	O
the	O
largest	O
and	O
most	O
widely	O
reported	O
GLUE	B-DatasetName
task	O
,	O
MNLI	B-DatasetName
,	O
BERT	B-MethodName
obtains	O
a	O
4.6	B-MetricValue
%	I-MetricValue
absolute	O
accuracy	B-MetricName
improvement	O
.	O

We	O
find	O
that	O
BERT	B-MethodName
LARGE	I-MethodName
significantly	O
outperforms	O
BERT	B-MethodName
BASE	I-MethodName
across	O
all	O
tasks	O
,	O
especially	O
those	O
with	O
very	O
little	O
training	O
data	O
.	O

The	O
effect	O
of	O
model	O
size	O
is	O
explored	O
more	O
thoroughly	O
in	O
Section	O
5.2	O
.	O
SQuAD	B-DatasetName
v1.1	I-DatasetName
.	O
The	O
Stanford	B-DatasetName
Question	I-DatasetName
Answering	I-DatasetName
Dataset	I-DatasetName
(	O
SQuAD	B-DatasetName
v1.1	I-DatasetName
)	O
is	O
a	O
collection	O
of	O
100k	O
crowdsourced	O
question	O
/	O
answer	O
pairs	O
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

Given	O
a	O
question	O
and	O
a	O
passage	O
from	O
9	O
The	O
GLUE	B-DatasetName
data	O
set	O
distribution	O
does	O
not	O
include	O
the	O
Test	O
labels	O
,	O
and	O
we	O
only	O
made	O
a	O
single	O
GLUE	B-DatasetName
evaluation	O
server	O
submission	O
for	O
each	O
of	O
BERTBASE	B-MethodName
and	O
BERTLARGE	B-MethodName
.	O

10	O
https://gluebenchmark.com/leaderboard	O
Wikipedia	O
containing	O
the	O
answer	O
,	O
the	O
task	O
is	O
to	O
predict	O
the	O
answer	O
text	O
span	O
in	O
the	O
passage	O
.	O

As	O
shown	O
in	O
Figure	O
1	O
,	O
in	O
the	O
question	B-TaskName
answering	I-TaskName
task	O
,	O
we	O
represent	O
the	O
input	O
question	O
and	O
passage	O
as	O
a	O
single	O
packed	O
sequence	O
,	O
with	O
the	O
question	O
using	O
the	O
A	O
embedding	O
and	O
the	O
passage	O
using	O
the	O
B	O
embedding	O
.	O

We	O
only	O
introduce	O
a	O
start	O
vector	O
S	O
∈	O
R	O
H	O
and	O
an	O
end	O
vector	O
E	O
∈	O
R	O
H	O
during	O
fine	O
-	O
tuning	O
.	O

The	O
probability	O
of	O
word	O
i	O
being	O
the	O
start	O
of	O
the	O
answer	O
span	O
is	O
computed	O
as	O
a	O
dot	O
product	O
between	O
T	O
i	O
and	O
S	O
followed	O
by	O
a	O
softmax	O
over	O
all	O
of	O
the	O
words	O
in	O
the	O
paragraph	O
:	O
P	O
i	O
=	O
e	O
S•T	O
i	O
j	O
e	O
S•T	O
j	O
.	O

The	O
analogous	O
formula	O
is	O
used	O
for	O
the	O
end	O
of	O
the	O
answer	O
span	O
.	O

The	O
score	O
of	O
a	O
candidate	O
span	O
from	O
position	O
i	O
to	O
position	O
j	O
is	O
defined	O
as	O
S•T	O
i	O
+	O
E•T	O
j	O
,	O
and	O
the	O
maximum	O
scoring	O
span	O
where	O
j	O
≥	O
i	O
is	O
used	O
as	O
a	O
prediction	O
.	O

The	O
training	O
objective	O
is	O
the	O
sum	O
of	O
the	O
log	O
-	O
likelihoods	O
of	O
the	O
correct	O
start	O
and	O
end	O
positions	O
.	O

We	O
fine	O
-	O
tune	O
for	O
3	O
epochs	O
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
5e-5	B-HyperparameterValue
and	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
32	B-HyperparameterValue
.	O

Table	O
2	O
shows	O
top	O
leaderboard	O
entries	O
as	O
well	O
as	O
results	O
from	O
top	O
published	O
systems	O
(	O
Seo	O
et	O
al	O
.	O
,	O
2017;Clark	O
and	O
Gardner	O
,	O
2018;Peters	O
et	O
al	O
.	O
,	O
2018a;Hu	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

The	O
top	O
results	O
from	O
the	O
SQuAD	B-DatasetName
leaderboard	O
do	O
not	O
have	O
up	O
-	O
to	O
-	O
date	O
public	O
system	O
descriptions	O
available	O
,	O
11	O
and	O
are	O
allowed	O
to	O
use	O
any	O
public	O
data	O
when	O
training	O
their	O
systems	O
.	O

We	O
therefore	O
use	O
modest	O
data	O
augmentation	O
in	O
our	O
system	O
by	O
first	O
fine	O
-	O
tuning	O
on	O
TriviaQA	B-DatasetName
(	O
Joshi	O
et	O
al	O
.	O
,	O
2017	O
)	O
befor	O
fine	O
-	O
tuning	O
on	O
SQuAD	B-DatasetName
.	O

Our	O
best	O
performing	O
system	O
outperforms	O
the	O
top	O
leaderboard	O
system	O
by	O
+1.5	B-MetricValue
F1	B-MetricName
in	O
ensembling	O
and	O
+1.3	B-MetricValue
F1	B-MetricName
as	O
a	O
single	O
system	O
.	O

In	O
fact	O
,	O
our	O
single	O
BERT	B-MethodName
model	O
outperforms	O
the	O
top	O
ensemble	O
system	O
in	O
terms	O
of	O
F1	B-MetricName
score	O
.	O

tuning	O
data	O
,	O
we	O
only	O
lose	O
0.1	B-MetricValue
-	O
0.4	B-MetricValue
F1	B-MetricName
,	O
still	O
outperforming	O
all	O
existing	O
systems	O
by	O
a	O
wide	O
margin	O
.	O

12	O
4.3	O
SQuAD	B-DatasetName
v2.0	I-DatasetName
The	O
SQuAD	B-DatasetName
2.0	I-DatasetName
task	O
extends	O
the	O
SQuAD	B-DatasetName
1.1	I-DatasetName
problem	O
definition	O
by	O
allowing	O
for	O
the	O
possibility	O
that	O
no	O
short	O
answer	O
exists	O
in	O
the	O
provided	O
paragraph	O
,	O
making	O
the	O
problem	O
more	O
realistic	O
.	O

We	O
use	O
a	O
simple	O
approach	O
to	O
extend	O
the	O
SQuAD	B-DatasetName
v1.1	I-DatasetName
BERT	B-MethodName
model	O
for	O
this	O
task	O
.	O

We	O
treat	O
questions	O
that	O
do	O
not	O
have	O
an	O
answer	O
as	O
having	O
an	O
answer	O
span	O
with	O
start	O
and	O
end	O
at	O
the	O
[	O
CLS	O
]	O
token	O
.	O

The	O
probability	O
space	O
for	O
the	O
start	O
and	O
end	O
answer	O
span	O
positions	O
is	O
extended	O
to	O
include	O
the	O
position	O
of	O
the	O
[	O
CLS	O
]	O
token	O
.	O

For	O
prediction	O
,	O
we	O
compare	O
the	O
score	O
of	O
the	O
no	O
-	O
answer	O
span	O
:	O
s	O
null	O
=	O
S•C	O
+	O
E•C	O
to	O
the	O
score	O
of	O
the	O
best	O
non	O
-	O
null	O
span	O
12	O
The	O
TriviaQA	B-DatasetName
data	O
we	O
used	O
consists	O
of	O
paragraphs	O
from	O
TriviaQA	B-DatasetName
-	I-DatasetName
Wiki	I-DatasetName
formed	O
of	O
the	O
first	O
400	O
tokens	O
in	O
documents	O
,	O
that	O
contain	O
at	O
least	O
one	O
of	O
the	O
provided	O
possible	O
answers	O
.	O

ŝ	O
i	O
,	O
j	O
=	O
max	O
j≥i	O
S•T	O
i	O
+	O
E•T	O
j	O
.	O

We	O
predict	O
a	O
non	O
-	O
null	O
answer	O
when	O
ŝ	O
i	O
,	O
j	O
>	O
s	O
null	O
+	O
τ	O
,	O
where	O
the	O
threshold	O
τ	O
is	O
selected	O
on	O
the	O
dev	O
set	O
to	O
maximize	O
F1	B-MetricName
.	O

We	O
did	O
not	O
use	O
TriviaQA	B-DatasetName
data	O
for	O
this	O
model	O
.	O

We	O
fine	O
-	O
tuned	O
for	O
2	O
epochs	O
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
5e-5	B-HyperparameterValue
and	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
48	B-HyperparameterValue
.	O

System	O
.	O

The	O
results	O
compared	O
to	O
prior	O
leaderboard	O
entries	O
and	O
top	O
published	O
work	O
(	O
Sun	O
et	O
al	O
.	O
,	O
2018;Wang	O
et	O
al	O
.	O
,	O
2018b	O
)	O
are	O
shown	O
in	O
Table	O
3	O
,	O
excluding	O
systems	O
that	O
use	O
BERT	B-MethodName
as	O
one	O
of	O
their	O
components	O
.	O

We	O
observe	O
a	O
+5.1	B-MetricValue
F1	B-MetricName
improvement	O
over	O
the	O
previous	O
best	O
system	O
.	O

SWAG	B-DatasetName
.	O

The	O
Situations	B-DatasetName
With	I-DatasetName
Adversarial	I-DatasetName
Generations	I-DatasetName
(	O
SWAG	B-DatasetName
)	O
dataset	O
contains	O
113k	O
sentence	O
-	O
pair	O
completion	O
examples	O
that	O
evaluate	O
grounded	O
commonsense	O
inference	O
(	O
Zellers	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

Given	O
a	O
sentence	O
,	O
the	O
task	O
is	O
to	O
choose	O
the	O
most	O
plausible	O
continuation	O
among	O
four	O
choices	O
.	O

When	O
fine	O
-	O
tuning	O
on	O
the	O
SWAG	B-DatasetName
dataset	O
,	O
we	O
construct	O
four	O
input	O
sequences	O
,	O
each	O
containing	O
the	O
concatenation	O
of	O
the	O
given	O
sentence	O
(	O
sentence	O
A	O
)	O
and	O
a	O
possible	O
continuation	O
(	O
sentence	O
B	O
)	O
.	O

The	O
only	O
task	O
-	O
specific	O
parameters	O
introduced	O
is	O
a	O
vector	O
whose	O
dot	O
product	O
with	O
the	O
[	O
CLS	O
]	O
token	O
representation	O
C	O
denotes	O
a	O
score	O
for	O
each	O
choice	O
which	O
is	O
normalized	O
with	O
a	O
softmax	O
layer	O
.	O

We	O
fine	O
-	O
tune	O
the	O
model	O
for	O
3	O
epochs	O
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
2e-5	B-HyperparameterValue
and	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
16	B-HyperparameterValue
.	O

Results	O
are	O
presented	O
in	O
Table	O
4	O
.	O

BERT	B-MethodName
LARGE	I-MethodName
outperforms	O
the	O
authors	O
'	O
baseline	O
ESIM+ELMo	B-MethodName
system	O
by	O
+27.1	B-MetricValue
%	I-MetricValue
and	O
OpenAI	B-MethodName
GPT	I-MethodName
by	O
8.3	B-MetricValue
%	I-MetricValue
.	O

Ablation	O
Studies	O
.	O

In	O
this	O
section	O
,	O
we	O
perform	O
ablation	O
experiments	O
over	O
a	O
number	O
of	O
facets	O
of	O
BERT	B-MethodName
in	O
order	O
to	O
better	O
understand	O
their	O
relative	O
importance	O
.	O

Additional	O
ablation	O
studies	O
can	O
be	O
found	O
in	O
Appendix	O
C.	O

Effect	O
of	O
Pre	O
-	O
training	O
Tasks	O
.	O

We	O
demonstrate	O
the	O
importance	O
of	O
the	O
deep	O
bidirectionality	O
of	O
BERT	B-MethodName
by	O
evaluating	O
two	O
pretraining	O
objectives	O
using	O
exactly	O
the	O
same	O
pretraining	O
data	O
,	O
fine	O
-	O
tuning	O
scheme	O
,	O
and	O
hyperparameters	O
as	O
BERT	B-MethodName
BASE	I-MethodName
:	O
No	O
NSP	O
:	O
A	O
bidirectional	O
model	O
which	O
is	O
trained	O
using	O
the	O
"	O
masked	O
LM	O
"	O
(	O
MLM	O
)	O
but	O
without	O
the	O
"	O
next	B-TaskName
sentence	I-TaskName
prediction	I-TaskName
"	O
(	O
NSP	B-TaskName
)	O
task	O
.	O

LTR	O
&	O
No	O
NSP	B-TaskName
:	O
.	O

A	O
left	O
-	O
context	O
-	O
only	O
model	O
which	O
is	O
trained	O
using	O
a	O
standard	O
Left	O
-	O
to	O
-	O
Right	O
(	O
LTR	O
)	O
LM	O
,	O
rather	O
than	O
an	O
MLM	O
.	O

The	O
left	O
-	O
only	O
constraint	O
was	O
also	O
applied	O
at	O
fine	O
-	O
tuning	O
,	O
because	O
removing	O
it	O
introduced	O
a	O
pre	O
-	O
train	O
/	O
fine	O
-	O
tune	O
mismatch	O
that	O
degraded	O
downstream	O
performance	O
.	O

Additionally	O
,	O
this	O
model	O
was	O
pre	O
-	O
trained	O
without	O
the	O
NSP	B-TaskName
task	O
.	O

This	O
is	O
directly	O
comparable	O
to	O
OpenAI	B-MethodName
GPT	I-MethodName
,	O
but	O
using	O
our	O
larger	O
training	O
dataset	O
,	O
our	O
input	O
representation	O
,	O
and	O
our	O
fine	O
-	O
tuning	O
scheme	O
.	O

We	O
first	O
examine	O
the	O
impact	O
brought	O
by	O
the	O
NSP	B-TaskName
task	O
.	O

In	O
Table	O
5	O
,	O
we	O
show	O
that	O
removing	O
NSP	B-TaskName
hurts	O
performance	O
significantly	O
on	O
QNLI	B-DatasetName
,	O
MNLI	B-DatasetName
,	O
and	O
SQuAD	B-DatasetName
1.1	I-DatasetName
.	O
Next	O
,	O
we	O
evaluate	O
the	O
impact	O
of	O
training	O
bidirectional	O
representations	O
by	O
comparing	O
"	O
No	O
NSP	B-TaskName
"	O
to	O
"	O
LTR	O
&	O
No	O
NSP	B-TaskName
"	O
.	O

The	O
LTR	O
model	O
performs	O
worse	O
than	O
the	O
MLM	O
model	O
on	O
all	O
tasks	O
,	O
with	O
large	O
drops	O
on	O
MRPC	B-DatasetName
and	O
SQuAD	B-DatasetName
.	O

For	O
SQuAD	B-DatasetName
it	O
is	O
intuitively	O
clear	O
that	O
a	O
LTR	O
model	O
will	O
perform	O
poorly	O
at	O
token	O
predictions	O
,	O
since	O
the	O
token	O
-	O
level	O
hidden	O
states	O
have	O
no	O
rightside	O
context	O
.	O

In	O
order	O
to	O
make	O
a	O
good	O
faith	O
attempt	O
at	O
strengthening	O
the	O
LTR	O
system	O
,	O
we	O
added	O
a	O
randomly	O
initialized	O
BiLSTM	B-MethodName
on	O
top	O
.	O

This	O
does	O
significantly	O
improve	O
results	O
on	O
SQuAD	B-DatasetName
,	O
but	O
the	O
results	O
are	O
still	O
far	O
worse	O
than	O
those	O
of	O
the	O
pretrained	O
bidirectional	O
models	O
.	O

The	O
BiLSTM	B-MethodName
hurts	O
performance	O
on	O
the	O
GLUE	B-DatasetName
tasks	O
.	O

However	O
:	O
(	O
a	O
)	O
this	O
is	O
twice	O
as	O
expensive	O
as	O
a	O
single	O
bidirectional	O
model	O
;	O
(	O
b	O
)	O
this	O
is	O
non	O
-	O
intuitive	O
for	O
tasks	O
like	O
QA	B-TaskName
,	O
since	O
the	O
RTL	O
model	O
would	O
not	O
be	O
able	O
to	O
condition	O
the	O
answer	O
on	O
the	O
question	O
;	O
(	O
c	O
)	O
this	O
it	O
is	O
strictly	O
less	O
powerful	O
than	O
a	O
deep	O
bidirectional	O
model	O
,	O
since	O
it	O
can	O
use	O
both	O
left	O
and	O
right	O
context	O
at	O
every	O
layer	O
.	O

We	O
recognize	O
that	O
it	O
would	O
also	O
be	O
possible	O
to	O
train	O
separate	O
LTR	O
and	O
RTL	O
models	O
and	O
represent	O
each	O
token	O
as	O
the	O
concatenation	O
of	O
the	O
two	O
models	O
,	O
as	O
ELMo	B-MethodName
does	O
.	O

Effect	O
of	O
Model	O
Size	O
.	O

In	O
this	O
section	O
,	O
we	O
explore	O
the	O
effect	O
of	O
model	O
size	O
on	O
fine	O
-	O
tuning	O
task	O
accuracy	O
.	O

Results	O
on	O
selected	O
GLUE	B-DatasetName
tasks	O
are	O
shown	O
in	O
Table	O
6	O
.	O

In	O
this	O
table	O
,	O
we	O
report	O
the	O
average	O
Dev	O
Set	O
accuracy	B-MetricName
from	O
5	O
random	O
restarts	O
of	O
fine	O
-	O
tuning	O
.	O

We	O
can	O
see	O
that	O
larger	O
models	O
lead	O
to	O
a	O
strict	O
accuracy	O
improvement	O
across	O
all	O
four	O
datasets	O
,	O
even	O
for	O
MRPC	B-DatasetName
which	O
only	O
has	O
3,600	O
labeled	O
training	O
examples	O
,	O
and	O
is	O
substantially	O
different	O
from	O
the	O
pre	O
-	O
training	O
tasks	O
.	O

It	O
is	O
also	O
perhaps	O
surprising	O
that	O
we	O
are	O
able	O
to	O
achieve	O
such	O
significant	O
improvements	O
on	O
top	O
of	O
models	O
which	O
are	O
already	O
quite	O
large	O
relative	O
to	O
the	O
existing	O
literature	O
.	O

For	O
example	O
,	O
the	O
largest	O
Transformer	B-MethodName
explored	O
in	O
Vaswani	O
et	O
al	O
.	O

(	O
2017	O
)	O
is	O
(	O
L=6	B-HyperparameterName
,	O
H=1024	B-HyperparameterName
,	O
A=16	B-HyperparameterName
)	O
with	O
100	O
M	O
parameters	O
for	O
the	O
encoder	O
,	O
and	O
the	O
largest	O
Transformer	O
we	O
have	O
found	O
in	O
the	O
literature	O
is	O
(	O
L=64	B-HyperparameterName
,	O
H=512	B-HyperparameterName
,	O
A=2	B-HyperparameterName
)	O
with	O
235	O
M	O
parameters	O
(	O
Al	O
-	O
Rfou	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

By	O
contrast	O
,	O
BERT	B-MethodName
BASE	I-MethodName
contains	O
110	O
M	O
parameters	O
and	O
BERT	B-MethodName
LARGE	I-MethodName
contains	O
340	O
M	O
parameters	O
.	O

It	O
has	O
long	O
been	O
known	O
that	O
increasing	O
the	O
model	O
size	O
will	O
lead	O
to	O
continual	O
improvements	O
on	O
large	O
-	O
scale	O
tasks	O
such	O
as	O
machine	B-TaskName
translation	I-TaskName
and	O
language	O
modeling	O
,	O
which	O
is	O
demonstrated	O
by	O
the	O
LM	O
perplexity	O
of	O
held	O
-	O
out	O
training	O
data	O
shown	O
in	O
Table	O
6	O
.	O

However	O
,	O
we	O
believe	O
that	O
this	O
is	O
the	O
first	O
work	O
to	O
demonstrate	O
convincingly	O
that	O
scaling	O
to	O
extreme	O
model	O
sizes	O
also	O
leads	O
to	O
large	O
improvements	O
on	O
very	O
small	O
scale	O
tasks	O
,	O
provided	O
that	O
the	O
model	O
has	O
been	O
sufficiently	O
pre	O
-	O
trained	O
.	O

Peters	O
et	O
al	O
.	O

(	O
2018b	O
)	O
presented	O
mixed	O
results	O
on	O
the	O
downstream	O
task	O
impact	O
of	O
increasing	O
the	O
pre	O
-	O
trained	O
bi	O
-	O
LM	O
size	O
from	O
two	O
to	O
four	O
layers	O
and	O
Melamud	O
et	O
al	O
.	O

(	O
2016	O
)	O
mentioned	O
in	O
passing	O
that	O
increasing	O
hidden	B-HyperparameterName
dimension	I-HyperparameterName
size	I-HyperparameterName
from	O
200	B-HyperparameterValue
to	O
600	B-HyperparameterValue
helped	O
,	O
but	O
increasing	O
further	O
to	O
1,000	B-HyperparameterValue
did	O
not	O
bring	O
further	O
improvements	O
.	O

Both	O
of	O
these	O
prior	O
works	O
used	O
a	O
featurebased	O
approach	O
-we	O
hypothesize	O
that	O
when	O
the	O
model	O
is	O
fine	O
-	O
tuned	O
directly	O
on	O
the	O
downstream	O
tasks	O
and	O
uses	O
only	O
a	O
very	O
small	O
number	O
of	O
randomly	O
initialized	O
additional	O
parameters	O
,	O
the	O
taskspecific	O
models	O
can	O
benefit	O
from	O
the	O
larger	O
,	O
more	O
expressive	O
pre	O
-	O
trained	O
representations	O
even	O
when	O
downstream	O
task	O
data	O
is	O
very	O
small	O
.	O

Feature	O
-	O
based	O
Approach	O
with	O
BERT	B-MethodName
.	O

In	O
Section	O
C.2	O
,	O
we	O
evaluate	O
the	O
impact	O
this	O
procedure	O
.	O

Additionally	O
,	O
because	O
random	O
replacement	O
only	O
occurs	O
for	O
1.5	O
%	O
of	O
all	O
tokens	O
(	O
i.e.	O
,	O
10	O
%	O
of	O
15	O
%	O
)	O
,	O
this	O
does	O
not	O
seem	O
to	O
harm	O
the	O
model	O
's	O
language	O
understanding	O
capability	O
.	O

Compared	O
to	O
standard	O
langauge	O
model	O
training	O
,	O
the	O
masked	O
LM	O
only	O
make	O
predictions	O
on	O
15	O
%	O
of	O
tokens	O
in	O
each	O
batch	O
,	O
which	O
suggests	O
that	O
more	O
pre	O
-	O
training	O
steps	O
may	O
be	O
required	O
for	O
the	O
model	O
.	O

The	O
purpose	O
of	O
this	O
is	O
to	O
bias	O
the	O
representation	O
towards	O
the	O
actual	O
observed	O
word	O
.	O

Masked	O
LM	O
and	O
the	O
Masking	O
Procedure	O
Assuming	O
the	O
unlabeled	O
sentence	O
is	O
my	O
dog	O
is	O
hairy	O
,	O
and	O
during	O
the	O
random	O
masking	O
procedure	O
we	O
chose	O
the	O
4	O
-	O
th	O
token	O
(	O
which	O
corresponding	O
to	O
hairy	O
)	O
,	O
our	O
masking	O
procedure	O
can	O
be	O
further	O
illustrated	O
by	O
•	O
10	O
%	O
of	O
the	O
time	O
:	O
Replace	O
the	O
word	O
with	O
a	O
random	O
word	O
,	O
e.g.	O
,	O
my	O
dog	O
is	O
hairy	O
→	O
my	O
dog	O
is	O
apple	O
•	O
10	O
%	O
of	O
the	O
time	O
:	O
Keep	O
the	O
word	O
unchanged	O
,	O
e.g.	O
,	O
my	O
dog	O
is	O
hairy	O
→	O
my	O
dog	O
is	O
hairy	O
.	O

A.1	O
Illustration	O
of	O
the	O
Pre	O
-	O
training	O
Tasks	O
We	O
provide	O
examples	O
of	O
the	O
pre	O
-	O
training	O
tasks	O
in	O
the	O
following	O
.	O

A	O
Additional	O
Details	O
for	O
BERT	B-MethodName
.	O

Appendix	O
for	O
"	O
BERT	B-MethodName
:	O
Pre	O
-	O
training	O
of	O
Deep	O
Bidirectional	B-MethodName
Transformers	I-MethodName
for	O
Language	O
Understanding	O
"	O
We	O
organize	O
the	O
appendix	O
into	O
three	O
sections	O
:	O
•	O
Additional	O
implementation	O
details	O
for	O
BERT	B-MethodName
are	O
presented	O
in	O
Appendix	O
A	O
;	O
.	O

Interestingly	O
,	O
using	O
only	O
the	O
RND	O
strategy	O
performs	O
much	O
worse	O
than	O
our	O
strategy	O
as	O
well	O
.	O

However	O
,	O
as	O
expected	O
,	O
using	O
only	O
the	O
MASK	O
strategy	O
was	O
problematic	O
when	O
applying	O
the	O
featurebased	O
approach	O
to	O
NER	B-TaskName
.	O

All	O
of	O
the	O
BERT	B-MethodName
results	O
presented	O
so	O
far	O
have	O
used	O
the	O
fine	O
-	O
tuning	O
approach	O
,	O
where	O
a	O
simple	O
classification	O
layer	O
is	O
added	O
to	O
the	O
pre	O
-	O
trained	O
model	O
,	O
and	O
all	O
parameters	O
are	O
jointly	O
fine	O
-	O
tuned	O
on	O
a	O
downstream	O
task	O
.	O

However	O
,	O
the	O
feature	O
-	O
based	O
approach	O
,	O
where	O
fixed	O
features	O
are	O
extracted	O
from	O
the	O
pretrained	O
model	O
,	O
has	O
certain	O
advantages	O
.	O

First	O
,	O
not	O
all	O
tasks	O
can	O
be	O
easily	O
represented	O
by	O
a	O
Transformer	B-MethodName
encoder	O
architecture	O
,	O
and	O
therefore	O
require	O
a	O
task	O
-	O
specific	O
model	O
architecture	O
to	O
be	O
added	O
.	O

Second	O
,	O
there	O
are	O
major	O
computational	O
benefits	O
to	O
pre	O
-	O
compute	O
an	O
expensive	O
representation	O
of	O
the	O
training	O
data	O
once	O
and	O
then	O
run	O
many	O
experiments	O
with	O
cheaper	O
models	O
on	O
top	O
of	O
this	O
representation	O
.	O

In	O
this	O
section	O
,	O
we	O
compare	O
the	O
two	O
approaches	O
by	O
applying	O
BERT	B-MethodName
to	O
the	O
CoNLL-2003	O
Named	B-TaskName
Entity	I-TaskName
Recognition	I-TaskName
(	O
NER	B-TaskName
)	O
task	O
(	O
Tjong	O
Kim	O
Sang	O
and	O
De	O
Meulder	O
,	O
2003	O
)	O
.	O

In	O
the	O
input	O
to	O
BERT	B-MethodName
,	O
we	O
use	O
a	O
case	O
-	O
preserving	O
WordPiece	O
model	O
,	O
and	O
we	O
include	O
the	O
maximal	O
document	O
context	O
provided	O
by	O
the	O
data	O
.	O

Following	O
standard	O
practice	O
,	O
we	O
formulate	O
this	O
as	O
a	O
tagging	O
task	O
but	O
do	O
not	O
use	O
a	O
CRF	O
layer	O
in	O
the	O
output	O
.	O

We	O
use	O
the	O
representation	O
of	O
the	O
first	O
sub	O
-	O
token	O
as	O
the	O
input	O
to	O
the	O
token	O
-	O
level	O
classifier	O
over	O
the	O
NER	B-TaskName
label	O
set	O
.	O

To	O
ablate	O
the	O
fine	O
-	O
tuning	O
approach	O
,	O
we	O
apply	O
the	O
feature	O
-	O
based	O
approach	O
by	O
extracting	O
the	O
activations	O
from	O
one	O
or	O
more	O
layers	O
without	O
fine	O
-	O
tuning	O
any	O
parameters	O
of	O
BERT	B-MethodName
.	O

These	O
contextual	O
embeddings	O
are	O
used	O
as	O
input	O
to	O
a	O
randomly	O
initialized	O
two	O
-	O
layer	O
768	O
-	O
dimensional	O
BiLSTM	B-MethodName
before	O
the	O
classification	O
layer	O
.	O

Results	O
are	O
presented	O
in	O
Table	O
7	O
.	O

BERT	B-MethodName
LARGE	I-MethodName
performs	O
competitively	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
.	O

The	O
best	O
performing	O
method	O
concatenates	O
the	O
token	O
representations	O
from	O
the	O
top	O
four	O
hidden	O
layers	O
of	O
the	O
pre	O
-	O
trained	O
Transformer	B-MethodName
,	O
which	O
is	O
only	O
0.3	B-MetricValue
F1	B-MetricName
behind	O
fine	O
-	O
tuning	O
the	O
entire	O
model	O
.	O

This	O
demonstrates	O
that	O
BERT	B-MethodName
is	O
effective	O
for	O
both	O
finetuning	O
and	O
feature	O
-	O
based	O
approaches	O
.	O

Conclusion	O
.	O

Recent	O
empirical	O
improvements	O
due	O
to	O
transfer	O
learning	O
with	O
language	O
models	O
have	O
demonstrated	O
that	O
rich	O
,	O
unsupervised	O
pre	O
-	O
training	O
is	O
an	O
integral	O
part	O
of	O
many	O
language	O
understanding	O
systems	O
.	O

In	O
particular	O
,	O
these	O
results	O
enable	O
even	O
low	O
-	O
resource	O
tasks	O
to	O
benefit	O
from	O
deep	O
unidirectional	O
architectures	O
.	O

Our	O
major	O
contribution	O
is	O
further	O
generalizing	O
these	O
findings	O
to	O
deep	O
bidirectional	O
architectures	O
,	O
allowing	O
the	O
same	O
pre	O
-	O
trained	O
model	O
to	O
successfully	O
tackle	O
a	O
broad	O
set	O
of	O
NLP	O
tasks	O
.	O

To	O
generate	O
each	O
training	O
input	O
sequence	O
,	O
we	O
sample	O
two	O
spans	O
of	O
text	O
from	O
the	O
corpus	O
,	O
which	O
we	O
refer	O
to	O
as	O
"	O
sentences	O
"	O
even	O
though	O
they	O
are	O
typically	O
much	O
longer	O
than	O
single	O
sentences	O
(	O
but	O
can	O
be	O
shorter	O
also	O
)	O
.	O

The	O
first	O
sentence	O
receives	O
the	O
A	O
embedding	O
and	O
the	O
second	O
receives	O
the	O
B	O
embedding	O
.	O

50	O
%	O
of	O
the	O
time	O
B	O
is	O
the	O
actual	O
next	O
sentence	O
that	O
follows	O
A	O
and	O
50	O
%	O
of	O
the	O
time	O
it	O
is	O
a	O
random	O
sentence	O
,	O
which	O
is	O
done	O
for	O
the	O
"	B-TaskName
next	I-TaskName
sentence	I-TaskName
prediction	I-TaskName
"	O
task	O
.	O

They	O
are	O
sampled	O
such	O
that	O
the	O
combined	O
length	O
is	O
≤	O
512	O
tokens	O
.	O

The	O
LM	O
masking	O
is	O
applied	O
after	O
WordPiece	O
tokenization	O
with	O
a	O
uniform	O
masking	O
rate	O
of	O
15	O
%	O
,	O
and	O
no	O
special	O
consideration	O
given	O
to	O
partial	O
word	O
pieces	O
.	O

We	O
train	O
with	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
256	B-HyperparameterValue
sequences	O
(	O
256	O
sequences	O
*	O
512	O
tokens	O
=	O
128,000	O
tokens	O
/	O
batch	O
)	O
for	O
1,000,000	B-HyperparameterValue
steps	B-HyperparameterName
,	O
which	O
is	O
approximately	O
40	O
epochs	O
over	O
the	O
3.3	O
billion	O
word	O
corpus	O
.	O

We	O
use	O
Adam	O
with	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
1e-4	B-HyperparameterValue
,	O
β	B-HyperparameterName
1	I-HyperparameterName
=	O
0.9	B-HyperparameterValue
,	O
β	B-HyperparameterName
2	I-HyperparameterName
=	O
0.999	B-HyperparameterValue
,	O
L2	B-HyperparameterName
weight	I-HyperparameterName
decay	I-HyperparameterName
of	O
0.01	B-HyperparameterValue
,	O
learning	O
rate	O
warmup	O
over	O
the	O
first	O
10,000	O
steps	O
,	O
and	O
linear	O
decay	O
of	O
the	O
learning	O
rate	O
.	O

We	O
use	O
a	O
dropout	B-HyperparameterName
probability	I-HyperparameterName
of	O
0.1	B-HyperparameterValue
on	O
all	O
layers	O
.	O

We	O
use	O
a	O
gelu	O
activation	O
(	O
Hendrycks	O
and	O
Gimpel	O
,	O
2016	O
)	O
rather	O
than	O
the	O
standard	O
relu	O
,	O
following	O
OpenAI	B-MethodName
GPT	I-MethodName
.	O

The	O
training	O
loss	O
is	O
the	O
sum	O
of	O
the	O
mean	O
masked	O
LM	O
likelihood	O
and	O
the	O
mean	O
next	O
sentence	O
prediction	O
likelihood	O
.	O

E	O
N	O
...	O

T	O
1	O
T	O
2	O
T	O
N	O
...	O

E	O
1	O
E	O
2	O
E	O
N	O
...	O

T	O
1	O
T	O
2	O
T	O
N	O
...	O

E	O
1	O
E	O
2	O
E	O
N	O
...	O

Training	O
of	O
BERT	B-MethodName
BASE	I-MethodName
was	O
performed	O
on	O
4	O
Cloud	O
TPUs	O
in	O
Pod	O
configuration	O
(	O
16	O
TPU	O
chips	O
total	O
)	O
.	O

13	O
Training	O
of	O
BERT	B-MethodName
LARGE	I-MethodName
was	O
performed	O
on	O
16	O
Cloud	O
TPUs	O
(	O
64	O
TPU	O
chips	O
total	O
)	O
.	O

Each	O
pretraining	O
took	O
4	O
days	O
to	O
complete	O
.	O

Longer	O
sequences	O
are	O
disproportionately	O
expensive	O
because	O
attention	O
is	O
quadratic	O
to	O
the	O
sequence	O
length	O
.	O

Then	O
,	O
we	O
train	O
the	O
rest	O
10	O
%	O
of	O
the	O
steps	O
of	O
sequence	O
of	O
512	O
to	O
learn	O
the	O
positional	O
embeddings	O
.	O

A.3	O
Fine	O
-	O
tuning	O
Procedure	O
.	O

For	O
fine	O
-	O
tuning	O
,	O
most	O
model	O
hyperparameters	O
are	O
the	O
same	O
as	O
in	O
pre	O
-	O
training	O
,	O
with	O
the	O
exception	O
of	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
,	O
learning	B-HyperparameterName
rate	I-HyperparameterName
,	O
and	O
number	B-HyperparameterName
of	I-HyperparameterName
training	I-HyperparameterName
epochs	I-HyperparameterName
.	O

The	O
dropout	B-HyperparameterName
probability	I-HyperparameterName
was	O
always	O
kept	O
at	O
0.1	B-HyperparameterValue
.	O
The	O
optimal	O
hyperparameter	O
values	O
are	O
task	O
-	O
specific	O
,	O
but	O
we	O
found	O
the	O
following	O
range	O
of	O
possible	O
values	O
to	O
work	O
well	O
across	O
all	O
tasks	O
:	O
•	O
Batch	B-HyperparameterName
size	I-HyperparameterName
:	O
16	B-HyperparameterValue
,	O
32	B-HyperparameterValue
We	O
also	O
observed	O
that	O
large	O
data	O
sets	O
(	O
e.g.	O
,	O
100k+	O
labeled	O
training	O
examples	O
)	O
were	O
far	O
less	O
sensitive	O
to	O
hyperparameter	O
choice	O
than	O
small	O
data	O
sets	O
.	O

Fine	O
-	O
tuning	O
is	O
typically	O
very	O
fast	O
,	O
so	O
it	O
is	O
reasonable	O
to	O
simply	O
run	O
an	O
exhaustive	O
search	O
over	O
the	O
above	O
parameters	O
and	O
choose	O
the	O
model	O
that	O
performs	O
best	O
on	O
the	O
development	O
set	O
.	O

A.4	O
Comparison	O
of	O
BERT	B-MethodName
,	O
ELMo	B-MethodName
,	O
and	O
OpenAI	B-MethodName
GPT	I-MethodName
Here	O
we	O
studies	O
the	O
differences	O
in	O
recent	O
popular	O
representation	O
learning	O
models	O
including	O
ELMo	B-MethodName
,	O
OpenAI	B-MethodName
GPT	I-MethodName
and	O
BERT	B-MethodName
.	O

The	O
comparisons	O
between	O
the	O
model	O
architectures	O
are	O
shown	O
visually	O
in	O
Figure	O
3	O
.	O

Note	O
that	O
in	O
addition	O
to	O
the	O
architecture	O
differences	O
,	O
BERT	B-MethodName
and	O
OpenAI	B-MethodName
GPT	I-MethodName
are	O
finetuning	O
approaches	O
,	O
while	O
ELMo	B-MethodName
is	O
a	O
feature	O
-	O
based	O
approach	O
.	O

The	O
most	O
comparable	O
existing	O
pre	O
-	O
training	O
method	O
to	O
BERT	B-MethodName
is	O
OpenAI	B-MethodName
GPT	I-MethodName
,	O
which	O
trains	O
a	O
left	O
-	O
to	O
-	O
right	O
Transformer	O
LM	O
on	O
a	O
large	O
text	O
corpus	O
.	O

In	O
fact	O
,	O
many	O
of	O
the	O
design	O
decisions	O
in	O
BERT	B-MethodName
were	O
intentionally	O
made	O
to	O
make	O
it	O
as	O
close	O
to	O
GPT	B-MethodName
as	O
possible	O
so	O
that	O
the	O
two	O
methods	O
could	O
be	O
minimally	O
compared	O
.	O

The	O
core	O
argument	O
of	O
this	O
work	O
is	O
that	O
the	O
bi	O
-	O
directionality	O
and	O
the	O
two	O
pretraining	O
tasks	O
presented	O
in	O
Section	O
3.1	O
account	O
for	O
the	O
majority	O
of	O
the	O
empirical	O
improvements	O
,	O
but	O
we	O
do	O
note	O
that	O
there	O
are	O
several	O
other	O
differences	O
between	O
how	O
BERT	B-MethodName
and	O
GPT	B-MethodName
were	O
trained	O
:	O
•	O
GPT	B-MethodName
is	O
trained	O
on	O
the	O
BooksCorpus	B-DatasetName
(	O
800	O
M	O
words	O
)	O
;	O
BERT	B-MethodName
is	O
trained	O
on	O
the	O
BooksCorpus	B-DatasetName
(	O
800	O
M	O
words	O
)	O
and	O
Wikipedia	B-DatasetName
(	O
2,500	O
M	O
words	O
)	O
.	O

•	O
GPT	B-MethodName
uses	O
a	O
sentence	O
separator	O
(	O
[	O
SEP	O
]	O
)	O
and	O
classifier	O
token	O
(	O
[	O
CLS	O
]	O
)	O
which	O
are	O
only	O
introduced	O
at	O
fine	O
-	O
tuning	O
time	O
;	O
BERT	B-MethodName
learns	O
[	O
SEP	O
]	O
,	O
[	O
CLS	O
]	O
and	O
sentence	O
A	O
/	O
B	O
embeddings	O
during	O
pre	O
-	O
training	O
.	O

•	O
GPT	B-MethodName
used	O
the	O
same	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
5e-5	B-HyperparameterValue
for	O
all	O
fine	O
-	O
tuning	O
experiments	O
;	O
BERT	B-MethodName
chooses	O
a	O
task	O
-	O
specific	O
fine	O
-	O
tuning	O
learning	O
rate	O
which	O
performs	O
the	O
best	O
on	O
the	O
development	O
set	O
.	O

To	O
isolate	O
the	O
effect	O
of	O
these	O
differences	O
,	O
we	O
perform	O
ablation	O
experiments	O
in	O
Section	O
5.1	O
which	O
demonstrate	O
that	O
the	O
majority	O
of	O
the	O
improvements	O
are	O
in	O
fact	O
coming	O
from	O
the	O
two	O
pre	O
-	O
training	O
tasks	O
and	O
the	O
bidirectionality	O
they	O
enable	O
.	O

A.5	O
Illustrations	O
of	O
Fine	O
-	O
tuning	O
on	O
Different	O
Tasks	O
.	O

The	O
illustration	O
of	O
fine	O
-	O
tuning	O
BERT	B-MethodName
on	O
different	O
tasks	O
can	O
be	O
seen	O
in	O
Figure	O
4	O
.	O

Our	O
task	O
-	O
specific	O
models	O
are	O
formed	O
by	O
incorporating	O
BERT	B-MethodName
with	O
one	O
additional	O
output	O
layer	O
,	O
so	O
a	O
minimal	O
number	O
of	O
parameters	O
need	O
to	O
be	O
learned	O
from	O
scratch	O
.	O

The	O
GLUE	B-DatasetName
benchmark	O
includes	O
the	O
following	O
datasets	O
,	O
the	O
descriptions	O
of	O
which	O
were	O
originally	O
summarized	O
in	O
Wang	O
et	O
al	O
.	O

Given	O
a	O
pair	O
of	O
sentences	O
,	O
the	O
goal	O
is	O
to	O
predict	O
whether	O
the	O
second	O
sentence	O
is	O
an	O
entailment	O
,	O
contradiction	O
,	O
or	O
neutral	O
with	O
respect	O
to	O
the	O
first	O
one	O
.	O

QNLI	B-DatasetName
Question	B-DatasetName
Natural	I-DatasetName
Language	I-DatasetName
Inference	I-DatasetName
is	O
a	O
version	O
of	O
the	O
Stanford	B-DatasetName
Question	I-DatasetName
Answering	I-DatasetName
Dataset	I-DatasetName
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O
2016	O
)	O
which	O
has	O
been	O
converted	O
to	O
a	O
binary	B-TaskName
classification	I-TaskName
task	O
(	O
Wang	O
et	O
al	O
.	O
,	O
2018a	O
...	O

E	O
N	O
E	O
1	O
'	O
...	O

E	O
M	O
'	O
C	O
T	O
1	O
T	O
[	O
SEP	O
]	O
...	O

T	O
N	O
T	O
1	O
'	O
...	O

T	O
M	O
'	O
[	O
CLS	O
]	O
Tok	O
1	O
[	O
SEP	O
]	O
...	O

...	O

[	O
CLS	O
]	O
E	O
[	O
CLS	O
]	O
E	O
1	O
E	O
2	O
E	O
N	O
C	O
T	O
1	O
T	O
2	O
T	O
N	O
Single	O
Sentence	O
B	O
-	O
PER	O
O	O
O	O
...	O

...	O

E	O
[	O
CLS	O
]	O
E	O
1	O
E	O
[	O
SEP	O
]	O
Class	O
Label	O
...	O

E	O
N	O
E	O
1	O
'	O
...	O

E	O
M	O
'	O
C	O
T	O
1	O
T	O
[	O
SEP	O
]	O
...	O

with	O
human	O
annotations	O
of	O
their	O
sentiment	O
(	O
Socher	O
et	O
al	O
.	O
,	O
2013	O
)	O
.	O

STS	O
-	O
B.	O

The	O
Semantic	B-DatasetName
Textual	I-DatasetName
Similarity	I-DatasetName
Benchmark	I-DatasetName
is	O
a	O
collection	O
of	O
sentence	O
pairs	O
drawn	O
from	O
news	O
headlines	O
and	O
other	O
sources	O
(	O
Cer	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

They	O
were	O
annotated	O
with	O
a	O
score	O
from	O
1	O
to	O
5	O
denoting	O
how	O
similar	O
the	O
two	O
sentences	O
are	O
in	O
terms	O
of	O
semantic	O
meaning	O
.	O

MRPC	B-DatasetName
Microsoft	B-DatasetName
Research	I-DatasetName
Paraphrase	I-DatasetName
Corpus	I-DatasetName
consists	O
of	O
sentence	O
pairs	O
automatically	O
extracted	O
from	O
online	O
news	O
sources	O
,	O
with	O
human	O
annotations	O
for	O
whether	O
the	O
sentences	O
in	O
the	O
pair	O
are	O
semantically	O
equivalent	O
(	O
Dolan	O
and	O
Brockett	O
,	O
2005	O
)	O
.	O

RTE	B-DatasetName
Recognizing	B-DatasetName
Textual	I-DatasetName
Entailment	I-DatasetName
is	O
a	O
binary	B-TaskName
entailment	I-TaskName
task	O
similar	O
to	O
MNLI	B-DatasetName
,	O
but	O
with	O
much	O
less	O
training	O
data	O
(	O
Bentivogli	O
et	O
al	O
.	O
,	O
2009	O
)	O
.	O

14	O
WNLI	B-DatasetName
Winograd	B-DatasetName
NLI	I-DatasetName
is	O
a	O
small	O
natural	B-TaskName
language	I-TaskName
inference	I-TaskName
dataset	O
(	O
Levesque	O
et	O
al	O
.	O
,	O
2011	O
)	O
.	O

For	O
our	O
GLUE	B-DatasetName
submission	O
,	O
we	O
always	O
predicted	O
the	O
majority	O
class	O
.	O

C	O
Additional	O
Ablation	O
Studies	O
.	O

C.2	O
Ablation	O
for	O
Different	O
Masking	O
Procedures	O
.	O

In	O
Section	O
3.1	O
,	O
we	O
mention	O
that	O
BERT	B-MethodName
uses	O
a	O
mixed	O
strategy	O
for	O
masking	O
the	O
target	O
tokens	O
when	O
pre	O
-	O
training	O
with	O
the	O
masked	O
language	O
model	O
(	O
MLM	O
)	O
objective	O
.	O

The	O
following	O
is	O
an	O
ablation	O
study	O
to	O
evaluate	O
the	O
effect	O
of	O
different	O
masking	O
strategies	O
.	O

Note	O
that	O
the	O
purpose	O
of	O
the	O
masking	O
strategies	O
is	O
to	O
reduce	O
the	O
mismatch	O
between	O
pre	O
-	O
training	O
and	O
fine	O
-	O
tuning	O
,	O
as	O
the	O
[	O
MASK	O
]	O
symbol	O
never	O
appears	O
during	O
the	O
fine	O
-	O
tuning	O
stage	O
.	O

We	O
report	O
the	O
Dev	O
results	O
for	O
both	O
MNLI	B-DatasetName
and	O
NER	B-TaskName
.	O

For	O
NER	B-TaskName
,	O
we	O
report	O
both	O
fine	O
-	O
tuning	O
and	O
feature	O
-	O
based	O
approaches	O
,	O
as	O
we	O
expect	O
the	O
mismatch	O
will	O
be	O
amplified	O
for	O
the	O
feature	O
-	O
based	O
approach	O
as	O
the	O
model	O
will	O
not	O
have	O
the	O
chance	O
to	O
adjust	O
the	O
representations	O
.	O

The	O
results	O
are	O
presented	O
in	O
Table	O
8	O
.	O

In	O
the	O
table	O
,	O
MASK	O
means	O
that	O
we	O
replace	O
the	O
target	O
token	O
with	O
the	O
[	O
MASK	O
]	O
symbol	O
for	O
MLM	O
;	O
SAME	O
means	O
that	O
we	O
keep	O
the	O
target	O
token	O
as	O
is	O
;	O
RND	O
means	O
that	O
we	O
replace	O
the	O
target	O
token	O
with	O
another	O
random	O
token	O
.	O

The	O
right	O
part	O
of	O
the	O
paper	O
represents	O
the	O
Dev	O
set	O
results	O
.	O

For	O
the	O
feature	O
-	O
based	O
approach	O
,	O
we	O
concatenate	O
the	O
last	O
4	O
layers	O
of	O
BERT	B-MethodName
as	O
the	O
features	O
,	O
which	O
was	O
shown	O
to	O
be	O
the	O
best	O
approach	O
in	O
Section	O
5.3	O
.	O
From	O
the	O
table	O
it	O
can	O
be	O
seen	O
that	O
fine	O
-	O
tuning	O
is	O
surprisingly	O
robust	O
to	O
different	O
masking	O
strategies	O
.	O

BERT	B-MethodName
alleviates	O
the	O
previously	O
mentioned	O
unidirectionality	O
constraint	O
by	O
using	O
a	O
"	O
masked	O
language	O
model	O
"	O
(	O
MLM	O
)	O
pre	O
-	O
training	O
objective	O
,	O
inspired	O
by	O
the	O
Cloze	O
task	O
(	O
Taylor	O
,	O
1953	O
)	O
.	O

BERT	B-MethodName
:	O
Pre	O
-	O
training	O
of	O
Deep	O
Bidirectional	O
Transformers	O
for	O
Language	O
Understanding	O
.	O

It	O
obtains	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
eleven	O
natural	O
language	O
processing	O
tasks	O
,	O
including	O
pushing	O
the	O
GLUE	B-MetricName
score	O
to	O
80.5	B-MetricValue
%	I-MetricValue
(	O
7.7	B-MetricValue
%	I-MetricValue
point	O
absolute	O
improvement	O
)	O
,	O
MultiNLI	B-DatasetName
accuracy	B-MetricName
to	O
86.7	B-MetricValue
%	I-MetricValue
(	O
4.6	B-MetricValue
%	I-MetricValue
absolute	O
improvement	O
)	O
,	O
SQuAD	B-DatasetName
v1.1	I-DatasetName
question	B-TaskName
answering	I-TaskName
Test	O
F1	B-MetricName
to	O
93.2	B-MetricValue
(	O
1.5	B-MetricValue
point	O
absolute	O
improvement	O
)	O
and	O
SQuAD	B-DatasetName
v2.0	I-DatasetName
Test	O
F1	B-MetricName
to	O
83.1	B-MetricValue
(	O
5.1	B-MetricValue
point	O
absolute	O
improvement	O
)	O
.	O

The	O
contributions	O
of	O
our	O
paper	O
are	O
as	O
follows	O
:	O
•	O
We	O
demonstrate	O
the	O
importance	O
of	O
bidirectional	O
pre	O
-	O
training	O
for	O
language	O
representations	O
.	O

In	O
addition	O
to	O
the	O
masked	O
language	O
model	O
,	O
we	O
also	O
use	O
a	O
"	O
next	O
sentence	O
prediction	O
"	O
task	O
that	O
jointly	O
pretrains	O
text	O
-	O
pair	O
representations	O
.	O

The	O
code	O
and	O
pre	O
-	O
trained	O
models	O
are	O
available	O
at	O
https://github.com/	O
google	O
-	O
research	O
/	O
bert	O
.	O

To	O
train	O
sentence	O
representations	O
,	O
prior	O
work	O
has	O
used	O
objectives	O
to	O
rank	O
candidate	O
next	O
sentences	O
(	O
Jernite	O
et	O
al	O
.	O
,	O
2017;Logeswaran	O
and	O
Lee	O
,	O
2018	O
)	O
,	O
left	O
-	O
to	O
-	O
right	O
generation	O
of	O
next	O
sentence	O
words	O
given	O
a	O
representation	O
of	O
the	O
previous	O
sentence	O
(	O
Kiros	O
et	O
al	O
.	O
,	O
2015	O
)	O
,	O
or	O
denoising	B-MethodName
autoencoder	I-MethodName
derived	O
objectives	O
(	O
Hill	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

(	O
2018	O
)	O
shows	O
that	O
the	O
cloze	O
task	O
can	O
be	O
used	O
to	O
improve	O
the	O
robustness	O
of	O
text	O
generation	O
models	O
.	O

More	O
recently	O
,	O
sentence	O
or	O
document	O
encoders	O
which	O
produce	O
contextual	O
token	O
representations	O
have	O
been	O
pre	O
-	O
trained	O
from	O
unlabeled	O
text	O
and	O
fine	O
-	O
tuned	O
for	O
a	O
supervised	O
downstream	O
task	O
(	O
Dai	O
and	O
Le	O
,	O
2015;Howard	O
and	O
Ruder	O
,	O
2018;Radford	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

Model	O
Architecture	O
BERT	B-MethodName
's	O
model	O
architecture	O
is	O
a	O
multi	O
-	O
layer	O
bidirectional	O
Transformer	O
encoder	O
based	O
on	O
the	O
original	O
implementation	O
described	O
in	O
Vaswani	O
et	O
al	O
.	O

1	O
Because	O
the	O
use	O
of	O
Transformers	O
has	O
become	O
common	O
and	O
our	O
implementation	O
is	O
almost	O
identical	O
to	O
the	O
original	O
,	O
we	O
will	O
omit	O
an	O
exhaustive	O
background	O
description	O
of	O
the	O
model	O
architecture	O
and	O
refer	O
readers	O
to	O
Vaswani	O
et	O
al	O
.	O

Task	O
#	O
1	O
:	O
Masked	O
LM	O
Intuitively	O
,	O
it	O
is	O
reasonable	O
to	O
believe	O
that	O
a	O
deep	O
bidirectional	O
model	O
is	O
strictly	O
more	O
powerful	O
than	O
either	O
a	O
left	O
-	O
to	O
-	O
right	O
model	O
or	O
the	O
shallow	O
concatenation	O
of	O
a	O
left	O
-	O
toright	O
and	O
a	O
right	O
-	O
to	O
-	O
left	O
model	O
.	O

We	O
refer	O
to	O
this	O
procedure	O
as	O
a	O
"	O
masked	O
LM	O
"	O
(	O
MLM	O
)	O
,	O
although	O
it	O
is	O
often	O
referred	O
to	O
as	O
a	O
Cloze	O
task	O
in	O
the	O
literature	O
(	O
Taylor	O
,	O
1953	O
)	O
.	O

We	O
compare	O
variations	O
of	O
this	O
procedure	O
in	O
Appendix	O
C.2	O
.	O
Task	O
#	O
2	O
:	O
Next	O
Sentence	O
Prediction	O
(	O
NSP	O
)	O
Many	O
important	O
downstream	O
tasks	O
such	O
as	O
Question	B-TaskName
Answering	I-TaskName
(	O
QA	B-TaskName
)	O
and	O
Natural	B-TaskName
Language	I-TaskName
Inference	I-TaskName
(	O
NLI	B-TaskName
)	O
are	O
based	O
on	O
understanding	O
the	O
relationship	O
between	O
two	O
sentences	O
,	O
which	O
is	O
not	O
directly	O
captured	O
by	O
language	O
modeling	O
.	O

As	O
we	O
show	O
in	O
Figure	O
1	O
,	O
C	O
is	O
used	O
for	O
next	O
sentence	O
prediction	O
(	O
NSP	O
)	O
.	O

6	O
he	O
likes	O
play	O
#	O
#	O
ing	O
[	O
SEP	O
]	O
my	O
dog	O
is	O
cute	O
[	O
SEP	O
]	O
Input	O
E	O
[	O
CLS	O
]	O
E	O
he	O
E	O
likes	O
E	O
play	O
E	O
#	O
#	O
ing	O
E	O
[	O
SEP	O
]	O
E	O
my	O
E	O
dog	O
E	O
is	O
E	O
cute	O
E	O
[	O
SEP	O
]	O
Token	O
Embeddings	O
E	O
A	O
E	O
B	O
E	O
B	O
E	O
B	O
E	O
B	O
E	O
B	O
E	O
A	O
E	O
A	O
E	O
A	O
E	O
A	O
E	O
A	O
Segment	O
Embeddings	O
E	O
0	O
E	O
6	O
E	O
7	O
E	O
8	O
E	O
9	O
E	O
10	O
E	O
1	O
E	O
2	O
E	O
3	O
E	O
4	O
E	O
5	O
Position	O
Embeddings	O
The	O
NSP	O
task	O
is	O
closely	O
related	O
to	O
representationlearning	O
objectives	O
used	O
in	O
Jernite	O
et	O
al	O
.	O

•	O
GPT	B-MethodName
was	O
trained	O
for	O
1	B-HyperparameterValue
M	I-HyperparameterValue
steps	B-HyperparameterName
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
32,000	B-HyperparameterValue
words	I-HyperparameterValue
;	O
BERT	B-MethodName
was	O
trained	O
for	O
1	B-HyperparameterValue
M	I-HyperparameterValue
steps	B-HyperparameterName
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
128,000	B-HyperparameterValue
words	I-HyperparameterValue
.	O

(	O
2018a	O
):	O
MNLI	B-DatasetName
Multi	B-DatasetName
-	I-DatasetName
Genre	I-DatasetName
Natural	I-DatasetName
Language	I-DatasetName
Inference	I-DatasetName
is	O
a	O
large	O
-	O
scale	O
,	O
crowdsourced	O
entailment	B-TaskName
classification	I-TaskName
task	O
(	O
Williams	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

QQP	B-DatasetName
Quora	B-DatasetName
Question	I-DatasetName
Pairs	I-DatasetName
is	O
a	O
binary	B-TaskName
classification	I-TaskName
task	O
where	O
the	O
goal	O
is	O
to	O
determine	O
if	O
two	O
questions	O
asked	O
on	O
Quora	O
are	O
semantically	O
equivalent	O
(	O
Chen	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

Tok	O
M	O
Question	O
Paragraph	O
BERT	B-MethodName
E	O
[	O
CLS	O
]	O
E	O
1	O
E	O
2	O
E	O
N	O
C	O
T	O
1	O
T	O
2	O
T	O
N	O
Single	O
Sentence	O
...	O

BERT	B-MethodName
Tok	O
1	O
Tok	O
2	O
Tok	O
N	O
...	O

CoLA	B-DatasetName
The	O
Corpus	B-DatasetName
of	I-DatasetName
Linguistic	I-DatasetName
Acceptability	I-DatasetName
is	O
a	O
binary	B-TaskName
single	I-TaskName
-	I-TaskName
sentence	I-TaskName
classification	I-TaskName
task	O
,	O
where	O
the	O
goal	O
is	O
to	O
predict	O
whether	O
an	O
English	O
sentence	O
is	O
linguistically	O
"	O
acceptable	O
"	O
or	O
not	O
(	O
Warstadt	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

We	O
therefore	O
exclude	O
this	O
set	O
to	O
be	O
fair	O
to	O
OpenAI	B-MethodName
GPT	I-MethodName
.	O

The	O
GLUE	B-DatasetName
webpage	O
notes	O
that	O
there	O
are	O
issues	O
with	O
the	O
construction	O
of	O
this	O
dataset	O
,	O
15	O
and	O
every	O
trained	O
system	O
that	O
's	O
been	O
submitted	O
to	O
GLUE	B-DatasetName
has	O
performed	O
worse	O
than	O
the	O
65.1	B-MetricValue
baseline	O
accuracy	B-MetricName
of	O
predicting	O
the	O
majority	O
class	O
.	O

The	O
numbers	O
in	O
the	O
left	O
part	O
of	O
the	O
table	O
represent	O
the	O
probabilities	O
of	O
the	O
specific	O
strategies	O
used	O
during	O
MLM	O
pre	O
-	O
training	O
(	O
BERT	B-MethodName
uses	O
80	B-HyperparameterValue
%	I-HyperparameterValue
,	O
10	B-HyperparameterValue
%	I-HyperparameterValue
,	O
10	B-HyperparameterValue
%	I-HyperparameterValue
)	O
.	O

The	O
advantage	O
of	O
this	O
procedure	O
is	O
that	O
the	O
Transformer	O
encoder	O
does	O
not	O
know	O
which	O
words	O
it	O
will	O
be	O
asked	O
to	O
predict	O
or	O
which	O
have	O
been	O
replaced	O
by	O
random	O
words	O
,	O
so	O
it	O
is	O
forced	O
to	O
keep	O
a	O
distributional	O
contextual	O
representation	O
of	O
every	O
input	O
token	O
.	O

