We	O
have	O
used	O
unsupervised	B-TaskName
NMT	I-TaskName
approach	O
of	O
MASS	B-MethodName
(	O
Song	O
et	O
al	O
.	O
,	O
2019	O
)	O
to	O
build	O
a	O
single	O
model	O
that	O
can	O
translate	O
in	O
both	O
the	O
directions	O
i.e.	O
Russian	O
to	O
Hindi	O
and	O
vice	O
-	O
versa	O
.	O

This	O
paper	O
presents	O
a	O
zero	B-TaskName
-	I-TaskName
shot	I-TaskName
NMT	I-TaskName
task	O
on	O
the	O
Russian	O
⇔	O
Hindi	O
translation	O
,	O
this	O
system	O
was	O
used	O
to	O
participate	O
in	O
the	O
LoResMT	O
2020	O
shared	O
task	O
.	O

In	O
future	O
,	O
we	O
will	O
use	O
IndicNLP	B-MethodName
tokenizer	I-MethodName
(	O
Kunchukuttan	O
,	O
2020	O
)	O
.	O

In	O
this	O
work	O
,	O
we	O
have	O
used	O
the	O
default	O
tokenizer	O
i.e.	O
Moses	B-MethodName
.	O

Task	O
BLEU	B-MetricName
Precision	B-MetricName
Recall	B-MetricName
F	B-MetricName
-	I-MetricName
measure	I-MetricName
RIBES	B-MetricName
.	O

The	O
results	O
are	O
evaluated	O
using	O
automatic	O
evaluation	O
metrics	O
,	O
BLEU	B-MetricName
(	O
Papineni	O
et	O
al	O
.	O
,	O
2002	O
)	O
,	O
precision	B-MetricName
,	O
recall	B-MetricName
,	O
F	B-MetricName
-	I-MetricName
measure	I-MetricName
and	O
RIBES	B-MetricName
(	O
Isozaki	O
et	O
al	O
.	O
,	O
2010	O
)	O
.	O

The	O
LoResMT	O
2020	O
shared	O
task	O
organizer	O
declared	O
the	O
evaluation	O
result	O
5	O
of	O
zero	B-TaskName
-	I-TaskName
shot	I-TaskName
NMT	I-TaskName
on	O
the	O
language	O
pairs	O
namely	O
,	O
Hindi	O
-	O
Bhojpuri	O
,	O
Hindi	O
-	O
Magahi	O
,	O
and	O
Russian	O
-	O
Hindi	O
,	O
and	O
participated	O
by	O
two	O
teams	O
only	O
.	O

Due	O
to	O
limited	O
computational	O
resources	O
,	O
we	O
have	O
used	O
256	B-HyperparameterValue
embedding	B-HyperparameterName
layers	I-HyperparameterName
with	O
batch	B-HyperparameterName
size	I-HyperparameterName
32	B-HyperparameterValue
,	O
tokens	B-HyperparameterName
per	I-HyperparameterName
batch	I-HyperparameterName
500	B-HyperparameterValue
and	O
dropout	B-HyperparameterName
0.1	B-HyperparameterValue
.	O
The	O
obtained	O
pre	O
-	O
trained	O
model	O
from	O
4.1	O
are	O
fine	O
-	O
tuned	O
with	O
pseudo	O
bilingual	O
corpus	O
through	O
self	O
-	O
generated	O
back	O
-	O
translation	O
data	O
following	O
default	O
settings	O
of	O
(	O
Song	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

In	O
the	O
pre	O
-	O
training	O
step	O
,	O
we	O
have	O
followed	O
the	O
default	O
settings	O
of	O
Transformer	O
model	O
-	O
based	O
Mass	B-MethodName
(	O
Song	O
et	O
al	O
.	O
,	O
2019	O
)	O
,	O
where	O
6	B-HyperparameterValue
layers	B-HyperparameterName
with	O
8	B-HyperparameterValue
attention	B-HyperparameterName
heads	I-HyperparameterName
are	O
used	O
.	O

During	O
pre	O
-	O
processing	O
of	O
the	O
data	O
,	O
following	O
(	O
Song	O
et	O
al	O
.	O
,	O
2019	O
)	O
and	O
using	O
the	O
code	O
provided	O
by	O
(	O
Lample	O
and	O
Conneau	O
,	O
2019	O
)	O
,	O
we	O
used	O
fastBPE	B-MethodName
4	O
to	O
learn	O
byte	O
pair	O
encoding	O
(	O
BPE	O
)	O
vocabulary	O
with	O
50,000	O
codes	O
.	O

(	O
1	O
)	O
Here	O
,	O
the	O
seq2seq	B-MethodName
model	O
learns	O
the	O
parameter	O
θ	O
to	O
compute	O
the	O
conditional	O
probability	O
.	O

For	O
the	O
pre	O
-	O
training	O
step	O
,	O
following	O
(	O
Song	O
et	O
al	O
.	O
,	O
2019	O
)	O
we	O
have	O
undertaken	O
the	O
log	B-HyperparameterValue
likelihood	I-HyperparameterValue
objective	B-HyperparameterName
function	I-HyperparameterName
(	O
LF	O
)	O
as	O
shown	O
in	O
Equation	O
1	O
.	O

The	O
MASS	B-MethodName
(	O
Song	O
et	O
al	O
.	O
,	O
2019	O
)	O
based	O
model	O
leverages	O
encode	O
-	O
decoder	O
framework	O
to	O
develop	O
complete	O
sentences	O
from	O
given	O
fractured	O
pieces	O
of	O
sentences	O
as	O
shown	O
in	O
Figure	O
1	O
.	O

Moses	B-MethodName
is	O
used	O
for	O
tokenization	O
(	O
Koehn	O
and	O
Hoang	O
,	O
2010	O
)	O
.	O

Our	O
system	O
consists	O
of	O
two	O
major	O
steps	O
namely	O
the	O
pre	O
-	O
training	O
and	O
then	O
the	O
fine	O
-	O
tuning	O
step	O
which	O
are	O
discussed	O
in	O
the	O
sub	O
-	O
sections	O
4.1	O
and	O
4.2	O
.	O
For	O
BPE	O
(	O
Sennrich	O
et	O
al	O
.	O
,	O
2016	O
)	O
and	O
vocabulary	O
creation	O
,	O
we	O
have	O
used	O
the	O
cross	B-MethodName
-	I-MethodName
language	I-MethodName
model	I-MethodName
(	O
XLM	O
)	O
(	O
Lample	O
and	O
Conneau	O
,	O
2019	O
)	O
codebase	O
as	O
given	O
in	O
their	O
repository	O
3	O
.	O

We	O
have	O
adopted	O
MASS	B-MethodName
based	I-MethodName
unsupervised	I-MethodName
NMT	I-MethodName
(	O
Song	O
et	O
al	O
.	O
,	O
2019	O
)	O
to	O
build	O
our	O
system	O
on	O
a	O
single	O
GPU	O
.	O

Additionally	O
,	O
we	O
have	O
used	O
external	O
monolingual	O
data	O
set	O
of	O
Hindi	O
(	O
9	O
GB	O
)	O
from	O
IITB	B-DatasetName
1	O
(	O
Kunchukuttan	O
et	O
al	O
.	O
,	O
2018;Bojar	O
et	O
al	O
.	O
,	O
2014	O
)	O
and	O
Russian	O
(	O
9	O
GB	O
)	O
from	O
WMT16	B-DatasetName
2	O
.	O

The	O
LoResMT	B-DatasetName
2020	I-DatasetName
shared	I-DatasetName
task	I-DatasetName
organizer	O
(	O
Ojha	O
et	O
al	O
.	O
,	O
2020	O
)	O
provided	O
the	O
Russian	B-DatasetName
-	I-DatasetName
Hindi	I-DatasetName
monolingual	I-DatasetName
dataset	O
of	O
train	O
,	O
valid	O
,	O
and	O
test	O
sets	O
,	O
which	O
is	O
summarized	O
in	O
Table	O
1	O
.	O

Figure	O
1	O
:	O
The	O
encoder	O
-	O
decoder	O
framework	O
of	O
the	O
MASS	B-MethodName
model	O
used	O
(	O
as	O
adopted	O
from	O
(	O
Song	O
et	O
al	O
.	O
,	O
2019	O
)	O
)	O
.	O

However	O
,	O
the	O
literature	O
survey	O
finds	O
work	O
on	O
unsupervised	B-MethodName
NMT	I-MethodName
using	I-MethodName
MASS	I-MethodName
(	O
Song	O
et	O
al	O
.	O
,	O
2019	O
)	O
which	O
outperform	O
previous	O
unsupervised	O
approaches	O
(	O
Lample	O
and	O
Conneau	O
,	O
2019;Lample	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

The	O
reason	O
behind	O
choosing	O
MASS	B-MethodName
-	I-MethodName
based	I-MethodName
unsupervised	I-MethodName
NMT	I-MethodName
is	O
that	O
it	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
the	O
unsupervised	O
English	O
-	O
French	O
pair	O
translation	O
.	O

In	O
this	O
paper	O
,	O
we	O
have	O
participated	O
in	O
the	O
LoResMT	O
2020	O
shared	O
task	O
of	O
zero	B-TaskName
-	I-TaskName
shot	I-TaskName
NMT	I-TaskName
approach	O
on	O
Russian	O
-	O
Hindi	O
pair	O
using	O
the	O
only	O
monolingual	O
corpus	O
and	O
the	O
same	O
has	O
been	O
implemented	O
using	O
MASS	B-MethodName
-	I-MethodName
based	I-MethodName
unsupervised	I-MethodName
NMT	I-MethodName
(	O
Song	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

(	O
Johnson	O
et	O
al	O
.	O
,	O
2017	O
)	O
introduced	O
a	O
zero	O
-	O
shot	O
approach	O
to	O
language	O
pair	O
translation	O
without	O
considering	O
the	O
parallel	O
data	O
using	O
multilingual	B-TaskName
-	I-TaskName
based	I-TaskName
NMT	I-TaskName
.	O

For	O
low	O
resource	O
language	O
pair	O
translation	O
,	O
pivot	B-MethodName
-	I-MethodName
based	I-MethodName
NMT	I-MethodName
(	O
Kim	O
et	O
al	O
.	O
,	O
2019	O
)	O
is	O
an	O
effective	O
approach	O
where	O
an	O
intermediate	O
language	O
is	O
considered	O
as	O
a	O
pivot	O
language	O
(	O
source	O
to	O
pivot	O
and	O
pivot	O
to	O
target	O
)	O
.	O

The	O
RNN	B-MethodName
based	I-MethodName
NMT	I-MethodName
approach	O
is	O
not	O
able	O
to	O
process	O
all	O
the	O
input	O
words	O
parallelly	O
,	O
to	O
solve	O
parallelization	O
transformer	O
-	O
based	O
NMT	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O
2017	O
)	O
is	O
proposed	O
by	O
using	O
a	O
self	O
-	O
attention	O
mechanism	O
.	O

The	O
end	B-MethodName
-	I-MethodName
to	I-MethodName
-	I-MethodName
end	I-MethodName
recurrent	I-MethodName
neural	I-MethodName
network	I-MethodName
(	O
RNN	B-MethodName
)	O
based	O
NMT	O
(	O
Cho	O
et	O
al	O
.	O
,	O
2014b	O
,	O
a	O
)	O
approach	O
attracts	O
attention	O
in	O
MT	B-TaskName
because	O
it	O
deals	O
with	O
many	O
challenges	O
like	O
variable	O
-	O
length	O
phrases	O
using	O
sequence	O
to	O
sequence	O
learning	O
concept	O
,	O
long	O
-	O
term	O
dependency	O
problem	O
adopting	O
long	O
short	O
term	O
memory	O
(	O
LSTM	O
)	O
(	O
Sutskever	O
et	O
al	O
.	O
,	O
2014	O
)	O
,	O
attention	O
mechanism	O
(	O
Bahdanau	O
et	O
al	O
.	O
,	O
2015;Luong	O
et	O
al	O
.	O
,	O
2015	O
)	O
which	O
pays	O
attention	O
globally	O
and	O
locally	O
to	O
all	O
source	O
words	O
.	O

And	O
for	O
Hindi	O
to	O
Russian	O
translation	O
,	O
we	O
have	O
achieved	O
BLEU	B-MetricName
,	O
precision	B-MetricName
,	O
recall	B-MetricName
,	O
F	B-MetricName
-	I-MetricName
measure	I-MetricName
,	O
and	O
RIBES	B-MetricName
score	O
of	O
1.11	B-MetricValue
,	O
4.72	B-MetricValue
,	O
4.41	B-MetricValue
,	O
4.56	B-MetricValue
,	O
and	O
0.026842	B-MetricValue
respectively	O
.	O

The	O
evaluated	O
results	O
are	O
declared	O
at	O
the	O
LoResMT	O
2020	O
shared	O
task	O
,	O
which	O
reports	O
that	O
our	O
system	O
achieves	O
the	O
bilingual	B-MetricName
evaluation	I-MetricName
understudy	I-MetricName
(	O
BLEU	B-MetricName
)	O
score	O
of	O
0.59	B-MetricValue
,	O
precision	B-MetricName
score	O
of	O
3.43	B-MetricValue
,	O
recall	B-MetricName
score	O
of	O
5.48	B-MetricValue
,	O
F	B-MetricName
-	I-MetricName
measure	I-MetricName
score	O
of	O
4.22	B-MetricValue
,	O
and	O
rank	B-MetricName
-	I-MetricName
based	I-MetricName
intuitive	I-MetricName
bilingual	I-MetricName
evaluation	I-MetricName
score	I-MetricName
(	O
RIBES	B-MetricName
)	O
of	O
0.180147	B-MetricValue
in	O
Russian	O
to	O
Hindi	O
translation	O
.	O

We	O
have	O
used	O
masked	B-MethodName
sequence	I-MethodName
to	I-MethodName
sequence	I-MethodName
pre	I-MethodName
-	I-MethodName
training	I-MethodName
for	I-MethodName
language	I-MethodName
generation	I-MethodName
(	O
MASS	B-MethodName
)	O
with	O
only	O
monolingual	O
corpus	O
following	O
the	O
unsupervised	B-TaskName
NMT	I-TaskName
architecture	O
.	O

Workshop	O
on	O
Technologies	O
for	O
MT	B-TaskName
of	O
Low	O
Resource	O
Languages	O
(	O
LoResMT	O
2020	O
)	O
organized	O
shared	O
tasks	O
of	O
low	O
resource	O
language	O
pair	O
translation	O
using	O
zero	B-TaskName
-	I-TaskName
shot	I-TaskName
NMT	I-TaskName
.	O

To	O
mitigate	O
this	O
issue	O
,	O
NMT	B-MethodName
attempts	O
to	O
utilize	O
a	O
monolingual	O
corpus	O
to	O
get	O
better	O
at	O
translation	O
for	O
low	O
resource	O
language	O
pairs	O
.	O

Neural	B-MethodName
machine	I-MethodName
translation	I-MethodName
(	O
NMT	B-MethodName
)	O
is	O
a	O
widely	O
accepted	O
approach	O
in	O
the	O
machine	B-TaskName
translation	I-TaskName
(	O
MT	B-TaskName
)	O
community	O
,	O
translating	O
from	O
one	O
natural	O
language	O
to	O
another	O
natural	O
language	O
.	O

Zero	B-TaskName
-	I-TaskName
Shot	I-TaskName
Neural	I-TaskName
Machine	I-TaskName
Translation	I-TaskName
:	O
Russian	O
-	O
Hindi	O
@LoResMT	O
2020	O
.	O

Also	O
,	O
thank	O
to	O
LoResMT	O
2020	O
shared	O
task	O
organizers	O
.	O

We	O
would	O
like	O
to	O
thank	O
Center	O
for	O
Natural	O
Language	O
Processing	O
(	O
CNLP	O
)	O
and	O
Department	O
of	O
Computer	O
Science	O
and	O
Engineering	O
at	O
National	O
Institute	O
of	O
Technology	O
,	O
Silchar	O
,	O
India	O
for	O
providing	O
the	O
requisite	O
support	O
and	O
infrastructure	O
to	O
execute	O
this	O
work	O
.	O

Acknowledgement	O
.	O

The	O
obtained	O
scores	O
and	O
closely	O
observed	O
predicted	O
output	O
remarks	O
that	O
our	O
future	O
works	O
require	O
significant	O
improvement	O
to	O
achieve	O
better	O
translation	O
accuracies	O
in	O
both	O
directions	O
.	O

Conclusion	O
and	O
Future	O
Work	O
.	O

This	O
tokenizer	O
is	O
specifically	O
designed	O
for	O
Indic	O
languages	O
,	O
in	O
order	O
to	O
improve	O
the	O
overall	O
performance	O
of	O
predictive	O
models	O
in	O
Hindi	O
languages	O
.	O

To	O
achieve	O
better	O
translation	O
accuracy	O
,	O
we	O
need	O
to	O
improve	O
both	O
adequacy	O
as	O
well	O
as	O
fluency	O
of	O
predicted	O
translations	O
.	O

Moreover	O
,	O
from	O
the	O
predicted	O
translation	O
as	O
shown	O
in	O
Figure	O
2	O
,	O
it	O
is	O
quite	O
clear	O
that	O
the	O
translation	O
accuracy	O
is	O
very	O
poor	O
in	O
terms	O
of	O
adequacy	O
but	O
better	O
in	O
the	O
fluency	O
factor	O
of	O
translation	O
.	O

However	O
,	O
it	O
is	O
to	O
be	O
noted	O
that	O
with	O
increasing	O
monolingual	O
data	O
,	O
the	O
performance	O
of	O
our	O
systems	O
improves	O
.	O

From	O
Table	O
2	O
,	O
it	O
is	O
observed	O
that	O
our	O
scores	O
are	O
very	O
low	O
.	O

We	O
have	O
submitted	O
two	O
systems	O
result	O
,	O
one	O
only	O
using	O
provided	O
monolingual	O
data	O
(	O
extension	O
-a	O
)	O
and	O
another	O
with	O
external	O
monolingual	O
data	O
addition	O
of	O
provided	O
monolingual	O
data	O
(	O
extension	O
-c	O
)	O
and	O
the	O
same	O
have	O
been	O
reported	O
in	O
Table	O
2	O
.	O

For	O
the	O
Russian	O
-	O
Hindi	O
language	O
pair	O
,	O
only	O
our	O
team	O
participated	O
and	O
our	O
team	O
name	O
is	O
CNLP	O
-	O
NITS	O
.	O

Result	O
and	O
Analysis	O
.	O

Also	O
,	O
for	O
leveraging	O
the	O
model	O
features	O
,	O
we	O
have	O
followed	O
the	O
settings	O
of	O
(	O
Song	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

Experimental	O
Setup	O
.	O

Experiment	O
.	O

Here	O
simply	O
backtranslation	O
is	O
employed	O
to	O
generate	O
pseudo	O
bilin-	O
.	O

Only	O
the	O
monolingual	O
data	O
is	O
used	O
here	O
.	O

Since	O
,	O
the	O
parallel	O
data	O
is	O
not	O
made	O
available	O
by	O
the	O
LoResMT	O
2020	O
organizers	O
for	O
this	O
specific	O
task	O
,	O
we	O
have	O
undertaken	O
the	O
unsupervised	O
approach	O
as	O
followed	O
by	O
(	O
Song	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

Fine	O
Tuning	O
.	O

t	O
denotes	O
the	O
word	O
position	O
.	O

LF	O
(	O
θ	O
;	O
S	O
)	O
=	O
1	O
|S|	O
Σ	O
s∈S	O
log	O
P	O
(	O
s	O
u	O
:	O
v	O
|s	O
\u	O
:	O
v	O
;	O
θ	O
)	O
=	O
1	O
|S|	O
Σ	O
s∈S	O
log	O
v	O
t	O
=	O
u	O
P	O
(	O
s	O
u	O
:	O
v	O
t	O
|s	O
u	O
:	O
v	O
<	O
t	O
,	O
s	O
\u	O
:	O
v	O
;	O
θ	O
)	O
.	O

And	O
in	O
a	O
particular	O
sentence	O
s	O
,	O
the	O
region	O
from	O
u	O
to	O
v	O
is	O
masked	O
,	O
such	O
that	O
the	O
sentence	O
length	O
remains	O
constant	O
.	O

Here	O
,	O
s	O
belongs	O
to	O
the	O
source	O
sentence	O
corpus	O
S.	O

Pre	O
-	O
training	O
.	O

The	O
model	O
details	O
are	O
further	O
described	O
in	O
Section	O
4.1	O
and	O
4.2	O
,	O
where	O
we	O
have	O
shown	O
the	O
pre	O
-	O
training	O
and	O
fine	O
tuning	O
step	O
respectively	O
.	O

System	O
Description	O
.	O

Dataset	O
Description	O
.	O

Attention	O
.	O

(	O
Song	O
et	O
al	O
.	O
,	O
2019	O
)	O
without	O
us-	O
X	O
8	O
X	O
4	O
X	O
7	O
X	O
1	O
X	O
2	O
X	O
3	O
_	O
X	O
6	O
_	O
_	O
_	O
_	O
_	O
Encoder	O
Decoder	O
_	O
_	O
X	O
5	O
.	O

There	O
is	O
a	O
lack	O
of	O
background	O
work	O
on	O
Russian	O
-	O
Hindi	O
translation	O
.	O

Related	O
Work	O
.	O

Generally	O
,	O
language	O
pairs	O
can	O
be	O
considered	O
as	O
low	O
-	O
resource	O
when	O
training	O
data	O
is	O
less	O
than	O
a	O
million	O
(	O
Kocmi	O
,	O
2020	O
)	O
.	O

Despite	O
modifying	O
NMT	O
architecture	O
,	O
it	O
needs	O
reasonable	O
parallel	O
training	O
data	O
which	O
is	O
a	O
challenge	O
for	O
low	O
resource	O
language	O
pair	O
translation	O
.	O

Introduction	O
.	O

We	O
have	O
participated	O
in	O
the	O
same	O
shared	O
task	O
with	O
our	O
team	O
name	O
CNLP	O
-	O
NITS	O
for	O
the	O
Russian	O
-	O
Hindi	O
language	O
pair	O
.	O

Here	O
,	O
the	O
parallel	O
corpus	O
is	O
not	O
used	O
and	O
only	O
monolingual	O
corpora	O
is	O
allowed	O
.	O

The	O
availability	O
of	O
a	O
parallel	O
corpus	O
in	O
low	O
resource	O
language	O
pairs	O
is	O
one	O
of	O
the	O
challenging	O
tasks	O
in	O
MT	O
.	O

Although	O
,	O
NMT	O
shows	O
remarkable	O
performance	O
in	O
both	O
high	O
and	O
low	O
resource	O
languages	O
,	O
it	O
needs	O
sufficient	O
training	O
corpus	O
.	O

