Zero	B-TaskName
-	I-TaskName
shot	I-TaskName
retrieval	I-TaskName
is	O
challenging	O
due	O
to	O
the	O
high	O
degree	O
of	O
ambiguity	O
and	O
variability	O
in	O
medical	O
corpora	O
,	O
making	O
it	O
difficult	O
to	O
build	O
an	O
accurate	O
similarity	O
measure	O
between	O
mentions	O
and	O
concepts	O
.	O

Current	O
approaches	O
tend	O
to	O
work	O
well	O
on	O
specific	O
medical	O
domains	O
but	O
generalize	O
poorly	O
to	O
unseen	O
sub	O
-	O
specialties	O
.	O

Medical	B-TaskName
entity	I-TaskName
retrieval	I-TaskName
is	O
an	O
integral	O
component	O
for	O
understanding	O
and	O
communicating	O
information	O
across	O
various	O
health	O
systems	O
.	O

Zero	O
-	O
shot	O
Medical	B-TaskName
Entity	I-TaskName
Retrieval	I-TaskName
without	O
Annotation	O
:	O
Learning	O
From	O
Rich	O
Knowledge	O
Graph	O
Semantics	O
.	O

Zero	O
-	O
shot	O
entity	O
retrieval	O
.	O

The	O
Code	O
section	O
contains	O
an	O
ICD-10	O
code	O
and	O
its	O
formal	O
medical	O
definition	O
,	O
denoted	O
by	O
N	O
CodeDescription	O
i	O
.	O

We	O
denote	O
it	O
by	O
N	O
T	O
itleConcatenation	O
i	O
.	O

Each	O
node	O
N	O
i	O
has	O
three	O
sections	O
:	O
The	O
Title	O
section	O
contains	O
a	O
subspecifier	O
(	O
e.g.	O
Chest	O
)	O
of	O
the	O
title	O
of	O
the	O
parent	O
(	O
e.g.	O
Pain	O
)	O
,	O
therefore	O
their	O
concatenation	O
gives	O
the	O
full	O
concept	O
description	O
(	O
e.g.	O
Chest	O
Pain	O
)	O
.	O

In	O
ICD-10	O
,	O
a	O
child	O
node	O
is	O
a	O
more	O
specific	O
medical	O
condition	O
compared	O
to	O
its	O
parent	O
(	O
e.g.	O
R07.9	O
Chest	O
pain	O
,	O
unspecified	O
is	O
a	O
child	O
of	O
R52	O
Pain	O
,	O
unspecified	O
)	O
.	O

Synonym	O
-	O
based	O
task	O
.	O

It	O
contains	O
over	O
69	O
K	O
concepts	O
,	O
organized	O
in	O
a	O
tree	O
structure	O
of	O
parent	O
-	O
child	O
relationships	O
.	O

The	O
10th	O
version	O
of	O
the	O
International	O
Statistical	O
Classification	O
of	O
Diseases	O
,	O
Clinical	O
Modification	O
(	O
ICD-10	O
)	O
is	O
one	O
of	O
the	O
most	O
widely	O
used	O
terminology	O
systems	O
for	O
medical	O
conditions	O
.	O

ICD-10	O
.	O

P	O
(	O
(	O
m	O
i	O
,	O
c	O
i	O
)	O
|θ	O
)	O
=	O
exp(S	O
θ	O
(	O
m	O
i	O
,	O
c	O
i	O
)	O
)	O
B	O
j=1	O
exp(S	O
θ	O
(	O
m	O
j	O
,	O
c	O
j	O
)	O
)	O
.	O

These	O
are	O
illustrated	O
below	O
for	O
three	O
major	O
medical	O
KG	O
:	O
ICD-10	O
,	O
SNOMED	O
and	O
UMLS	O
.	O

We	O
define	O
two	O
major	O
types	O
of	O
tasks	O
:	O
synonym	O
-	O
based	O
tasks	O
and	O
graph	O
-	O
based	O
tasks	O
.	O

Since	O
each	O
structure	O
implies	O
its	O
own	O
measure	O
of	O
similarity	O
,	O
we	O
design	O
learning	O
tasks	O
by	O
finding	O
very	O
similar	O
or	O
closely	O
related	O
textual	O
descriptions	O
and	O
use	O
them	O
to	O
construct	O
(	O
m	O
,	O
c	O
)	O
pairs	O
.	O

The	O
goal	O
is	O
to	O
capture	O
multiple	O
layers	O
of	O
semantics	O
from	O
a	O
KG	O
by	O
leveraging	O
its	O
unique	O
structure	O
.	O

Loss	O
function	O
for	O
a	O
batch	B-HyperparameterName
of	I-HyperparameterName
size	I-HyperparameterName
B	B-HyperparameterName
is	O
defined	O
as	O
mean	O
negative	O
log	O
likelihood	O
:	O
L	O
=	O
−	O
1	O
B	O
B	O
i=1	O
log(P	O
(	O
(	O
m	O
i	O
,	O
c	O
i	O
)	O
|θ	O
)	O
)	O
where	O
the	O
conditional	O
probability	O
of	O
each	O
mentionconcept	O
pair	O
(	O
m	O
i	O
,	O
c	O
i	O
)	O
in	O
the	O
batch	O
is	O
modeled	O
as	O
a	O
softmax	O
:	O
We	O
design	O
our	O
learning	O
tasks	O
by	O
constructing	O
mention	O
-	O
concept	O
pairs	O
(	O
m	O
,	O
c	O
)	O
.	O

We	O
use	O
in	O
-	O
batch	O
negatives	O
for	O
optimization	O
.	O

Assume	O
model	O
parameter	O
is	O
θ	O
.	O

Optimization	O
.	O

Similarity	O
between	O
a	O
mention	O
and	O
a	O
concept	O
is	O
then	O
measured	O
as	O
the	O
inner	O
product	O
:	O
S(m	O
,	O
c	O
)	O
=	O
e	O
m	O
,	O
e	O
c	O
.	O

T	O
is	O
also	O
referred	O
to	O
as	O
an	O
encoder	O
,	O
for	O
which	O
we	O
use	O
the	O
Transformer	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O
2017	O
)	O
encoder	O
in	O
this	O
work	O
.	O

Mention	O
m	O
and	O
concept	O
c	O
are	O
firstly	O
embedded	O
into	O
vectors	O
,	O
using	O
a	O
shared	O
function	O
T	O
:	O
e	O
m	O
=	O
T	O
(	O
m	O
)	O
,	O
e	O
c	O
=	O
T	O
(	O
c	O
)	O
.	O

Siamese	B-MethodName
architecture	I-MethodName
.	O

Model	O
Architecture	O
.	O

2	O
)	O
zero	O
-	O
shot	O
on	O
mentions	O
and	O
concepts	O
,	O
which	O
assumes	O
both	O
to	O
be	O
unseen	O
at	O
test	O
time	O
.	O

We	O
examine	O
two	O
zero	O
-	O
shot	O
scenarios	O
:	O
1	O
)	O
zero	O
-	O
shot	O
on	O
mentions	O
only	O
,	O
which	O
assumes	O
unseen	O
mentions	O
but	O
allows	O
seen	O
concepts	O
at	O
test	O
time	O
.	O

We	O
use	O
them	O
interchangeably	O
below	O
.	O

A	O
concept	O
is	O
also	O
referred	O
to	O
as	O
a	O
node	O
in	O
a	O
KG	O
.	O

Given	O
a	O
mention	O
m	O
and	O
a	O
concept	O
c	O
∈	O
KG	O
=	O
{	O
c	O
1	O
,	O
c	O
2	O
,	O
...	O
,	O
c	O
n	O
}	O
,	O
the	O
goal	O
is	O
to	O
learn	O
a	O
similarity	O
measurement	O
S(m	O
,	O
c	O
)	O
,	O
so	O
that	O
the	O
most	O
relevant	O
concept	O
is	O
assigned	O
the	O
highest	O
score	O
.	O

2	O
Formulation	O
Entity	B-MethodName
retrieval	I-MethodName
.	O

(	O
3	O
)	O
When	O
annotations	O
are	O
available	O
,	O
we	O
show	O
that	O
the	O
proposed	O
framework	O
can	O
be	O
easily	O
plugged	O
into	O
an	O
existing	O
supervised	O
approach	O
and	O
in	O
so	O
doing	O
,	O
deliver	O
consistent	O
improvements	O
.	O

(	O
2	O
)	O
We	O
apply	O
the	O
framework	O
to	O
major	O
medical	O
ontologies	O
and	O
conduct	O
extensive	O
experiments	O
to	O
establish	O
the	O
effectiveness	O
of	O
our	O
framework	O
.	O

Our	O
contributions	O
are	O
as	O
follows	O
.	O

(	O
1	O
)	O
We	O
pro	O
-	O
pose	O
a	O
framework	O
which	O
allows	O
the	O
information	O
in	O
medical	O
KGs	O
to	O
be	O
incorporated	O
into	O
entity	O
retrieval	O
models	O
,	O
thereby	O
enabling	O
robust	O
zero	O
-	O
shot	O
performance	O
without	O
the	O
need	O
of	O
human	O
annotations	O
.	O

Trained	O
with	O
our	O
proposed	O
tasks	O
,	O
a	O
simple	O
Siamese	B-MethodName
architecture	I-MethodName
significantly	O
outperforms	O
common	O
zero	O
-	O
shot	O
benchmarks	O
across	O
multiple	O
major	O
medical	O
ontologies	O
including	O
UMLS	O
,	O
SNOMED	O
and	O
ICD10	O
.	O

(	O
2019	O
)	O
also	O
integrates	O
graph	O
structure	O
information	O
during	O
pretraining	O
.	O

In	O
recent	O
years	O
,	O
large	O
scale	O
pretraining	O
(	O
Devlin	O
et	O
al	O
.	O
,	O
2019	O
)	O
has	O
been	O
widely	O
adopted	O
in	O
the	O
medical	O
domain	O
such	O
as	O
Clinical	O
BERT	O
(	O
Alsentzer	O
et	O
al	O
.	O
,	O
2019	O
)	O
and	O
BioBERT	O
(	O
Lee	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

Early	O
entity	O
retrieval	O
systems	O
use	O
string	O
matching	O
methods	O
such	O
as	O
exact	O
match	O
,	O
approximate	O
match	O
(	O
Hanisch	O
et	O
al	O
.	O
,	O
2005	O
)	O
and	O
weighted	O
keyword	O
match	O
e.g.	O
BM25	O
(	O
Wang	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

It	O
is	O
difficult	O
to	O
build	O
an	O
accurate	O
similarity	O
measure	O
which	O
can	O
detect	O
the	O
true	O
relatedness	O
between	O
a	O
mention	O
and	O
a	O
concept	O
even	O
when	O
their	O
surface	O
forms	O
differ	O
greatly	O
.	O

Zero	B-TaskName
-	I-TaskName
shot	I-TaskName
retrieval	I-TaskName
is	O
challenging	O
due	O
to	O
the	O
complexity	O
of	O
medical	O
corpora	O
-large	O
numbers	O
of	O
ambiguous	O
terms	O
,	O
copious	O
acronyms	O
and	O
synonymous	O
terms	O
.	O

Hence	O
,	O
a	O
robust	O
medical	O
entity	O
retrieval	O
system	O
is	O
expected	O
to	O
have	O
decent	O
performance	O
in	O
a	O
zero	O
-	O
shot	O
scenario	O
.	O

Training	O
an	O
effective	O
entity	O
retrieval	O
system	O
often	O
requires	O
high	O
quality	O
annotations	O
,	O
which	O
are	O
expensive	O
and	O
slow	O
to	O
produce	O
in	O
the	O
medical	O
domain	O
.	O

For	O
evaluations	O
of	O
zero	O
-	O
shot	O
on	O
mentions	O
only	O
(	O
e.g.	O
UMLS	O
tasks	O
evaluated	O
on	O
MedMention	B-DatasetName
which	O
is	O
UMLS	O
annotated	O
)	O
,	O
we	O
observe	O
12	O
%	O
to	O
45	O
%	O
gain	O
for	O
R@1	O
compared	O
to	O
benchmarks	O
.	O

Clinical	B-MethodName
BERT	I-MethodName
consistently	O
outperforms	O
the	O
other	O
pre	O
-	O
trained	O
counterparts	O
,	O
which	O
are	O
therefore	O
omitted	O
.	O

We	O
report	O
overall	O
results	O
in	O
Table	O
3	O
.	O

Results	O
.	O

We	O
also	O
assume	O
that	O
each	O
mention	O
has	O
a	O
valid	O
gold	O
concept	O
in	O
the	O
KG	O
.	O

Top	O
-	O
k	O
retrieval	O
recalls	B-MetricName
(	O
R@1	O
,	O
R@25	O
)	O
are	O
used	O
as	O
metrics	O
.	O

Evaluation	O
metrics	O
.	O

We	O
use	O
the	O
BertAdam	B-HyperparameterValue
optimizer	B-HyperparameterName
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
128	B-HyperparameterValue
,	O
the	O
initial	B-HyperparameterName
learning	I-HyperparameterName
rate	I-HyperparameterName
of	O
3	B-HyperparameterValue
×	I-HyperparameterValue
10	I-HyperparameterValue
−5	I-HyperparameterValue
,	O
warm	B-HyperparameterName
-	I-HyperparameterName
up	I-HyperparameterName
ratio	I-HyperparameterName
of	O
0.02	B-HyperparameterValue
,	O
max	O
epochs	B-HyperparameterName
of	O
50	B-HyperparameterValue
,	O
followed	O
by	O
a	O
linear	O
learning	O
rate	O
decay	O
.	O

For	O
our	O
Siamese	B-MethodName
architecture	I-MethodName
,	O
the	O
transformer	O
encoder	O
is	O
initialized	O
with	O
BERT	O
base	O
.	O

We	O
report	O
sizes	O
of	O
the	O
test	O
sets	O
in	O
Table	O
2	O
Hyperparameters	O
.	O

Zero	O
-	O
shot	O
performance	O
is	O
evaluated	O
on	O
the	O
corresponding	O
test	O
sets	O
.	O

The	O
annotation	O
follows	O
the	O
i2b2	O
challenge	O
(	O
Uzuner	O
et	O
al	O
.	O
,	O
2011	O
)	O
guidelines	O
.	O

It	O
has	O
two	O
sets	O
of	O
annotations	O
:	O
one	O
with	O
ICD-10	O
(	O
ICD	O
split	O
)	O
,	O
another	O
with	O
SNOMED	O
(	O
SN	O
split	O
)	O
.	O

We	O
also	O
use	O
a	O
de	O
-	O
identified	O
corpus	O
of	O
dictated	O
doctor	O
's	O
notes	O
named	O
3DNotes	B-DatasetName
(	O
Zhu	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

It	O
provides	O
four	O
train	O
,	O
dev	O
,	O
test	O
splits	O
:	O
Stratified	O
-	O
General	O
(	O
SG	O
)	O
,	O
Stratified	O
-	O
Specific	O
(	O
SS	O
)	O
,	O
Zeroshot	O
-	O
General	O
(	O
ZG	O
)	O
,	O
Zeroshot	O
-	O
Specific	O
(	O
ZS	O
)	O
.	O

COMETA	B-DatasetName
(	O
Basaldella	O
et	O
al	O
.	O
,	O
2020	O
)	O
is	O
one	O
of	O
the	O
largest	O
public	O
corpora	O
of	O
social	O
media	O
data	O
with	O
SNOMED	O
annotations	O
.	O

MedMention	B-DatasetName
(	O
Mohan	O
and	O
Li	O
,	O
2019	O
)	O
is	O
a	O
publicly	O
available	O
corpus	O
of	O
4,392	O
PubMed	O
1	O
abstracts	O
with	O
biomedical	O
entities	O
annotated	O
with	O
UMLS	O
concepts	O
.	O

We	O
include	O
three	O
datasets	O
in	O
zero	O
-	O
shot	O
evaluations	O
.	O

Datasets	O
.	O

Conclusion	O
.	O

We	O
present	O
a	O
framework	O
for	O
allowing	O
entity	O
retrieval	O
models	O
to	O
mine	O
rich	O
semantics	O
from	O
a	O
medical	O
KG	O
.	O

We	O
show	O
its	O
effectiveness	O
in	O
zero	O
-	O
shot	O
set	O
-	O
tings	O
through	O
extensive	O
experiments	O
.	O

In	O
addition	O
,	O
we	O
demonstrate	O
the	O
ease	O
with	O
which	O
the	O
framework	O
can	O
be	O
adapted	O
to	O
serve	O
as	O
an	O
auxiliary	O
task	O
when	O
annotations	O
are	O
available	O
.	O

Future	O
research	O
should	O
explore	O
more	O
fine	O
-	O
grained	O
approaches	O
to	O
combine	O
tasks	O
.	O

In	O
addition	O
,	O
the	O
graphical	O
structure	O
of	O
a	O
KG	O
contains	O
information	O
on	O
how	O
concepts	O
are	O
related	O
to	O
each	O
other	O
and	O
so	O
can	O
be	O
used	O
as	O
another	O
valuable	O
resource	O
for	O
building	O
an	O
effective	O
similarity	O
measure	O
.	O

One	O
important	O
entry	O
is	O
the	O
synonym	O
,	O
whereby	O
two	O
medical	O
terms	O
may	O
be	O
used	O
interchangeably	O
.	O

As	O
an	O
alternative	O
to	O
manually	O
annotating	O
a	O
corpus	O
,	O
the	O
rich	O
semantics	O
inside	O
a	O
KG	O
itself	O
can	O
be	O
utilized	O
(	O
Chang	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

Most	O
of	O
them	O
,	O
however	O
,	O
require	O
a	O
finetuning	O
step	O
on	O
annotated	O
training	O
data	O
(	O
Wu	O
et	O
al	O
.	O
,	O
2020	O
)	O
before	O
being	O
applied	O
to	O
entity	O
retrieval	O
.	O

Agarwal	O
et	O
al	O
.	O

Although	O
annotated	O
training	O
data	O
is	O
not	O
required	O
,	O
such	O
systems	O
typically	O
lack	O
the	O
ability	O
to	O
handle	O
synonyms	O
and	O
paraphrases	O
with	O
large	O
surface	O
form	O
differences	O
.	O

It	O
is	O
therefore	O
not	O
feasible	O
to	O
annotate	O
enough	O
data	O
to	O
cover	O
the	O
millions	O
of	O
concepts	O
in	O
a	O
medical	O
KG	O
,	O
and	O
difficult	O
to	O
adapt	O
quickly	O
enough	O
to	O
those	O
newly	O
appeared	O
medical	O
conditions	O
and	O
drug	O
treatments	O
under	O
a	O
public	O
health	O
crisis	O
.	O

This	O
allows	O
medical	O
researchers	O
and	O
clinicians	O
to	O
search	O
medical	O
literature	O
easily	O
using	O
standardized	O
codes	O
and	O
terms	O
to	O
improve	O
patient	O
care	O
.	O

Entity	B-TaskName
retrieval	I-TaskName
is	O
the	O
task	O
of	O
linking	O
mentions	O
of	O
named	O
entities	O
to	O
concepts	O
in	O
a	O
curated	O
knowledge	O
graph	O
(	O
KG	O
)	O
.	O

Introduction	O
.	O

To	O
take	O
advantage	O
of	O
this	O
valuable	O
information	O
,	O
we	O
propose	O
a	O
suite	O
of	O
learning	O
tasks	O
designed	O
for	O
training	O
efficient	O
zero	O
-	O
shot	O
entity	B-TaskName
retrieval	I-TaskName
models	O
.	O

Without	O
requiring	O
any	O
human	O
annotation	O
,	O
our	O
knowledge	O
graph	O
enriched	O
architecture	O
significantly	O
outperforms	O
common	O
zero	O
-	O
shot	O
benchmarks	O
including	O
BM25	B-MethodName
and	O
Clinical	B-MethodName
BERT	I-MethodName
with	O
7	B-MetricValue
%	I-MetricValue
to	O
30	B-MetricValue
%	I-MetricValue
higher	O
recall	B-MetricName
across	O
multiple	O
major	O
medical	O
ontologies	O
,	O
such	O
as	O
UMLS	O
,	O
SNOMED	O
and	O
ICD-10	O
.	O

Medical	O
knowledge	O
graphs	O
(	O
KG	O
)	O
,	O
however	O
,	O
contain	O
rich	O
semantics	O
including	O
large	O
numbers	O
of	O
synonyms	O
as	O
well	O
as	O
its	O
curated	O
graphical	O
structures	O
.	O

This	O
is	O
of	O
increasing	O
concern	O
under	O
a	O
public	O
health	O
crisis	O
as	O
new	O
medical	O
conditions	O
and	O
drug	O
treatments	O
come	O
to	O
light	O
frequently	O
.	O

In	O
practice	O
,	O
if	O
the	O
target	O
medical	O
ontology	O
is	O
a	O
private	O
KG	O
(	O
Wise	O
et	O
al	O
.	O
,	O
2020;Bhatia	O
et	O
al	O
.	O
,	O
2020	O
)	O
,	O
one	O
can	O
also	O
consider	O
customizing	O
the	O
learning	O
tasks	O
that	O
follow	O
the	O
synonym	O
and	O
graphbased	O
frameworks	O
outlined	O
in	O
this	O
work	O
to	O
bring	O
greater	O
gains	O
.	O

Private	O
KG	O
.	O

Since	O
most	O
annotations	O
cover	O
no	O
more	O
than	O
a	O
couple	O
thousands	O
concepts	O
,	O
which	O
is	O
a	O
tiny	O
portion	O
of	O
a	O
typical	O
medical	O
KG	O
's	O
size	O
,	O
this	O
demonstrates	O
the	O
generalizing	O
capacity	O
of	O
our	O
approach	O
on	O
the	O
vast	O
majority	O
of	O
unseen	O
concepts	O
.	O

2	O
.	O

We	O
observe	O
an	O
8	O
%	O
increase	O
in	O
R@25	O
,	O
illustrated	O
in	O
Fig	O
.	O

We	O
evaluate	O
zero	O
-	O
shot	O
performance	O
on	O
COMETA	O
-	O
ZS	O
.	O

Using	O
the	O
3DNotes	O
-	O
SN	O
's	O
annotated	O
training	O
set	O
to	O
train	O
the	O
primary	O
supervised	O
task	O
,	O
we	O
set	O
the	O
comb	O
task	O
as	O
its	O
auxiliary	O
counterpart	O
by	O
summing	O
the	O
losses	O
.	O

When	O
annotations	O
are	O
available	O
,	O
our	O
learning	O
tasks	O
can	O
be	O
used	O
as	O
an	O
auxiliary	O
to	O
the	O
primary	O
loss	O
.	O

Auxiliary	O
task	O
.	O

A	O
model	O
trained	O
using	O
the	O
graph	O
task	O
performs	O
better	O
when	O
mention	O
and	O
concept	O
have	O
an	O
is	O
a	O
relationship	O
(	O
lines	O
3	O
,	O
4	O
)	O
.	O

A	O
model	O
trained	O
using	O
the	O
synonym	O
task	O
makes	O
better	O
predictions	O
for	O
scenarios	O
involving	O
medical	O
synonyms	O
and	O
acronym	O
(	O
lines	O
1	O
,	O
2	O
)	O
.	O

To	O
further	O
understand	O
the	O
difference	O
between	O
synonym	O
-	O
based	O
tasks	O
and	O
graphbased	O
tasks	O
,	O
we	O
illustrate	O
qualitative	O
examples	O
in	O
Table	O
4	O
.	O

Task	O
comparison	O
.	O

Analysis	O
and	O
Discussion	O
.	O

Comb	O
task	O
has	O
the	O
most	O
balanced	O
performance	O
gains	O
across	O
all	O
datasets	O
.	O

For	O
evaluations	O
of	O
zeroshot	O
on	O
mentions	O
and	O
concepts	O
(	O
e.g.	O
UMLS	O
tasks	O
evaluated	O
on	O
COMETA	O
which	O
is	O
SNOMED	O
annotated	O
)	O
,	O
7	O
%	O
to	O
30	O
%	O
higher	O
R@1	O
is	O
observed	O
.	O

A	O
summary	O
can	O
be	O
found	O
in	O
Table	O
1	O
.	O

We	O
also	O
define	O
a	O
comb	O
task	O
,	O
where	O
all	O
the	O
tasks	O
are	O
firstly	O
downsampled	O
to	O
equal	O
sizes	O
and	O
then	O
combined	O
.	O

For	O
each	O
task	O
mentioned	O
above	O
,	O
the	O
(	O
m	O
,	O
c	O
)	O
pairs	O
generated	O
at	O
each	O
node	O
are	O
combined	O
and	O
split	O
into	O
train	O
and	O
dev	O
in	O
a	O
80:20	O
ratio	O
.	O

UMLS	O
has	O
almost	O
the	O
same	O
structure	O
as	O
SNOMED	O
,	O
therefore	O
we	O
define	O
the	O
synonym	O
-	O
based	O
task	O
and	O
graph	O
-	O
based	O
task	O
in	O
a	O
similar	O
fashion	O
to	O
that	O
of	O
SNOMED	O
.	O

The	O
Unified	O
Medical	O
Language	O
System	O
(	O
UMLS	O
)	O
is	O
a	O
compendium	O
of	O
a	O
large	O
number	O
of	O
curated	O
biomedical	O
vocabularies	O
with	O
over	O
1MM	O
concepts	O
.	O

UMLS	O
.	O

A	O
direct	O
connection	O
between	O
two	O
nodes	O
is	O
likely	O
to	O
imply	O
a	O
certain	O
degree	O
of	O
similarity	O
,	O
thus	O
we	O
define	O
the	O
SNOMED	O
graph	O
-	O
based	O
task	O
as	O
:	O
m	O
=	O
l	O
1	O
i	O
,	O
c	O
=	O
l	O
1	O
j	O
N	O
i	O
.is	O
connected(N	O
j	O
)	O
.	O

SNOMED	O
is	O
a	O
directed	O
graph	O
with	O
107	O
possible	O
relationship	O
types	O
(	O
e.g.	O
is	O
a	O
,	O
finding	O
site	O
,	O
relative	O
to	O
)	O
.	O

Graph	O
-	O
based	O
task	O
.	O

We	O
therefore	O
define	O
SNOMED	O
synonym	O
-	O
based	O
task	O
as	O
:	O
m	O
=	O
l	O
p	O
i	O
,	O
c	O
=	O
l	O
q	O
i	O
,	O
p	O
>	O
q	O
d	O
*	O
(	O
d−1)2	O
unique	O
(	O
m	O
,	O
c	O
)	O
pairs	O
are	O
constructed	O
at	O
each	O
node	O
.	O

Each	O
node	O
N	O
i	O
in	O
SNOMED	O
has	O
multiple	O
synonymous	O
descriptions	O
{	O
l	O
1	O
i	O
,	O
l	O
2	O
i	O
,	O
...	O
,	O
l	O
d	O
i	O
}	O
,	O
with	O
l	O
1	O
i	O
as	O
the	O
main	O
description	O
.	O

Synonym	O
-	O
based	O
task	O
.	O

Systematized	O
Nomenclature	O
of	O
Medicine	O
-Clinical	O
Terms	O
(	O
SNOMED	O
)	O
is	O
a	O
standardized	O
clinical	O
terminology	O
used	O
for	O
the	O
electronic	O
exchange	O
of	O
clinical	O
health	O
information	O
with	O
over	O
360	O
K	O
active	O
concepts	O
.	O

SNOMED	O
.	O

To	O
incorporate	O
the	O
semantics	O
of	O
parent	O
-	O
child	O
relationships	O
into	O
learning	O
,	O
we	O
define	O
ICD-10	O
graph	O
-	O
based	O
task	O
as	O
:	O
m	O
=	O
N	O
CodeDescription	O
i	O
,	O
c	O
=	O
N	O
CodeDescription	O
j	O
N	O
i	O
.is	O
parent(N	O
j	O
)	O
.	O

Graph	O
-	O
based	O
task	O
.	O

These	O
three	O
sections	O
describe	O
the	O
same	O
medical	O
condition	O
with	O
different	O
surface	O
forms	O
,	O
therefore	O
we	O
define	O
the	O
ICD-10	O
synonym	O
-	O
based	O
task	O
as	O
:	O
m	O
=	O
N	O
L	O
i	O
,	O
c	O
=	O
N	O
R	O
i	O
N	O
L	O
i	O
,	O
N	O
R	O
i	O
∈	O
{	O
N	O
T	O
itleConcatenation	O
i	O
,	O
N	O
CodeDescription	O
i	O
,	O
N	O
SeeAlso	O
i	O
}	O
,	O
L	O
=	O
R	O
We	O
illustrate	O
it	O
with	O
an	O
example	O
in	O
Figure	O
1	O
.	O

The	O
SeeAlso	O
section	O
contains	O
a	O
similar	O
concept	O
,	O
denoted	O
by	O
N	O
SeeAlso	O
i	O
.	O

We	O
therefore	O
design	O
synonym	O
-	O
based	O
tasks	O
and	O
graph	O
-	O
based	O
tasks	O
to	O
mine	O
a	O
medical	O
KG	O
.	O

As	O
discussed	O
in	O
§	O
1	O
,	O
this	O
task	O
is	O
usually	O
divided	O
into	O
two	O
subtasks	O
.	O

Apart	O
from	O
sentence	O
-	O
level	O
sentiment	B-TaskName
analysis	I-TaskName
(	O
Lin	O
and	O
He	O
,	O
2009;Kim	O
,	O
2014	O
)	O
,	O
targeted	O
sentiment	B-TaskName
analysis	I-TaskName
,	O
which	O
requires	O
the	O
detection	O
of	O
sentiments	O
towards	O
mentioned	O
entities	O
in	O
the	O
open	O
domain	O
,	O
is	O
also	O
an	O
important	O
research	O
topic	O
.	O

Moreover	O
,	O
we	O
find	O
that	O
the	O
pipeline	B-MethodName
model	I-MethodName
consistently	O
surpasses	O
both	O
the	O
joint	B-MethodName
model	I-MethodName
and	O
the	O
collapsed	B-MethodName
model	I-MethodName
.	O

TAG	B-MethodName
SPAN	B-MethodName
suitable	O
for	O
long	O
sentences	O
.	O

Sentence	O
.	O

This	O
work	O
was	O
supported	O
by	O
the	O
National	O
Key	O
Research	O
and	O
Development	O
Program	O
of	O
China	O
(	O
2016YFB1000101	O
)	O
.	O

We	O
also	O
thank	O
Li	O
Dong	O
for	O
his	O
helpful	O
comments	O
and	O
suggestions	O
.	O

We	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
insightful	O
feedback	O
.	O

Acknowledgments	O
.	O

Model	O
analysis	O
reveals	O
that	O
the	O
main	O
performance	O
improvement	O
comes	O
from	O
the	O
span	O
-	O
level	O
polarity	O
classifier	O
,	O
and	O
the	O
multi	O
-	O
target	O
extractor	O
is	O
more	O
.	O

Our	O
approach	O
firmly	O
outperforms	O
the	O
sequence	O
tagging	O
baseline	O
as	O
well	O
as	O
previous	O
stateof	O
-	O
the	O
-	O
art	O
methods	O
on	O
three	O
benchmark	O
datasets	O
.	O

On	O
top	O
of	O
it	O
,	O
we	O
design	O
a	O
multi	O
-	O
target	O
extractor	O
for	O
proposing	O
multiple	O
candidate	O
targets	O
with	O
an	O
heuristic	O
multispan	O
decoding	O
algorithm	O
,	O
and	O
introduce	O
a	O
polarity	O
classifier	O
that	O
predicts	O
the	O
sentiment	O
towards	O
each	O
candidate	O
using	O
its	O
summarized	O
span	O
representation	O
.	O

The	O
framework	O
contains	O
a	O
pre	O
-	O
trained	O
Transformer	O
encoder	O
as	O
the	O
backbone	O
network	O
.	O

We	O
re	O
-	O
examine	O
the	O
drawbacks	O
of	O
sequence	O
tagging	O
methods	O
in	O
open	O
-	O
domain	O
targeted	O
sentiment	B-TaskName
analysis	I-TaskName
,	O
and	O
propose	O
an	O
extract	B-MethodName
-	I-MethodName
then	I-MethodName
-	I-MethodName
classify	I-MethodName
framework	I-MethodName
with	O
the	O
span	B-MethodName
-	I-MethodName
based	I-MethodName
labeling	I-MethodName
scheme	I-MethodName
instead	O
.	O

Conclusion	O
.	O

Our	O
polarity	O
classifier	O
,	O
however	O
,	O
can	O
avoid	O
such	O
problem	O
by	O
using	O
the	O
target	O
span	O
representation	O
to	O
predict	O
the	O
sentiment	O
.	O

Moreover	O
,	O
we	O
find	O
that	O
the	O
tagging	O
method	O
sometimes	O
fails	O
to	O
predict	O
the	O
correct	O
sen-	O
timent	O
class	O
,	O
especially	O
when	O
the	O
target	O
consists	O
of	O
multiple	O
words	O
(	O
e.g.	O
,	O
"	O
battery	O
cycle	O
count	O
"	O
in	O
(	O
5	O
)	O
and	O
"	O
Casa	O
La	O
Femme	O
"	O
in	O
(	O
6	O
)	O
)	O
,	O
indicating	O
the	O
tagger	O
can	O
not	O
effectively	O
maintain	O
sentiment	O
consistency	O
across	O
words	O
.	O

For	O
example	O
,	O
it	O
additionally	O
predicts	O
the	O
entity	O
"	O
food	O
"	O
as	O
a	O
target	O
in	O
the	O
second	O
example	O
.	O

In	O
contrast	O
,	O
the	O
tagging	O
method	O
does	O
not	O
rely	O
on	O
a	O
threshold	O
and	O
is	O
observed	O
to	O
have	O
a	O
higher	O
recall	O
.	O

As	O
a	O
result	O
,	O
the	O
model	O
only	O
makes	O
cautious	O
but	O
confident	O
predictions	O
.	O

We	O
find	O
that	O
our	O
approach	O
may	O
sometimes	O
fail	O
to	O
propose	O
target	O
entities	O
(	O
e.g.	O
,	O
"	O
adjustments	O
"	O
in	O
(	O
3	O
)	O
and	O
"	O
feel	O
"	O
in	O
(	O
4	O
)	O
)	O
,	O
which	O
is	O
due	O
to	O
the	O
fact	O
that	O
a	O
relatively	O
large	O
has	O
been	O
set	O
.	O

But	O
when	O
it	O
comes	O
to	O
shorter	O
inputs	O
(	O
e.g.	O
,	O
the	O
third	O
and	O
the	O
fourth	O
examples	O
)	O
,	O
the	O
tagging	O
baseline	O
usually	O
performs	O
better	O
than	O
our	O
approach	O
.	O

A	O
likely	O
reason	O
of	O
its	O
failure	O
is	O
that	O
the	O
input	O
sentences	O
are	O
relatively	O
longer	O
,	O
and	O
the	O
tagging	O
method	O
is	O
less	O
effective	O
when	O
dealing	O
with	O
them	O
.	O

As	O
observed	O
in	O
the	O
first	O
two	O
examples	O
,	O
the	O
"	O
TAG	B-MethodName
"	O
model	O
incorrectly	O
predicts	O
the	O
target	O
span	O
by	O
either	O
missing	O
the	O
word	O
"	O
Mac	O
"	O
or	O
proposing	O
a	O
phrase	O
across	O
two	O
targets	O
(	O
"	O
scallps	O
and	O
prawns	O
"	O
)	O
.	O

Table	O
4	O
shows	O
some	O
qualitative	O
cases	O
sampled	O
from	O
the	O
pipeline	O
methods	O
.	O

Case	O
Study	O
.	O

Our	O
span	O
-	O
based	O
method	O
,	O
on	O
the	O
contrary	O
,	O
can	O
naturally	O
alleviate	O
such	O
problem	O
because	O
the	O
polarity	O
is	O
classified	O
by	O
taking	O
all	O
target	O
words	O
into	O
account	O
.	O

It	O
demonstrates	O
that	O
the	O
tagging	O
method	O
indeed	O
suffers	O
from	O
the	O
sentiment	O
inconsistency	O
problem	O
when	O
it	O
comes	O
to	O
multi	O
-	O
word	O
target	O
entities	O
.	O

The	O
performance	O
of	O
tagging	O
baseline	O
,	O
however	O
,	O
significantly	O
decreases	O
as	O
the	O
target	O
becomes	O
longer	O
.	O

We	O
find	O
that	O
the	O
accuracy	O
of	O
span	O
-	O
level	O
classifier	O
only	O
drops	O
a	O
little	O
as	O
the	O
number	O
of	O
words	O
increases	O
on	O
the	O
LAPTOP	B-DatasetName
and	O
REST	B-DatasetName
datasets	O
.	O

To	O
gain	O
more	O
insights	O
on	O
performance	O
improvements	O
,	O
we	O
plot	O
the	O
accuracy	O
of	O
both	O
methods	O
with	O
respect	O
to	O
different	O
target	O
lengths	O
in	O
Figure	O
7	O
.	O

The	O
large	O
improvement	O
over	O
the	O
tagging	O
baseline	O
suggests	O
that	O
detecting	O
sentiment	O
with	O
the	O
entire	O
span	O
representation	O
is	O
much	O
more	O
beneficial	O
than	O
predicting	O
polarities	O
over	O
each	O
word	O
,	O
as	O
the	O
semantics	O
of	O
the	O
given	O
target	O
has	O
been	O
fully	O
considered	O
.	O

The	O
results	O
show	O
that	O
our	O
approach	O
significantly	O
outperforms	O
the	O
tagging	O
baseline	O
by	O
achieving	O
9.97	B-MetricValue
%	I-MetricValue
,	O
8.15	B-MetricValue
%	I-MetricValue
and	O
15.4	B-MetricValue
%	I-MetricValue
absolute	O
gains	O
on	O
three	O
datasets	O
,	O
and	O
firmly	O
surpasses	O
previous	O
stateof	O
-	O
the	O
-	O
art	O
models	O
on	O
LAPTOP	B-DatasetName
.	O

To	O
assess	O
the	O
polarity	O
classification	O
subtask	O
,	O
we	O
compare	O
the	O
performance	O
of	O
our	O
span	B-MethodName
-	I-MethodName
level	I-MethodName
polarity	I-MethodName
classifier	I-MethodName
with	O
the	O
CRF	B-MethodName
-	I-MethodName
based	I-MethodName
tagger	I-MethodName
in	O
Table	O
5	O
.	O

Analysis	O
on	O
Polarity	O
Classification	O
.	O

Moreover	O
,	O
removing	O
the	O
non	O
-	O
maximum	O
suppression	O
(	O
NMS	O
)	O
leads	O
to	O
significant	O
performance	O
degradations	O
,	O
suggesting	O
that	O
it	O
is	O
crucial	O
to	O
prune	O
redundant	O
spans	O
that	O
refer	O
to	O
the	O
same	O
text	O
.	O

The	O
model	O
without	O
length	O
heuristics	O
is	O
very	O
likely	O
to	O
output	O
the	O
whole	O
phrase	O
as	O
a	O
single	O
target	O
,	O
thus	O
being	O
totally	O
wrong	O
.	O

By	O
sampling	O
incorrect	O
predictions	O
we	O
find	O
that	O
there	O
are	O
many	O
targets	O
closely	O
aligned	O
with	O
each	O
other	O
,	O
such	O
as	O
"	O
perfect	O
[	O
size	O
]	O
+	O
and	O
[	O
speed	O
]	O
+	O
"	O
,	O
"	O
[	O
portions	O
]	O
+	O
all	O
at	O
a	O
reasonable	O
[	O
price	O
]	O
+	O
"	O
,	O
and	O
so	O
on	O
.	O

As	O
can	O
be	O
seen	O
from	O
Figure	O
6	O
,	O
ablating	O
the	O
length	O
heuristics	O
results	O
in	O
consistent	O
performance	O
drops	O
across	O
two	O
datasets	O
.	O

Since	O
a	O
trade	O
-	O
off	O
between	O
precision	B-MetricName
and	O
recall	B-MetricName
can	O
be	O
adjusted	O
according	O
to	O
the	O
threshold	O
in	O
our	O
extractor	O
,	O
we	O
further	O
plot	O
the	O
precision	O
-	O
recall	O
curves	O
under	O
different	O
ablations	O
to	O
show	O
the	O
effects	O
of	O
heuristic	O
multi	O
-	O
span	O
decoding	O
algorithm	O
.	O

The	O
above	O
result	O
demonstrates	O
that	O
our	O
extractor	O
is	O
more	O
suitable	O
for	O
long	O
sentences	O
due	O
to	O
the	O
fact	O
that	O
its	O
search	O
space	O
only	O
increases	O
linearly	O
with	O
the	O
sentence	O
length	O
.	O

A	O
likely	O
reason	O
for	O
this	O
observation	O
is	O
that	O
the	O
lengths	O
of	O
input	O
sentences	O
on	O
these	O
datasets	O
are	O
usually	O
small	O
(	O
e.g.	O
,	O
98	O
%	O
of	O
sentences	O
are	O
less	O
than	O
40	O
words	O
in	O
REST	B-DatasetName
)	O
,	O
which	O
limits	O
the	O
tagger	O
's	O
search	O
space	O
(	O
the	O
power	O
set	O
of	O
all	O
sentence	O
words	O
)	O
.	O

Our	O
extractor	O
manages	O
to	O
surpass	O
the	O
tagger	O
by	O
16.1	B-MetricValue
F1	B-MetricName
and	O
1.0	B-MetricValue
F1	B-MetricName
when	O
the	O
length	O
exceeds	O
40	O
on	O
LAPTOP	B-DatasetName
and	O
REST	B-DatasetName
,	O
respectively	O
.	O

We	O
observe	O
that	O
the	O
performance	O
of	O
BIO	B-MethodName
tagger	I-MethodName
dramatically	O
decreases	O
as	O
the	O
sentence	O
length	O
increases	O
,	O
while	O
our	O
extractor	O
is	O
more	O
robust	O
for	O
long	O
sentences	O
.	O

In	O
order	O
to	O
confirm	O
the	O
above	O
hypothesis	O
,	O
we	O
plot	O
the	O
F1	B-MetricName
score	O
with	O
respect	O
to	O
different	O
sentence	O
lengths	O
in	O
Figure	O
5	O
.	O

As	O
a	O
result	O
,	O
the	O
computational	O
complexity	O
has	O
been	O
largely	O
reduced	O
,	O
which	O
is	O
beneficial	O
for	O
the	O
tagging	O
method	O
.	O

We	O
find	O
that	O
the	O
BIO	B-MethodName
tagger	I-MethodName
outperforms	O
our	O
extractor	O
on	O
LAPTOP	B-DatasetName
and	O
REST	B-DatasetName
.	O

To	O
analyze	O
the	O
performance	O
on	O
target	O
extraction	O
,	O
we	O
run	O
both	O
the	O
tagging	O
baseline	O
and	O
the	O
multitarget	O
extractor	O
on	O
three	O
datasets	O
,	O
as	O
shown	O
in	O
Table	O
3	O
.	O

Analysis	O
on	O
Target	O
Extraction	O
.	O

Model	O
.	O

The	O
conclusion	O
is	O
also	O
supported	O
by	O
the	O
result	O
of	O
SPANcollapsed	O
method	O
,	O
which	O
severely	O
drops	O
across	O
all	O
datasets	O
,	O
implying	O
that	O
merging	O
polarity	O
labels	O
into	O
target	O
spans	O
does	O
not	O
address	O
the	O
task	O
effectively	O
.	O

This	O
suggests	O
that	O
there	O
is	O
only	O
a	O
weak	O
connection	O
between	O
target	O
extraction	O
and	O
polarity	O
classification	O
.	O

(	O
2015	O
)	O
.	O

(	O
2013	O
)	O
;	O
Zhang	O
et	O
al	O
.	O

Second	O
,	O
among	O
the	O
span	O
-	O
based	O
methods	O
,	O
the	O
SPAN	B-MethodName
-	I-MethodName
pipeline	I-MethodName
achieves	O
the	O
best	O
performance	O
,	O
which	O
is	O
similar	O
to	O
the	O
results	O
of	O
Mitchell	O
et	O
al	O
.	O

The	O
best	O
span	O
-	O
based	O
method	O
achieves	O
1.55	B-MetricValue
%	I-MetricValue
,	O
0.94	B-MetricValue
%	I-MetricValue
and	O
3.43	B-MetricValue
%	I-MetricValue
absolute	O
gains	O
on	O
three	O
datasets	O
compared	O
to	O
the	O
best	O
tagging	O
method	O
,	O
indicating	O
the	O
efficacy	O
of	O
our	O
extract	B-MethodName
-	I-MethodName
then	I-MethodName
-	I-MethodName
classify	I-MethodName
framework	I-MethodName
.	O

First	O
,	O
despite	O
that	O
the	O
"	O
TAG	B-MethodName
"	O
baselines	O
already	O
outperform	O
previous	O
best	O
approach	O
(	O
"	O
UNIFIED	B-MethodName
"	O
)	O
,	O
they	O
are	O
all	O
beaten	O
by	O
the	O
"	O
SPAN	B-MethodName
"	O
methods	O
.	O

Two	O
main	O
observations	O
can	O
be	O
obtained	O
from	O
the	O
Table	O
.	O

We	O
denote	O
our	O
approach	O
as	O
"	O
SPAN	B-MethodName
"	O
,	O
and	O
use	O
BERT	O
LARGE	O
as	O
backbone	O
networks	O
for	O
both	O
the	O
"	O
TAG	B-MethodName
"	O
and	O
"	O
SPAN	B-MethodName
"	O
models	O
to	O
make	O
the	O
comparison	O
fair	O
.	O

We	O
compare	O
models	O
under	O
either	O
the	O
sequence	O
tagging	O
scheme	O
or	O
the	O
span	O
-	O
based	O
labeling	O
scheme	O
,	O
and	O
show	O
the	O
results	O
in	O
Table	O
2	O
.	O

Main	O
Results	O
.	O

TNet	B-MethodName
(	O
Li	O
et	O
al	O
.	O
,	O
2018	O
)	O
is	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
model	O
on	O
polarity	O
classification	O
,	O
which	O
consists	O
of	O
a	O
multi	O
-	O
layer	O
context	O
-	O
preserving	O
network	O
architecture	O
and	O
uses	O
CNNs	O
as	O
feature	O
extractor	O
5	O
.	O

Finally	O
,	O
the	O
polarity	O
classifier	O
is	O
compared	O
with	O
the	O
following	O
methods	O
:	O
MGAN	B-MethodName
(	O
Fan	O
et	O
al	O
.	O
,	O
2018	O
)	O
uses	O
a	O
multi	O
-	O
grained	O
attention	O
mechanism	O
to	O
capture	O
interactions	O
between	O
targets	O
and	O
sentences	O
for	O
polarity	O
classification	O
.	O

We	O
also	O
compare	O
our	O
multi	O
-	O
target	O
extractor	O
with	O
the	O
following	O
method	O
:	O
DE	B-MethodName
-	I-MethodName
CNN	I-MethodName
(	O
Xu	O
et	O
al	O
.	O
,	O
2018	O
)	O
is	O
the	O
current	O
stateof	O
-	O
the	O
-	O
art	O
model	O
on	O
target	O
extraction	O
,	O
which	O
combines	O
a	O
double	O
embeddings	O
mechanism	O
with	O
convolutional	O
neural	O
networks	O
(	O
CNNs	O
)	O
4	O
.	O

It	O
contains	O
two	O
stacked	O
recurrent	O
neural	O
networks	O
enhanced	O
with	O
multi	O
-	O
task	O
learning	O
and	O
adopts	O
the	O
collapsed	O
tagging	O
scheme	O
.	O

UNIFIED	B-MethodName
(	O
Li	O
et	O
al	O
.	O
,	O
2019	O
)	O
is	O
the	O
current	O
stateof	O
-	O
the	O
-	O
art	O
model	O
on	O
targeted	O
sentiment	O
analysis	O
3	O
.	O

"	O
pipeline	B-MethodName
"	O
and	O
"	O
joint	B-MethodName
"	O
denote	O
the	O
pipeline	O
and	O
joint	O
approaches	O
that	O
utilize	O
the	O
BIO	O
and	O
+	O
/-/0	O
tagging	O
schemes	O
,	O
while	O
"	O
collapsed	B-MethodName
"	O
is	O
the	O
model	O
following	O
the	O
collapsed	O
tagging	O
scheme	O
(	O
Figure	O
2(a	O
)	O
)	O
.	O

We	O
compare	O
the	O
proposed	O
span	O
-	O
based	O
approach	O
with	O
the	O
following	O
methods	O
:	O
TAG-{pipeline	B-MethodName
,	I-MethodName
joint	I-MethodName
,	I-MethodName
collapsed	I-MethodName
}	I-MethodName
are	O
the	O
sequence	O
tagging	O
baselines	O
that	O
involve	O
a	O
BERT	O
encoder	O
and	O
a	O
CRF	O
decoder	O
.	O

Baseline	O
Methods	O
.	O

Model	O
settings	O
.	O

All	O
experiments	O
are	O
conducted	O
on	O
a	O
single	O
NVIDIA	O
P100	O
GPU	O
card	O
.	O

The	O
threshold	O
is	O
manually	O
tuned	O
on	O
each	O
dataset	O
.	O

The	O
number	B-HyperparameterName
of	I-HyperparameterName
candidate	I-HyperparameterName
M	B-HyperparameterName
is	O
set	O
as	O
20	B-HyperparameterName
while	O
the	O
maximum	B-HyperparameterName
number	I-HyperparameterName
of	I-HyperparameterName
proposed	I-HyperparameterName
targets	I-HyperparameterName
K	B-HyperparameterName
is	O
10	B-HyperparameterValue
(	O
Algorithm	O
1	O
)	O
.	O

The	O
batch	B-HyperparameterName
size	I-HyperparameterName
is	O
32	B-HyperparameterValue
and	O
a	O
dropout	B-HyperparameterName
probability	I-HyperparameterName
of	O
0.1	B-HyperparameterValue
is	O
used	O
.	O

We	O
use	O
Adam	B-HyperparameterValue
optimizer	B-HyperparameterName
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
2e-5	B-HyperparameterValue
and	O
warmup	B-HyperparameterValue
over	O
the	O
first	B-HyperparameterValue
10	I-HyperparameterValue
%	I-HyperparameterValue
steps	I-HyperparameterValue
to	O
train	O
for	O
3	B-HyperparameterValue
epochs	B-HyperparameterValue
.	O

(	O
2018	O
)	O
for	O
details	O
on	O
model	O
sizes	O
.	O

and	O
refer	O
readers	O
to	O
Devlin	O
et	O
al	O
.	O

To	O
separately	O
analyze	O
the	O
performance	O
of	O
two	O
subtasks	O
,	O
precision	B-MetricName
,	O
recall	B-MetricName
,	O
and	O
F1	B-MetricName
are	O
also	O
used	O
for	O
the	O
target	O
extraction	O
subtask	O
,	O
while	O
the	O
accuracy	B-MetricName
(	O
ACC	B-MetricName
)	O
metric	O
is	O
applied	O
to	O
polarity	O
classification	O
.	O

A	O
predicted	O
target	O
is	O
correct	O
only	O
if	O
it	O
exactly	O
matches	O
the	O
gold	O
target	O
entity	O
and	O
the	O
corresponding	O
polarity	O
.	O

Metrices	O
We	O
adopt	O
the	O
precision	B-MetricName
(	O
P	B-MetricName
)	O
,	O
recall	B-MetricName
(	O
R	B-MetricName
)	O
,	O
and	O
F1	B-MetricName
score	O
as	O
evaluation	O
metrics	O
.	O

For	O
each	O
dataset	O
,	O
the	O
gold	O
target	O
span	O
boundaries	O
are	O
available	O
,	O
and	O
the	O
targets	O
are	O
labeled	O
with	O
three	O
sentiment	O
polarities	O
,	O
namely	O
positive	O
(	O
+	O
)	O
,	O
negative	O
(	O
-	O
)	O
,	O
and	O
neutral	O
(	O
0	O
)	O
.	O

(	O
2019	O
)	O
,	O
we	O
report	O
the	O
ten	O
-	O
fold	O
cross	O
validation	O
results	O
for	O
TWITTER	B-DatasetName
,	O
as	O
there	O
is	O
no	O
train	O
-	O
test	O
split	O
.	O

(	O
2015	O
)	O
;	O
Li	O
et	O
al	O
.	O

Following	O
Zhang	O
et	O
al	O
.	O

(	O
2013	O
)	O
,	O
consisting	O
of	O
twitter	O
posts	O
.	O

TWITTER	B-DatasetName
is	O
built	O
by	O
Mitchell	O
et	O
al	O
.	O

REST	B-DatasetName
is	O
the	O
union	O
set	O
of	O
the	O
restaurant	O
domain	O
from	O
SemEval	O
2014	O
,	O
2015	O
and	O
2016	O
(	O
Pontiki	O
et	O
al	O
.	O
,	O
2015(Pontiki	O
et	O
al	O
.	O
,	O
,	O
2016	O
)	O
)	O
.	O

LAPTOP	B-DatasetName
contains	O
product	O
reviews	O
from	O
the	O
laptop	O
domain	O
in	O
SemEval	B-DatasetName
2014	I-DatasetName
ABSA	I-DatasetName
challenges	O
(	O
Pontiki	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

Datasets	O
We	O
conduct	O
experiments	O
on	O
three	O
benchmark	O
sentiment	O
analysis	O
datasets	O
,	O
as	O
shown	O
in	O
Table	O
1	O
.	O

Setup	O
.	O

Experiments	O
.	O

During	O
inference	O
,	O
the	O
heuristic	O
multi	O
-	O
span	O
decoding	O
algorithm	O
is	O
performed	O
on	O
each	O
set	O
of	O
scores	O
(	O
e.g.	O
,	O
g	O
s+	O
and	O
g	O
e+	O
)	O
,	O
and	O
the	O
output	O
sets	O
O	O
+	O
,	O
O	O
,	O
and	O
O	O
0	O
are	O
aggregated	O
as	O
the	O
final	O
prediction	O
.	O

Then	O
,	O
we	O
define	O
three	O
objectives	O
to	O
optimize	O
towards	O
each	O
polarity	O
.	O

We	O
then	O
modify	O
the	O
multi	O
-	O
target	O
extractor	O
by	O
producing	O
three	O
sets	O
of	O
probabilities	O
of	O
the	O
start	O
and	O
end	O
positions	O
,	O
where	O
each	O
set	O
corresponds	O
to	O
one	O
sentiment	O
class	O
(	O
e.g.	O
,	O
p	O
s+	O
and	O
p	O
e+	O
for	O
positive	O
targets	O
)	O
.	O

For	O
example	O
,	O
the	O
sentence	O
in	O
Figure	O
2(b	O
(	O
11-	O
,	O
11-	O
)	O
.	O

We	O
combine	O
target	O
span	O
boundaries	O
and	O
sentiment	O
polarities	O
into	O
one	O
label	O
space	O
.	O

Collapsed	B-MethodName
model	I-MethodName
.	O

The	O
inference	O
procedure	O
is	O
the	O
same	O
as	O
the	O
pipeline	B-MethodName
model	I-MethodName
.	O

A	O
joint	O
training	O
loss	O
L	O
+	O
J	O
is	O
used	O
to	O
optimize	O
the	O
whole	O
model	O
.	O

Joint	B-MethodName
model	I-MethodName
In	O
this	O
model	O
,	O
each	O
sentence	O
is	O
fed	O
into	O
a	O
shared	O
BERT	O
backbone	O
network	O
that	O
finally	O
branches	O
into	O
two	O
sibling	O
output	O
layers	O
:	O
one	O
for	O
proposing	O
multiple	O
candidate	O
targets	O
and	O
another	O
for	O
predicting	O
the	O
sentiment	O
polarity	O
over	O
each	O
extracted	O
target	O
.	O

Two	O
models	O
are	O
separately	O
trained	O
and	O
combined	O
as	O
a	O
pipeline	O
during	O
inference	O
.	O

Then	O
,	O
a	O
second	O
backbone	O
network	O
is	O
used	O
to	O
provide	O
contextual	O
sentence	O
vectors	O
for	O
the	O
polarity	O
classifier	O
.	O

(	O
2015	O
)	O
,	O
we	O
investigate	O
three	O
kinds	O
of	O
models	O
under	O
the	O
extract	B-MethodName
-	I-MethodName
then	I-MethodName
-	I-MethodName
classify	I-MethodName
framework	I-MethodName
:	O
Pipeline	B-MethodName
model	I-MethodName
We	O
first	O
build	O
a	O
multi	O
-	O
target	O
extractor	O
where	O
a	O
BERT	O
encoder	O
is	O
exclusively	O
used	O
.	O

(	O
2013	O
)	O
;	O
Zhang	O
et	O
al	O
.	O

Following	O
Mitchell	O
et	O
al	O
.	O

Model	O
Variants	O
.	O

During	O
inference	O
,	O
the	O
polarity	O
probability	O
is	O
calculated	O
for	O
each	O
candidate	O
target	O
span	O
in	O
the	O
set	O
O	O
,	O
and	O
the	O
sentiment	O
class	O
that	O
possesses	O
the	O
maximum	O
value	O
in	O
p	O
p	O
is	O
chosen	O
.	O

We	O
minimize	O
the	O
negative	O
log	O
probabilities	O
of	O
the	O
true	O
polarity	O
on	O
the	O
predicted	O
probability	O
as	O
:	O
J	O
=	O
X	O
k	O
i=1	O
y	O
p	O
i	O
log(p	O
p	O
i	O
)	O
where	O
y	O
p	O
is	O
an	O
one	O
-	O
hot	O
label	O
indicating	O
the	O
true	O
polarity	O
,	O
and	O
k	O
is	O
the	O
number	O
of	O
sentiment	O
classes	O
.	O

The	O
polarity	O
score	O
is	O
obtained	O
by	O
applying	O
two	O
linear	O
transformations	O
with	O
a	O
Tanh	O
activation	O
in	O
between	O
,	O
and	O
is	O
normalized	O
with	O
the	O
softmax	O
function	O
to	O
output	O
the	O
polarity	O
probability	O
as	O
:	O
g	O
p	O
=	O
W	O
p	O
tanh(W	O
v	O
v	O
)	O
,	O
p	O
p	O
=	O
softmax(g	O
p	O
)	O
where	O
W	O
v	O
2	O
R	O
h	O
⇥	O
h	O
and	O
W	O
p	O
2	O
R	O
k	O
⇥	O
h	O
are	O
two	O
trainable	O
parameter	O
matrices	O
.	O

(	O
2018	O
):	O
↵	O
=	O
softmax(w	O
↵	O
h	O
L	O
s	O
i	O
:	O
e	O
j	O
)	O
v	O
=	O
X	O
e	O
j	O
t	O
=	O
s	O
i	O
↵	O
t	O
s	O
i	O
+1	O
h	O
L	O
t	O
where	O
w	O
↵	O
2	O
R	O
h	O
is	O
a	O
trainable	O
weight	O
vector	O
.	O

(	O
2017	O
)	O
and	O
He	O
et	O
al	O
.	O

Specifically	O
,	O
given	O
a	O
target	O
span	O
r	O
,	O
we	O
calculate	O
a	O
summarized	O
vector	O
v	O
using	O
the	O
attention	O
mechanism	O
(	O
Bahdanau	O
et	O
al	O
.	O
,	O
2014	O
)	O
over	O
tokens	O
in	O
its	O
corrsponding	O
bound	O
(	O
s	O
i	O
,	O
e	O
j	O
)	O
,	O
similar	O
to	O
Lee	O
et	O
al	O
.	O

Instead	O
,	O
we	O
propose	O
to	O
summarize	O
the	O
target	O
representation	O
from	O
contextual	O
sentence	O
vectors	O
according	O
to	O
its	O
span	O
boundary	O
,	O
and	O
use	O
feed	O
-	O
forward	O
neural	O
networks	O
to	O
predict	O
the	O
sentiment	O
polarity	O
,	O
as	O
shown	O
in	O
Figure	O
3(b	O
)	O
.	O

Typically	O
,	O
polarity	O
classification	O
is	O
solved	O
using	O
either	O
sequence	O
tagging	O
methods	O
or	O
sophisticated	O
neural	O
networks	O
that	O
separately	O
encode	O
the	O
target	O
and	O
the	O
sentence	O
.	O

Polarity	O
Classifier	O
.	O

Input	O
:	O
g	O
s	O
,	O
g	O
e	O
,	O
,	O
K	O
g	O
s	O
denotes	O
the	O
score	O
of	O
start	O
positions	O
g	O
e	O
denotes	O
the	O
score	O
of	O
end	O
positions	O
is	O
a	O
minimum	O
score	O
threshold	O
K	O
is	O
the	O
maximum	O
number	O
of	O
proposed	O
targets	O
1	O
:	O
Initialize	O
R	O
,	O
U	O
,	O
O	O
=	O
{	O
}	O
,	O
{	O
}	O
,	O
{	O
}	O
2	O
:	O
Get	O
top	O
-	O
M	O
indices	O
S	O
,	O
E	O
from	O
g	O
s	O
,	O
g	O
e	O
3	O
:	O
for	O
si	O
in	O
S	O
do	O
4	O
:	O
for	O
ej	O
in	O
E	O
do	O
5	O
:	O
if	O
si	O
	O
ej	O
and	O
g	O
s	O
s	O
i	O
+	O
g	O
e	O
e	O
j	O
then	O
6	O
:	O
u	O
l	O
=	O
g	O
s	O
s	O
i	O
+	O
g	O
e	O
e	O
j	O
(	O
ej	O
si	O
+	O
1	O
)	O
7	O
:	O
r	O
l	O
=	O
(	O
si	O
,	O
ej	O
)	O
8	O
:	O
R	O
=	O
R	O
[	O
{	O
r	O
l	O
}	O
,	O
U	O
=	O
U	O
[	O
{	O
u	O
l	O
}	O
9	O
:	O
while	O
R	O
6	O
=	O
{	O
}	O
and	O
size(O	O
)	O
<	O
K	O
do	O
10	O
:	O
l	O
=	O
arg	O
max	O
U	O
11	O
:	O
O	O
=	O
O	O
[	O
{	O
r	O
l	O
}	O
;	O
R	O
=	O
R	O
{	O
r	O
l	O
}	O
;	O
U	O
=	O
U	O
{	O
u	O
l	O
}	O
12	O
:	O
for	O
r	O
k	O
in	O
R	O
do	O
13	O
:	O
if	O
f1(r	O
l	O
,	O
r	O
k	O
)	O
6	O
=	O
0	O
then	O
14	O
:	O
R	O
=	O
R	O
{	O
r	O
k	O
}	O
;	O
U	O
=	O
U	O
{	O
u	O
k	O
}	O
15	O
:	O
return	O
O.	O

Algorithm	O
1	O
Heuristic	O
multi	O
-	O
span	O
decoding	O
.	O

This	O
process	O
is	O
repeated	O
for	O
remaining	O
spans	O
in	O
R	O
,	O
until	O
R	O
is	O
empty	O
or	O
top	O
-	O
K	O
target	O
spans	O
have	O
been	O
proposed	O
(	O
line	O
9	O
)	O
.	O

We	O
also	O
delete	O
any	O
span	O
r	O
k	O
that	O
is	O
overlapped	O
with	O
r	O
l	O
,	O
which	O
is	O
measured	O
with	O
the	O
word	O
-	O
level	O
F1	O
function	O
(	O
line	O
12	O
-	O
14	O
)	O
.	O

Specifically	O
,	O
we	O
remove	O
the	O
span	O
r	O
l	O
that	O
possesses	O
the	O
maximum	O
score	O
u	O
l	O
from	O
the	O
set	O
R	O
and	O
add	O
it	O
to	O
the	O
set	O
O	O
(	O
line	O
10	O
-	O
11	O
)	O
.	O

Next	O
,	O
we	O
prune	O
redundant	O
spans	O
in	O
R	O
using	O
the	O
non	O
-	O
maximum	O
suppression	O
algorithm	O
(	O
Rosenfeld	O
and	O
Thurston	O
,	O
1971	O
)	O
.	O

Note	O
that	O
we	O
heuristically	O
calculate	O
u	O
l	O
as	O
the	O
sum	O
of	O
two	O
scores	O
minus	O
the	O
span	O
length	O
(	O
line	O
6	O
)	O
,	O
which	O
turns	O
out	O
to	O
be	O
critical	O
to	O
the	O
performance	O
as	O
targets	O
are	O
usually	O
short	O
entities	O
.	O

For	O
each	O
example	O
,	O
top	O
-	O
M	O
indices	O
are	O
first	O
chosen	O
from	O
the	O
two	O
predicted	O
scores	O
g	O
s	O
and	O
g	O
e	O
(	O
line	O
2	O
)	O
,	O
and	O
the	O
candidate	O
span	O
(	O
s	O
i	O
,	O
e	O
j	O
)	O
(	O
denoted	O
as	O
r	O
l	O
)	O
along	O
with	O
its	O
heuristicregularized	O
score	O
u	O
l	O
are	O
then	O
added	O
to	O
the	O
lists	O
R	O
and	O
U	O
respectively	O
,	O
under	O
the	O
constraints	O
that	O
the	O
end	O
position	O
is	O
no	O
less	O
than	O
the	O
start	O
position	O
as	O
well	O
as	O
the	O
addition	O
of	O
two	O
scores	O
exceeds	O
a	O
threshold	O
(	O
line	O
3	O
-	O
8)	O
.	O

To	O
adapt	O
to	O
multi	O
-	O
target	O
scenarios	O
,	O
we	O
propose	O
an	O
heuristic	O
multi	O
-	O
span	O
decoding	O
algorithm	O
as	O
shown	O
in	O
Algorithm	O
1	O
.	O

Sentence	O
:	O
Great	O
food	O
but	O
the	O
service	O
was	O
dreadful	O
!	O
Targets	O
:	O
food	O
,	O
service	O
Predictions	O
:	O
food	O
but	O
the	O
service	O
,	O
food	O
,	O
Great	O
food	O
,	O
service	O
,	O
service	O
was	O
dreadful	O
,	O
...	O

Figure	O
4	O
gives	O
a	O
qualitative	O
example	O
to	O
illustrate	O
this	O
phenomenon	O
.	O

Moreover	O
,	O
simply	O
taking	O
top	O
-	O
K	O
spans	O
according	O
to	O
the	O
addition	O
of	O
two	O
scores	O
is	O
also	O
not	O
optimal	O
,	O
as	O
multiple	O
candidates	O
may	O
refer	O
to	O
the	O
same	O
text	O
.	O

However	O
,	O
such	O
decoding	O
method	O
is	O
not	O
suitable	O
for	O
the	O
multi	O
-	O
target	O
extraction	O
task	O
.	O

Then	O
,	O
we	O
define	O
the	O
training	O
objective	O
as	O
the	O
sum	O
of	O
the	O
negative	O
log	O
probabilities	O
of	O
the	O
true	O
start	O
and	O
end	O
positions	O
on	O
two	O
predicted	O
probabilities	O
as	O
:	O
L	O
=	O
X	O
n+2	O
i=1	O
y	O
s	O
i	O
log(p	O
s	O
i	O
)	O
X	O
n+2	O
j=1	O
y	O
e	O
j	O
log(p	O
e	O
j	O
)	O
At	O
inference	O
time	O
,	O
previous	O
works	O
choose	O
the	O
span	O
(	O
k	O
,	O
l	O
)	O
(	O
k	O
	O
l	O
)	O
with	O
the	O
maximum	O
value	O
of	O
g	O
s	O
k	O
+	O
g	O
e	O
l	O
as	O
the	O
final	O
prediction	O
.	O

As	O
a	O
result	O
,	O
we	O
can	O
obtain	O
a	O
vector	O
y	O
s	O
2	O
R	O
(	O
n+2	O
)	O
,	O
where	O
each	O
element	O
y	O
s	O
i	O
indicates	O
whether	O
the	O
i	O
-	O
th	O
token	O
starts	O
a	O
target	O
,	O
and	O
also	O
get	O
another	O
vector	O
y	O
e	O
2	O
R	O
(	O
n+2	O
)	O
for	O
labeling	O
the	O
end	O
positions	O
.	O

Similarly	O
,	O
we	O
can	O
get	O
the	O
probability	O
of	O
the	O
end	O
position	O
along	O
with	O
its	O
confidence	O
score	O
by	O
:	O
g	O
e	O
=	O
w	O
e	O
h	O
L	O
,	O
p	O
e	O
=	O
softmax(g	O
e	O
)	O
During	O
training	O
,	O
since	O
each	O
sentence	O
may	O
contain	O
multiple	O
targets	O
,	O
we	O
label	O
the	O
span	O
boundaries	O
for	O
all	O
target	O
entities	O
in	O
the	O
list	O
T.	O

We	O
obtain	O
the	O
unnormalized	O
score	O
as	O
well	O
as	O
the	O
probability	O
distribution	O
of	O
the	O
start	O
position	O
as	O
:	O
g	O
s	O
=	O
w	O
s	O
h	O
L	O
,	O
p	O
s	O
=	O
softmax(g	O
s	O
)	O
where	O
w	O
s	O
2	O
R	O
h	O
is	O
a	O
trainable	O
weight	O
vector	O
.	O

Rather	O
than	O
finding	O
targets	O
via	O
sequence	O
tagging	O
methods	O
,	O
we	O
detect	O
candidate	O
targets	O
by	O
predicting	O
the	O
start	O
and	O
end	O
positions	O
of	O
the	O
target	O
in	O
the	O
sentence	O
,	O
as	O
suggested	O
in	O
extractive	O
question	O
answering	O
(	O
Wang	O
and	O
Jiang	O
,	O
2017;Seo	O
et	O
al	O
.	O
,	O
2017;Hu	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

Multi	O
-	O
target	O
extractor	O
aims	O
to	O
propose	O
multiple	O
candidate	O
opinion	O
targets	O
(	O
Figure	O
3(a	O
)	O
)	O
.	O

Multi	O
-	O
Target	O
Extractor	O
.	O

(	O
2017	O
)	O
for	O
more	O
details	O
.	O

Next	O
,	O
we	O
use	O
a	O
series	O
of	O
L	O
stacked	O
Transformer	O
blocks	O
to	O
project	O
the	O
input	O
embeddings	O
into	O
a	O
sequence	O
of	O
contextual	O
vectors	O
h	O
i	O
2	O
R	O
(	O
n+2)	O
⇥	O
h	O
as	O
:	O
h	O
i	O
=	O
TransformerBlock(h	O
i	O
1	O
)	O
,	O
8i	O
2	O
[	O
1	O
,	O
L	O
]	O
Here	O
,	O
we	O
omit	O
an	O
exhaustive	O
description	O
of	O
the	O
block	O
architecture	O
and	O
refer	O
readers	O
to	O
Vaswani	O
et	O
al	O
.	O

Then	O
for	O
each	O
token	O
xi	O
in	O
x	O
,	O
we	O
convert	O
it	O
into	O
vector	O
space	O
by	O
summing	O
the	O
token	O
,	O
segment	O
,	O
and	O
position	O
embeddings	O
,	O
thus	O
yielding	O
the	O
input	O
embeddings	O
h	O
0	O
2	O
R	O
(	O
n+2)	O
⇥	O
h	O
,	O
where	O
h	O
is	O
the	O
hidden	O
size	O
.	O

We	O
first	O
tokenize	O
the	O
sentence	O
x	O
using	O
a	O
30,522	O
wordpiece	O
vocabulary	O
,	O
and	O
then	O
generate	O
the	O
input	O
sequence	O
x	O
by	O
concatenating	O
a	O
[	O
CLS	O
]	O
token	O
,	O
the	O
tokenized	O
sentence	O
,	O
and	O
a	O
[	O
SEP	O
]	O
token	O
.	O

We	O
use	O
Bidirectional	O
Encoder	O
Representations	O
from	O
Transformers	O
(	O
BERT	O
)	O
(	O
Devlin	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
a	O
pre	O
-	O
trained	O
bidirectional	O
Transformer	O
encoder	O
that	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performances	O
across	O
a	O
variety	O
of	O
NLP	O
tasks	O
,	O
as	O
our	O
backbone	O
network	O
.	O

We	O
further	O
investigate	O
three	O
different	O
approaches	O
under	O
this	O
framework	O
,	O
namely	O
the	O
pipeline	B-MethodName
,	O
joint	B-MethodName
,	O
and	O
collapsed	B-MethodName
models	O
in	O
§	O
3.4	O
.	O
BERT	O
as	O
Backbone	O
Network	O
.	O

Then	O
,	O
a	O
polarity	O
classifier	O
is	O
designed	O
to	O
predict	O
the	O
sentiment	O
towards	O
each	O
extracted	O
candidate	O
using	O
its	O
summarized	O
span	O
representation	O
(	O
§	O
3.3	O
)	O
.	O

A	O
multitarget	O
extractor	O
is	O
first	O
used	O
to	O
propose	O
multiple	O
candidate	O
targets	O
from	O
the	O
sentence	O
(	O
§	O
3.2	O
)	O
.	O

The	O
basis	O
of	O
our	O
frame	O
-	O
work	O
is	O
the	O
BERT	O
encoder	O
(	O
Devlin	O
et	O
al	O
.	O
,	O
2018	O
):	O
we	O
map	O
word	O
embeddings	O
into	O
contextualized	O
token	O
representations	O
using	O
pre	O
-	O
trained	O
Transformer	O
blocks	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O
2017	O
)	O
(	O
§	O
3.1	O
)	O
.	O

The	O
overall	O
illustration	O
of	O
the	O
proposed	O
framework	O
is	O
shown	O
in	O
Figure	O
3	O
.	O

The	O
goal	O
is	O
to	O
find	O
all	O
targets	O
from	O
the	O
sentence	O
as	O
well	O
as	O
predict	O
their	O
polarities	O
.	O

Instead	O
of	O
formulating	O
the	O
open	O
-	O
domain	O
targeted	O
sentiment	B-TaskName
analysis	I-TaskName
task	O
as	O
a	O
sequence	O
tagging	O
problem	O
,	O
we	O
propose	O
to	O
use	O
a	O
span	B-MethodName
-	I-MethodName
based	I-MethodName
labeling	I-MethodName
scheme	I-MethodName
as	O
follows	O
:	O
given	O
an	O
input	O
sentence	O
x	O
=	O
(	O
x	O
1	O
,	O
...	O
,	O
x	O
n	O
)	O
with	O
length	O
n	O
,	O
and	O
a	O
target	O
list	O
T	O
=	O
{	O
t	O
1	O
,	O
...	O
,	O
t	O
m	O
}	O
,	O
where	O
the	O
number	O
of	O
targets	O
is	O
m	O
and	O
each	O
target	O
t	O
i	O
is	O
annotated	O
with	O
its	O
start	O
position	O
,	O
its	O
end	O
position	O
,	O
and	O
its	O
sentiment	O
polarity	O
.	O

Extract	B-MethodName
-	I-MethodName
then	I-MethodName
-	I-MethodName
Classify	I-MethodName
Framework	I-MethodName
.	O

However	O
,	O
unlike	O
these	O
works	O
that	O
extract	O
one	O
span	O
as	O
the	O
final	O
answer	O
,	O
our	O
approach	O
is	O
designed	O
to	O
dynamically	O
output	O
one	O
or	O
multiple	O
opinion	O
targets	O
.	O

Our	O
approach	O
is	O
related	O
to	O
this	O
line	O
of	O
work	O
.	O

Wang	O
and	O
Jiang	O
(	O
2017	O
)	O
explore	O
two	O
answer	O
prediction	O
methods	O
,	O
namely	O
the	O
sequence	O
method	O
and	O
the	O
boundary	O
method	O
,	O
finding	O
that	O
the	O
later	O
performs	O
better	O
.	O

(	O
2016	O
)	O
investigate	O
several	O
predicting	O
strategies	O
,	O
such	O
as	O
BIO	O
prediction	O
,	O
boundary	O
prediction	O
,	O
and	O
the	O
results	O
show	O
that	O
predicting	O
the	O
two	O
endpoints	O
of	O
the	O
answer	O
is	O
more	O
beneficial	O
than	O
the	O
tagging	O
method	O
.	O

To	O
solve	O
this	O
task	O
,	O
Lee	O
et	O
al	O
.	O

The	O
proposed	O
span	O
-	O
based	O
labeling	O
scheme	O
is	O
inspired	O
by	O
recent	O
advances	O
in	O
machine	O
comprehension	O
and	O
question	O
answering	O
(	O
Seo	O
et	O
al	O
.	O
,	O
2017;Hu	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
where	O
the	O
task	O
is	O
to	O
extract	O
a	O
continuous	O
span	O
of	O
text	O
from	O
the	O
document	O
as	O
the	O
answer	O
to	O
the	O
question	O
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

Our	O
work	O
differs	O
from	O
these	O
approaches	O
in	O
that	O
we	O
formulate	O
this	O
task	O
as	O
a	O
spanlevel	B-DatasetName
extract	I-DatasetName
-	I-DatasetName
then	I-DatasetName
-	I-DatasetName
classify	I-DatasetName
process	I-DatasetName
instead	O
.	O

word	O
detection	O
.	O

The	O
last	O
block	O
's	O
hidden	O
states	O
are	O
used	O
to	O
(	O
a	O
)	O
propose	O
one	O
or	O
multiple	O
candidate	O
targets	O
based	O
on	O
the	O
probabilities	O
of	O
the	O
start	O
and	O
end	O
positions	O
,	O
(	O
b	O
)	O
predict	O
the	O
sentiment	O
polarity	O
using	O
the	O
span	O
representation	O
of	O
the	O
given	O
target	O
.	O

(	O
2019	O
)	O
have	O
proposed	O
a	O
unified	B-MethodName
model	O
that	O
contains	O
two	O
stacked	O
LSTMs	O
along	O
with	O
carefully	O
-	O
designed	O
components	O
for	O
maintaining	O
sentiment	O
consistency	O
and	O
improving	O
target	O
(	O
Devlin	O
et	O
al	O
.	O
,	O
2018	O
)	O
that	O
contains	O
L	O
pre	O
-	O
trained	O
Transformer	O
blocks	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

Recently	O
,	O
Li	O
et	O
al	O
.	O

(	O
2015	O
)	O
further	O
leverage	O
these	O
linguistic	O
features	O
to	O
enhance	O
a	O
neural	O
CRF	O
model	O
.	O

Zhang	O
et	O
al	O
.	O

(	O
2013	O
)	O
formulate	O
the	O
whole	O
task	O
as	O
a	O
sequence	B-TaskName
tagging	I-TaskName
problem	O
and	O
propose	O
to	O
use	O
CRF	B-MethodName
with	O
hand	O
-	O
crafted	O
linguistic	O
features	O
.	O

Specifically	O
,	O
Mitchell	O
et	O
al	O
.	O

Rather	O
than	O
solving	O
these	O
two	O
subtasks	O
with	O
separate	O
models	O
,	O
a	O
more	O
practical	O
approach	O
is	O
to	O
directly	O
predict	O
the	O
sentiment	O
towards	O
an	O
entity	O
along	O
with	O
discovering	O
the	O
entity	O
itself	O
.	O

Recent	O
works	O
mainly	O
focus	O
on	O
capturing	O
the	O
interaction	O
between	O
the	O
target	O
and	O
the	O
sentence	O
,	O
by	O
utilizing	O
various	O
neural	O
architectures	O
such	O
as	O
LSTMs	B-MethodName
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997;Tang	O
et	O
al	O
.	O
,	O
2016a	O
)	O
with	O
attention	O
mechanism	O
(	O
Wang	O
et	O
al	O
.	O
,	O
2016b;Li	O
et	O
al	O
.	O
,	O
2018;Fan	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
CNNs	B-MethodName
(	O
Xue	O
and	O
Li	O
,	O
2018;Huang	O
and	O
Carley	O
,	O
2018	O
)	O
,	O
and	O
Memory	B-MethodName
Networks	I-MethodName
(	O
Tang	O
et	O
al	O
.	O
,	O
2016b;Chen	O
et	O
al	O
.	O
,	O
2017;Li	O
and	O
Lam	O
,	O
2017	O
)	O
.	O

Next	O
,	O
polarity	B-TaskName
classification	I-TaskName
aims	O
to	O
predict	O
the	O
sentiment	O
polarities	O
over	O
the	O
extracted	O
target	O
entities	O
(	O
Jiang	O
et	O
al	O
.	O
,	O
2011;Dong	O
et	O
al	O
.	O
,	O
2014;Tang	O
et	O
al	O
.	O
,	O
2016a;Wang	O
et	O
al	O
.	O
,	O
2016b;Chen	O
et	O
al	O
.	O
,	O
2017;Xue	O
and	O
Li	O
,	O
2018;Li	O
et	O
al	O
.	O
,	O
2018;Fan	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

This	O
subtask	O
,	O
which	O
is	O
usually	O
denoted	O
as	O
target	B-TaskName
extraction	I-TaskName
,	O
can	O
be	O
solved	O
by	O
sequence	O
tagging	O
methods	O
(	O
Jakob	O
and	O
Gurevych	O
,	O
2010;Liu	O
et	O
al	O
.	O
,	O
2015;Wang	O
et	O
al	O
.	O
,	O
2016a;Poria	O
et	O
al	O
.	O
,	O
2016;Shu	O
et	O
al	O
.	O
,	O
2017;He	O
et	O
al	O
.	O
,	O
2017;Xu	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

We	O
further	O
investigate	O
three	O
approaches	O
under	O
this	O
framework	O
,	O
namely	O
the	O
pipeline	O
,	O
joint	O
,	O
and	O
collapsed	O
models	O
.	O

The	O
second	O
is	O
polarity	O
classification	O
,	O
assuming	O
that	O
the	O
target	O
entities	O
are	O
given	O
.	O

Recently	O
,	O
many	O
works	O
concentrate	O
on	O
leveraging	O
deep	O
neural	O
networks	O
to	O
tackle	O
this	O
task	O
,	O
e.g.	O
,	O
using	O
CNNs	B-MethodName
(	O
Poria	O
et	O
al	O
.	O
,	O
2016;Xu	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
RNNs	B-MethodName
(	O
Liu	O
et	O
al	O
.	O
,	O
2015;He	O
et	O
al	O
.	O
,	O
2017	O
)	O
,	O
and	O
so	O
on	O
.	O

Traditionally	O
,	O
Conditional	O
Random	O
Fields	O
(	O
CRF	O
)	O
(	O
Lafferty	O
et	O
al	O
.	O
,	O
2001	O
)	O
have	O
been	O
widely	O
explored	O
(	O
Jakob	O
and	O
Gurevych	O
,	O
2010;Wang	O
et	O
al	O
.	O
,	O
2016a;Shu	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

The	O
first	O
is	O
target	O
extraction	O
for	O
identifying	O
entities	O
from	O
the	O
input	O
sentence	O
.	O

Related	O
Work	O
.	O

1	O
https://github.com/huminghao16/SpanABSA	O
.	O

Source	O
code	O
is	O
released	O
to	O
facilitate	O
future	O
research	O
in	O
this	O
field	O
1	O
.	O

In	O
addition	O
,	O
the	O
pipeline	O
model	O
firmly	O
improves	O
over	O
both	O
the	O
joint	O
and	O
collapsed	O
models	O
.	O

Extensive	O
experiments	O
on	O
three	O
benchmark	O
datasets	O
show	O
that	O
our	O
models	O
consistently	O
outperform	O
sequence	O
tagging	O
baselines	O
.	O

Second	O
,	O
following	O
previous	O
works	O
(	O
Mitchell	O
et	O
al	O
.	O
,	O
2013;Zhang	O
et	O
al	O
.	O
,	O
2015	O
)	O
,	O
we	O
compare	O
the	O
pipeline	O
,	O
joint	O
,	O
and	O
collapsed	O
models	O
under	O
the	O
span	O
-	O
based	O
labeling	O
scheme	O
.	O

First	O
,	O
we	O
make	O
an	O
elaborate	O
comparison	O
between	O
tagging	O
-	O
based	O
models	O
and	O
span	O
-	O
based	O
models	O
.	O

We	O
take	O
BERT	O
(	O
Devlin	O
et	O
al	O
.	O
,	O
2018	O
)	O
as	O
the	O
default	O
backbone	O
network	O
,	O
and	O
explore	O
two	O
research	O
questions	O
.	O

Moreover	O
,	O
since	O
the	O
polarity	O
is	O
decided	O
using	O
the	O
targeted	O
span	O
representation	O
,	O
the	O
model	O
is	O
able	O
to	O
take	O
all	O
target	O
words	O
into	O
account	O
before	O
making	O
predictions	O
,	O
thus	O
naturally	O
avoiding	O
sentiment	O
inconsistency	O
.	O

The	O
advantage	O
of	O
this	O
approach	O
is	O
that	O
the	O
extractive	O
search	O
space	O
can	O
be	O
reduced	O
linearly	O
with	O
the	O
sentence	O
length	O
,	O
which	O
is	O
far	O
less	O
than	O
the	O
tagging	O
method	O
.	O

Under	O
such	O
annotation	O
,	O
we	O
introduce	O
an	O
extract	O
-	O
then	O
-	O
classify	O
framework	O
that	O
first	O
extracts	O
multiple	O
opinion	O
targets	O
using	O
an	O
heuristic	O
multispan	O
decoding	O
algorithm	O
,	O
and	O
then	O
classifies	O
their	O
polarities	O
with	O
corresponding	O
summarized	O
span	O
representations	O
.	O

The	O
key	O
insight	O
is	O
to	O
annotate	O
each	O
opinion	O
target	O
with	O
its	O
span	O
boundary	O
followed	O
by	O
its	O
sentiment	O
polarity	O
.	O

To	O
address	O
the	O
problems	O
,	O
we	O
propose	O
a	O
spanbased	O
labeling	O
scheme	O
for	O
open	O
-	O
domain	O
targeted	O
sentiment	O
analysis	O
,	O
as	O
shown	O
in	O
Figure	O
2(b	O
)	O
.	O

For	O
example	O
,	O
there	O
is	O
a	O
chance	O
that	O
the	O
words	O
"	O
Windows	O
"	O
and	O
"	O
7	O
"	O
in	O
Figure	O
2(a	O
)	O
are	O
predicted	O
to	O
have	O
different	O
polarities	O
due	O
to	O
word	O
-	O
level	O
tagging	O
decisions	O
.	O

(	O
2019	O
)	O
.	O

Second	O
,	O
since	O
predicted	O
polarities	O
over	O
target	O
words	O
may	O
be	O
different	O
,	O
the	O
sentiment	O
consistency	O
of	O
multi	O
-	O
word	O
entity	O
can	O
not	O
be	O
guaranteed	O
,	O
as	O
mentioned	O
by	O
Li	O
et	O
al	O
.	O

First	O
,	O
tagging	O
polarity	O
over	O
each	O
word	O
ignores	O
the	O
semantics	O
of	O
the	O
entire	O
opinion	O
target	O
.	O

As	O
for	O
polarity	O
classification	O
,	O
the	O
sequence	O
tagging	O
scheme	O
turns	O
out	O
to	O
be	O
problematic	O
for	O
two	O
reasons	O
.	O

(	O
2016	O
)	O
show	O
that	O
,	O
when	O
using	O
BIO	O
tags	O
for	O
extractive	O
question	O
answering	O
tasks	O
,	O
the	O
model	O
must	O
consider	O
a	O
huge	O
search	O
space	O
due	O
to	O
the	O
compositionality	O
of	O
labels	O
(	O
the	O
power	O
set	O
of	O
all	O
sentence	O
words	O
)	O
,	O
thus	O
being	O
less	O
effective	O
.	O

Lee	O
et	O
al	O
.	O

However	O
,	O
the	O
above	O
annotation	O
scheme	O
has	O
several	O
disadvantages	O
in	O
target	O
extraction	O
and	O
polarity	O
classification	O
.	O

As	O
a	O
result	O
,	O
the	O
entire	O
task	O
is	O
formulated	O
as	O
a	O
sequence	O
tagging	O
problem	O
,	O
and	O
solved	O
using	O
either	O
a	O
pipeline	O
model	O
,	O
a	O
joint	O
model	O
,	O
or	O
a	O
collapsed	O
model	O
under	O
the	O
same	O
network	O
architecture	O
.	O

The	O
key	O
insight	O
is	O
to	O
label	O
each	O
word	O
with	O
a	O
set	O
of	O
target	O
tags	O
(	O
e.g.	O
,	O
B	O
,	O
I	O
,	O
O	O
)	O
as	O
well	O
as	O
a	O
set	O
of	O
polarity	O
tags	O
(	O
e.g.	O
,	O
+	O
,	O
-	O
,	O
0	O
)	O
,	O
or	O
use	O
a	O
more	O
collapsed	O
set	O
of	O
tags	O
(	O
e.g.	O
,	O
B+	O
,	O
I-	O
)	O
to	O
directly	O
indicate	O
the	O
boundary	O
of	O
targeted	O
sentiment	O
,	O
as	O
shown	O
in	O
Figure	O
2(a	O
)	O
.	O

Rather	O
than	O
using	O
separate	O
models	O
for	O
each	O
subtask	O
,	O
some	O
works	O
attempt	O
to	O
solve	O
the	O
task	O
in	O
a	O
more	O
integrated	O
way	O
,	O
by	O
jointly	O
extracting	O
targets	O
and	O
predicting	O
their	O
sentiments	O
(	O
Mitchell	O
et	O
al	O
.	O
,	O
2013;Zhang	O
et	O
al	O
.	O
,	O
2015;Li	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

Although	O
lots	O
of	O
efforts	O
have	O
been	O
made	O
to	O
design	O
sophisticated	O
classifiers	O
for	O
this	O
subtask	O
,	O
they	O
all	O
assume	O
that	O
the	O
targets	O
are	O
already	O
given	O
.	O

Since	O
opinion	O
targets	O
are	O
not	O
given	O
,	O
we	O
need	O
to	O
first	O
detect	O
the	O
targets	O
from	O
the	O
input	O
text	O
.	O

Typically	O
,	O
the	O
whole	O
task	O
can	O
be	O
decoupled	O
into	O
two	O
subtasks	O
.	O

Taking	O
Figure	O
1	O
as	O
an	O
example	O
,	O
the	O
goal	O
is	O
to	O
first	O
identify	O
"	O
Windows	O
7	O
"	O
and	O
"	O
Vista	O
"	O
as	O
opinion	O
targets	O
and	O
then	O
predict	O
their	O
corresponding	O
sentiment	O
classes	O
.	O

Open	O
-	O
domain	O
targeted	O
sentiment	B-TaskName
analysis	I-TaskName
is	O
a	O
fundamental	O
task	O
in	O
opinion	O
mining	O
and	O
sentiment	O
analysis	O
(	O
Pang	O
et	O
al	O
.	O
,	O
2008;Liu	O
,	O
2012	O
)	O
.	O

Introduction	O
.	O

Moreover	O
,	O
we	O
find	O
that	O
the	O
pipeline	O
model	O
achieves	O
the	O
best	O
performance	O
compared	O
with	O
the	O
other	O
two	O
models	O
.	O

Experiments	O
on	O
three	O
benchmark	O
datasets	O
show	O
that	O
our	O
approach	O
consistently	O
outperforms	O
the	O
sequence	O
tagging	O
baseline	O
.	O

To	O
address	O
these	O
problems	O
,	O
we	O
propose	O
a	O
span	B-MethodName
-	I-MethodName
based	I-MethodName
extract	I-MethodName
-	I-MethodName
then	I-MethodName
-	I-MethodName
classify	I-MethodName
framework	I-MethodName
,	O
where	O
multiple	O
opinion	O
targets	O
are	O
directly	O
extracted	O
from	O
the	O
sentence	O
under	O
the	O
supervision	O
of	O
target	O
span	O
boundaries	O
,	O
and	O
corresponding	O
polarities	O
are	O
then	O
classified	O
using	O
their	O
span	O
representations	O
.	O

Compared	O
to	O
traditional	O
sentence	O
-	O
level	O
sentiment	B-TaskName
analysis	I-TaskName
tasks	O
(	O
Lin	O
and	O
He	O
,	O
2009;Kim	O
,	O
2014	O
)	O
,	O
the	O
task	O
requires	O
detecting	O
target	O
entities	O
mentioned	O
in	O
the	O
sentence	O
along	O
with	O
their	O
sentiment	O
polarities	O
,	O
thus	O
being	O
more	O
challenging	O
.	O

However	O
,	O
such	O
formulation	O
suffers	O
from	O
problems	O
such	O
as	O
huge	O
search	O
space	O
and	O
sentiment	O
inconsistency	O
.	O

Prior	O
work	O
typically	O
formulates	O
this	O
task	O
as	O
a	O
sequence	O
tagging	O
problem	O
.	O

Open	O
-	O
domain	O
targeted	O
sentiment	B-TaskName
analysis	I-TaskName
aims	O
to	O
detect	O
opinion	O
targets	O
along	O
with	O
their	O
sentiment	O
polarities	O
from	O
a	O
sentence	O
.	O

Open	O
-	O
Domain	O
Targeted	O
Sentiment	B-TaskName
Analysis	I-TaskName
via	O
Span	O
-	O
Based	O
Extraction	O
and	O
Classification	O
.	O

For	O
example	O
,	O
word	O
"	O
allocation	O
"	O
has	O
a	O
new	O
distance-2	O
word	O
"	O
topics	O
"	O
after	O
merging	O
.	O

Experiments	O
on	O
two	O
datasets	O
show	O
EGTRF	B-MethodName
achieves	O
better	O
performance	O
than	O
GTRF	B-MethodName
and	O
LDA	B-MethodName
,	O
which	O
confirm	O
our	O
assumption	O
that	O
adding	O
topical	O
dependency	O
of	O
distance-2	B-HyperparameterName
words	O
and	O
incorporating	O
word	O
similarity	O
information	O
can	O
improve	O
model	O
performance	O
.	O

Word	O
topics	O
are	O
drawed	O
by	O
Extended	B-MethodName
Global	I-MethodName
Random	I-MethodName
Field(EGRF	I-MethodName
)	O
instead	O
of	O
Multinomial	O
,	O
the	O
conditional	O
independence	O
of	O
word	O
topic	O
assignment	O
is	O
thus	O
relaxed	O
.	O

In	O
this	O
paper	O
,	O
we	O
extended	O
Global	B-MethodName
Topic	I-MethodName
Random	I-MethodName
Field(GTRF	I-MethodName
)	O
and	O
proposed	O
a	O
novel	O
topic	O
model	O
Extended	B-MethodName
Global	I-MethodName
Topic	I-MethodName
Random	I-MethodName
Field(EGTRF	I-MethodName
)	O
which	O
can	O
model	O
dependency	O
relation	O
between	O
adjacent	O
words	O
and	O
distance-2	B-HyperparameterName
words	O
.	O

And	O
we	O
define	O
the	O
topic	B-TaskName
model	I-TaskName
based	O
on	O
EGRF	B-MethodName
as	O
Extended	B-MethodName
Global	I-MethodName
Topic	I-MethodName
Random	I-MethodName
Field(EGTRF	I-MethodName
)	O
.	O

We	O
define	O
the	O
random	O
field	O
as	O
in	O
Equation	O
(	O
1	O
)	O
a	O
Extended	B-MethodName
Global	I-MethodName
Random	I-MethodName
Field	I-MethodName
(	O
EGRF	B-MethodName
)	O
.	O

In	O
this	O
section	O
,	O
we	O
first	O
present	O
Extended	B-MethodName
Global	I-MethodName
Random	I-MethodName
Field(EGRF	I-MethodName
)	O
in	O
section	O
2.1	O
,	O
then	O
show	O
how	O
to	O
model	O
topical	O
dependencies	O
using	O
EGRF	B-MethodName
in	O
section	O
2.2	O
.	O
We	O
incorporate	O
word	O
similarity	O
information	O
into	O
model	O
in	O
section	O
2.3	O
.	O
1	O
https://code.google.com/p/word2vec/.	O

In	O
this	O
paper	O
,	O
we	O
extend	O
GTRF	B-MethodName
model	O
and	O
present	O
a	O
novel	O
model	O
Extended	B-MethodName
Global	I-MethodName
Topic	I-MethodName
Random	I-MethodName
Field	I-MethodName
(	O
EGTRF	B-MethodName
)	O
to	O
exploit	O
topical	O
dependency	O
between	O
words	O
.	O

In	O
Global	B-MethodName
Topic	I-MethodName
Random	I-MethodName
Field(GTRF	I-MethodName
)	O
model	O
(	O
Li	O
et	O
al	O
,	O
2014	O
)	O
,	O
sentences	O
of	O
a	O
document	O
are	O
parsed	O
into	O
dependency	O
trees	O
(	O
Marneffe	O
et	O
al	O
,	O
2008	O
)	O
(	O
Manning	O
et	O
al	O
,	O
2014	O
)	O
(	O
Marneffe	O
et	O
al	O
,	O
2006	O
)	O
.	O

Probabilistic	O
topic	O
model	O
such	O
as	O
Latent	B-MethodName
Dirichlet	I-MethodName
Allocation(LDA	I-MethodName
)	O
(	O
Blei	O
et	O
al	O
,	O
2003	O
)	O
has	O
been	O
widely	O
used	O
for	O
discovering	B-TaskName
latent	I-TaskName
topics	I-TaskName
from	O
document	O
collections	O
by	O
capturing	O
words	O
'	O
cooccuring	O
relation	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
new	O
model	O
Extended	B-MethodName
Global	I-MethodName
Topic	I-MethodName
Random	I-MethodName
Field	I-MethodName
(	O
EGTRF	B-MethodName
)	O
to	O
model	O
non	O
-	O
linear	O
dependencies	O
between	O
words	O
.	O

Topic	O
Model	O
such	O
as	O
Latent	B-MethodName
Dirichlet	I-MethodName
Allocation(LDA	I-MethodName
)	O
makes	O
assumption	O
that	O
topic	O
assignment	O
of	O
different	O
words	O
are	O
conditionally	O
independent	O
.	O

As	O
we	O
discussed	O
in	O
section	O
1	O
,	O
word	O
similarity	O
information	O
S	O
w	O
1	O
,	O
w	O
2	O
works	O
as	O
a	O
confidence	O
score	O
to	O
model	O
how	O
likely	O
two	O
words	O
on	O
an	O
edge	O
have	O
same	O
topic	O
.	O

S	O
w	O
1	O
,	O
w	O
2	O
is	O
the	O
similarity	O
measure	O
between	O
word	O
w	O
1	O
and	O
w	O
2	O
.	O

And	O
we	O
make	O
assumption	O
that	O
two	O
words	O
are	O
more	O
likely	O
to	O
have	O
same	O
topic	O
if	O
they	O
have	O
a	O
higher	O
similarity	O
score	O
.	O

To	O
get	O
the	O
similarity	O
score	O
between	O
words	O
,	O
we	O
use	O
word2vec	O
tool	O
to	O
learn	O
the	O
word	O
representation	O
of	O
each	O
word	O
from	O
pre	O
-	O
trained	O
model	O
.	O

Normalized	O
similarity	O
between	O
word	O
vectors	O
can	O
be	O
regarded	O
as	O
the	O
confidence	O
score	O
of	O
how	O
possible	O
two	O
words	O
have	O
same	O
topic	O
.	O

E	O
2	O
is	O
distance-2	O
edge	O
set	O
,	O
E	O
2	O
=	O
{	O
(	O
w	O
i	O
,	O
w	O
j	O
)	O
|∃path	O
between	O
w	O
i	O
,	O
w	O
j	O
that	O
length	O
is	O
2	O
}	O
.	O

E	O
1	O
is	O
distance-1	O
edge	O
set	O
,	O
E	O
1	O
=	O
{	O
(	O
w	O
i	O
,	O
w	O
j	O
)	O
|∃path	O
between	O
w	O
i	O
,	O
w	O
j	O
that	O
length	O
is	O
1	O
}	O
.	O

After	O
representing	O
document	O
to	O
undirected	O
graph	O
on	O
previous	O
section	O
,	O
we	O
extend	O
Global	B-MethodName
Random	I-MethodName
Field	I-MethodName
and	O
give	O
the	O
definition	O
of	O
Extended	B-MethodName
Global	I-MethodName
Random	I-MethodName
Field	I-MethodName
to	O
model	O
the	O
graph	O
as	O
below	O
:	O
Given	O
an	O
undirected	O
graph	O
G	O
,	O
word	O
vertex	O
set	O
is	O
denoted	O
as	O
W	O
=	O
{	O
w	O
i	O
|i	O
=	O
1	O
,	O
2	O
,	O
..	O
n	O
}	O
,	O
where	O
w	O
i	O
is	O
a	O
word	O
vertex	O
,	O
and	O
n	O
is	O
the	O
number	O
of	O
unique	O
words	O
in	O
a	O
document	O
.	O

Extended	B-MethodName
Global	I-MethodName
Topic	I-MethodName
Random	I-MethodName
Field	I-MethodName
.	O

We	O
use	O
the	O
pretrained	O
model	O
from	O
Google	B-DatasetName
News	I-DatasetName
dataset(about	O
100	O
billion	O
words	O
)	O
using	O
word2vec	O
1	O
tool	O
to	O
represent	O
each	O
word	O
as	O
a	O
300	O
-	O
dimensional	O
word	O
vector	O
,	O
and	O
apply	O
normalized	O
word	O
similarity	O
as	O
a	O
confidence	O
score	O
to	O
indicate	O
how	O
possible	O
two	O
word	O
vertices	O
share	O
same	O
topic	O
.	O

Theoretically	O
,	O
we	O
can	O
also	O
model	O
the	O
distance	O
further	O
than	O
2	O
,	O
however	O
,	O
it	O
leads	O
to	O
more	O
complicated	O
computation	O
and	O
small	O
increase	O
of	O
performance	O
.	O

In	O
EGTRF	B-MethodName
,	O
the	O
topic	O
assignment	O
of	O
a	O
word	O
is	O
assumed	O
to	O
depend	O
on	O
both	O
distance-1	O
and	O
distance-2	O
word	O
vertices	O
.	O

However	O
,	O
GTRF	B-MethodName
assumes	O
topic	O
assignment	O
of	O
a	O
word	O
vertex	O
depends	O
on	O
the	O
topic	O
mixture	O
of	O
the	O
document	O
and	O
its	O
neighboring	O
word	O
vertices	O
,	O
ignoring	O
the	O
fact	O
that	O
word	O
vertex	O
can	O
also	O
be	O
influenced	O
by	O
the	O
distance-2	O
or	O
further	O
word	O
vertices	O
.	O

Specifically	O
,	O
we	O
parse	O
sentences	O
into	O
dependency	O
trees	O
and	O
represent	O
them	O
as	O
a	O
graph	O
,	O
and	O
assume	O
the	O
topic	B-TaskName
assignment	I-TaskName
of	O
a	O
word	O
is	O
influenced	O
by	O
its	O
adjacent	O
words	O
and	O
distance-2	O
words	O
.	O

Extended	B-MethodName
Topic	I-MethodName
Model	I-MethodName
for	O
Word	O
Dependency	O
.	O

We	O
believe	O
modeling	O
distance-2	B-HyperparameterName
word	O
vertices	O
can	O
exploit	O
more	O
semantically	O
or	O
syntactically	O
word	O
dependencies	O
from	O
document	O
,	O
and	O
word	O
similarity	O
information	O
obtained	O
from	O
large	O
corpus	O
can	O
make	O
up	O
the	O
lack	O
of	O
sufficient	O
information	O
from	O
the	O
original	O
corpus	O
.	O

Therefore	O
,	O
adding	O
the	O
influence	O
of	O
distance-2	B-HyperparameterName
word	O
vertices	O
and	O
word	O
similarity	O
information	O
can	O
improve	O
performance	O
of	O
topic	B-TaskName
modeling	I-TaskName
.	O

Figure	O
2	O
shows	O
the	O
experimental	O
results	O
of	O
four	O
models	O
:	O
lda	B-MethodName
,	O
gtrf	B-MethodName
,	O
egtrf(EGTRF	B-MethodName
without	B-MethodName
word	I-MethodName
similarity	I-MethodName
information	I-MethodName
)	O
,	O
and	O
egtrf+s(EGTRF	B-MethodName
with	B-MethodName
word	I-MethodName
similarity	I-MethodName
information	I-MethodName
)	O
on	O
two	O
datasets	O
.	O

The	O
results	O
show	O
EGTRF	B-MethodName
outperforms	O
LDA	B-MethodName
and	O
GTRF	B-MethodName
in	O
general	O
,	O
and	O
EGTRF	B-MethodName
with	I-MethodName
word	I-MethodName
similarity	I-MethodName
information	I-MethodName
achieves	O
best	O
performance	O
.	O

We	O
choose	O
10	B-HyperparameterValue
,	O
20	B-HyperparameterValue
,	O
30	B-HyperparameterValue
,	O
50	B-HyperparameterValue
topics	B-HyperparameterName
for	O
20	B-HyperparameterValue
news	B-DatasetName
dataset	I-DatasetName
,	O
10	B-HyperparameterValue
,	O
15	B-HyperparameterValue
,	O
20	B-HyperparameterValue
,	O
25	B-HyperparameterValue
topics	O
for	O
NIPS	B-DatasetName
dataset	O
.	O

Word	O
is	O
represented	O
as	O
vector	O
from	O
pretrained	O
Google	B-DatasetName
News	I-DatasetName
dataset	O
,	O
we	O
use	O
the	O
word	O
vector	O
learned	O
from	O
original	O
corpus	O
when	O
the	O
word	O
does	O
not	O
exist	O
in	O
pre	O
-	O
trained	O
Google	B-DatasetName
News	I-DatasetName
dataset	O
.	O

We	O
set	O
λ	B-HyperparameterName
4	I-HyperparameterName
=	O
1.2	B-HyperparameterValue
to	O
give	O
lower(even	O
negative	O
)	O
reward	O
to	O
edges	O
from	O
E	O
2	O
that	O
the	O
two	O
word	O
vertices	O
have	O
same	O
topic	O
in	O
EGTRF	B-MethodName
,	O
since	O
the	O
distance-1	B-HyperparameterName
words	O
are	O
expected	O
to	O
have	O
greater	O
topical	O
affects	O
than	O
distance-2	B-HyperparameterName
words	O
.	O

We	O
implement	O
GTRF	B-MethodName
without	O
adding	O
self	O
defined	O
edges	O
from	O
the	O
original	O
paper	O
,	O
and	O
set	O
λ	B-HyperparameterName
2	I-HyperparameterName
=	O
0.2	B-HyperparameterValue
to	O
give	O
higher	O
reward	O
to	O
edges	O
from	O
E	O
1	O
that	O
the	O
two	O
word	O
vertices	O
have	O
same	O
topic	O
.	O

Lower	O
perplexity	B-MetricName
,	O
higher	O
log	B-MetricName
predictive	I-MetricName
probability	I-MetricName
indicate	O
better	O
generalization	O
performance	O
.	O

We	O
evaluate	O
how	O
well	O
a	O
model	O
fits	O
the	O
data	O
with	O
held	B-MetricName
-	I-MetricName
out	I-MetricName
perplexity	I-MetricName
(	O
Blei	O
et	O
al	O
,	O
2003	O
)	O
and	O
predictive	B-MetricName
distribution	I-MetricName
(	O
Hoffman	O
et	O
al	O
,	O
2013	O
)	O
.	O

•	O
NIPS	B-DatasetName
data	I-DatasetName
(	O
Globerson	O
et	O
al	O
,	O
2004	O
):	O
Spanning	O
from	O
2000	O
to	O
2005	O
.	O

•	O
20	B-DatasetName
News	I-DatasetName
Groups	I-DatasetName
:	O
After	O
processing	O
,	O
it	O
contains	O
13706	O
documents	O
with	O
a	O
vocabulary	O
of	O
5164	O
terms	O
.	O

In	O
this	O
section	O
we	O
study	O
the	O
empirical	O
performance	O
of	O
EGTRF	B-MethodName
on	O
two	O
datasets	O
.	O

The	O
updating	O
rule	O
of	O
α	O
and	O
β	O
are	O
same	O
to	O
LDA	B-MethodName
,	O
γ	O
is	O
updated	O
using	O
Newton	O
method	O
since	O
we	O
can	O
not	O
obtain	O
the	O
direct	O
updating	O
rule	O
for	O
γ	O
.	O

All	O
terms	O
except	O
P	O
(	O
z|θ	O
)	O
in	O
likelihood	O
function	O
are	O
also	O
same	O
to	O
LDA	B-MethodName
,	O
Based	O
on	O
Equation	O
(	O
9	O
)	O
,	O
we	O
obtain	O
:	O
Eq[log	O
P	O
egrf	O
(	O
z	O
|	O
θ	O
)	O
]	O
≈Eq[log	O
(	O
n	O
M	O
ulti(zw	O
n	O
|	O
θ))]+	O
1	O
−	O
λ2	O
ζ1	O
Eq(|	O
E	O
C	O
1	O
|	O
)	O
+	O
1	O
−	O
λ4	O
ζ1	O
Eq(|	O
E	O
C	O
2	O
|)+	O
(	O
|	O
E1	O
|	O
λ2	O
+	O
|	O
E2	O
|	O
λ4	O
ζ1	O
−	O
|	O
E1	O
|	O
+	O
|	O
E2	O
|	O
ζ2	O
)	O
Eq(θ	O
T	O
θ)+	O
log	O
ζ1	O
−	O
log	O
ζ2(11	O
)	O
We	O
get	O
the	O
approximation	O
in	O
Equation	O
(	O
11	O
)	O
from	O
Taylor	O
series	O
,	O
where	O
ζ	O
1	O
and	O
ζ	O
2	O
are	O
Taylor	O
approximation	O
.	O

The	O
variational	O
function	O
q	O
is	O
same	O
to	O
the	O
original	O
LDA	B-MethodName
paper	O
(	O
Blei	O
et	O
al	O
,	O
2003	O
)	O
.	O

The	O
expectation	O
of	O
the	O
number	O
of	O
edges	O
in	O
E	O
c	O
i	O
can	O
be	O
computed	O
as	O
:	O
E(|	O
E	O
C	O
i	O
|	O
)	O
=	O
(	O
w	O
1	O
,	O
w	O
2	O
)	O
∈E	O
i	O
φ	O
T	O
w	O
1	O
φw	O
2	O
Sw	O
1	O
,	O
w	O
2	O
(	O
10	O
)	O
φ	O
is	O
the	O
K	O
dimensional	O
variational	O
multinomial	O
parameters	O
and	O
can	O
be	O
thought	O
as	O
the	O
posterior	O
probability	O
of	O
a	O
word	O
given	O
the	O
topic	B-TaskName
assignment	I-TaskName
.	O

If	O
|E	O
2	O
|	O
=	O
0	O
,	O
|E	O
1	O
|	O
=	O
0	O
,	O
EGTRF	B-MethodName
is	O
equivalent	O
to	O
GTRF	B-MethodName
.	O

If	O
(	O
7	O
)	O
and	O
(	O
8)	O
hold	O
true	O
,	O
Equation	O
(	O
3	O
)	O
is	O
an	O
EGRF	B-MethodName
.	O

In	O
order	O
to	O
model	O
Equation	O
(	O
3	O
)	O
as	O
an	O
EGRF	B-MethodName
,	O
it	O
must	O
satisfy	O
all	O
the	O
four	O
constraints	O
in	O
Equation	O
(	O
1	O
)	O
.	O

According	O
to	O
EGRF	B-MethodName
described	O
in	O
section	O
2.1	O
,	O
we	O
define	O
the	O
probability	O
of	O
topic	O
sequence	O
z	O
as	O
below	O
:	O
P	O
egrf	O
(	O
z	O
|	O
θ	O
)	O
=	O
1	O
|	O
E1	O
|	O
+	O
|	O
E2	O
|	O
w∈V	O
f	O
(	O
zw)×	O
(	O
(	O
w	O
1	O
,	O
w	O
1	O
)	O
∈E	O
1	O
f	O
(	O
1	O
)	O
(	O
z	O
w	O
1	O
,	O
z	O
w	O
1	O
)	O
+	O
(	O
w	O
2	O
,	O
w	O
2	O
)	O
∈E	O
2	O
f	O
(	O
2	O
)	O
(	O
z	O
w	O
2	O
,	O
z	O
w	O
2	O
)	O
)	O
(	O
3	O
)	O
where	O
f	O
(	O
zw	O
)	O
=	O
M	O
ulti(zw|θ	O
)	O
(	O
4	O
)	O
f	O
(	O
1	O
)	O
(	O
z	O
w	O
1	O
,	O
z	O
w	O
1	O
)	O
=	O
σz	O
w	O
1	O
=	O
z	O
w	O
1	O
λ1	O
+	O
σ	O
z	O
w	O
1	O
=	O
z	O
w	O
1	O
λ2	O
(	O
5	O
)	O
f	O
(	O
2	O
)	O
(	O
z	O
w	O
2	O
,	O
z	O
w	O
2	O
)	O
=	O
σz	O
w	O
2	O
=	O
z	O
w	O
2	O
λ3	O
+	O
σ	O
z	O
w	O
2	O
=	O
z	O
w	O
2	O
λ4	O
(	O
6	O
)	O
σ	O
is	O
an	O
indicator	O
function	O
and	O
equals	O
1	O
if	O
the	O
topic	O
assignments	O
of	O
two	O
words	O
on	O
an	O
edge	O
are	O
same	O
.	O

So	O
the	O
word	B-TaskName
topic	I-TaskName
assignment	I-TaskName
is	O
no	O
longer	O
conditionally	O
independent	O
.	O

We	O
define	O
Extended	B-MethodName
Global	I-MethodName
Topic	I-MethodName
Random	I-MethodName
Field	I-MethodName
based	O
on	O
EGRF	B-MethodName
.	O

Topic	B-TaskName
Model	I-TaskName
Using	O
EGRF	B-MethodName
.	O

And	O
EGRF	B-MethodName
does	O
not	O
have	O
normalization	B-HyperparameterName
factor	I-HyperparameterName
,	O
which	O
is	O
much	O
simplier	O
than	O
models	O
with	O
intractable	O
normalizing	B-HyperparameterName
factor	I-HyperparameterName
.	O

The	O
state(topic	B-TaskName
assignment	I-TaskName
)	O
of	O
a	O
word	O
vertex	O
w	O
is	O
generated	O
from	O
Z	O
=	O
{	O
z	O
i	O
|i	O
=	O
1	O
,	O
2	O
,	O
...	O
,	O
k	O
}	O
,	O
k	O
is	O
the	O
number	O
of	O
topics	O
.	O

Another	O
advantage	O
of	O
EGTRF	B-MethodName
is	O
it	O
incorporates	O
word	O
features	O
.	O

Word	O
similarity	O
information	O
learned	O
from	O
large	O
corpus	O
is	O
incorporated	O
into	O
the	O
model	O
.	O

Conclusion	O
.	O

After	O
processing	O
,	O
it	O
contains	O
843	O
documents	O
with	O
a	O
vocabulary	O
of	O
6098	O
terms	O
.	O

Eighty	O
percent	O
data	O
are	O
used	O
for	O
training	O
,	O
others	O
for	O
testing	O
.	O

For	O
each	O
dataset	O
,	O
we	O
remove	O
very	O
short	O
documents	O
,	O
and	O
compute	O
a	O
vocabulary	O
by	O
removing	O
stop	O
words	O
,	O
rare	O
words	O
,	O
frequent	O
words	O
.	O

Experiment	O
.	O

We	O
run	O
such	O
iterations	O
until	O
convergence	O
.	O

At	O
M	O
-	O
step	O
,	O
we	O
update	O
new	O
α	O
and	O
β	O
based	O
on	O
obtained	O
γ	O
and	O
φ	O
.	O

At	O
E	O
-	O
step	O
,	O
we	O
estimate	O
the	O
best	O
γ	O
and	O
φ	O
given	O
current	O
α	O
and	O
β	O
.	O

φ	O
can	O
be	O
approximated	O
as	O
:	O
φw	O
n	O
,	O
i	O
∝	O
βi	O
,	O
vexp(Ψ(γi)+	O
1	O
−	O
λ2	O
ζ1	O
×	O
(	O
wn	O
,	O
wm)∈E	O
1	O
φw	O
m	O
,	O
i	O
Sm	O
,	O
n+	O
1	O
−	O
λ4	O
ζ1	O
×	O
(	O
wn	O
,	O
wp)∈E	O
2	O
φw	O
p	O
,	O
i	O
Sp	O
,	O
n	O
)	O
(	O
12	O
)	O
EM	O
algorithm	O
is	O
applied	O
using	O
above	O
updating	O
rules	O
.	O

E	O
q	O
(	O
|	O
E	O
C	O
i	O
|	O
)	O
is	O
obtained	O
directly	O
from	O
(	O
10	O
)	O
,	O
E	O
q	O
(	O
θ	O
T	O
θ	O
)	O
is	O
from	O
the	O
property	O
of	O
Dirichlet	O
distribution	O
.	O

We	O
derive	O
Variational	O
Inference	O
for	O
posterior	O
inference	O
.	O

Posterior	O
Inference	O
and	O
Parameter	O
Estimation	O
.	O

In	O
this	O
way	O
,	O
knowledge	O
from	O
large	O
corpus	O
other	O
than	O
current	O
document	O
collections	O
is	O
incorporated	O
to	O
guide	O
topic	O
modeling	O
.	O

The	O
word	O
representations	O
are	O
computed	O
using	O
neural	O
networks	O
,	O
and	O
the	O
learned	O
representations	O
explicitly	O
encode	O
many	O
linguistic	O
regularities	O
and	O
patterns	O
from	O
the	O
corpus	O
.	O

Then	O
equation	O
(	O
3	O
)	O
can	O
be	O
represented	O
as	O
below	O
:	O
P	O
egrf	O
(	O
z	O
|	O
θ	O
)	O
=	O
1	O
|	O
E1	O
|	O
+	O
|	O
E2	O
|	O
w∈V	O
M	O
ulti(zw	O
|	O
θ)×	O
(	O
|	O
E	O
C	O
1	O
|	O
λ1	O
+	O
|	O
E	O
N	O
C	O
1	O
|	O
λ2	O
+	O
|	O
E	O
C	O
2	O
|	O
λ3	O
+	O
|	O
E	O
N	O
C	O
2	O
|	O
λ4	O
)	O
=	O
w∈V	O
M	O
ulti(zw	O
|	O
θ	O
)	O
(	O
|	O
E1	O
|	O
+	O
|	O
E2	O
|)θ	O
T	O
θ	O
×	O
(	O
|	O
E	O
C	O
1	O
|	O
(	O
1	O
−	O
λ2)+	O
|	O
E1	O
|	O
λ2θ	O
T	O
θ+	O
|	O
E	O
C	O
2	O
|	O
(	O
1	O
−	O
λ4)+	O
|	O
E2	O
|	O
λ4θ	O
T	O
θ	O
)	O
(	O
9	O
)	O
From	O
the	O
second	O
line	O
to	O
the	O
third	O
line	O
of	O
Equation	O
(	O
9	O
)	O
,	O
we	O
represent	O
λ	O
1	O
,	O
λ	O
3	O
as	O
the	O
function	O
of	O
λ	O
2	O
,	O
λ	O
4	O
based	O
on	O
(	O
7	O
)	O
and	O
(	O
8)	O
.	O

E	O
C	O
i	O
includes	O
all	O
coherent	O
edges	O
,	O
E	O
N	O
C	O
i	O
contains	O
all	O
non	O
-	O
coherent	O
edges	O
.	O

In	O
distance	O
-	O
i	O
edge	O
set	O
,	O
i=	O
1	O
,	O
2	O
.	O

The	O
coherent	O
edge	O
is	O
the	O
edge	O
that	O
the	O
two	O
linked	O
words	O
have	O
same	O
topic	O
.	O

Word	O
Similarity	O
Information	O
.	O

If	O
|E	O
1	O
|	O
=	O
0	O
,	O
|E	O
2	O
|	O
=	O
0	O
,	O
EGTRF	O
is	O
equiva-	O
lent	O
to	O
LDA	O
.	O

Equation	O
(	O
4	O
)	O
defines	O
word	O
vertex	O
as	O
multinomial	O
distribution	O
,	O
and	O
we	O
assign	O
λ	O
1	O
,	O
λ	O
2	O
,	O
λ	O
3	O
and	O
λ	O
4	O
nonzero	O
values	O
,	O
then	O
it	O
is	O
clear	O
to	O
verify	O
constraint	O
Lower	O
λ	O
2	O
,	O
λ	O
4	O
give	O
higher	O
reward	O
to	O
the	O
edge	O
that	O
connects	O
two	O
word	O
vertices	O
with	O
same	O
topic	O
.	O

We	O
obtain	O
the	O
marginal	O
distribution	O
of	O
a	O
document	O
:	O
p(w	O
|	O
α	O
,	O
β	O
)	O
=	O
P	O
(	O
θ	O
|	O
α	O
)	O
z	O
P	O
egrf	O
(	O
z	O
|	O
θ	O
)	O
n	O
P	O
(	O
wn	O
|	O
zw	O
n	O
,	O
β)dθ(2	O
)	O
We	O
can	O
see	O
the	O
marginal	O
distribution	O
is	O
similar	O
to	O
LDA	O
except	O
topic	O
assignment	O
of	O
word	O
is	O
sampled	O
by	O
Extended	O
Global	O
Random	O
Field	O
instead	O
of	O
Multinomial	O
.	O

Given	O
Dirichlet	O
prior	O
α	O
,	O
word	O
distribution	O
of	O
topics	O
β	O
,	O
topic	O
mixture	O
of	O
document	O
θ	O
,	O
topic	O
assignments	O
z	O
and	O
words	O
w.	O

For	O
each	O
of	O
the	O
n	O
words	O
w	O
n	O
in	O
d	O
:	O
Choose	O
topic	O
z	O
n	O
∼	O
P	O
egrf	O
(	O
z	O
|	O
θ	O
)	O
,	O
Choose	O
word	O
w	O
n	O
∼	O
M	O
ulti(β	O
zn	O
,	O
wn	O
)	O
.	O

Choose	O
θ	O
∼	O
Dir(α	O
)	O
.	O

The	O
generative	O
process	O
for	O
word	O
sequence	O
of	O
a	O
document	O
is	O
described	O
as	O
below	O
:	O
For	O
each	O
document	O
d	O
in	O
corpus	O
D	O
:	O
Transform	O
document	O
d	O
into	O
graph	O
.	O

EGTRF	O
is	O
a	O
generative	O
proba	O
-	O
bilistic	O
model	O
,	O
the	O
basic	O
idea	O
is	O
that	O
documents	O
are	O
represented	O
as	O
mixtures	O
of	O
topics	O
,	O
words	O
are	O
generated	O
depending	O
on	O
the	O
topic	O
mixtures	O
and	O
graph	O
structure	O
of	O
current	O
document	O
.	O

If	O
Equation	O
(	O
1	O
)	O
satisfies	O
all	O
the	O
four	O
constraints	O
,	O
it	O
is	O
easy	O
to	O
verify	O
P	O
(	O
G	O
)	O
is	O
also	O
a	O
probability	O
measure	O
since	O
summing	O
over	O
all	O
possible	O
samples	O
g	O
equals	O
to	O
1	O
.	O

g	O
is	O
one	O
sample	O
of	O
word	O
topic	O
assignments	O
from	O
graph	O
G.	O

So	O
f	O
(	O
z	O
)	O
f	O
(	O
z	O
)	O
f	O
(	O
1	O
)	O
(	O
z	O
,	O
z	O
)	O
and	O
f	O
(	O
z	O
)	O
f	O
(	O
z	O
)	O
f	O
(	O
2	O
)	O
(	O
z	O
,	O
z	O
)	O
are	O
probability	O
measure	O
.	O

f	O
(	O
1	O
)	O
and	O
f	O
(	O
2	O
)	O
are	O
not	O
necessarily	O
probability	O
measure	O
,	O
however	O
,	O
summing	O
over	O
all	O
possible	O
states	O
of	O
the	O
product	O
of	O
the	O
edge	O
and	O
the	O
linked	O
word	O
pair	O
should	O
equal	O
to	O
1	O
,	O
which	O
are	O
from	O
constraints	O
3	O
and	O
4	O
.	O

f	O
(	O
1	O
)	O
(	O
z	O
,	O
z	O
)	O
and	O
f	O
(	O
2	O
)	O
(	O
z	O
,	O
z	O
)	O
are	O
the	O
function	O
defined	O
on	O
edge	O
set	O
E	O
1	O
and	O
E	O
2	O
.	O

z	O
,	O
z	O
∈Z	O
f	O
(	O
z	O
)	O
f	O
(	O
z	O
)	O
f	O
(	O
2	O
)	O
(	O
z	O
,	O
z	O
)	O
=	O
1	O
In	O
Equation	O
(	O
1	O
)	O
,	O
f	O
(	O
z	O
)	O
is	O
the	O
function	O
defined	O
on	O
word	O
vertex	O
,	O
which	O
is	O
a	O
probability	O
measure	O
because	O
of	O
the	O
constraints	O
1	O
and	O
2	O
.	O

z	O
,	O
z	O
∈Z	O
f	O
(	O
z	O
)	O
f	O
(	O
z	O
)	O
f	O
(	O
1	O
)	O
(	O
z	O
,	O
z	O
)	O
=	O
1	O
4	O
.	O

z∈Z	O
f	O
(	O
z	O
)	O
=	O
1	O
3	O
.	O

P	O
(	O
G	O
)	O
=	O
f	O
G	O
(	O
g	O
)	O
=	O
1	O
|	O
E1	O
|	O
+	O
|	O
E2	O
|	O
w∈W	O
f	O
(	O
zw)×	O
(	O
(	O
w	O
1	O
,	O
w	O
1	O
)	O
∈E	O
1	O
f	O
(	O
1	O
)	O
(	O
z	O
w	O
1	O
,	O
z	O
w	O
1	O
)	O
+	O
(	O
w	O
2	O
,	O
w	O
2	O
)	O
∈E	O
2	O
f	O
(	O
2	O
)	O
(	O
z	O
w	O
2	O
,	O
z	O
w	O
2	O
)	O
)	O
(	O
1	O
)	O
s.t	O
.	O
1.f	O
(	O
z	O
)	O
>	O
0	O
,	O
f	O
(	O
1	O
)	O
(	O
z	O
,	O
z	O
)	O
>	O
0	O
,	O
f	O
(	O
2	O
)	O
(	O
z	O
,	O
z	O
)	O
>	O
0	O
2	O
.	O

Extended	O
Global	O
Random	O
Field	O
.	O

We	O
organized	O
the	O
paper	O
as	O
below	O
:	O
EGTRF	O
is	O
presented	O
in	O
Section	O
2	O
,	O
variational	O
inference	O
and	O
parameter	O
estimation	O
are	O
derived	O
in	O
Section	O
3	O
,	O
experiments	O
on	O
two	O
datasets	O
are	O
showed	O
in	O
Section	O
4	O
,	O
we	O
conclude	O
the	O
paper	O
in	O
Section	O
5	O
.	O

The	O
word	O
vector	O
representations	O
are	O
very	O
interesting	O
because	O
the	O
learned	O
vectors	O
explicitly	O
encode	O
many	O
linguistic	O
regularities	O
and	O
patterns	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
.	O

Therefore	O
,	O
EGTRF	O
can	O
exploit	O
more	O
semantically	O
or	O
syntactically	O
word	O
dependencies	O
.	O

Some	O
hidden	O
dependency	O
relations	O
can	O
also	O
be	O
extracted	O
by	O
merging	O
dependency	O
trees	O
.	O

The	O
two	O
sentences	O
are	O
parsed	O
into	O
dependency	O
trees	O
respectively	O
,	O
and	O
then	O
merged	O
into	O
a	O
graph	O
.	O

An	O
example	O
of	O
a	O
simple	O
document	O
that	O
has	O
two	O
sentences	O
shows	O
in	O
Figure	O
1	O
.	O

Then	O
they	O
propose	O
GTRF	O
to	O
model	O
non	O
-	O
linear	O
topical	O
dependencies	O
,	O
word	O
topics	O
are	O
sampled	O
based	O
on	O
graph	O
structure	O
instead	O
of	O
"	O
bag	O
of	O
words	O
"	O
representation	O
,	O
the	O
conditional	O
independence	O
of	O
word	O
topic	O
assignment	O
is	O
thus	O
relaxed	O
.	O

They	O
show	O
topics	O
of	O
semantically	O
or	O
syntactically	O
dependent	O
words	O
achieve	O
the	O
highest	O
similarity	O
and	O
are	O
able	O
to	O
provide	O
more	O
useful	O
information	O
in	O
topic	O
modeling	O
,	O
which	O
is	O
also	O
the	O
basic	O
assumption	O
of	O
our	O
model	O
.	O

In	O
Syntactic	O
topic	O
models	O
(	O
Boyd	O
-	O
Graber	O
et	O
al	O
,	O
2009	O
)	O
,	O
each	O
word	O
of	O
a	O
sentence	O
is	O
generated	O
by	O
a	O
distribution	O
that	O
combines	O
document	O
-	O
specific	O
topic	O
weights	O
and	O
parsetree	O
-	O
specific	O
syntactic	O
transitions	O
.	O

Most	O
of	O
the	O
models	O
above	O
are	O
limited	O
to	O
model	O
linear	O
topical	O
dependencies	O
between	O
words	O
,	O
word	O
topical	O
dependencies	O
can	O
also	O
be	O
modeled	O
by	O
a	O
non	O
-	O
linear	O
way	O
.	O

Zhu	O
(	O
Zhu	O
et	O
al	O
,	O
2010	O
)	O
incorporates	O
Markov	O
dependency	O
between	O
topic	O
assignments	O
of	O
neighboring	O
words	O
,	O
and	O
employs	O
a	O
general	O
structure	O
of	O
the	O
GLM	O
to	O
define	O
a	O
conditional	O
distribution	O
of	O
latent	O
topic	O
assignments	O
over	O
words	O
.	O

Gruber	O
(	O
Gruber	O
et	O
al	O
,	O
2007	O
)	O
models	O
the	O
topics	O
of	O
words	O
in	O
the	O
document	O
as	O
a	O
Markov	O
chain	O
,	O
and	O
assumes	O
all	O
words	O
in	O
the	O
same	O
sentence	O
are	O
more	O
likely	O
to	O
have	O
the	O
same	O
topic	O
.	O

Wallach	O
(	O
Wallach	O
,	O
2006	O
)	O
explores	O
a	O
hierarchical	O
generative	O
probabilistic	O
model	O
that	O
incorporates	O
both	O
n	O
-	O
gram	O
statistics	O
and	O
latent	O
topic	O
variables	O
.	O

To	O
relax	O
the	O
"	O
bag	O
of	O
words	O
"	O
assumption	O
,	O
many	O
extended	O
topic	O
models	O
have	O
been	O
proposed	O
to	O
address	O
the	O
limitation	O
of	O
conditional	O
independence	O
.	O

However	O
,	O
the	O
"	O
bag	O
of	O
words	O
"	O
assumption	O
is	O
employed	O
in	O
most	O
existing	O
topic	O
models	O
,	O
it	O
assumes	O
the	O
order	O
of	O
words	O
can	O
be	O
ignored	O
and	O
topic	O
assignment	O
of	O
each	O
word	O
is	O
conditionally	O
independent	O
given	O
the	O
topic	O
mixture	O
of	O
a	O
document	O
.	O

Introduction	O
.	O

Parameters	O
are	O
estimated	O
efficiently	O
by	O
variational	O
inference	O
and	O
experimental	O
results	O
on	O
two	O
datasets	O
show	O
EGTRF	O
achieves	O
lower	O
perplexity	O
and	O
higher	O
log	O
predictive	O
probability	O
.	O

Word	O
similarity	O
information	O
learned	O
from	O
large	O
corpus	O
is	O
incorporated	O
to	O
enhance	O
word	O
topic	O
assignment	O
.	O

This	O
work	O
was	O
supported	O
by	O
EPSRC	O
grant	O
EP	O
/	O
L027623/1	O
.	O

We	O
propose	O
these	O
techniques	O
as	O
practical	O
approaches	O
to	O
including	O
target	O
syntax	O
in	O
NMT	B-TaskName
.	O

We	O
further	O
improve	O
on	O
the	O
individual	O
results	O
via	O
a	O
decoding	O
strategy	O
allowing	O
ensembling	B-MethodName
of	I-MethodName
models	I-MethodName
producing	O
different	O
output	O
representations	O
,	O
such	O
as	O
subword	O
units	O
and	O
syntax	O
.	O

We	O
train	O
these	O
models	O
using	O
a	O
delayed	B-MethodName
SGD	I-MethodName
update	I-MethodName
training	I-MethodName
procedure	I-MethodName
that	O
is	O
especially	O
effective	O
for	O
the	O
long	O
representations	O
that	O
arise	O
from	O
including	O
target	O
language	O
syntactic	O
information	O
in	O
the	O
output	O
.	O

We	O
report	O
strong	O
performance	O
with	O
individual	O
models	O
that	O
meets	O
or	O
improves	O
over	O
the	O
recent	O
best	O
WAT	B-MethodName
Ja	I-MethodName
-	I-MethodName
En	I-MethodName
ensemble	I-MethodName
results	O
.	O

However	O
,	O
we	O
found	O
that	O
this	O
gives	O
little	O
improvement	O
in	O
BLEU	B-MetricName
over	O
unconstrained	O
decoding	O
although	O
it	O
remains	O
an	O
interesting	O
line	O
of	O
research	O
.	O

We	O
find	O
that	O
the	O
syntax	B-MethodName
model	I-MethodName
is	O
often	O
more	O
grammatical	O
,	O
even	O
when	O
the	O
plain	B-MethodName
BPE	I-MethodName
model	O
may	O
share	O
more	O
vocabulary	O
with	O
the	O
reference	O
(	O
Table	O
2	O
)	O
.	O

To	O
highlight	O
these	O
,	O
we	O
examine	O
hypotheses	O
generated	O
by	O
the	O
plain	B-MethodName
BPE	I-MethodName
and	O
linearized	B-MethodName
derivation	I-MethodName
models	O
.	O

However	O
,	O
an	O
ensemble	B-MethodName
of	I-MethodName
models	I-MethodName
producing	O
plain	B-MethodName
BPE	I-MethodName
and	O
linearized	B-MethodName
derivations	I-MethodName
improves	O
by	O
0.5	B-MetricValue
BLEU	B-MetricName
over	O
the	O
plain	B-MethodName
BPE	I-MethodName
baseline	O
.	O

Ensembles	B-MethodName
of	I-MethodName
two	I-MethodName
identical	I-MethodName
models	I-MethodName
trained	O
with	O
different	O
seeds	O
only	O
slightly	O
improve	O
over	O
the	O
single	B-MethodName
model	I-MethodName
(	O
Table	O
5	O
)	O
.	O

It	O
has	O
been	O
suggested	O
that	O
decaying	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
can	O
have	O
a	O
similar	O
effect	O
to	O
large	O
batch	O
training	O
(	O
Smith	O
et	O
al	O
.	O
,	O
2017	O
)	O
,	O
but	O
reducing	O
the	O
initial	O
learning	B-HyperparameterName
rate	I-HyperparameterName
by	O
a	O
factor	B-HyperparameterValue
of	I-HyperparameterValue
8	I-HyperparameterValue
alone	O
did	O
not	O
give	O
the	O
same	O
improvements	O
.	O

Accumulating	O
the	O
gradient	O
over	O
8	B-HyperparameterValue
batches	B-HyperparameterName
of	O
size	O
4096	B-HyperparameterValue
gives	O
a	O
3	B-MetricValue
BLEU	B-MetricName
improvement	O
for	O
the	O
linear	B-MethodName
derivation	I-MethodName
model	O
.	O

English	O
constituency	O
trees	O
are	O
obtained	O
using	O
CKYlark	B-MethodName
(	O
Oda	O
et	O
al	O
.	O
,	O
2015	O
)	O
,	O
with	O
words	O
replaced	O
by	O
BPE	O
subwords	O
.	O

We	O
report	O
all	O
experiments	O
for	O
Japanese	O
-	O
English	O
,	O
using	O
the	O
first	B-HyperparameterValue
1	I-HyperparameterValue
M	I-HyperparameterValue
training	B-HyperparameterName
sentences	I-HyperparameterName
of	O
the	O
Japanese	O
-	O
English	O
ASPEC	B-DatasetName
data	O
(	O
Nakazawa	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

For	O
these	O
models	O
we	O
use	O
embedding	B-HyperparameterName
size	I-HyperparameterName
400	B-HyperparameterValue
,	O
a	O
single	B-HyperparameterValue
BiLSTM	B-HyperparameterName
layer	I-HyperparameterName
of	O
size	O
750	B-HyperparameterValue
,	O
and	O
batch	B-HyperparameterName
size	I-HyperparameterName
80	B-HyperparameterValue
.	O

For	O
comparison	O
with	O
earlier	O
target	O
syntax	O
work	O
,	O
we	O
also	O
train	O
two	O
RNN	B-MethodName
attention	I-MethodName
-	I-MethodName
based	I-MethodName
seq2seq	I-MethodName
models	O
(	O
Bahdanau	O
et	O
al	O
.	O
,	O
2015	O
)	O
with	O
normal	O
SGD	O
to	O
produce	O
plain	O
BPE	O
sequences	O
and	O
linearized	O
derivations	O
.	O

In	O
all	O
cases	O
we	O
decode	O
using	O
SGNMT	B-MethodName
(	O
Stahlberg	O
et	O
al	O
.	O
,	O
2017	O
)	O
with	O
beam	B-HyperparameterName
size	I-HyperparameterName
4	B-HyperparameterValue
,	O
using	O
the	O
average	O
of	O
the	O
final	O
20	B-HyperparameterValue
checkpoints	B-HyperparameterName
.	O

All	O
Transformer	O
architectures	O
are	O
Ten	B-MethodName
-	I-MethodName
sor2Tensor	I-MethodName
's	O
base	O
Transformer	O
model	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O
2018	O
)	O
with	O
a	O
batch	O
size	O
of	O
4096	O
.	O

Each	O
multirepresentation	B-MethodName
ensemble	I-MethodName
consists	O
of	O
the	O
plain	O
BPE	O
model	O
and	O
one	O
other	O
individual	O
model	O
.	O

We	O
decode	O
with	O
individual	O
models	O
and	O
two	B-MethodName
-	I-MethodName
model	I-MethodName
ensembles	I-MethodName
,	O
comparing	O
results	O
for	O
single	O
-	O
representation	O
and	O
multi	B-MethodName
-	I-MethodName
representation	I-MethodName
ensembles	I-MethodName
.	O

To	O
compare	O
target	O
representations	O
we	O
train	O
Transformer	B-MethodName
models	I-MethodName
with	O
target	O
representations	O
(	O
1	O
)	O
,	O
(	O
2	O
)	O
,	O
(	O
4	O
)	O
and	O
(	O
5	O
)	O
shown	O
in	O
Table	O
1	O
,	O
using	O
delayed	B-MethodName
SGD	I-MethodName
updates	I-MethodName
every	O
8	B-HyperparameterValue
batches	B-HyperparameterName
.	O

We	O
first	O
explore	O
the	O
effect	O
of	O
our	O
delayed	B-MethodName
SGD	I-MethodName
update	I-MethodName
training	O
scheme	O
on	O
single	O
models	O
,	O
contrasting	O
updates	O
every	O
batch	O
with	O
accumulated	O
updates	O
every	O
8	B-HyperparameterValue
batches	B-HyperparameterName
.	O

This	O
lets	O
us	O
effectively	O
use	O
very	O
large	O
batch	B-HyperparameterName
sizes	I-HyperparameterName
without	O
requiring	O
multiple	O
GPUs	O
.	O
Ensembling	O
Representations	O
.	O

We	O
accumulate	O
gradients	O
over	O
a	O
fixed	O
number	B-HyperparameterName
of	I-HyperparameterName
batches	I-HyperparameterName
before	O
using	O
the	O
accumulated	O
gradients	O
to	O
update	O
the	O
model	O
1	O
.	O

Our	O
strategy	O
avoids	O
this	O
problem	O
by	O
using	O
delayed	B-MethodName
SGD	I-MethodName
updates	I-MethodName
.	O

During	O
NMT	B-TaskName
training	O
,	O
by	O
default	O
,	O
the	O
gradients	O
used	O
to	O
update	O
model	O
parameters	O
are	O
calculated	O
over	O
individual	O
batches	O
.	O

The	O
Ten	B-MethodName
-	I-MethodName
sor2Tensor	I-MethodName
framework	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O
2018	O
)	O
defines	O
batch	O
size	O
as	O
the	O
number	O
of	O
tokens	O
per	O
batch	O
,	O
so	O
batches	O
will	O
contain	O
fewer	O
sequences	O
if	O
their	O
average	O
length	O
increases	O
.	O

Delayed	B-MethodName
SGD	I-MethodName
Update	I-MethodName
Training	O
for	O
Long	O
Sequences	O
.	O

Garmash	O
and	O
Monz	O
(	O
2016	O
)	O
show	O
translation	O
improvements	O
with	O
multi	B-MethodName
-	I-MethodName
source	I-MethodName
-	I-MethodName
language	I-MethodName
NMT	I-MethodName
ensembles	I-MethodName
.	O

Hokamp	O
(	O
2017	O
)	O
shows	O
improvements	O
in	O
the	O
quality	B-TaskName
estimation	I-TaskName
task	O
using	O
ensembles	O
of	O
NMT	B-TaskName
models	O
with	O
multiple	O
input	O
representations	O
which	O
share	O
an	O
output	O
representation	O
.	O

(	O
2017	O
)	O
combine	O
recurrent	B-MethodName
neural	I-MethodName
network	I-MethodName
grammar	I-MethodName
(	I-MethodName
RNNG	I-MethodName
)	I-MethodName
models	O
(	O
Dyer	O
et	O
al	O
.	O
,	O
2016	O
)	O
with	O
attention	B-MethodName
-	I-MethodName
based	I-MethodName
models	O
to	O
produce	O
well	O
-	O
formed	O
dependency	O
trees	O
.	O

(	O
2017	O
)	O
perform	O
NMT	B-TaskName
with	O
syntax	O
annotation	O
in	O
the	O
form	O
of	O
Combinatory	O
Categorial	O
Grammar	O
(	O
CCG	O
)	O
supertags	O
.	O

Long	O
sequences	O
make	O
training	O
more	O
difficult	O
(	O
Bahdanau	O
et	O
al	O
.	O
,	O
2015	O
)	O
,	O
which	O
we	O
address	O
with	O
an	O
adjusted	O
training	O
procedure	O
for	O
the	O
Transformer	O
architecture	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O
2017	O
)	O
,	O
using	O
delayed	B-MethodName
SGD	I-MethodName
updates	I-MethodName
which	O
accumulate	O
gradients	O
over	O
multiple	O
batches	O
.	O

As	O
part	O
of	O
our	O
investigation	O
we	O
suggest	O
strategies	O
for	O
practical	O
NMT	B-TaskName
with	O
very	O
long	O
target	O
sequences	O
.	O

We	O
hypothesize	O
that	O
an	O
NMT	B-MethodName
ensemble	I-MethodName
would	O
be	O
strengthened	O
if	O
its	O
component	O
models	O
were	O
complementary	O
in	O
this	O
way	O
.	O

However	O
,	O
ensembling	B-MethodName
often	O
requires	O
component	O
models	O
to	O
make	O
predictions	O
relating	O
to	O
the	O
same	O
output	O
sequence	O
position	O
at	O
each	O
time	O
step	O
.	O

Previous	O
work	O
has	O
observed	O
that	O
NMT	B-TaskName
models	O
trained	O
to	O
generate	O
target	O
syntax	O
can	O
exhibit	O
improved	O
sentence	O
structure	O
(	O
Aharoni	O
and	O
Goldberg	O
,	O
2017;Eriguchi	O
et	O
al	O
.	O
,	O
2017	O
)	O
relative	O
to	O
those	O
trained	O
on	O
plain	O
-	O
text	O
,	O
while	O
plain	O
-	O
text	O
models	O
produce	O
shorter	O
sequences	O
and	O
so	O
may	O
encode	O
lexical	O
information	O
more	O
easily	O
(	O
Nadejde	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

Ensembles	O
of	O
multiple	O
NMT	B-TaskName
models	O
consistently	O
and	O
significantly	O
improve	O
over	O
single	O
models	O
(	O
Garmash	O
and	O
Monz	O
,	O
2016	O
)	O
.	O

We	O
formulate	O
beam	O
search	O
over	O
such	O
ensembles	O
using	O
WFSTs	B-MethodName
,	O
and	O
describe	O
a	O
delayed	B-MethodName
SGD	I-MethodName
update	I-MethodName
training	I-MethodName
procedure	I-MethodName
that	O
is	O
especially	O
effective	O
for	O
long	O
representations	O
like	O
linearized	O
syntax	O
.	O

We	O
explore	O
strategies	O
for	O
incorporating	O
target	O
syntax	O
into	O
Neural	B-TaskName
Machine	I-TaskName
Translation	I-TaskName
.	O

Multi	O
-	O
representation	O
Ensembles	O
and	O
Delayed	O
SGD	O
Updates	O
Improve	O
Syntax	B-TaskName
-	I-TaskName
based	I-TaskName
NMT	I-TaskName
.	O

Acknowledgments	O
.	O

Conclusions	O
.	O

It	O
is	O
also	O
possible	O
to	O
constrain	O
decoding	O
of	O
linearized	O
trees	O
and	O
derivations	O
to	O
wellformed	O
outputs	O
.	O

Our	O
solution	O
was	O
to	O
penalise	O
scores	O
of	O
non	O
-	O
terminals	O
under	O
the	O
syntax	O
model	O
by	O
a	O
constant	O
factor	O
.	O

In	O
ensembling	O
plain	O
-	O
text	O
with	O
a	O
syntax	O
external	O
representation	O
we	O
observed	O
that	O
in	O
a	O
small	O
proportion	O
of	O
cases	O
non	O
-	O
terminals	O
were	O
over	O
-	O
generated	O
,	O
due	O
to	O
the	O
mismatch	O
in	O
target	O
sequence	O
lengths	O
.	O

By	O
ensembling	O
syntax	O
and	O
plain	O
-	O
text	O
we	O
hope	O
to	O
benefit	O
from	O
their	O
complementary	O
strengths	O
.	O

Our	O
syntax	O
models	O
achieve	O
similar	O
results	O
despite	O
producing	O
much	O
longer	O
sequences	O
.	O

Our	O
plain	O
BPE	O
baseline	O
(	O
Table	O
4	O
)	O
outperforms	O
the	O
current	O
best	O
system	O
on	O
WAT	O
Ja	O
-	O
En	O
,	O
an	O
8	O
-	O
model	O
ensemble	O
(	O
Morishita	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

Our	O
first	O
results	O
in	O
Table	O
3	O
show	O
that	O
large	O
batch	O
training	O
can	O
significantly	O
improve	O
the	O
performance	O
of	O
single	O
Transformers	O
,	O
particularly	O
when	O
trained	O
to	O
produce	O
longer	O
sequences	O
.	O

Results	O
and	O
Discussion	O
.	O

The	O
linearized	O
derivation	O
uses	O
additional	O
tokens	O
for	O
non	O
-	O
terminals	O
with	O
<	O
/R	O
>	O
.	O

Non	O
-	O
terminals	O
are	O
included	O
as	O
separate	O
tokens	O
.	O

We	O
train	O
separate	O
Japanese	O
(	O
lowercased	O
)	O
and	O
English	O
(	O
cased	O
)	O
BPE	O
vocabularies	O
on	O
the	O
plain	O
-	O
text	O
,	O
with	O
30	O
K	O
merges	O
each	O
.	O

All	O
models	O
use	O
plain	O
BPE	O
Japanese	O
source	O
sentences	O
.	O

Experiments	O
.	O

Symbols	O
in	O
the	O
internal	O
representation	O
are	O
consumed	O
as	O
needed	O
to	O
stay	O
synchronized	O
with	O
the	O
external	O
representation	O
,	O
as	O
illustrated	O
in	O
Figure	O
1	O
;	O
epsilons	O
are	O
consumed	O
with	O
a	O
probability	O
of	O
1	O
.	O

As	O
search	O
proceeds	O
,	O
each	O
model	O
score	O
is	O
updated	O
separately	O
with	O
its	O
appropriate	O
representation	O
.	O

This	O
leads	O
to	O
an	O
outer	O
beam	O
search	O
over	O
external	O
representations	O
with	O
inner	O
beam	O
searches	O
for	O
the	O
best	O
matching	O
internal	O
representations	O
.	O

The	O
ensembled	O
score	O
of	O
h	O
is	O
then	O
:	O
P	O
(	O
h	O
j	O
|h	O
<	O
j	O
)	O
=	O
P	O
o	O
(	O
h	O
j	O
|h	O
<	O
j	O
)	O
×	O
(	O
3	O
)	O
max	O
(	O
x	O
,	O
y)∈M	O
(	O
h	O
)	O
P	O
i	O
(	O
i(y)|i(x	O
)	O
)	O
The	O
max	O
performed	O
for	O
each	O
partial	O
hypothesis	O
h	O
is	O
itself	O
approximated	O
by	O
a	O
beam	O
search	O
.	O

The	O
set	O
of	O
partial	O
paths	O
yielding	O
h	O
are	O
:	O
M	O
(	O
h	O
)	O
=	O
(	O
2	O
)	O
{	O
(	O
x	O
,	O
y)|xyz	O
∈	O
P	O
,	O
o(x	O
)	O
=	O
h	O
<	O
j	O
,	O
o(xy	O
)	O
=	O
h	O
}	O
2	O
See	O
the	O
tokenization	O
wrappers	O
in	O
https://	O
github.com/ucam-smt/sgnmt	O
Here	O
z	O
is	O
the	O
path	O
suffix	O
.	O

h	O
j	O
be	O
a	O
partial	O
hypothesis	O
in	O
the	O
output	O
representation	O
.	O

Let	O
h	O
=	O
h	O
1	O
.	O

In	O
practice	O
,	O
beam	O
decoding	O
is	O
performed	O
in	O
the	O
external	O
representation	O
,	O
i.e.	O
over	O
projections	O
of	O
paths	O
in	O
P	O
2	O
.	O

A	O
path	O
(	O
Sennrich	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

Let	O
P	O
be	O
the	O
paths	O
in	O
T	O
leading	O
from	O
the	O
start	O
state	O
to	O
any	O
final	O
state	O
.	O

Mapping	O
from	O
word	O
to	O
BPE	O
representations	O
is	O
straightforward	O
,	O
and	O
mapping	O
from	O
(	O
linearized	O
)	O
syntax	O
to	O
plain	O
-	O
text	O
simply	O
deletes	O
non	O
-	O
terminals	O
.	O

The	O
complexity	O
of	O
the	O
transduction	O
depends	O
on	O
the	O
representations	O
.	O

To	O
formulate	O
an	O
ensembling	O
decoder	O
over	O
pairs	O
of	O
these	O
representations	O
,	O
we	O
assume	O
we	O
have	O
a	O
transducer	O
T	O
that	O
maps	O
from	O
one	O
representation	O
to	O
the	O
other	O
representation	O
.	O

Table	O
1	O
shows	O
several	O
different	O
representations	O
of	O
the	O
same	O
hypothesis	O
.	O

Training	O
on	O
multiple	O
GPUs	O
is	O
one	O
way	O
to	O
increase	O
the	O
amount	O
of	O
data	O
used	O
to	O
estimate	O
gradients	O
,	O
but	O
it	O
requires	O
significant	O
resources	O
.	O

However	O
,	O
with	O
such	O
large	O
batches	O
the	O
model	O
size	O
may	O
exceed	O
available	O
GPU	O
memory	O
.	O

Previous	O
research	O
has	O
used	O
very	O
large	O
batches	O
to	O
improve	O
training	O
convergence	O
while	O
requiring	O
fewer	O
model	O
updates	O
(	O
Smith	O
et	O
al	O
.	O
,	O
2017;Neishi	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

A	O
possible	O
consequence	O
is	O
that	O
batches	O
containing	O
fewer	O
sequences	O
per	O
update	O
may	O
have	O
'	O
noisier	O
'	O
estimated	O
gradients	O
than	O
batches	O
with	O
more	O
sequences	O
.	O

We	O
suggest	O
a	O
training	O
strategy	O
for	O
the	O
Transformer	O
model	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O
2017	O
)	O
which	O
gives	O
improved	O
performance	O
for	O
long	O
sequences	O
,	O
like	O
syntax	O
representations	O
,	O
without	O
requiring	O
additional	O
GPU	O
memory	O
.	O

We	O
map	O
words	O
to	O
subwords	O
as	O
described	O
in	O
Section	O
3	O
.	O

The	O
original	O
tree	O
can	O
be	O
directly	O
reproduced	O
from	O
the	O
sequence	O
,	O
so	O
that	O
structure	O
information	O
is	O
maintained	O
.	O

Our	O
linearized	O
derivation	O
representation	O
(	O
(	O
4	O
)	O
in	O
Table	O
1	O
)	O
consists	O
of	O
the	O
derivation	O
's	O
right	O
-	O
hand	O
side	O
tokens	O
with	O
an	O
end	O
-	O
of	O
-	O
rule	O
marker	O
,	O
<	O
/R	O
>	O
,	O
marking	O
the	O
last	O
non	O
-	O
terminal	O
in	O
each	O
rule	O
.	O

We	O
therefore	O
propose	O
a	O
derivation	O
-	O
based	O
representation	O
which	O
is	O
much	O
more	O
compact	O
than	O
a	O
linearized	O
parse	O
tree	O
(	O
examples	O
in	O
Table	O
1	O
)	O
.	O

We	O
wish	O
to	O
ensemble	O
using	O
models	O
which	O
generate	O
linearized	O
constituency	O
trees	O
but	O
these	O
representations	O
can	O
be	O
very	O
long	O
and	O
difficult	O
to	O
model	O
.	O

Ensembles	O
of	O
Syntax	O
Models	O
.	O

Previous	O
approaches	O
to	O
ensembling	O
diverse	O
models	O
focus	O
on	O
model	O
inputs	O
.	O

(	O
2017	O
)	O
similarly	O
produce	O
both	O
words	O
and	O
arcstandard	O
algorithm	O
actions	O
(	O
Nivre	O
,	O
2004	O
)	O
.	O

Wu	O
et	O
al	O
.	O

Eriguchi	O
et	O
al	O
.	O

They	O
demonstrate	O
improved	O
target	O
language	O
reordering	O
when	O
producing	O
syntax	O
.	O

Aharoni	O
and	O
Goldberg	O
(	O
2017	O
)	O
translate	O
from	O
source	O
BPE	O
into	O
target	O
linearized	O
parse	O
trees	O
,	O
but	O
omit	O
POS	O
tags	O
to	O
reduce	O
sequence	O
length	O
.	O

Nadejde	O
et	O
al	O
.	O

Related	O
Work	O
.	O

We	O
also	O
suggest	O
a	O
syntax	O
representation	O
which	O
results	O
in	O
much	O
shorter	O
sequences	O
.	O

These	O
long	O
sequences	O
may	O
arise	O
through	O
the	O
use	O
of	O
linearized	O
constituency	O
trees	O
and	O
can	O
be	O
much	O
longer	O
than	O
their	O
plain	O
byte	O
pair	O
encoded	O
(	O
BPE	O
)	O
equivalent	O
representations	O
(	O
Table	O
1	O
)	O
.	O

We	O
propose	O
an	O
approach	O
to	O
decoding	O
ensembles	O
of	O
models	O
generating	O
different	O
representations	O
,	O
focusing	O
on	O
models	O
generating	O
syntax	O
.	O

Models	O
producing	O
different	O
sentence	O
representations	O
are	O
necessarily	O
synchronized	O
to	O
enable	O
this	O
.	O

Introduction	O
.	O

Our	O
approach	O
gives	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
a	O
difficult	O
Japanese	O
-	O
English	O
task	O
.	O

We	O
specifically	O
focus	O
on	O
syntax	O
in	O
ensembles	O
containing	O
multiple	O
sentence	O
representations	O
.	O

As	O
shown	O
in	O
Table	O
4	O
,	O
out	O
of	O
all	O
the	O
settings	O
,	O
the	O
minimum	B-MetricName
phrase	I-MetricName
table	I-MetricName
size	I-MetricName
after	O
oracle	B-MethodName
-	I-MethodName
BLEU	I-MethodName
re	I-MethodName
-	I-MethodName
estimation	I-MethodName
is	O
only	O
3.28	B-MetricValue
%	I-MetricValue
of	O
baseline	O
(	O
i.e.	O
,	O
a	O
reduction	B-MetricName
of	O
96.72	B-MetricValue
%	I-MetricValue
)	O
while	O
it	O
is	O
7.6	B-MetricValue
%	I-MetricValue
for	O
forced	B-MethodName
decoding	I-MethodName
.	O

Our	O
experimental	O
results	O
show	O
BLEU	B-MetricName
score	O
improvements	O
of	O
up	O
to	O
+0.8	B-MetricValue
points	O
for	O
oracle	B-MethodName
-	I-MethodName
BLEU	I-MethodName
re	I-MethodName
-	I-MethodName
estimation	I-MethodName
over	O
a	O
strong	O
baseline	O
along	O
with	O
a	O
substantially	O
reduced	O
size	B-MetricName
of	I-MetricName
the	I-MetricName
re	I-MetricName
-	I-MetricName
estimated	I-MetricName
phrase	I-MetricName
table	I-MetricName
(	O
3.3	B-MetricValue
%	I-MetricValue
of	O
the	O
baseline	O
)	O
.	O

As	O
an	O
additional	O
benefit	O
,	O
the	O
phrase	B-MetricName
table	I-MetricName
size	I-MetricName
is	O
reduced	O
dramatically	O
to	O
only	O
3	B-MetricValue
%	I-MetricValue
of	O
the	O
original	O
size	O
.	O

Re	B-MethodName
-	I-MethodName
estimation	I-MethodName
of	O
the	O
translation	O
models	O
from	O
the	O
n	O
-	O
best	O
translation	O
of	O
the	O
bitext	O
could	O
re	O
-	O
enforce	O
the	O
probabilities	O
of	O
the	O
low	O
frequency	O
phrase	O
pairs	O
in	O
the	O
re	O
-	O
estimated	O
models	O
leading	O
to	O
over	O
-	O
fitting	O
.	O

Finally	O
,	O
we	O
re	O
-	O
train	O
the	O
phrase	O
translations	O
,	O
re	O
-	O
ordering	O
and	O
BiLM	B-MethodName
on	O
these	O
translations	O
and	O
alignments	O
.	O

We	O
use	O
a	O
BiLM	B-MethodName
specifically	O
as	O
an	O
instance	O
of	O
a	O
reordering	O
model	O
in	O
order	O
to	O
determine	O
the	O
effect	O
of	O
re	O
-	O
estimating	O
re	O
-	O
ordering	O
decisions	O
from	O
oracle	O
-	O
BLEU	O
translations	O
.	O

Along	O
with	O
the	O
phrase	O
translation	O
and	O
language	O
models	O
,	O
we	O
also	O
train	O
a	O
bilingual	B-MethodName
language	I-MethodName
model	I-MethodName
(	I-MethodName
BiLM	I-MethodName
)	I-MethodName
(	O
Niehues	O
et	O
al	O
.	O
,	O
2011;Garmash	O
and	O
Monz	O
,	O
2014	O
)	O
,	O
as	O
well	O
as	O
lexicalized	O
(	O
Tillman	O
,	O
2004	O
)	O
and	O
hierarchical	O
reordering	B-MethodName
models	I-MethodName
(	O
Galley	O
and	O
Manning	O
,	O
2008	O
)	O
.	O

For	O
obtaining	O
the	O
oracle	O
-	O
BLEU	O
translations	O
,	O
we	O
first	O
train	O
the	O
translation	O
models	O
from	O
the	O
bitext	O
using	O
the	O
standard	O
pipeline	O
of	O
word	B-TaskName
alignment	I-TaskName
and	O
heuristic	B-TaskName
extraction	I-TaskName
.	O

The	O
lower	O
scores	O
for	O
this	O
setting	O
as	O
compared	O
to	O
the	O
baseline	O
verified	O
that	O
using	O
only	O
the	O
best	O
BLEU	O
translation	O
indeed	O
degrades	O
the	O
performance	O
of	O
the	O
re	O
-	O
estimated	O
models	O
.	O

Oracle	O
BLEU	O
.	O

Oracle	O
BLEU	O
translations	O
have	O
been	O
previously	O
used	O
for	O
different	O
analytical	O
purposes	O
in	O
SMT	B-TaskName
(	O
Srivastava	O
et	O
al	O
.	O
,	O
2011;Dreyer	O
et	O
al	O
.	O
,	O
2007;Wisniewski	O
et	O
al	O
.	O
,	O
2010	O
)	O
.	O

Given	O
a	O
source	O
and	O
its	O
reference	O
translation	O
,	O
the	O
oracle	O
-	O
BLEU	O
translation	O
is	O
defined	O
as	O
the	O
translation	O
output	O
with	O
highest	O
BLEU	B-MetricName
score	O
.	O

The	O
idea	O
of	O
our	O
approach	O
is	O
to	O
re	O
-	O
estimate	O
the	O
models	O
with	O
n	O
-	O
best	O
oracle	O
-	O
BLEU	O
translations	O
and	O
sentence	O
alignments	O
resulting	O
from	O
decoding	O
the	O
source	O
sentence	O
.	O

Model	B-MethodName
Re	I-MethodName
-	I-MethodName
estimation	I-MethodName
.	O

However	O
,	O
the	O
main	O
focus	O
of	O
their	O
work	O
is	O
translation	O
model	O
adaptation	O
by	O
augmenting	O
the	O
bitext	O
with	O
additional	O
training	O
data	O
and	O
not	O
the	O
reestimation	O
of	O
the	O
translation	O
models	O
trained	O
on	O
the	O
parallel	O
data	O
.	O

Thus	O
forced	O
alignment	O
is	O
a	O
reestimation	O
technique	O
where	O
translation	O
probabilities	O
are	O
calculated	O
based	O
on	O
their	O
frequency	O
in	O
best	O
-	O
scoring	O
hypotheses	O
instead	O
of	O
the	O
frequencies	O
of	O
all	O
possible	O
phrase	O
pairs	O
in	O
the	O
bitext	O
.	O

In	O
forced	O
alignment	O
,	O
given	O
a	O
sentence	O
pair	O
(	O
F	O
,	O
E	O
)	O
,	O
a	O
decoder	O
determines	O
the	O
best	O
phrase	O
segmentation	O
and	O
alignment	O
which	O
will	O
result	O
in	O
a	O
translation	O
of	O
F	O
into	O
E.	O

An	O
important	O
novelty	O
of	O
our	O
approach	O
is	O
that	O
it	O
also	O
allows	O
for	O
the	O
re	O
-	O
estimation	O
of	O
re	O
-	O
ordering	O
models	O
which	O
can	O
yield	O
further	O
improvements	O
in	O
SMT	B-TaskName
performance	O
as	O
demonstrated	O
by	O
the	O
re	B-MethodName
-	I-MethodName
estimation	I-MethodName
of	I-MethodName
a	I-MethodName
BiLM	I-MethodName
.	O

In	O
this	O
paper	O
,	O
we	O
proposed	O
a	O
novel	O
technique	O
for	O
improving	O
the	O
reliability	O
of	O
SMT	B-TaskName
models	O
by	O
model	B-MethodName
re	I-MethodName
-	I-MethodName
estimation	I-MethodName
from	I-MethodName
oracle	I-MethodName
-	I-MethodName
BLEU	I-MethodName
translations	I-MethodName
of	O
the	O
source	O
sentences	O
in	O
the	O
bitext	O
.	O

In	O
addition	O
to	O
the	O
BLEU	B-MetricName
improvements	O
,	O
our	O
approach	O
also	O
results	O
in	O
a	O
re	O
-	O
estimated	O
phrase	O
table	O
with	O
a	O
significantly	O
reduced	O
size	O
as	O
compared	O
to	O
the	O
baseline	O
.	O

On	O
the	O
other	O
hand	O
,	O
oracle	B-MethodName
-	I-MethodName
BLEU	I-MethodName
re	I-MethodName
-	I-MethodName
estimation	I-MethodName
by	O
its	O
own	O
not	O
only	O
performs	O
better	O
than	O
forced	B-MethodName
decoding	I-MethodName
,	O
but	O
also	O
gives	O
a	O
performance	O
equal	O
to	O
forced	B-MethodName
decoding	I-MethodName
with	I-MethodName
leave	I-MethodName
-	I-MethodName
oneout	I-MethodName
when	O
interpolated	O
with	O
baseline	O
phrase	O
table	O
.	O

This	O
implies	O
that	O
only	O
in	O
combination	O
with	O
the	O
original	O
phrase	O
table	O
does	O
forced	B-MethodName
-	I-MethodName
decoding	I-MethodName
with	O
leave	O
-	O
one	O
-	O
out	O
outperform	O
the	O
baseline	O
.	O

As	O
shown	O
in	O
Table	O
3	O
,	O
even	O
with	O
leaveone	O
-	O
out	O
,	O
forced	B-MethodName
decoding	I-MethodName
performance	O
drops	O
below	O
the	O
baseline	O
by	O
-0.3	B-MetricValue
BLEU	B-MetricName
.	O

Additionally	O
,	O
we	O
also	O
compare	O
oracle	B-MethodName
-	I-MethodName
BLEU	I-MethodName
re	I-MethodName
-	I-MethodName
estimation	I-MethodName
to	O
forced	B-MethodName
decoding	I-MethodName
with	O
leave	O
-	O
oneout	O
(	O
Wuebker	O
et	O
al	O
.	O
,	O
2010	O
)	O
by	O
evaluating	O
both	O
on	O
a	O
concatenation	O
of	O
5	O
test	O
sets	O
(	O
MT03	B-DatasetName
,	O
MT05	B-DatasetName
-	I-DatasetName
MT09	I-DatasetName
)	O
.	O

However	O
,	O
improvements	O
achieved	O
with	O
this	O
interpolation	O
did	O
not	O
surpass	O
the	O
best	O
result	O
obtained	O
for	O
the	O
oracle	B-MethodName
-	I-MethodName
BLEU	I-MethodName
re	I-MethodName
-	I-MethodName
estimation	I-MethodName
.	O

For	O
an	O
additional	O
anal-	O
ysis	O
,	O
we	O
experimented	O
with	O
the	O
interpolation	O
of	O
both	O
the	O
re	O
-	O
estimated	O
phrase	O
table	O
(	O
forced	B-MethodName
decoding	I-MethodName
and	O
oracle	B-MethodName
-	I-MethodName
BLEU	I-MethodName
)	O
with	O
the	O
baseline	O
.	O

Note	O
that	O
re	B-MethodName
-	I-MethodName
estimation	I-MethodName
of	I-MethodName
BiLM	I-MethodName
or	O
re	B-MethodName
-	I-MethodName
ordering	I-MethodName
models	I-MethodName
is	O
not	O
possible	O
for	O
forced	B-MethodName
decoding	I-MethodName
due	O
to	O
the	O
constraint	O
of	O
having	O
to	O
match	O
the	O
exact	O
reference	O
.	O

The	O
highest	O
BLEU	B-MetricName
improvement	O
of	O
+0.8	B-MetricValue
is	O
achieved	O
by	O
using	O
a	O
re	B-MethodName
-	I-MethodName
estimated	I-MethodName
BiLM	I-MethodName
and	O
an	O
interpolated	B-MethodName
phrase	I-MethodName
table	I-MethodName
.	O

The	O
last	O
row	O
of	O
Table	O
2	O
shows	O
that	O
the	O
re	B-MethodName
-	I-MethodName
estimated	I-MethodName
BiLM	I-MethodName
on	O
its	O
own	O
adds	O
BLEU	B-MetricName
improvement	O
of	O
up	O
to	O
+0.5	B-MetricValue
(	O
for	O
MT09	B-DatasetName
)	O
.	O

For	O
all	O
test	O
sets	O
,	O
BiLM	B-MethodName
re	I-MethodName
-	I-MethodName
estimation	I-MethodName
provides	O
additional	O
improvements	O
over	O
simple	O
phrase	O
table	O
interpolation	O
,	O
demonstrating	O
that	O
reestimation	O
of	O
re	B-MethodName
-	I-MethodName
ordering	I-MethodName
models	I-MethodName
can	O
further	O
improve	O
translation	O
performance	O
.	O

Here	O
we	O
provide	O
the	O
results	O
for	O
the	O
re	O
-	O
estimation	O
of	O
a	O
BiLM	B-MethodName
.	O

An	O
important	O
novelty	O
of	O
oracle	B-MethodName
-	I-MethodName
BLEU	I-MethodName
reestimation	I-MethodName
is	O
that	O
it	O
also	O
allows	O
for	O
re	O
-	O
training	O
of	O
other	O
models	O
alongside	O
the	O
phrase	O
table	O
.	O

It	O
is	O
important	O
to	O
note	O
here	O
that	O
although	O
linear	B-MethodName
interpolation	I-MethodName
extinguishes	O
the	O
advantage	O
of	O
a	O
smaller	O
phrase	O
table	O
size	O
obtained	O
by	O
re	O
-	O
estimation	O
,	O
the	O
improvement	O
achieved	O
by	O
interpolation	O
for	O
oracle	B-MethodName
-	I-MethodName
BLEU	I-MethodName
re	I-MethodName
-	I-MethodName
estimation	I-MethodName
are	O
significantly	O
higher	O
as	O
compared	O
to	O
forced	B-MethodName
decoding	I-MethodName
.	O

On	O
the	O
other	O
hand	O
,	O
oracle	B-MethodName
-	I-MethodName
BLEU	I-MethodName
re	I-MethodName
-	I-MethodName
estimation	I-MethodName
shows	O
consistent	O
improvements	O
for	O
all	O
test	O
sets	O
with	O
a	O
maximum	B-MetricName
gain	I-MetricName
of	O
up	O
to	O
+0.7	B-MetricValue
for	O
MT06	B-DatasetName
.	O

For	O
the	O
linear	B-MethodName
interpolation	I-MethodName
of	O
the	O
re	O
-	O
estimated	O
phrase	O
table	O
with	O
the	O
baseline	O
,	O
forced	O
decoding	O
shows	O
only	O
a	O
slight	O
improvement	O
for	O
MT06	B-DatasetName
,	O
MT08	B-DatasetName
and	O
MT09	B-DatasetName
and	O
still	O
suffers	O
from	O
a	O
substantial	O
drop	O
for	O
MT05	B-DatasetName
.	O

One	O
can	O
see	O
in	O
Table	O
2	O
that	O
while	O
phrase	O
table	O
re	O
-	O
estimation	O
drops	O
substantially	O
for	O
forced	B-MethodName
decoding	I-MethodName
for	O
all	O
test	O
sets	O
(	O
up	O
to	O
-1.4	B-MetricValue
for	O
MT09	B-DatasetName
)	O
,	O
oracle	B-MethodName
-	I-MethodName
BLEU	I-MethodName
phrase	I-MethodName
table	I-MethodName
re	I-MethodName
-	I-MethodName
estimation	I-MethodName
shows	O
either	O
slight	O
improvements	O
or	O
negligible	O
drops	O
compared	O
to	O
the	O
baseline	O
.	O

Table	O
2	O
provides	O
a	O
comparison	O
between	O
BLEU	B-MetricName
improvements	O
achieved	O
by	O
forced	B-MethodName
decoding	I-MethodName
(	O
n	B-HyperparameterName
=	O
100	B-HyperparameterValue
best	O
)	O
and	O
our	O
oracle	B-MethodName
-	I-MethodName
BLEU	I-MethodName
re	I-MethodName
-	I-MethodName
estimation	I-MethodName
approach	O
(	O
n	B-HyperparameterName
=	O
1	B-HyperparameterValue
best	O
)	O
over	O
the	O
baseline	O
for	O
different	O
models	O
.	O

(	O
2010	O
)	O
,	O
where	O
the	O
best	O
improvements	O
are	O
obtained	O
for	O
n	B-HyperparameterName
=	O
100	B-HyperparameterValue
.	O

Surprisingly	O
,	O
this	O
is	O
in	O
contrast	O
with	O
forced	B-MethodName
decoding	I-MethodName
as	O
discussed	O
in	O
Wuebker	O
et	O
al	O
.	O

The	O
best	O
improvements	O
over	O
the	O
baseline	O
are	O
obtained	O
by	O
using	O
only	O
1	O
-	O
best	O
(	O
n=	B-HyperparameterName
1	B-HyperparameterValue
)	O
alignments	O
as	O
shown	O
in	O
Table	O
1	O
.	O

Re	B-MethodName
-	I-MethodName
estimated	I-MethodName
models	I-MethodName
with	O
three	O
different	O
values	O
of	O
n	B-HyperparameterName
∈	O
{	O
1	B-HyperparameterValue
,	O
10	B-HyperparameterValue
,	O
100	B-HyperparameterValue
}	O
were	O
evaluated	O
under	O
three	O
settings	O
:	O
phrase	B-MethodName
table	I-MethodName
re	I-MethodName
-	I-MethodName
estimation	I-MethodName
,	O
interpolation	O
,	O
and	O
BiLM	B-MethodName
re	I-MethodName
-	I-MethodName
estimation	I-MethodName
.	O

We	O
discuss	O
the	O
experimental	O
results	O
of	O
our	O
oracle	B-MethodName
-	I-MethodName
BLEU	I-MethodName
re	I-MethodName
-	I-MethodName
estimation	I-MethodName
approach	O
for	O
different	O
mod	O
-	O
els	O
and	O
settings	O
and	O
provide	O
a	O
comparison	O
with	O
the	O
baseline	O
(	O
heuristic	B-MethodName
training	I-MethodName
)	O
and	O
forced	B-MethodName
alignment	I-MethodName
.	O

Approximate	B-MetricName
randomization	I-MetricName
(	O
Noreen	O
.	O
,	O
1989;Riezler	O
and	O
Maxwell	O
,	O
2005	O
)	O
is	O
used	O
to	O
detect	O
statistically	O
significant	O
differences	O
.	O

Case	B-MetricName
-	I-MetricName
insensitive	I-MetricName
4	I-MetricName
-	I-MetricName
gram	I-MetricName
BLEU	I-MetricName
(	O
Papineni	O
et	O
al	O
.	O
,	O
2002	O
)	O
is	O
used	O
as	O
evaluation	O
metric	O
.	O

We	O
evaluate	O
against	O
4	O
test	O
sets	O
:	O
MT05	B-DatasetName
,	O
MT06	B-DatasetName
,	O
MT08	B-DatasetName
,	O
and	O
MT09	B-DatasetName
.	O

For	O
testing	O
the	O
performance	O
of	O
the	O
re	B-MethodName
-	I-MethodName
estimated	I-MethodName
models	I-MethodName
,	O
we	O
tune	O
different	O
systems	O
while	O
replacing	O
the	O
baseline	O
models	O
with	O
the	O
corresponding	O
re	O
-	O
estimated	O
models	O
.	O

Hierarchical	B-MethodName
and	I-MethodName
lexicalized	I-MethodName
re	I-MethodName
-	I-MethodName
ordering	I-MethodName
models	I-MethodName
as	O
well	O
as	O
the	O
BiLM	B-MethodName
are	O
re	O
-	O
trained	O
using	O
the	O
source	O
sentences	O
,	O
oracle	O
-	O
BLEU	O
translations	O
and	O
word	O
alignments	O
.	O

This	O
system	O
is	O
identical	O
to	O
the	O
baseline	O
system	O
except	O
for	O
the	O
removal	O
of	O
low	O
-	O
frequency	O
phrase	O
pairs	O
from	O
the	O
baseline	O
phrase	O
table	O
as	O
described	O
in	O
Section	O
3.3	O
.	O
To	O
obtain	O
the	O
n	O
-	O
best	O
oracle	O
-	O
BLUE	O
translations	O
,	O
we	O
experiment	O
with	O
different	O
values	O
of	O
n	B-HyperparameterName
,	O
where	O
n	B-HyperparameterValue
∈	O
{	O
1	B-HyperparameterName
,	O
10	B-HyperparameterName
,	O
100	B-HyperparameterName
}	O
.	O

To	O
obtain	O
oracle	O
-	O
BLEU	O
translations	O
,	O
we	O
first	O
train	O
an	O
initial	O
SMT	B-TaskName
system	O
and	O
use	O
it	O
to	O
decode	O
the	O
bitext	O
.	O

Oracle	B-MethodName
BLEU	I-MethodName
re	I-MethodName
-	I-MethodName
estimation	I-MethodName
.	O

In	O
order	O
to	O
increase	O
the	O
chances	O
of	O
producing	O
the	O
exact	O
reference	O
,	O
we	O
follow	O
Foster	O
and	O
Kuhn	O
(	O
2012	O
)	O
and	O
relax	O
the	O
standard	O
decoding	O
parameters	O
as	O
follows	O
:	O
distortion	B-HyperparameterName
limit=∞	I-HyperparameterName
,	O
stack	B-HyperparameterName
size=2000	I-HyperparameterName
,	O
beam	B-HyperparameterName
width=10e-30	I-HyperparameterName
,	O
and	O
no	O
threshold	B-HyperparameterName
pruning	I-HyperparameterName
of	O
the	O
translation	O
model	O
.	O

For	O
forced	B-MethodName
alignment	I-MethodName
we	O
use	O
the	O
existing	O
implementation	O
within	O
the	O
Moses	B-MethodName
SMT	I-MethodName
toolkit	I-MethodName
(	O
Koehn	O
et	O
al	O
.	O
,	O
2007	O
)	O
trained	O
on	O
the	O
baseline	O
phrase	O
translation	O
model	O
.	O

For	O
all	O
settings	O
in	O
this	O
paper	O
,	O
weights	O
were	O
optimized	O
on	O
NIST	B-DatasetName
's	I-DatasetName
MT04	I-DatasetName
data	O
set	O
using	O
pairwise	O
ranked	O
optimization	O
(	O
Hopkins	O
and	O
May	O
,	O
2011	O
)	O
.	O

We	O
use	O
an	O
in	O
-	O
house	O
phrase	O
-	O
based	O
SMT	B-TaskName
system	O
similar	O
to	O
Moses	B-MethodName
.	O

The	O
English	B-MethodName
5	I-MethodName
-	I-MethodName
gram	I-MethodName
target	I-MethodName
language	I-MethodName
model	I-MethodName
is	O
trained	O
with	O
Kneser	O
-	O
Ney	O
smoothing	O
on	O
news	B-DatasetName
data	O
of	O
nearly	O
1.6B	O
tokens	O
.	O

Phrase	O
table	O
,	O
distortion	B-MethodName
models	I-MethodName
and	O
the	O
lexical	B-MethodName
BiLM	I-MethodName
are	O
trained	O
with	O
initial	O
alignments	O
obtained	O
using	O
GIZA++	B-DatasetName
(	O
Och	O
and	O
Ney	O
,	O
2003	O
)	O
.	O

The	O
initial	O
training	O
corpus	O
we	O
use	O
is	O
a	O
collection	O
of	O
parallel	O
sentences	O
taken	O
from	O
OpenMT	B-DatasetName
data	I-DatasetName
sources	I-DatasetName
released	O
by	O
the	O
LDC	O
.	O

We	O
establish	O
a	O
baseline	O
system	O
by	O
training	O
models	O
on	O
this	O
bitext	O
and	O
then	O
compare	O
this	O
to	O
a	O
forced	B-MethodName
decoding	I-MethodName
implementation	O
and	O
to	O
oracle	B-MethodName
-	I-MethodName
BLEU	I-MethodName
re	I-MethodName
-	I-MethodName
estimation	I-MethodName
using	O
the	O
same	O
bitext	O
.	O

Our	O
experiments	O
are	O
carried	O
out	O
for	O
an	O
Arabic	B-DatasetName
-	I-DatasetName
English	I-DatasetName
parallel	I-DatasetName
corpus	I-DatasetName
of	O
approximately	O
1	O
million	O
sentence	O
pairs	O
.	O

However	O
,	O
in	O
our	O
approach	O
,	O
we	O
do	O
not	O
impose	O
a	O
constraint	O
to	O
produce	O
the	O
exact	O
translation	O
,	O
instead	O
we	O
use	O
the	O
highest	O
BLEU	B-MetricName
translations	O
which	O
may	O
be	O
very	O
different	O
from	O
the	O
references	O
.	O

Finally	O
,	O
we	O
extract	O
n	O
-	O
best	O
candidate	O
translations	O
from	O
the	O
graphs	O
ranked	O
on	O
BLEU	B-MetricName
score	O
as	O
defined	O
in	O
Equation	O
(	O
3	O
)	O
.	O

This	O
finding	O
for	O
the	O
optimal	O
value	O
of	O
µ	B-HyperparameterName
has	O
also	O
been	O
established	O
in	O
(	O
Chiang	O
et	O
al	O
.	O
,	O
2008	O
)	O
through	O
a	O
series	O
of	O
experiments	O
.	O

We	O
set	O
µ=0.5	B-HyperparameterName
to	O
balance	O
between	O
BLEU	B-MetricName
scores	O
almost	O
as	O
high	O
as	O
the	O
max	O
-	O
BLEU	O
translations	O
,	O
while	O
staying	O
close	O
to	O
translations	O
preferred	O
by	O
the	O
model	O
.	O

We	O
also	O
conducted	O
a	O
set	O
of	O
experiments	O
with	O
µ=0	B-HyperparameterName
(	O
pure	O
or	O
absolute	O
BLEU	B-MetricName
)	O
in	O
order	O
to	O
verify	O
the	O
necessity	O
for	O
the	O
optimal	O
combination	O
.	O

Hence	O
following	O
them	O
,	O
we	O
use	O
a	O
weighted	O
combination	O
of	O
BLEU	B-MetricName
and	O
model	O
score	O
to	O
select	O
the	O
n	O
-	O
best	O
list	O
:	O
e	O
*	O
=	O
argmax	O
e	O
(	O
B(e	O
)	O
−	O
µ	O
•	O
(	O
B(e	O
)	O
−	O
h(e).w	O
)	O
)	O
(	O
3	O
)	O
where	O
B(e	O
)	O
and	O
h(e	O
)	O
are	O
the	O
BLEU	B-MetricName
and	O
model	O
scores	O
of	O
the	O
candidate	O
translation	O
and	O
w	O
is	O
the	O
optimised	O
weights	O
for	O
the	O
models	O
,	O
µ	O
controls	O
the	O
preference	O
between	O
BLEU	B-MetricName
and	O
model	O
scores	O
to	O
determine	O
oracle	O
translations	O
.	O

(	O
2008	O
)	O
is	O
that	O
due	O
to	O
noise	O
in	O
the	O
training	O
data	O
,	O
a	O
high	O
-	O
BLEU	B-MetricName
translation	O
may	O
contain	O
certain	O
rules	O
which	O
are	O
unlikely	O
to	O
be	O
used	O
by	O
the	O
model	O
.	O

Then	O
the	O
BLEU	B-MetricName
score	O
for	O
a	O
sentence	O
pairs	O
(	O
f	O
,	O
r	O
)	O
and	O
translation	O
e	O
*	O
is	O
defined	O
as	O
:	O
B(e	O
;	O
f	O
,	O
r	O
)	O
=	O
(	O
O	O
f	O
+	O
|f	O
|	O
)	O
•	O
BLEU	O
(	O
O	O
+	O
c(e	O
*	O
;	O
r	O
)	O
)	O
(	O
2	O
)	O
The	O
second	O
problem	O
as	O
discussed	O
in	O
Chiang	O
et	O
al	O
.	O

Hence	O
,	O
following	O
their	O
work	O
and	O
(	O
Watanabe	O
et	O
al	O
.	O
,	O
2007	O
)	O
,	O
we	O
calculate	O
BLEU	B-MetricName
for	O
a	O
sentence	O
in	O
the	O
context	O
of	O
a	O
exponentially	O
-	O
weighted	O
moving	O
average	O
of	O
previous	O
translations	O
.	O

However	O
there	O
are	O
two	O
problems	O
in	O
calculating	O
BLEU	B-MetricName
for	O
individual	O
sentence	O
:	O
First	O
,	O
as	O
discussed	O
in	O
(	O
Chiang	O
et	O
al	O
.	O
,	O
2008	O
)	O
,	O
BLEU	B-MetricName
is	O
not	O
designed	O
to	O
be	O
used	O
for	O
sentences	O
in	O
isolation	O
where	O
it	O
can	O
exhibit	O
rather	O
volatile	O
behavior	O
.	O

Figure	O
1	O
shows	O
example	O
of	O
word	O
alignment	O
obtained	O
from	O
EM	B-MethodName
training	I-MethodName
,	O
segmentations	O
and	O
alignment	O
obtained	O
from	O
forced	B-MethodName
decoding	I-MethodName
and	O
oracle	B-MethodName
-	I-MethodName
BLEU	I-MethodName
re	I-MethodName
-	I-MethodName
estimation	I-MethodName
.	O

In	O
this	O
work	O
,	O
we	O
propose	O
that	O
aligning	O
source	O
sentences	O
to	O
their	O
oracle	O
BLEU	B-MetricName
translations	O
provides	O
a	O
more	O
realistic	O
estimate	O
of	O
the	O
models	O
from	O
the	O
decoding	O
perspective	O
instead	O
of	O
aligning	O
them	O
to	O
high	O
quality	O
human	O
translations	O
as	O
in	O
forced	O
decoding	O
.	O

(	O
2011	O
)	O
who	O
use	O
a	O
self	O
-	O
enhancing	O
strategy	O
to	O
utilize	O
additional	O
mono-	O
lingual	O
source	O
language	O
data	O
by	O
aligning	O
it	O
to	O
its	O
target	O
language	O
translation	O
obtained	O
by	O
using	O
an	O
SMT	B-TaskName
system	O
to	O
rank	O
sentence	O
translation	O
probabilities	O
.	O

However	O
,	O
one	O
limitation	O
of	O
forced	B-MethodName
alignment	I-MethodName
is	O
that	O
only	O
the	O
phrase	O
translation	O
model	O
can	O
be	O
re	O
-	O
estimated	O
since	O
it	O
is	O
restricted	O
to	O
align	O
the	O
source	O
sentence	O
to	O
the	O
given	O
target	O
reference	O
,	O
thus	O
fixing	O
the	O
choice	O
of	O
reordering	O
decisions	O
.	O

The	O
forced	B-MethodName
alignment	I-MethodName
technique	O
of	O
Wuebker	O
et	O
al	O
.	O

An	O
important	O
contribution	O
of	O
our	O
approach	O
is	O
that	O
unlike	O
previous	O
approaches	O
such	O
as	O
forced	B-MethodName
alignment	I-MethodName
(	O
Wuebker	O
et	O
al	O
.	O
,	O
2010	O
)	O
,	O
reordering	O
and	O
language	O
models	O
can	O
also	O
be	O
re	O
-	O
estimated	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
solution	O
which	O
is	O
to	O
re	O
-	O
estimate	O
the	O
models	O
from	O
the	O
best	O
BLEU	B-MetricName
translation	O
of	O
each	O
source	O
sentence	O
in	O
the	O
bitext	O
.	O

However	O
,	O
from	O
an	O
SMT	B-TaskName
perspective	O
it	O
is	O
important	O
that	O
the	O
models	O
reflect	O
probability	O
distributions	O
which	O
are	O
preferred	O
by	O
the	O
decoding	O
process	O
,	O
i.e.	O
,	O
phrase	O
translations	O
which	O
are	O
likely	O
to	O
be	O
used	O
frequently	O
to	O
achieve	O
better	O
translations	O
should	O
get	O
higher	O
scores	O
and	O
phrases	O
which	O
are	O
less	O
likely	O
to	O
be	O
used	O
should	O
get	O
low	O
scores	O
.	O

In	O
phrase	B-TaskName
-	I-TaskName
based	I-TaskName
SMT	I-TaskName
,	O
the	O
phrase	O
pairs	O
in	O
the	O
translation	O
model	O
are	O
traditionally	O
trained	O
by	O
applying	O
a	O
heuristic	B-MethodName
extraction	I-MethodName
method	O
(	O
Och	O
and	O
Ney	O
,	O
2000	O
)	O
which	O
extracts	O
phrase	O
pairs	O
based	O
on	O
consistency	O
of	O
word	O
alignments	O
from	O
a	O
word	O
-	O
aligned	O
bilingual	O
training	O
data	O
.	O

Experiments	O
show	O
an	O
improvement	O
of	O
up	O
to	O
0.8	B-MetricValue
BLEU	B-MetricName
for	O
our	O
approach	O
over	O
a	O
competitive	O
Arabic	O
-	O
English	O
baseline	O
trained	O
directly	O
on	O
the	O
word	O
-	O
aligned	O
bitext	O
using	O
heuristic	O
extraction	O
.	O

We	O
present	O
a	O
novel	O
technique	O
for	O
training	O
translation	O
models	O
for	O
statistical	B-TaskName
machine	I-TaskName
translation	I-TaskName
by	O
aligning	O
source	O
sentences	O
to	O
their	O
oracle	B-MethodName
-	I-MethodName
BLEU	I-MethodName
translations	O
.	O

Improving	O
Statistical	B-TaskName
Machine	I-TaskName
Translation	I-TaskName
Performance	O
by	O
Oracle	B-MethodName
-	I-MethodName
BLEU	I-MethodName
Model	I-MethodName
Re	I-MethodName
-	I-MethodName
estimation	I-MethodName
.	O

We	O
thank	O
Arianna	O
Bisazza	O
and	O
the	O
anonymous	O
reviewers	O
for	O
their	O
comments	O
.	O

This	O
research	O
was	O
funded	O
in	O
part	O
by	O
the	O
Netherlands	O
Organization	O
for	O
Scientific	O
Research	O
(	O
NWO	O
)	O
under	O
project	O
numbers	O
639.022.213	O
and	O
612.001.218	O
.	O

Acknowledgments	O
.	O

Conclusions	O
.	O

When	O
interpolated	O
with	O
the	O
baseline	O
phrase	O
table	O
,	O
both	O
approaches	O
show	O
significant	O
improvements	O
over	O
the	O
baseline	O
.	O

In	O
contrast	O
,	O
phrase	O
tables	O
re	O
-	O
estimated	O
from	O
oracle	O
-	O
BLEU	O
translation	O
achieves	O
the	O
same	O
performance	O
as	O
the	O
baseline	O
.	O

Results	O
.	O

We	O
also	O
experiment	O
with	O
the	O
interpolation	O
of	O
re	O
-	O
estimated	O
models	O
with	O
the	O
respective	O
baseline	O
models	O
.	O

From	O
these	O
oracle	O
-	O
BLEU	O
translations	O
and	O
alignments	O
all	O
phrases	O
that	O
were	O
used	O
in	O
the	O
derivation	O
of	O
these	O
nbest	O
sentences	O
are	O
extracted	O
and	O
the	O
models	O
are	O
reestimated	O
by	O
re	O
-	O
calculating	O
the	O
translation	O
probabilities	O
.	O

Baseline	O
and	O
forced	O
decoding	O
.	O

Experimental	O
set	O
up	O
.	O

Instead	O
,	O
we	O
handle	O
the	O
problem	O
by	O
simply	O
removing	O
all	O
the	O
phrase	O
pairs	O
below	O
a	O
threshold	O
count	O
which	O
in	O
our	O
case	O
is	O
2	O
,	O
φ	O
init	O
=	O
φ	O
baseline	O
−	O
φ	O
C(e	O
,	O
f	O
)	O
<	O
2(4	O
)	O
therefore	O
removing	O
phrase	O
pairs	O
with	O
high	O
probability	O
but	O
low	O
frequency	O
.	O

Thus	O
it	O
is	O
not	O
strictly	O
necessary	O
to	O
apply	O
leave	O
-	O
one	O
-	O
out	O
in	O
our	O
approach	O
as	O
a	O
solution	O
to	O
over	O
-	O
fitting	O
.	O

(	O
2010	O
)	O
address	O
this	O
problem	O
by	O
using	O
a	O
leave	O
-	O
one	O
-	O
out	O
approach	O
where	O
they	O
modify	O
the	O
phrase	O
translation	O
probabilities	O
for	O
each	O
sentence	O
pair	O
by	O
removing	O
the	O
counts	O
of	O
all	O
phrases	O
that	O
were	O
extracted	O
from	O
that	O
particular	O
sentence	O
.	O

Within	O
forced	O
decoding	O
,	O
Wuebker	O
et	O
al	O
.	O

Avoiding	O
over	O
-	O
fitting	O
.	O

Using	O
the	O
word	O
alignments	O
from	O
the	O
initial	O
phrase	O
table	O
,	O
we	O
extract	O
the	O
alignments	O
between	O
each	O
source	O
sentence	O
and	O
each	O
of	O
their	O
n	O
-	O
best	O
oracle	O
-	O
BLEU	O
translations	O
.	O

Using	O
the	O
target	O
sentences	O
,	O
we	O
convert	O
the	O
translation	O
lattice	O
to	O
an	O
isomorphic	O
oracle	O
-	O
BLEU	O
lattice	O
which	O
has	O
the	O
same	O
set	O
of	O
nodes	O
but	O
the	O
edges	O
represent	O
BLEU	O
score	O
differences	O
corresponding	O
to	O
each	O
transition	O
.	O

Along	O
with	O
the	O
1best	O
translation	O
(	O
based	O
on	O
model	O
scores	O
)	O
,	O
we	O
also	O
store	O
search	O
graphs	O
or	O
lattices	O
generated	O
during	O
the	O
translations	O
process	O
.	O

We	O
use	O
the	O
decoder	O
trained	O
on	O
these	O
models	O
to	O
translate	O
the	O
training	O
bitext	O
.	O

Training	O
.	O

We	O
briefly	O
discuss	O
the	O
computation	O
from	O
(	O
Chiang	O
et	O
al	O
.	O
,	O
2008	O
)	O
as	O
follows	O
:	O
Given	O
a	O
source	O
sentence	O
f	O
,	O
and	O
its	O
reference	O
translation	O
r	O
,	O
for	O
an	O
n	O
-	O
best	O
translation	O
e	O
*	O
,	O
let	O
c(e	O
)	O
be	O
defined	O
as	O
the	O
vector	O
of	O
target	O
length	O
|e|	O
,	O
source	O
length	O
|f|	O
,	O
reference	O
length	O
|r|	O
,	O
and	O
the	O
number	O
of	O
n	O
-	O
gram	O
matches	O
between	O
e	O
and	O
r	O
,	O
then	O
two	O
pseudo	O
document	O
parameters	O
O	O
and	O
O	O
f	O
are	O
defined	O
as	O
:	O
O	O
←	O
0.9	O
•	O
(	O
O	O
+	O
c(e	O
*	O
)	O
)	O
,	O
O	O
f	O
←	O
0.9	O
•	O
(	O
O	O
f	O
+	O
|f	O
|	O
)	O
(	O
1	O
)	O
O	O
is	O
an	O
exponentially	O
-	O
weighted	O
moving	O
average	O
of	O
the	O
vectors	O
from	O
previous	O
sentences	O
and	O
O	O
f	O
is	O
the	O
correction	O
of	O
source	O
length	O
with	O
respect	O
to	O
the	O
previous	O
sentences	O
.	O

Ideally	O
,	O
one	O
would	O
like	O
to	O
re	O
-	O
estimate	O
translation	O
models	O
directly	O
from	O
the	O
n	O
-	O
best	O
BLEU	O
translations	O
.	O

However	O
,	O
our	O
approach	O
specifically	O
proposes	O
a	O
novel	O
method	O
for	O
training	O
models	O
using	O
oracle	O
BLEU	O
translations	O
.	O

Another	O
relevant	O
line	O
of	O
research	O
relates	O
tuning	O
(	O
weight	O
optimisation	O
)	O
,	O
where	O
our	O
work	O
lies	O
between	O
forced	O
decoding	O
(	O
Wuebker	O
et	O
al	O
.	O
,	O
2010	O
)	O
and	O
the	O
bold	O
updating	O
approach	O
of	O
(	O
Liang	O
et	O
al	O
.	O
,	O
2006	O
)	O
.	O

(	O
2011	O
)	O
and	O
Schwenk	O
et	O
al	O
.	O

A	O
similar	O
line	O
of	O
work	O
is	O
proposed	O
by	O
Lambert	O
et	O
al	O
.	O

At	O
the	O
end	O
,	O
the	O
phrase	O
table	O
is	O
re	O
-	O
estimated	O
using	O
the	O
phrase	O
pair	O
segmentations	O
obtained	O
from	O
forced	O
decoding	O
.	O

The	O
best	O
segmentation	O
is	O
defined	O
as	O
the	O
one	O
which	O
maximizes	O
the	O
probability	O
of	O
translating	O
the	O
source	O
sentence	O
into	O
the	O
given	O
target	O
sentence	O
.	O

(	O
2010	O
)	O
forms	O
the	O
main	O
motivation	O
for	O
our	O
work	O
.	O

Related	O
Work	O
.	O

This	O
means	O
that	O
translation	O
probabilities	O
are	O
distributed	O
over	O
a	O
very	O
large	O
number	O
of	O
phrase	O
translation	O
candidates	O
most	O
of	O
which	O
never	O
lead	O
to	O
the	O
best	O
possible	O
translation	O
of	O
a	O
sentence	O
.	O

In	O
addition	O
,	O
the	O
heuristic	O
extraction	O
algorithm	O
generates	O
all	O
possible	O
,	O
consistent	O
phrases	O
including	O
overlapping	O
phrases	O
.	O

A	O
notable	O
shortcoming	O
of	O
this	O
approach	O
is	O
that	O
the	O
translation	O
model	O
probabilities	O
thus	O
calculated	O
from	O
the	O
training	O
bitext	O
can	O
be	O
unintuitive	O
and	O
unreliable	O
(	O
Marcu	O
and	O
Wong	O
,	O
2002;Foster	O
et	O
al	O
.	O
,	O
2006	O
)	O
as	O
they	O
reflect	O
only	O
the	O
distribution	O
over	O
the	O
phrase	O
pairs	O
observed	O
in	O
the	O
training	O
data	O
.	O

The	O
probabilities	O
of	O
the	O
translation	O
model	O
are	O
then	O
calculated	O
based	O
on	O
the	O
relative	O
frequencies	O
of	O
the	O
extracted	O
phrase	O
pairs	O
.	O

Introduction	O
.	O

In	O
contrast	O
to	O
previous	O
approaches	O
which	O
are	O
constrained	O
to	O
phrase	O
training	O
,	O
our	O
method	O
also	O
allows	O
the	O
re	O
-	O
estimation	O
of	O
reordering	O
models	O
along	O
with	O
the	O
translation	O
model	O
.	O

Acknowledgements	O
.	O

As	O
for	O
ET	B-TaskName
,	O
we	O
adopted	O
the	O
evaluation	O
metrics	O
of	O
loose	O
F1	B-MetricName
score	O
following	O
the	O
same	O
evaluation	O
criteria	O
used	O
in	O
previous	O
works	O
(	O
Ling	O
and	O
Weld	O
,	O
2012;Wang	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

The	O
evaluation	O
metrics	O
for	O
the	O
RE	B-TaskName
task	O
were	O
F1	B-MetricName
score	O
in	O
the	O
Scikit	O
-	O
learn	O
library	O
(	O
Pedregosa	O
et	O
al	O
.	O
,	O
2011	O
)	O
.	O

Conversely	O
,	O
Character	B-MetricName
F1	I-MetricName
is	O
a	O
metric	O
that	O
evaluates	O
each	O
type	O
of	O
syllable	O
in	O
a	O
sentence	O
individually	O
.	O

Entity	B-MetricName
F1	I-MetricName
is	O
a	O
metric	O
that	O
is	O
recognized	O
as	O
a	O
correct	O
answer	O
only	O
when	O
all	O
types	O
included	O
in	O
an	O
entity	O
are	O
matched	O
accurately	O
.	O

Specifically	O
,	O
the	O
evaluation	O
metrics	O
for	O
NER	B-TaskName
task	O
were	O
Entity	B-MetricName
F1	I-MetricName
and	O
Character	B-MetricName
F1	I-MetricName
based	O
on	O
previous	O
research	O
(	O
Park	O
et	O
al	O
.	O
,	O
2021b	O
)	O
.	O

We	O
evaluated	O
our	O
system	O
by	O
employing	O
F1	B-MetricName
score	O
,	O
which	O
is	O
standard	O
metric	O
for	O
classification	O
tasks	O
.	O

In	O
the	O
case	O
of	O
max	B-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
,	O
the	O
lengths	O
of	O
256	B-HyperparameterValue
and	O
128	B-HyperparameterValue
were	O
used	O
for	O
each	O
task	O
.	O

The	O
batch	B-HyperparameterName
size	I-HyperparameterName
in	O
training	O
and	O
testing	O
procedures	O
was	O
set	O
to	O
128	B-HyperparameterValue
in	O
NER	B-TaskName
,	O
RE	B-TaskName
and	O
256	B-HyperparameterValue
in	O
ET	B-TaskName
.	O

The	O
number	B-HyperparameterName
of	I-HyperparameterName
training	I-HyperparameterName
epochs	I-HyperparameterName
was	O
set	O
to	O
10	B-HyperparameterValue
in	O
NER	B-TaskName
,	O
RE	B-TaskName
and	O
3	B-HyperparameterValue
in	O
ET	B-TaskName
.	O

The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
and	O
weight	B-HyperparameterName
decay	I-HyperparameterName
were	O
consistently	O
set	O
at	O
5e-5	B-HyperparameterValue
and	O
0.01	B-HyperparameterValue
across	O
all	O
three	O
tasks	O
.	O

The	O
hyperparameters	O
in	O
the	O
fine	O
-	O
tuning	O
step	O
were	O
set	O
as	O
follows	O
.	O

Further	O
,	O
we	O
set	O
our	O
environment	O
for	O
the	O
experiment	O
with	O
four	O
A6000	O
GPUs	O
and	O
384	O
GB	O
memory	O
.	O

In	O
all	O
the	O
model	O
experiments	O
,	O
the	O
performance	O
of	O
each	O
model	O
was	O
measured	O
five	O
times	O
,	O
and	O
the	O
average	O
of	O
each	O
result	O
was	O
evaluated	O
as	O
the	O
final	O
result	O
.	O

As	O
the	O
baseline	O
models	O
,	O
we	O
employed	O
two	O
global	O
language	O
models	O
:	O
multilingual	B-MethodName
bidirectional	I-MethodName
encoder	I-MethodName
representations	I-MethodName
from	I-MethodName
transformers	I-MethodName
(	O
BERT	B-MethodName
)	O
(	O
Devlin	O
et	O
al	O
.	O
,	O
2019	O
)	O
and	O
a	O
cross	O
-	O
lingual	O
language	O
model	O
XLM	B-MethodName
-	I-MethodName
RoBERTa	I-MethodName
-	I-MethodName
base	I-MethodName
(	O
Conneau	O
et	O
al	O
.	O
,	O
2020	O
)	O
containing	O
the	O
Korean	O
language	O
,	O
and	O
two	O
KLUE	O
language	O
models	O
:	O
KLUE	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
base	I-MethodName
,	O
KLUE	B-MethodName
-	I-MethodName
RoBERTa	I-MethodName
-	I-MethodName
base	I-MethodName
,	O
which	O
were	O
recently	O
published	O
covering	O
various	O
Korean	O
downstream	O
tasks	O
.	O

A	O
Experimental	O
Setup	O
.	O

We	O
hope	O
that	O
the	O
continuous	O
effort	O
to	O
preserve	O
cultural	O
heritage	O
with	O
the	O
effective	O
management	O
of	O
digitized	O
documents	O
containing	O
cultural	O
artifacts	O
is	O
encouraged	O
by	O
this	O
research	O
.	O

Above	O
all	O
,	O
the	O
most	O
significant	O
contributing	O
point	O
is	O
that	O
the	O
disclosure	O
of	O
our	O
corpus	O
is	O
expected	O
to	O
serve	O
as	O
a	O
cornerstone	O
for	O
the	O
development	O
of	O
IE	B-TaskName
tasks	O
for	O
a	O
traditional	O
cultural	O
heritage	O
.	O

Furthermore	O
,	O
we	O
proved	O
the	O
applicability	O
of	O
our	O
entity	O
-	O
abundant	O
corpus	O
with	O
the	O
experiments	O
employing	O
the	O
various	O
pre	O
-	O
trained	O
language	O
models	O
and	O
provided	O
practical	O
insights	O
regarding	O
the	O
statistical	O
,	O
diachronic	O
,	O
and	O
linguistic	O
analysis	O
.	O

Unlike	O
the	O
existing	O
public	O
Korean	O
datasets	O
with	O
additional	O
restrictions	O
,	O
KOCHET	B-DatasetName
obviated	O
the	O
cumbersome	O
prerequisite	O
and	O
can	O
be	O
freely	O
modified	O
and	O
redistributed	O
.	O

In	O
this	O
paper	O
,	O
we	O
introduced	O
KOCHET	B-DatasetName
-a	O
Korean	O
cultural	O
heritage	O
corpus	O
for	O
three	O
typical	O
entityrelated	O
tasks	O
,	O
i.e.	O
,	O
NER	B-TaskName
,	O
RE	B-TaskName
,	O
and	O
ET	B-TaskName
.	O

Conclusion	O
.	O

On	O
the	O
other	O
hand	O
,	O
as	O
the	O
RE	B-TaskName
or	O
ET	B-TaskName
task	O
does	O
not	O
classify	O
all	O
tokens	O
in	O
a	O
sentence	O
,	O
the	O
correct	O
answer	O
can	O
be	O
satisfactorily	O
inferred	O
from	O
only	O
the	O
given	O
Korean	O
words	O
;	O
thereby	O
,	O
the	O
language	O
models	O
pre	O
-	O
trained	O
in	O
Korean	O
show	O
better	O
performance	O
in	O
the	O
two	O
tasks	O
compared	O
to	O
the	O
multilingual	O
model	O
.	O

It	O
is	O
attributed	O
that	O
the	O
NER	B-TaskName
task	O
requires	O
more	O
polyglot	O
features	O
of	O
the	O
model	O
compared	O
to	O
the	O
other	O
tasks	O
,	O
i.e.	O
,	O
RE	B-TaskName
and	O
ET	B-TaskName
,	O
which	O
has	O
the	O
properties	O
of	O
sentence	O
classification	O
tasks	O
.	O

In	O
addition	O
,	O
in	O
Table	O
7	O
,	O
the	O
Korean	O
models	O
,	O
i.e.	O
,	O
KLUE	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
base	I-MethodName
and	O
KLUE	B-MethodName
-	I-MethodName
RoBERTa	I-MethodName
-	I-MethodName
base	I-MethodName
show	O
a	O
significantly	O
higher	O
ratio	O
of	O
unknown	O
tokens	O
than	O
the	O
multilingual	O
language	O
models	O
.	O

Experimental	O
Analysis	O
As	O
the	O
token	O
classification	O
tasks	O
are	O
directly	O
affected	O
by	O
segmentation	O
(	O
Kim	O
et	O
al	O
.	O
,	O
2021;Park	O
et	O
al	O
.	O
,	O
2021a	O
)	O
,	O
models	O
with	O
linguistic	O
knowledge	O
of	O
Chinese	O
and	O
Japanese	O
overperform	O
in	O
such	O
tasks	O
(	O
Pires	O
et	O
al	O
.	O
,	O
2019	O
)	O
other	O
words	O
,	O
the	O
multilingual	O
models	O
are	O
considered	O
to	O
segment	O
better	O
each	O
token	O
composed	O
of	O
various	O
languages	O
,	O
especially	O
in	O
the	O
NER	B-DatasetName
corpus	I-DatasetName
.	O

The	O
other	O
is	O
that	O
in	O
the	O
RE	B-TaskName
and	O
ET	B-TaskName
tasks	O
,	O
the	O
performances	O
of	O
the	O
Korean	O
models	O
were	O
at	O
least	O
1.1	B-MetricValue
%	I-MetricValue
higher	O
than	O
those	O
of	O
the	O
multilingual	O
models	O
.	O

One	O
is	O
that	O
in	O
the	O
NER	B-TaskName
task	O
,	O
the	O
multilingual	O
models	O
,	O
i.e.	O
,	O
multilingual	B-MethodName
BERT	I-MethodName
and	O
xlm	B-MethodName
-	I-MethodName
RoBERTa	I-MethodName
-	I-MethodName
base	I-MethodName
,	O
showed	O
better	O
performance	O
by	O
more	O
than	O
30	B-MetricValue
%	I-MetricValue
difference	O
in	O
both	O
Entity	B-MetricName
F1	I-MetricName
and	O
Character	B-MetricName
F1	I-MetricName
scores	O
compared	O
to	O
the	O
Korean	O
models	O
,	O
i.e.	O
,	O
KLUE	B-DatasetName
-	I-DatasetName
BERT	I-DatasetName
-	I-DatasetName
base	I-DatasetName
and	O
KLUE	B-DatasetName
-	I-DatasetName
RoBERTa	I-DatasetName
-	I-DatasetName
base	I-DatasetName
.	O

Experimental	O
results	O
According	O
to	O
Table	O
6	O
,	O
two	O
tendencies	O
are	O
observed	O
.	O

The	O
detailed	O
experimental	O
settings	O
are	O
in	O
Appendix	O
A.	O

Experiment	O
.	O

On	O
the	O
other	O
hand	O
,	O
the	O
entity	O
types	O
to	O
which	O
belong	O
to	O
the	O
AM	O
,	O
PT	O
,	O
MT	O
,	O
and	O
EV	O
almost	O
account	O
for	O
under	O
1.0	O
%	O
.	O

Finally	O
,	O
among	O
the	O
fine	O
-	O
grained	O
entity	O
types	O
,	O
the	O
"	O
AF_DOCUMENTS	O
"	O
type	O
,	O
such	O
as	O
historical	O
documents	O
,	O
occupies	O
the	O
largest	O
part	O
with	O
17.9	O
%	O
,	O
and	O
"	O
PS_NAME	O
"	O
including	O
the	O
names	O
of	O
historical	O
figures	O
,	O
takes	O
second	O
place	O
by	O
occupying	O
11.5	O
%	O
.	O

"	O
A	O
hasDestroyed	O
B	O
"	O
has	O
the	O
smallest	O
proportion	O
with	O
ten	O
relations	O
in	O
total	O
because	O
,	O
in	O
actual	O
history	O
,	O
significant	O
events	O
such	O
as	O
the	O
collapse	O
of	O
a	O
nation	O
or	O
the	O
loss	O
of	O
cultural	O
properties	O
are	O
not	O
as	O
diverse	O
as	O
the	O
types	O
of	O
general	O
cultural	O
assets	O
.	O

"	O
A	O
depicts	O
B	O
"	O
and	O
"	O
A	O
documents	O
B	O
"	O
include	O
cultural	O
assets	O
left	O
in	O
a	O
specific	O
form	O
such	O
as	O
records	O
,	O
drawings	O
,	O
and	O
photographs	O
,	O
whereas	O
"	O
A	O
hasSection	O
B	O
"	O
contains	O
cultural	O
heritage	O
or	O
historical	O
landmarks	O
located	O
at	O
a	O
particular	O
place	O
.	O

Among	O
them	O
,	O
"	O
A	O
depicts	O
B	O
,	O
"	O
"	O
A	O
documents	O
B	O
,	O
"	O
and	O
"	O
A	O
hasSection	O
B	O
"	O
are	O
the	O
most	O
relationship	O
labels	O
with	O
approximately	O
22	O
%	O
,	O
16	O
%	O
,	O
and	O
10	O
%	O
of	O
the	O
total	O
,	O
respectively	O
.	O

In	O
the	O
case	O
of	O
"	O
A	O
depicts	O
B	O
"	O
and	O
"	O
A	O
documents	O
B	O
,	O
"	O
cultural	O
assets	O
left	O
in	O
a	O
specific	O
form	O
such	O
as	O
records	O
,	O
drawings	O
,	O
and	O
photographs	O
are	O
included	O
,	O
whereas	O
"	O
A	O
hasSection	O
B	O
"	O
contains	O
cultural	O
heritage	O
or	O
historical	O
landmarks	O
located	O
at	O
a	O
specific	O
place	O
.	O

Second	O
,	O
Table	O
2	O
demonstrates	O
the	O
distribution	O
of	O
14	O
RE	B-TaskName
labels	O
.	O

On	O
the	O
other	O
hand	O
,	O
the	O
EV	O
type	O
occupies	O
the	O
most	O
minor	O
proportion	O
,	O
approximately	O
0.8	O
%	O
,	O
because	O
our	O
corpus	O
especially	O
aims	O
to	O
concentrate	O
on	O
the	O
cultural	O
heritage	O
.	O

The	O
AF	O
type	O
includes	O
cultural	O
assets	O
and	O
historical	O
landmarks	O
,	O
the	O
TM	O
type	O
includes	O
patterns	O
or	O
traces	O
engraved	O
on	O
certain	O
cultural	O
assets	O
,	O
and	O
the	O
PS	O
type	O
particularly	O
includes	O
not	O
only	O
general	O
people	O
but	O
also	O
particular	O
types	O
of	O
persons	O
such	O
as	O
mythical	O
figures	O
.	O

AF	O
,	O
PS	O
,	O
and	O
TM	O
entities	O
possess	O
approximately	O
36	O
%	O
,	O
20	O
%	O
,	O
and	O
10	O
%	O
,	O
respectively	O
,	O
which	O
are	O
used	O
as	O
crucial	O
information	O
in	O
the	O
cultural	O
heritage	O
domain	O
.	O

Due	O
to	O
the	O
properties	O
of	O
the	O
cultural	O
heritage	O
domain	O
,	O
the	O
three	O
primary	O
entity	O
types	O
,	O
i.e.	O
,	O
artifacts	O
(	O
AF	O
)	O
,	O
person	O
(	O
PS	O
)	O
,	O
and	O
term	O
(	O
TM	O
)	O
,	O
account	O
for	O
the	O
majority	O
of	O
the	O
total	O
entity	O
population	O
.	O

First	O
,	O
as	O
shown	O
in	O
Table	O
1	O
,	O
we	O
used	O
12	O
entity	O
types	O
for	O
our	O
cultural	O
heritage	O
NER	B-DatasetName
corpus	I-DatasetName
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
our	O
corpus	O
statistically	O
in	O
the	O
order	O
of	O
NER	B-TaskName
,	O
RE	B-TaskName
,	O
and	O
ET	B-TaskName
.	O

The	O
annotated	O
corpus	O
was	O
divided	O
into	O
three	O
subsets	O
for	O
each	O
task	O
,	O
i.e.	O
,	O
a	O
ratio	B-HyperparameterName
of	O
8:1:1	B-HyperparameterValue
for	O
training	B-HyperparameterName
,	O
development	B-HyperparameterName
,	O
and	O
testing	B-HyperparameterName
,	O
respectively	O
.	O

For	O
the	O
ET	B-DatasetName
corpus	I-DatasetName
,	O
332,830	O
entity	O
mentions	O
from	O
113,198	O
examples	O
were	O
annotated	O
in	O
total	O
.	O

For	O
the	O
RE	B-DatasetName
corpus	I-DatasetName
,	O
79,942	O
relations	O
from	O
38,765	O
examples	O
were	O
annotated	O
in	O
total	O
.	O

For	O
the	O
NER	B-DatasetName
corpus	I-DatasetName
,	O
457,232	O
entities	O
from	O
112,362	O
examples	O
in	O
total	O
.	O

The	O
overall	O
statistics	O
of	O
KOCHET	B-DatasetName
are	O
showed	O
in	O
Table	O
5	O
.	O

Statistics	O
.	O

Therefore	O
,	O
to	O
fully	O
understand	O
such	O
expression	O
types	O
in	O
our	O
corpus	O
,	O
multilingual	O
factors	O
of	O
language	O
models	O
should	O
be	O
considered	O
;	O
particularly	O
in	O
the	O
case	O
of	O
token	O
classification	O
tasks	O
,	O
in	O
which	O
the	O
meaning	O
of	O
each	O
token	O
directly	O
affects	O
the	O
model	O
performance	O
.	O

For	O
example	O
,	O
as	O
shown	O
in	O
sentence	O
2	O
in	O
Table	O
4	O
,	O
the	O
description	O
with	O
Chinese	O
characters	O
in	O
parentheses	O
follows	O
the	O
entity	O
"	O
안창호씨	O
,	O
"	O
and	O
is	O
usually	O
written	O
such	O
as	O
"	O
안창호씨(安昌浩氏	O
)	O
.	O
"	O
Further	O
,	O
Japanese	O
characters	O
are	O
also	O
present	O
throughout	O
the	O
corpus	O
,	O
enhancing	O
the	O
polyglot	O
property	O
of	O
the	O
corpus	O
,	O
as	O
shown	O
in	O
sentence	O
3	O
.	O

Second	O
,	O
several	O
entities	O
contained	O
in	O
KOCHET	B-DatasetName
written	O
in	O
Korean	O
are	O
followed	O
by	O
the	O
descriptions	O
written	O
in	O
either	O
Chinese	O
or	O
Japanese	O
characters	O
.	O

There	O
is	O
a	O
phrase	O
'	O
한번사신레꼬	O
-	O
드는승질상밧고거	O
-	O
나믈느지는안슴니다'(archaic	O
Korean	O
)	O
on	O
the	O
left	O
corner	O
of	O
the	O
front	O
side	O
.	O

When	O
translating	O
the	O
sentence	O
with	O
quotation	O
marks	O
into	O
modern	O
Korean	O
,	O
it	O
can	O
be	O
expressed	O
as	O
"	O
한번	O
사신	O
레코드는	O
성질상	O
바꾸거나	O
무르지는	O
않습니다	O
(	O
Once	O
a	O
record	O
is	O
purchased	O
,	O
it	O
can	O
not	O
be	O
exchanged	O
or	O
refunded	O
due	O
to	O
its	O
characteristics	O
)	O
.	O
"	O
Index	O
Example	O
sentences	O
1	O
앞면	O
좌측	O
하단에	O
'	O
한번사신레꼬	O
-	O
드는승질상밧고거	O
-	O
나믈느지는안슴니다	O
'	O
문구가	O
있음	O
.	O

Although	O
it	O
is	O
written	O
using	O
syllables	O
of	O
modern	O
Korean	O
,	O
the	O
grammar	O
and	O
the	O
vocabulary	O
are	O
fairly	O
dissimilar	O
from	O
those	O
of	O
contemporary	O
Korean	O
,	O
such	O
as	O
word	O
spacing	O
and	O
syllabification	O
,	O
i.e.	O
,	O
separation	O
rule	O
between	O
the	O
units	O
of	O
the	O
word	O
.	O

Let	O
us	O
consider	O
the	O
phrase	O
"	O
한번사신레꼬	O
-	O
드는승질상밧고거	O
-	O
나믈느지는안슴니다	O
"	O
in	O
sentence	O
1	O
in	O
Table	O
4	O
.	O

Specifically	O
,	O
such	O
expressions	O
continually	O
appear	O
when	O
ancient	O
documents	O
or	O
historical	O
artifacts	O
are	O
quoted	O
.	O

First	O
,	O
archaic	O
expressions	O
that	O
are	O
not	O
used	O
in	O
modern	O
times	O
are	O
frequently	O
shown	O
in	O
our	O
corpus	O
.	O

There	O
are	O
mainly	O
two	O
differences	O
between	O
the	O
entities	O
in	O
the	O
proposed	O
corpus	O
and	O
those	O
commonly	O
used	O
.	O

Diachronic	O
and	O
Linguistic	O
Analysis	O
.	O

Analysis	O
on	O
KOCHET	B-DatasetName
.	O

•	O
12	O
general	O
types	O
:	O
PS	O
,	O
AF	O
,	O
AM	O
,	O
CV	O
,	O
DT	O
,	O
EV	O
,	O
PT	O
,	O
MT	O
,	O
TM	O
,	O
LC	O
,	O
LCG	O
,	O
LCP	O
•	O
94	O
fine	O
-	O
grained	O
types	O
,	O
which	O
were	O
mapped	O
to	O
the	O
cultural	O
heritage	O
-	O
specialized	O
finegrained	O
entity	O
labels	O
,	O
were	O
inspired	O
by	O
prior	O
works	O
(	O
Ling	O
and	O
Weld	O
,	O
2012;Gillick	O
et	O
al	O
.	O
,	O
2014;Choi	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

Label	O
Description	O
.	O

The	O
category	O
to	O
which	O
the	O
most	O
entities	O
belong	O
is	O
"	O
AF_DOCUMENTS	O
,	O
"	O
which	O
possesses	O
17.9	O
%	O
,	O
and	O
that	O
on	O
the	O
second	O
place	O
is	O
"	O
PS_NAME	O
,	O
"	O
having	O
16.7	O
%	O
.	O

Each	O
example	O
includes	O
2.94	O
fine	O
-	O
grained	O
entities	O
on	O
average	O
;	O
there	O
are	O
up	O
to	O
nine	O
several	O
fine	O
-	O
grained	O
9	O
www.cha.go.kr	O
entity	O
types	O
per	O
entity	O
.	O

For	O
Figure	O
1	O
,	O
the	O
circle	O
on	O
the	O
left	O
shows	O
the	O
visualization	O
of	O
fine	O
-	O
grained	O
entity	O
types	O
that	O
possess	O
approximately	O
84	O
%	O
among	O
all	O
labels	O
in	O
the	O
corpus	O
,	O
and	O
the	O
set	O
on	O
the	O
right	O
shows	O
the	O
detailed	O
distributions	O
of	O
all	O
fine	O
-	O
grained	O
types	O
.	O

Accordingly	O
,	O
they	O
can	O
let	O
models	O
understand	O
richly	O
the	O
noun	O
phrases	O
including	O
entity	O
,	O
compared	O
to	O
when	O
the	O
models	O
are	O
trained	O
to	O
predict	O
only	O
relatively	O
coarse	O
types	O
.	O

Our	O
fine	O
-	O
grained	O
entity	O
types	O
can	O
embrace	O
all	O
the	O
existing	O
general	O
types	O
and	O
categorize	O
them	O
in	O
greater	O
detail	O
.	O

Given	O
a	O
sentence	O
with	O
an	O
entity	O
mention	O
,	O
the	O
appropriate	O
type	O
that	O
describes	O
the	O
role	O
of	O
the	O
entity	O
span	O
in	O
the	O
sentence	O
should	O
be	O
predicted	O
.	O

Table	O
3	O
lists	O
three	O
example	O
sentences	O
with	O
entity	O
mention	O
that	O
can	O
represent	O
several	O
fine	O
-	O
grained	O
types	O
.	O

The	O
fine	O
-	O
grained	O
entities	O
for	O
entity	O
-	O
related	O
downstream	O
tasks	O
in	O
the	O
cultural	O
heritage	O
domain	O
enable	O
a	O
more	O
detailed	O
contextualized	O
representation	O
for	O
each	O
entity	O
mention	O
than	O
the	O
previous	O
typing	O
schemas	O
,	O
which	O
only	O
predict	O
relatively	O
coarse	O
types	O
of	O
entities	O
.	O

All	O
fine	O
-	O
grained	O
entity	O
types	O
are	O
detailed	O
in	O
Figure	O
1	O
.	O

Particularly	O
,	O
the	O
cultural	O
taxonomy	O
defined	O
in	O
the	O
Cultural	O
Properties	O
Protection	O
Law	O
9	O
was	O
applied	O
to	O
AF	O
,	O
and	O
the	O
2004	O
Cavalier	O
-	O
Smith	O
's	O
classification	O
system	O
(	O
Cavalier	O
-	O
Smith	O
,	O
2004	O
)	O
was	O
applied	O
to	O
the	O
biological	O
scope	O
of	O
PT	O
and	O
AM	O
.	O

Considering	O
the	O
properties	O
of	O
the	O
cultural	O
heritage	O
domain	O
,	O
we	O
categorized	O
the	O
12	O
general	O
entity	O
types	O
aforementioned	O
in	O
the	O
NER	B-TaskName
task	O
(	O
Section	O
3.2.1	O
)	O
into	O
a	O
fine	O
-	O
grained	O
set	O
of	O
94	O
types	O
with	O
detailed	O
meanings	O
.	O

The	O
schema	O
for	O
the	O
ET	B-TaskName
task	O
was	O
designed	O
with	O
reference	O
to	O
the	O
data	O
construction	O
process	O
of	O
the	O
Fine	B-DatasetName
-	I-DatasetName
Grained	I-DatasetName
Entity	I-DatasetName
Recognition	I-DatasetName
dataset	I-DatasetName
(	O
Ling	O
and	O
Weld	O
,	O
2012	O
)	O
.	O

In	O
dealing	O
with	O
this	O
data	O
scarcity	O
problem	O
and	O
promoting	O
universal	O
studies	O
,	O
we	O
release	O
a	O
Korean	B-DatasetName
ET	I-DatasetName
task	I-DatasetName
corpus	I-DatasetName
for	O
the	O
first	O
time	O
,	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
.	O

Unlike	O
high	O
resource	O
languages	O
,	O
we	O
found	O
that	O
the	O
Korean	O
corpus	O
for	O
the	O
ET	B-TaskName
task	O
has	O
not	O
been	O
released	O
.	O

(	O
Kim	O
Hong	O
-	O
do	O
was	O
a	O
painter	O
of	O
the	O
Joseon	O
era	O
of	O
Korea	O
.	O
)	O
,	O
"	O
Joseon	O
should	O
be	O
typed	O
as	O
"	O
dynasty	O
/	O
Date	O
"	O
and	O
not	O
"	O
country	O
/	O
Location	O
.	O
"	O
This	O
typification	O
is	O
crucial	O
for	O
context	O
-	O
sensitive	O
tasks	O
such	O
as	O
RE	B-TaskName
,	O
coreference	B-TaskName
resolution	I-TaskName
,	O
and	O
question	B-TaskName
answering	I-TaskName
(	O
e.g.	O
,	O
"	O
In	O
which	O
era	O
was	O
Kim	O
Hongdo	O
,	O
an	O
artist	O
?	O
"	O
)	O
.	O

For	O
example	O
,	O
in	O
"	O
김홍도는	O
조선	O
후기의	O
화가이다	O
.	O

Given	O
a	O
sentence	O
and	O
entity	O
mention	O
within	O
it	O
,	O
the	O
ET	B-TaskName
task	O
predicts	O
a	O
set	O
of	O
noun	O
phrases	O
that	O
describe	O
the	O
mention	O
type	O
.	O

Fine	O
-	O
grained	O
Entity	B-TaskName
Typing	I-TaskName
.	O

is	O
destroyed	O
at	O
a	O
particular	O
period	O
.	O

•	O
"	O
A	O
hasDestroyed	O
B	O
"	O
implies	O
the	O
event	O
that	O
caused	O
destruction	O
such	O
as	O
"	O
War	O
-	O
Destroyed	O
place	O
,	O
"	O
or	O
the	O
collapse	O
of	O
a	O
country	O
in	O
a	O
specific	O
year	O
such	O
as	O
"	O
Country	O
-	O
Year	O
,	O
"	O
or	O
the	O
relationship	O
in	O
which	O
a	O
building	O
,	O
structure	O
,	O
monument	O
,	O
etc	O
.	O

•	O
"	O
A	O
hasCarriedOut	O
B	O
"	O
indicates	O
"	O
-is	O
caused	O
by	O
∼.	O
"	O
It	O
can	O
represent	O
a	O
relationship	O
between	O
a	O
specific	O
organization	O
or	O
group	O
and	O
an	O
event	O
conducted	O
by	O
it	O
,	O
such	O
as	O
a	O
festival	O
or	O
social	O
movement	O
.	O

may	O
correspond	O
to	O
the	O
object	O
argument	O
.	O

•	O
"	O
A	O
hasTime	O
B	O
"	O
implies	O
"	O
∼	O
has	O
happened	O
at	O
-	O
.	O
"	O
For	O
example	O
,	O
it	O
can	O
indicate	O
the	O
relationship	O
between	O
a	O
particular	O
event	O
and	O
a	O
specific	O
date	O
,	O
such	O
as	O
"	O
Presidential	O
election-1928	O
.	O
"	O
The	O
relation	O
between	O
a	O
specific	O
date	O
and	O
a	O
certain	O
work	O
,	O
such	O
as	O
the	O
year	O
of	O
production	O
of	O
a	O
work	O
and	O
the	O
year	O
of	O
construction	O
of	O
a	O
building	O
,	O
can	O
fall	O
under	O
this	O
category	O
,	O
for	O
example	O
,	O
"	O
Year	O
-	O
Craftwork	O
.	O
"	O
•	O
"	O
A	O
wears	O
B	O
"	O
implies	O
"	O
∼	O
puts	O
-on	O
.	O
"	O
For	O
instance	O
,	O
not	O
only	O
clothes	O
such	O
as	O
school	O
uniforms	O
but	O
also	O
crafts	O
,	O
etc	O
.	O

•	O
"	O
A	O
fallsWithin	O
B	O
"	O
implies	O
"	O
∼	O
is	O
denominated	O
as	O
-	O
.	O
"	O
It	O
indicates	O
the	O
relationship	O
of	O
alternate	O
names	O
such	O
as	O
"	O
Person	O
-	O
Specific	O
name	O
,	O
"	O
or	O
between	O
a	O
name	O
and	O
designation	O
in	O
front	O
of	O
the	O
name	O
,	O
or	O
between	O
words	O
that	O
refer	O
to	O
synonymous	O
concepts	O
such	O
as	O
"	O
Verse	O
-	O
Poetry	O
.	O
"	O
•	O
"	O
A	O
isUsedIn	O
B	O
"	O
indicates	O
"	O
∼	O
is	O
used	O
for	O
the	O
purpose	O
of	O
-	O
"	O
or	O
literally	O
"	O
∼	O
is	O
used	O
in	O
-	O
.	O
"	O
For	O
example	O
,	O
it	O
can	O
also	O
indicate	O
the	O
material	O
used	O
for	O
a	O
certain	O
object	O
,	O
such	O
as	O
"	O
Raw	O
material	O
-	O
Clothes	O
.	O
"	O
The	O
relationship	O
between	O
an	O
object	O
and	O
the	O
place	O
where	O
the	O
object	O
is	O
used	O
,	O
such	O
as	O
a	O
signboard	O
and	O
a	O
palace	O
,	O
or	O
the	O
relationship	O
between	O
certain	O
means	O
of	O
performing	O
a	O
function	O
and	O
an	O
object	O
such	O
as	O
"	O
Bowl	O
-	O
Rice	O
cake	O
"	O
can	O
correspond	O
to	O
this	O
category	O
.	O

The	O
relationships	O
between	O
two	O
positions	O
or	O
a	O
person	O
and	O
the	O
position	O
he	O
or	O
she	O
holds	O
do	O
not	O
fall	O
into	O
this	O
.	O

•	O
"	O
A	O
isConnectedWith	O
B	O
"	O
represents	O
a	O
personto	O
-	O
person	O
association	O
.	O

•	O
"	O
A	O
consistsOf	O
B	O
"	O
refers	O
to	O
the	O
relation	O
between	O
an	O
object	O
and	O
its	O
raw	O
ingredients	O
,	O
such	O
as	O
soil	O
,	O
iron	O
,	O
and	O
wood	O
that	O
constitute	O
an	O
object	O
.	O

•	O
"	O
A	O
OriginatedIn	O
B	O
"	O
means	O
"	O
∼	O
is	O
discovered	O
at	O
-	O
"	O
or	O
"	O
∼	O
is	O
produced	O
at	O
-(time	O
)	O
.	O
"	O
It	O
indicates	O
that	O
cultural	O
property	O
is	O
produced	O
at	O
a	O
specific	O
time	O
such	O
as	O
"	O
Craft	O
-	O
Year	O
"	O
or	O
is	O
discovered	O
at	O
a	O
particular	O
place	O
such	O
as	O
"	O
Object	O
-	O
Place	O
,	O
"	O
or	O
is	O
produced	O
at	O
a	O
certain	O
site	O
such	O
as	O
"	O
Document	O
-	O
Place	O
.	O
"	O
For	O
example	O
,	O
the	O
relation	O
between	O
earrings	O
and	O
tombs	O
or	O
a	O
newspaper	O
and	O
the	O
company	O
of	O
the	O
newspaper	O
fall	O
into	O
this	O
.	O

•	O
"	O
A	O
hasCreated	O
B	O
"	O
demonstrates	O
,	O
for	O
example	O
,	O
"	O
Person	O
-	O
Documents	O
"	O
or	O
"	O
Person	O
-	O
Painting	O
,	O
"	O
which	O
refers	O
to	O
the	O
relationship	O
between	O
a	O
person	O
and	O
a	O
document	O
such	O
as	O
a	O
book	O
,	O
map	O
,	O
or	O
drawing	O
,	O
or	O
his	O
/	O
her	O
activities	O
to	O
record	O
works	O
.	O

•	O
"	O
A	O
servedAs	O
B	O
"	O
implies	O
"	O
∼	O
is	O
the	O
role	O
of	O
-	O
,	O
"	O
which	O
corresponds	O
to	O
the	O
relationship	O
between	O
a	O
person	O
,	O
and	O
his	O
/	O
her	O
position	O
or	O
occupation	O
,	O
etc	O
.	O

It	O
also	O
indicates	O
the	O
relationship	O
like	O
a	O
record	O
written	O
on	O
an	O
object	O
such	O
as	O
"	O
Postcard	O
-	O
Explanation	O
"	O
or	O
a	O
specific	O
language	O
written	O
on	O
a	O
document	O
such	O
as	O
"	O
Record	O
-	O
Chinese	O
characters	O
.	O
"	O
•	O
"	O
A	O
hasSection	O
B	O
"	O
indicates	O
"	O
∼	O
is	O
located	O
at	O
-	O
.	O
"	O
It	O
represents	O
the	O
relationship	O
between	O
a	O
statue	O
,	O
building	O
,	O
or	O
specific	O
attraction	O
and	O
a	O
location	O
,	O
such	O
as	O
a	O
certain	O
city	O
and	O
place	O
.	O

It	O
can	O
also	O
represent	O
a	O
descriptive	O
relationship	O
such	O
as	O
"	O
Picture	O
of	O
a	O
place	O
-	O
the	O
place	O
where	O
it	O
was	O
taken	O
"	O
or	O
"	O
Picture	O
of	O
a	O
person	O
-	O
the	O
person	O
who	O
is	O
the	O
object	O
of	O
the	O
painting	O
.	O
"	O
•	O
"	O
A	O
documents	O
B	O
"	O
implies	O
"	O
∼	O
records	O
-	O
.	O
"	O
;	O
a	O
relationship	O
such	O
as	O
"	O
Record	O
-	O
The	O
person	O
who	O
records	O
it	O
"	O
can	O
be	O
represented	O
by	O
this	O
.	O

For	O
example	O
,	O
"	O
Green	O
Door	O
"	O
corresponds	O
to	O
this	O
relationship	O
.	O

•	O
"	O
A	O
depicts	O
B	O
"	O
implies	O
the	O
relationship	O
between	O
an	O
object	O
and	O
its	O
color	O
,	O
shape	O
or	O
pattern	O
,	O
etc	O
.	O

Label	O
Description	O
.	O

As	O
shown	O
in	O
Table	O
2	O
,	O
our	O
RE	B-DatasetName
corpus	I-DatasetName
consists	O
of	O
14	O
labels	O
,	O
and	O
these	O
were	O
defined	O
based	O
on	O
the	O
Encyves	O
ontology	O
research	O
of	O
the	O
National	O
Culture	O
Research	O
Institute	O
8	O
.	O

A	O
relationship	O
in	O
the	O
form	O
of	O
a	O
selfrelationship	O
between	O
identical	O
tokens	O
does	O
not	O
exist	O
.	O

In	O
the	O
case	O
of	O
certain	O
tokens	O
,	O
it	O
can	O
be	O
a	O
subject	O
or	O
an	O
object	O
depending	O
on	O
the	O
relationship	O
with	O
other	O
tokens	O
.	O

We	O
consider	O
the	O
relations	O
between	O
annotated	O
entities	O
in	O
the	O
NER	B-TaskName
annotation	O
procedure	O
.	O

Unlike	O
the	O
other	O
existing	O
corpora	O
,	O
our	O
corpus	O
has	O
the	O
advantage	O
of	O
capturing	O
various	O
relationships	O
between	O
multiple	O
entities	O
that	O
are	O
included	O
in	O
a	O
sentence	O
because	O
more	O
than	O
one	O
relation	O
can	O
exist	O
per	O
raw	O
sentence	O
.	O

Relation	O
Extraction	O
.	O

In	O
principle	O
,	O
social	O
movements	O
and	O
declarations	O
,	O
wars	O
,	O
revolutions	O
,	O
events	O
,	O
festivals	O
,	O
etc	O
.	O
,	O
fall	O
under	O
this	O
category	O
and	O
should	O
be	O
classified	O
only	O
if	O
they	O
exist	O
as	O
a	O
separate	O
entity	O
.	O

•	O
Event	O
(	O
EV	O
)	O
contains	O
entities	O
for	O
a	O
specific	O
event	O
/	O
accident	O
.	O

If	O
it	O
is	O
applied	O
as	O
a	O
subject	O
of	O
a	O
picture	O
,	O
it	O
is	O
also	O
included	O
in	O
the	O
category	O
of	O
animals	O
and	O
plants	O
.	O

•	O
Animal	O
(	O
AM	O
)	O
and	O
Plant	O
(	O
PT	O
)	O
are	O
defined	O
as	O
animals	O
and	O
plants	O
,	O
respectively	O
,	O
excluding	O
humans	O
.	O

•	O
Geographical	O
location	O
(	O
LCG	O
)	O
,	O
Political	O
location	O
(	O
LCP	O
)	O
,	O
and	O
Location	O
(	O
LC	O
)	O
are	O
defined	O
as	O
geographical	O
names	O
,	O
administrative	O
districts	O
,	O
and	O
other	O
places	O
,	O
respectively	O
.	O

When	O
an	O
entity	O
can	O
be	O
tagged	O
as	O
both	O
natural	O
objects	O
(	O
AM	O
,	O
PT	O
)	O
and	O
MT	O
,	O
tagging	O
as	O
MT	O
takes	O
precedence	O
.	O

In	O
other	O
words	O
,	O
it	O
indicates	O
the	O
entity	O
corresponding	O
to	O
the	O
detailed	O
classification	O
of	O
a	O
substance	O
(	O
metal	O
,	O
rock	O
,	O
wood	O
,	O
etc	O
.	O
)	O
.	O

•	O
Material	O
(	O
MT	O
)	O
includes	O
a	O
substance	O
used	O
as	O
a	O
material	O
or	O
an	O
expression	O
for	O
the	O
substance	O
.	O

However	O
,	O
in	O
the	O
case	O
of	O
an	O
unclear	O
period	O
that	O
can	O
not	O
be	O
tagged	O
with	O
a	O
separate	O
entity	O
,	O
tagging	O
is	O
not	O
performed	O
.	O

•	O
Date	O
(	O
DT	O
)	O
includes	O
all	O
entities	O
related	O
to	O
date	O
and	O
time	O
,	O
such	O
as	O
date	O
,	O
period	O
,	O
specific	O
day	O
,	O
or	O
season	O
,	O
month	O
,	O
year	O
,	O
era	O
/	O
dynasty	O
.	O

It	O
targets	O
words	O
classified	O
by	O
detailed	O
civilizations	O
/	O
cultures	O
,	O
such	O
as	O
clothing	O
and	O
food	O
.	O

•	O
Civilization	O
(	O
CV	O
)	O
is	O
defined	O
as	O
terms	O
related	O
to	O
civilization	O
/	O
culture	O
.	O

Patterns	O
and	O
drawings	O
are	O
classified	O
as	O
TM	O
,	O
owing	O
to	O
the	O
characteristics	O
of	O
movable	O
cultural	O
properties	O
.	O

•	O
Term	O
(	O
TM	O
)	O
includes	O
the	O
color	O
,	O
direction	O
,	O
shape	O
,	O
or	O
form	O
that	O
describes	O
an	O
artifact	O
.	O

•	O
Person	O
(	O
PS	O
)	O
is	O
a	O
category	O
for	O
content	O
related	O
to	O
people	O
,	O
including	O
real	O
persons	O
,	O
mythical	O
figures	O
,	O
fictional	O
characters	O
in	O
games	O
/	O
novels	O
,	O
occupations	O
,	O
and	O
human	O
relationships	O
.	O

Therefore	O
,	O
artificial	O
materials	O
such	O
as	O
buildings	O
,	O
civil	O
engineering	O
constructions	O
,	O
playground	O
names	O
,	O
apartments	O
,	O
and	O
bridges	O
fall	O
under	O
this	O
category	O
.	O

•	O
Artifacts	O
(	O
AF	O
)	O
generally	O
refer	O
to	O
objects	O
created	O
by	O
humans	O
corresponding	O
to	O
common	O
and	O
proper	O
nouns	O
and	O
also	O
include	O
cultural	O
properties	O
.	O

Label	O
Description	O
.	O

For	O
example	O
,	O
"	O
아시아	O
(	O
Asia	O
):	O
Geographical	O
Location	O
(	O
LCG	O
)	O
"	O
is	O
tagged	O
as	O
"	O
아	O
:	O
B	O
-	O
LCG	O
,	O
"	O
"	O
시	O
:	O
I	O
-	O
LCG	O
,	O
"	O
"	O
아	O
:	O
I	O
-	O
LCG	O
.	O
"	O
Therefore	O
,	O
we	O
evaluated	O
the	O
model	O
not	O
only	O
with	O
entity	B-MetricName
-	I-MetricName
level	I-MetricName
F1	I-MetricName
score	O
but	O
also	O
with	O
character	B-MetricName
-	I-MetricName
level	I-MetricName
F1	I-MetricName
score	O
(	O
Park	O
et	O
al	O
.	O
,	O
2021b	O
)	O
.	O

They	O
were	O
tagged	O
with	O
the	O
character	O
-	O
level	O
beginning	O
-	O
inside	O
-	O
outside	O
(	O
BIO	O
)	O
tagging	O
scheme	O
,	O
which	O
is	O
the	O
generally	O
adopted	O
method	O
for	O
sequence	O
labeling	O
problems	O
.	O

As	O
described	O
in	O
Table	O
1	O
,	O
we	O
defined	O
12	O
entity	O
types	O
.	O

Schema	O
for	O
.	O

Table	O
1	O
:	O
The	O
counts	O
of	O
entities	O
and	O
their	O
distributions	O
(	O
%	O
)	O
in	O
our	O
NER	B-TaskName
data	O
.	O

These	O
procedures	O
allowed	O
the	O
reliability	O
and	O
validity	O
of	O
KOCHET	B-DatasetName
on	O
the	O
cultural	O
heritage	O
objects	O
to	O
be	O
improved	O
.	O

The	O
discrepancy	O
between	O
annotators	O
on	O
the	O
annotated	O
entity	O
types	O
and	O
relations	O
is	O
also	O
discussed	O
and	O
agreed	O
upon	O
in	O
the	O
period	O
.	O

All	O
entity	O
types	O
and	O
relations	O
were	O
reviewed	O
by	O
four	O
crosschecking	O
annotators	O
,	O
afterward	O
,	O
were	O
additionally	O
checked	O
by	O
two	O
expert	O
supervisors	O
.	O

The	O
annotation	O
team	O
met	O
once	O
every	O
week	O
to	O
review	O
and	O
discuss	O
each	O
member	O
's	O
work	O
during	O
the	O
annotation	O
process	O
.	O

All	O
annotators	O
were	O
trained	O
for	O
a	O
week	O
,	O
and	O
each	O
of	O
them	O
was	O
familiarized	O
with	O
the	O
annotation	O
guideline	O
and	O
conducted	O
practice	O
annotation	O
on	O
test	O
samples	O
.	O

Annotator	O
Training	O
and	O
Cross	O
-	O
Checking	O
We	O
recruited	O
34	O
college	O
and	O
graduate	O
annotators	O
who	O
have	O
been	O
professionally	O
educated	O
on	O
the	O
cultural	O
heritage	O
domain	O
in	O
Korea	O
to	O
participate	O
in	O
the	O
annotation	O
process	O
.	O

In	O
both	O
cases	O
,	O
tagging	O
is	O
determined	O
according	O
to	O
the	O
pre	O
-	O
defined	O
priority	O
.	O

The	O
second	O
refers	O
to	O
the	O
case	O
where	O
it	O
may	O
vary	O
depending	O
on	O
the	O
context	O
.	O

The	O
first	O
case	O
is	O
where	O
the	O
entity	O
belongs	O
to	O
two	O
semantic	O
categories	O
regardless	O
of	O
the	O
context	O
.	O

There	O
are	O
two	O
cases	O
in	O
which	O
this	O
principle	O
should	O
be	O
applied	O
.	O

This	O
guideline	O
grants	O
a	O
single	O
tag	O
to	O
a	O
semantically	O
suitable	O
word	O
and	O
refers	O
to	O
assigning	O
only	O
one	O
tag	O
by	O
prioritizing	O
it	O
accordingly	O
.	O

For	O
the	O
principle	O
of	O
unique	O
tagging	O
,	O
there	O
are	O
cases	O
of	O
duplication	O
in	O
entities	O
that	O
belong	O
to	O
two	O
or	O
more	O
semantic	O
regions	O
.	O

It	O
is	O
not	O
tagged	O
in	O
the	O
case	O
of	O
Chinese	O
characters	O
and	O
English	O
,	O
but	O
if	O
it	O
is	O
read	O
in	O
Korean	O
,	O
it	O
is	O
included	O
in	O
the	O
tagging	O
range	O
.	O

In	O
addition	O
,	O
it	O
is	O
applied	O
only	O
to	O
cases	O
written	O
in	O
Korean	O
,	O
where	O
the	O
notation	O
is	O
possible	O
.	O

The	O
minimum	O
unit	O
is	O
based	O
on	O
one	O
word	O
for	O
the	O
tagging	O
units	O
and	O
categories	O
.	O

The	O
annotators	O
were	O
instructed	O
to	O
follow	O
two	O
types	O
of	O
rules	O
by	O
the	O
aforementioned	O
entity	O
guidelines	O
in	O
Section	O
1	O
;	O
one	O
is	O
related	O
to	O
tagging	O
units	O
and	O
categories	O
,	O
and	O
the	O
other	O
is	O
the	O
principle	O
of	O
unique	O
tagging	O
.	O

The	O
raw	O
corpus	O
annotated	O
by	O
each	O
annotator	O
is	O
equally	O
divided	O
by	O
the	O
category	O
.	O

Annotation	O
Guidelines	O
.	O

To	O
improve	O
the	O
quality	O
of	O
annotations	O
on	O
our	O
entityrich	O
corpus	O
related	O
to	O
cultural	O
heritage	O
,	O
we	O
conducted	O
the	O
annotation	O
process	O
based	O
on	O
expertise	O
in	O
the	O
cultural	O
heritage	O
domain	O
.	O

Annotation	O
Process	O
.	O

In	O
this	O
section	O
,	O
we	O
report	O
the	O
annotation	O
process	O
and	O
guidelines	O
in	O
detail	O
.	O

Following	O
the	O
guidelines	O
of	O
Korean	O
institutional	O
organizations	O
,	O
KOCHET	B-DatasetName
is	O
a	O
domain	O
specialized	O
corpus	O
for	O
cultural	O
heritage	O
,	O
which	O
ensures	O
quality	O
and	O
can	O
be	O
freely	O
accessed	O
.	O

KOCHET	B-DatasetName
.	O

The	O
tasks	O
include	O
natural	B-TaskName
language	I-TaskName
inference	I-TaskName
,	O
semantic	B-TaskName
textual	I-TaskName
similarity	I-TaskName
,	O
dependency	B-TaskName
parsing	I-TaskName
,	O
NER	B-TaskName
,	O
and	O
RE	B-TaskName
.	O

Korean	B-DatasetName
Language	I-DatasetName
Understanding	I-DatasetName
Evaluation	I-DatasetName
(	O
KLUE	B-DatasetName
)	O
dataset	O
was	O
recently	O
released	O
to	O
evaluate	O
the	O
ability	O
of	O
Korean	O
models	O
to	O
understand	O
natural	O
languages	O
with	O
eight	O
diverse	O
and	O
typical	O
tasks	O
(	O
Park	O
et	O
al	O
.	O
,	O
2021b	O
)	O
.	O

In	O
addition	O
to	O
public	O
datasets	O
opened	O
by	O
public	O
institutions	O
,	O
there	O
is	O
a	O
Korean	O
dataset	O
publicly	O
available	O
for	O
free	O
without	O
the	O
requirement	O
for	O
an	O
access	O
request	O
.	O

Institute	O
,	O
as	O
part	O
of	O
the	O
Exo	O
-	O
brain	O
project	O
6	O
,	O
provides	O
corpora	O
for	O
NLP	O
tasks	O
such	O
as	O
morphological	B-TaskName
analysis	I-TaskName
,	O
entity	B-TaskName
recognition	I-TaskName
,	O
dependency	B-TaskName
parsing	I-TaskName
,	O
and	O
question	B-TaskName
answering	I-TaskName
,	O
and	O
guidelines	O
for	O
building	O
such	O
high	O
-	O
quality	O
corpora	O
7	O
.	O

Electronics	O
and	O
Telecommunications	O
Research	O
.	O

To	O
support	O
the	O
development	O
of	O
the	O
Korean	O
artificial	O
intelligence	O
industry	O
for	O
the	O
NLP	O
field	O
,	O
the	O
NIA	O
disclosed	O
domain	O
-	O
specific	O
corpora	O
and	O
27	O
datasets	O
have	O
been	O
released	O
or	O
are	O
being	O
prepared	O
.	O

AI	O
HUB	O
is	O
a	O
massive	O
dataset	O
integration	O
platform	O
4	O
hosted	O
by	O
the	O
National	O
Information	O
Society	O
Agency	O
(	O
NIA	O
)	O
5	O
,	O
a	O
government	O
-	O
affiliated	O
organization	O
.	O

The	O
National	O
Institute	O
of	O
Korean	O
Language	O
,	O
which	O
is	O
an	O
institution	O
that	O
has	O
established	O
the	O
norms	O
for	O
Korean	O
linguistics	O
,	O
constructed	O
a	O
largescale	O
dataset	O
3	O
for	O
the	O
study	O
of	O
new	O
computational	O
linguistics	O
of	O
Korean	O
(	O
Kim	O
,	O
2006	O
)	O
.	O

Korean	O
public	O
corpora	O
.	O

For	O
example	O
,	O
these	O
include	O
a	O
Czech	B-DatasetName
NER	I-DatasetName
corpus	I-DatasetName
constructed	O
based	O
on	O
public	O
optical	O
character	O
recognition	O
data	O
of	O
Czech	O
historical	O
newspapers	O
(	O
Hubková	O
et	O
al	O
.	O
,	O
2020	O
)	O
,	O
a	O
Chinese	O
corpus	O
suitable	O
for	O
the	O
computational	O
analysis	O
of	O
historical	O
lexicon	O
and	O
semantic	O
change	O
(	O
Zinin	O
and	O
Xu	O
,	O
2020	O
)	O
,	O
and	O
an	O
English	O
corpus	O
that	O
is	O
one	O
of	O
the	O
most	O
commonly	O
used	O
large	O
corpora	O
in	O
diachronic	O
studies	O
in	O
English	O
(	O
Alatrash	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

Moreover	O
,	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
the	O
ET	B-DatasetName
corpus	I-DatasetName
is	O
the	O
first	O
proposed	O
corpus	O
in	O
Korean	O
.	O

There	O
have	O
been	O
the	O
disclosures	O
of	O
corpora	O
in	O
an	O
effort	O
to	O
preserve	O
traditional	O
culture	O
including	O
the	O
2	O
https://www.korean.go.kr	O
cultural	O
heritage	O
,	O
composing	O
data	O
from	O
the	O
perspective	O
of	O
the	O
entity	O
-	O
related	O
tasks	O
that	O
we	O
deal	O
with	O
.	O

General	O
cultural	O
heritage	O
corpora	O
.	O

Despite	O
such	O
demand	O
,	O
Korean	O
does	O
not	O
yet	O
have	O
a	O
corpus	O
specialized	O
in	O
the	O
cultural	O
heritage	O
area	O
,	O
unlike	O
other	O
languages	O
.	O

As	O
domains	O
that	O
require	O
expertise	O
,	O
such	O
as	O
the	O
cultural	O
heritage	O
,	O
contain	O
entities	O
or	O
relationships	O
that	O
rarely	O
appear	O
in	O
general	O
domains	O
,	O
the	O
necessity	O
of	O
a	O
corpus	O
specialized	O
in	O
the	O
domain	O
is	O
obvious	O
.	O

Related	O
Works	O
.	O

•	O
We	O
prove	O
the	O
applicability	O
of	O
our	O
entityabundant	O
corpus	O
in	O
each	O
task	O
by	O
providing	O
statistics	O
and	O
linguistic	O
analysis	O
,	O
along	O
with	O
the	O
experiments	O
with	O
pre	O
-	O
trained	O
language	O
models	O
.	O

•	O
We	O
categorized	O
the	O
detailed	O
entity	O
types	O
specialized	O
in	O
the	O
cultural	O
heritage	O
domain	O
,	O
which	O
is	O
essential	O
for	O
preserving	O
our	O
cultural	O
and	O
historical	O
artifacts	O
,	O
thereby	O
contributing	O
as	O
an	O
alternative	O
to	O
the	O
increased	O
demand	O
for	O
the	O
digitalized	O
archiving	O
of	O
cultural	O
heritage	O
documents	O
.	O

This	O
guarantees	O
a	O
high	O
-	O
quality	O
corpus	O
without	O
restrictions	O
regarding	O
modification	O
and	O
redistribution	O
.	O

Our	O
contributions	O
are	O
summarized	O
as	O
follows	O
:	O
•	O
We	O
introduce	O
KOCHET	B-DatasetName
designed	O
for	O
entityrelated	O
tasks	O
.	O

In	O
addition	O
to	O
providing	O
these	O
values	O
,	O
this	O
paper	O
provides	O
detailed	O
statistics	O
and	O
linguistic	O
analysis	O
of	O
KOCHET	B-DatasetName
for	O
each	O
entity	O
-	O
related	O
task	O
to	O
demonstrate	O
their	O
applicability	O
and	O
enhance	O
understanding	O
of	O
the	O
data	O
,	O
along	O
with	O
baseline	O
experiments	O
with	O
language	O
models	O
.	O

Furthermore	O
,	O
the	O
ET	B-TaskName
of	O
KOCHET	B-DatasetName
is	O
the	O
first	O
freely	O
available	O
corpus	O
for	O
the	O
ET	B-TaskName
task	O
in	O
Korea	O
.	O

As	O
our	O
corpus	O
focuses	O
on	O
the	O
entity	O
features	O
,	O
it	O
has	O
more	O
detailed	O
and	O
abundant	O
entity	O
types	O
including	O
diverse	O
cultural	O
heritage	O
artifacts	O
,	O
compared	O
to	O
the	O
existing	O
accessible	O
datasets	O
that	O
aim	O
to	O
deal	O
with	O
several	O
downstream	O
tasks	O
in	O
addition	O
to	O
entityrelated	O
tasks	O
.	O

For	O
the	O
annotation	O
,	O
the	O
categorization	O
for	O
classes	O
and	O
attributes	O
appropriate	O
was	O
defined	O
and	O
developed	O
following	O
the	O
2020	O
Named	O
Entity	O
Corpus	O
Research	O
Analysis	O
2	O
which	O
was	O
published	O
under	O
the	O
guidelines	O
as	O
institutional	O
organizations	O
.	O

We	O
selectively	O
used	O
resources	O
from	O
the	O
museums	O
in	O
which	O
the	O
details	O
of	O
artifacts	O
were	O
registered	O
;	O
moreover	O
,	O
for	O
the	O
completeness	O
of	O
the	O
attribute	O
data	O
,	O
we	O
limited	O
the	O
chronological	O
range	O
of	O
the	O
data	O
from	O
the	O
prehistoric	O
era	O
to	O
the	O
Korean	O
Empire	O
era	O
,	O
excluding	O
the	O
Japanese	O
colonial	O
period	O
.	O

For	O
corpus	O
construction	O
,	O
we	O
crawled	O
the	O
e	O
-	O
museum	O
digitized	O
data	O
of	O
the	O
National	O
Museum	O
of	O
Korea	O
1	O
(	O
including	O
data	O
from	O
all	O
50	O
museums	O
)	O
as	O
the	O
source	O
text	O
which	O
is	O
for	O
the	O
interested	O
public	O
.	O

To	O
address	O
these	O
difficulties	O
against	O
the	O
conservation	O
of	O
Korean	O
cultural	O
heritage	O
,	O
we	O
introduce	O
a	O
new	O
dataset	O
collection	O
called	O
KOCHET	B-DatasetName
-Korean	O
Cultural	O
Heritage	O
corpus	O
for	O
Entity	O
-	O
related	O
Tasks	O
,	O
a	O
high	O
-	O
quality	O
Korean	O
cultural	O
heritage	O
domainspecialized	O
dataset	O
for	O
NER	B-TaskName
,	O
RE	B-TaskName
,	O
and	O
ET	B-TaskName
tasks	O
.	O

These	O
cumbersome	O
procedures	O
and	O
restrictions	O
have	O
been	O
stumbling	O
blocks	O
for	O
researchers	O
against	O
the	O
rapid	O
increase	O
in	O
digitized	O
cultural	O
heritage	O
materials	O
over	O
the	O
past	O
few	O
decades	O
.	O

Furthermore	O
,	O
not	O
in	O
the	O
cultural	O
heritage	O
domain	O
,	O
the	O
existing	O
entity	O
-	O
related	O
datasets	O
supervised	O
by	O
the	O
public	O
institutions	O
have	O
a	O
complicated	O
procedure	O
for	O
data	O
acquisition	O
,	O
and	O
they	O
are	O
also	O
restricted	O
from	O
modification	O
and	O
redistribution	O
.	O

This	O
absence	O
of	O
cultural	O
heritage	O
domain	O
-	O
specialized	O
corpus	O
and	O
narrow	O
coverage	O
of	O
entity	O
types	O
hinders	O
the	O
effective	O
digitization	O
of	O
domestic	O
historical	O
documents	O
because	O
training	O
the	O
model	O
with	O
general	O
corpus	O
for	O
entity	O
-	O
related	O
tasks	O
can	O
not	O
afford	O
to	O
learn	O
enough	O
significant	O
entity	O
types	O
such	O
as	O
pagodas	O
,	O
historical	O
sites	O
and	O
intangible	O
heritage	O
,	O
and	O
their	O
relations	O
.	O

Moreover	O
,	O
conventional	O
entity	O
-	O
related	O
systems	O
deal	O
only	O
with	O
a	O
coarse	O
set	O
of	O
entity	O
types	O
such	O
as	O
person	O
,	O
location	O
,	O
and	O
organization	O
which	O
is	O
significantly	O
limited	O
in	O
terms	O
of	O
application	O
(	O
Kim	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

Despite	O
the	O
necessity	O
of	O
a	O
well	O
-	O
refined	O
entitycentric	O
corpus	O
specialized	O
in	O
domestic	O
cultural	O
her	O
-	O
itage	O
,	O
unfortunately	O
,	O
there	O
no	O
exists	O
any	O
cultural	O
heritage	O
domain	O
-	O
specialized	O
corpus	O
in	O
Korean	O
.	O

As	O
the	O
amount	O
of	O
digitized	O
information	O
materials	O
increases	O
rapidly	O
,	O
information	B-TaskName
extraction	I-TaskName
(	O
IE	B-TaskName
)	O
tasks	O
in	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
,	O
such	O
as	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
NER	B-TaskName
)	O
,	O
relation	B-TaskName
extraction	I-TaskName
(	O
RE	B-TaskName
)	O
,	O
and	O
entity	B-TaskName
typing	I-TaskName
(	O
ET	B-TaskName
)	O
,	O
have	O
become	O
an	O
essential	O
and	O
fundamental	O
step	O
in	O
the	O
field	O
of	O
historical	O
document	O
analysis	O
.	O

Recently	O
there	O
has	O
been	O
an	O
increasing	O
interest	O
in	O
the	O
preservation	O
of	O
national	O
historical	O
artifacts	O
and	O
traditional	O
cultural	O
heritage	O
,	O
and	O
also	O
grows	O
up	O
the	O
importance	O
of	O
effective	O
management	O
of	O
them	O
through	O
digitization	O
and	O
archival	O
.	O

Introduction	O
.	O

Our	O
corpus	O
is	O
freely	O
available	O
at	O
https://github.com/Gyeongmin47/KoCHET	O
.	O

We	O
also	O
provide	O
practical	O
insights	O
of	O
KOCHET	B-DatasetName
in	O
terms	O
of	O
statistical	O
and	O
linguistic	O
analysis	O
.	O

Our	O
experimental	O
results	O
make	O
the	O
practical	O
usability	O
of	O
KO	B-DatasetName
-	I-DatasetName
CHET	I-DatasetName
more	O
valuable	O
in	O
terms	O
of	O
cultural	O
heritage	O
.	O

Moreover	O
,	O
unlike	O
the	O
existing	O
public	O
corpora	O
,	O
modified	O
redistribution	O
can	O
be	O
allowed	O
both	O
domestic	O
and	O
foreign	O
researchers	O
.	O

Advised	O
by	O
cultural	O
heritage	O
experts	O
based	O
on	O
the	O
data	O
construction	O
guidelines	O
of	O
government	O
-	O
affiliated	O
organizations	O
,	O
KOCHET	B-DatasetName
consists	O
of	O
respectively	O
112,362	O
,	O
38,765	O
,	O
113,198	O
examples	O
for	O
NER	B-TaskName
,	O
RE	B-TaskName
,	O
and	O
ET	B-TaskName
tasks	O
,	O
covering	O
all	O
entity	O
types	O
related	O
to	O
Korean	O
cultural	O
heritage	O
.	O

To	O
achieve	O
this	O
,	O
we	O
propose	O
KOCHET	B-DatasetName
-a	O
Korean	O
cultural	O
heritage	O
corpus	O
for	O
the	O
typical	O
entity	O
-	O
related	O
tasks	O
,	O
i.e.	O
,	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
NER	B-TaskName
)	O
,	O
relation	B-TaskName
extraction	I-TaskName
(	O
RE	B-TaskName
)	O
,	O
and	O
entity	B-TaskName
typing	I-TaskName
(	O
ET	B-TaskName
)	O
.	O

As	O
digitized	O
traditional	O
cultural	O
heritage	O
documents	O
have	O
rapidly	O
increased	O
,	O
resulting	O
in	O
an	O
increased	O
need	O
for	O
preservation	O
and	O
management	O
,	O
practical	O
recognition	O
of	O
entities	O
and	O
typification	O
of	O
their	O
classes	O
has	O
become	O
essential	O
.	O

KOCHET	B-DatasetName
:	O
a	O
Korean	O
Cultural	O
Heritage	O
corpus	O
for	O
Entity	O
-	O
related	O
Tasks	O
.	O

Since	O
most	O
available	O
caption	O
datasets	O
have	O
been	O
constructed	O
for	O
English	O
language	O
,	O
there	O
are	O
few	O
datasets	O
for	O
Japanese	O
.	O

In	O
this	O
paper	O
,	O
we	O
particularly	O
consider	O
generating	O
Japanese	O
captions	O
for	O
images	O
.	O

As	O
a	O
result	O
,	O
we	O
showed	O
the	O
necessity	O
of	O
STAIR	B-DatasetName
Captions	I-DatasetName
.	O

In	O
addition	O
,	O
we	O
confirmed	O
that	O
Japanese	O
captions	O
can	O
be	O
generated	O
simply	O
by	O
adapting	O
the	O
existing	O
caption	B-TaskName
generation	I-TaskName
method	O
.	O

Moreover	O
,	O
by	O
using	O
both	O
Japanese	O
and	O
English	O
captions	O
,	O
we	O
will	O
develop	O
multi	O
-	O
lingual	O
caption	B-TaskName
generation	I-TaskName
models	O
.	O

In	O
our	O
experiment	O
,	O
we	O
compared	O
the	O
performance	O
of	O
Japanese	O
caption	O
generation	O
by	O
a	O
neural	O
network	O
-	O
based	O
model	O
with	O
and	O
without	O
STAIR	B-DatasetName
Captions	I-DatasetName
to	O
highlight	O
the	O
necessity	O
of	O
Japanese	O
captions	O
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
STAIR	B-DatasetName
Captions	I-DatasetName
is	O
currently	O
the	O
largest	O
Japanese	O
image	O
caption	O
dataset	O
.	O

In	O
STAIR	B-DatasetName
Captions	I-DatasetName
,	O
Japanese	O
captions	O
are	O
provided	O
for	O
all	O
the	O
images	O
of	O
MS	B-DatasetName
-	I-DatasetName
COCO	I-DatasetName
.	O

In	O
this	O
paper	O
,	O
we	O
constructed	O
a	O
new	O
Japanese	O
image	O
caption	O
dataset	O
called	O
STAIR	B-DatasetName
Captions	I-DatasetName
.	O

In	O
the	O
example	O
at	O
the	O
bottom	O
of	O
the	O
table	O
,	O
En	B-MethodName
-	I-MethodName
generator	I-MethodName
→	I-MethodName
MT	I-MethodName
yielded	O
the	O
incorrect	O
caption	O
by	O
translating	O
"	O
A	O
bunch	O
of	O
food	O
"	O
as	O
"	O
食べ物の束	O
(	O
A	O
bundle	O
of	O
food	O
)	O
.	O
"	O
By	O
contrast	O
,	O
Ja	B-MethodName
-	I-MethodName
generator	I-MethodName
correctly	O
recognized	O
that	O
the	O
food	O
pictured	O
in	O
the	O
image	O
is	O
a	O
donut	O
,	O
and	O
expressed	O
it	O
as	O
"	O
ドーナツがたくさん	O
(	O
A	O
bunch	O
of	O
donuts	O
)	O
.	O
"	O
.	O

By	O
contrast	O
,	O
Jagenerator	B-MethodName
generated	O
"	O
二階建てのバス	O
(	O
two	O
-	O
story	O
bus	O
)	O
,	O
"	O
which	O
is	O
appropriate	O
as	O
the	O
Japanese	O
translation	O
of	O
A	O
double	O
decker	O
bus	O
.	O

In	O
the	O
example	O
at	O
the	O
top	O
in	O
Table	O
3	O
,	O
En	B-MethodName
-	I-MethodName
generator	I-MethodName
first	O
generated	O
the	O
term	O
,	O
"	O
A	O
double	O
decker	O
bus	O
.	O
"	O
MT	O
translated	O
the	O
term	O
into	O
as	O
"	O
二重デッカーバ	O
ス	O
"	O
,	O
but	O
the	O
translation	O
is	O
word	O
-	O
by	O
-	O
word	O
and	O
inappropriate	O
as	O
a	O
Japanese	O
term	O
.	O

Table	O
3	O
shows	O
two	O
examples	O
where	O
Ja	B-MethodName
-	I-MethodName
generator	I-MethodName
generated	O
appropriate	O
captions	O
,	O
whereas	O
Engenerator	B-MethodName
→	I-MethodName
MT	I-MethodName
generated	O
unnatural	O
ones	O
.	O

The	O
results	O
show	O
that	O
Ja	B-MethodName
-	I-MethodName
generator	I-MethodName
,	O
that	O
is	O
,	O
the	O
approach	O
in	O
which	O
Japanese	O
captions	O
were	O
used	O
as	O
training	O
data	O
,	O
outperformed	O
En	B-MethodName
-	I-MethodName
generator	I-MethodName
→	I-MethodName
MT	I-MethodName
,	O
which	O
was	O
trained	O
without	O
Japanese	O
captions	O
.	O

The	O
hyper	O
-	O
parameters	O
of	O
the	O
neural	O
network	O
were	O
tuned	O
based	O
on	O
CIDEr	B-MetricName
scores	O
by	O
using	O
the	O
validation	O
set	O
.	O

With	O
the	O
optimization	B-HyperparameterName
of	O
LSTM	B-MethodName
,	O
we	O
used	O
mini	B-HyperparameterValue
-	I-HyperparameterValue
batch	I-HyperparameterValue
RMSProp	I-HyperparameterValue
,	O
and	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
was	O
set	O
to	O
20	B-HyperparameterValue
.	O

We	O
used	O
VGG	B-MethodName
with	O
16	B-HyperparameterValue
layers	B-HyperparameterName
as	O
CNN	B-MethodName
,	O
where	O
the	O
VGG	B-MethodName
parameters	O
were	O
the	O
pre	O
-	O
trained	O
ones5	O
.	O

In	O
both	O
the	O
methods	O
,	O
following	O
Karpathy	O
and	O
Fei	O
-	O
Fei	O
,	O
we	O
only	O
trained	O
LSTM	B-MethodName
parameters	O
,	O
while	O
CNN	B-MethodName
parameters	O
were	O
fixed	O
.	O

En	B-MethodName
-	I-MethodName
generator	I-MethodName
→	I-MethodName
MT	I-MethodName
:	O
テーブルの上にある食べ物の束	O
。	O
Ja	B-MethodName
-	I-MethodName
generator	I-MethodName
:	O
.	O

En	B-MethodName
-	I-MethodName
generator	I-MethodName
:	O
.	O

ストリートを運転する二重デッカーバス	O
。	O
Ja	B-MethodName
-	I-MethodName
generator	I-MethodName
:	O
.	O

En	B-MethodName
-	I-MethodName
generator	I-MethodName
→	I-MethodName
MT	I-MethodName
:	O
.	O

En	B-MethodName
-	I-MethodName
generator	I-MethodName
:	O
.	O

Ja	B-MethodName
-	I-MethodName
generator	I-MethodName
was	O
trained	O
with	O
Japanese	O
captions	O
.	O

En	B-MethodName
-	I-MethodName
generator	I-MethodName
→	I-MethodName
MT	I-MethodName
is	O
the	O
pipeline	O
method	O
:	O
it	O
first	O
generates	O
English	O
caption	O
and	O
performs	O
machine	O
translation	O
subsequently	O
.	O

En	B-MethodName
-	I-MethodName
generator	I-MethodName
denotes	O
the	O
caption	O
generator	O
trained	O
with	O
MS	B-DatasetName
-	I-DatasetName
COCO	I-DatasetName
.	O

As	O
mentioned	O
in	O
Section	O
4	O
,	O
we	O
used	O
the	O
method	O
proposed	O
by	O
Karpathy	O
and	O
Fei	O
-	O
Fei	O
(	O
2015	O
)	O
as	O
caption	B-TaskName
generation	I-TaskName
models	O
for	O
both	O
En	B-MethodName
-	I-MethodName
generator	I-MethodName
→	I-MethodName
MT	I-MethodName
and	O
Ja	B-MethodName
-	I-MethodName
generator	I-MethodName
.	O

Unlike	O
MS	B-DatasetName
-	I-DatasetName
COCO	I-DatasetName
→	O
MT	O
,	O
this	O
method	O
directly	O
generate	O
a	O
Japanese	O
caption	O
from	O
a	O
given	O
image	O
.	O

•	O
Ja	O
-	O
generator	O
:	O
This	O
method	O
trains	O
a	O
neural	O
network	O
using	O
STAIR	B-DatasetName
Captions	I-DatasetName
.	O

In	O
the	O
test	O
phase	O
,	O
given	O
an	O
image	O
,	O
we	O
first	O
generate	O
an	O
English	O
caption	O
to	O
the	O
image	O
by	O
the	O
trained	O
neural	O
network	O
,	O
and	O
then	O
translate	O
the	O
generated	O
caption	O
into	O
Japanese	O
one	O
by	O
machine	B-TaskName
translation	I-TaskName
.	O

This	O
method	O
trains	O
a	O
neural	O
network	O
,	O
which	O
generates	O
English	O
captions	O
,	O
with	O
MS	B-DatasetName
-	I-DatasetName
COCO	I-DatasetName
.	O

•	O
En	O
-	O
generator	O
→	O
MT	O
:	O
A	O
pipeline	O
method	O
of	O
English	O
caption	B-TaskName
generation	I-TaskName
and	O
English	O
-	O
Japanese	O
machine	B-TaskName
translation	I-TaskName
.	O

Although	O
BLEU	B-MetricName
and	O
ROUGE	B-MetricName
were	O
developed	O
originally	O
for	O
evaluating	O
machine	O
translation	O
and	O
text	O
summarization	O
,	O
we	O
use	O
them	O
here	O
because	O
they	O
are	O
often	O
used	O
for	O
measuring	O
the	O
quality	O
of	O
caption	B-TaskName
generation	I-TaskName
.	O

Following	O
the	O
literature	O
(	O
Chen	O
et	O
al	O
.	O
,	O
2015;Karpathy	O
and	O
Fei	O
-	O
Fei	O
,	O
2015	O
)	O
,	O
we	O
use	O
BLEU	B-MetricName
(	O
Papineni	O
et	O
al	O
.	O
,	O
2002	O
)	O
,	O
ROUGE	B-MetricName
(	O
Lin	O
,	O
2004	O
)	O
,	O
andCIDEr	B-MetricName
(	O
Vedantam	O
et	O
al	O
.	O
,	O
2015	O
)	O
as	O
evaluation	O
measures	O
.	O

In	O
particular	O
,	O
we	O
evaluate	O
quantitatively	O
and	O
qualitatively	O
how	O
fluent	O
Japanese	O
captions	O
can	O
be	O
generated	O
by	O
using	O
a	O
neural	O
network	O
-	O
based	O
caption	B-TaskName
generation	I-TaskName
model	O
trained	O
on	O
STAIR	O
Captions	O
.	O

In	O
this	O
section	O
,	O
we	O
perform	O
an	O
experiment	O
which	O
generates	O
Japanese	O
captions	O
using	O
STAIR	B-DatasetName
Captions	I-DatasetName
.	O

In	O
the	O
training	O
phase	O
,	O
given	O
the	O
training	O
data	O
,	O
we	O
train	O
W	O
(	O
i	O
m	O
)	O
,	O
b	O
(	O
i	O
m	O
)	O
,	O
W	O
*	O
,	O
b	O
*	O
,	O
CNN	B-MethodName
,	O
and	O
LSTM	B-MethodName
parameters	O
,	O
where	O
*	O
represents	O
wild	O
card	O
.	O

The	O
generation	O
process	O
is	O
repeated	O
until	O
LSTM	B-MethodName
outputs	O
the	O
symbol	O
that	O
indicates	O
the	O
end	O
of	O
sentence	O
.	O

Then	O
,	O
caption	O
generation	O
is	O
defined	O
as	O
follows	O
:	O
x	O
(	O
i	O
m	O
)	O
=	O
CNN(I	B-MethodName
)	O
,	O
h	O
0	O
=	O
tanh	O
(	O
W	O
(	O
i	O
m	O
)	O
x	O
(	O
i	O
m	O
)	O
+	O
b	O
(	O
i	O
m	O
)	O
)	O
,	O
c	O
0	O
=	O
0	O
,	O
h	O
t	O
,	O
c	O
t	O
=	O
LSTM	B-MethodName
(	O
x	O
t	O
,	O
h	O
t−1	O
,	O
c	O
t−1	O
)	O
(	O
t	O
≥	O
1	O
)	O
,	O
y	O
t	O
=	O
softmax	O
(	O
W	O
o	O
h	O
t	O
+	O
b	O
o	O
)	O
,	O
where	O
CNN(•	B-MethodName
)	O
is	O
a	O
function	O
that	O
outputs	O
the	O
image	O
features	O
extracted	O
by	O
CNN	B-MethodName
,	O
that	O
is	O
,	O
the	O
final	O
layer	O
of	O
CNN	B-MethodName
,	O
and	O
y	O
t	O
is	O
the	O
tth	O
output	O
word	O
.	O

Specifically	O
,	O
CNN	B-MethodName
first	O
extracts	O
features	O
from	O
a	O
given	O
image	O
,	O
and	O
then	O
,	O
LSTM	B-MethodName
generates	O
a	O
caption	O
from	O
the	O
extracted	O
features	O
.	O

This	O
method	O
consists	O
of	O
a	O
convolutional	B-MethodName
neural	I-MethodName
network	I-MethodName
(	O
CNN	B-MethodName
)	O
and	O
long	B-MethodName
short	I-MethodName
-	I-MethodName
term	I-MethodName
memory	I-MethodName
(	O
LSTM)3	B-MethodName
.	O

In	O
this	O
section	O
,	O
we	O
briefly	O
review	O
the	O
caption	B-TaskName
generation	I-TaskName
method	O
proposed	O
by	O
Karpathy	O
and	O
Fei	O
-	O
Fei	O
(	O
2015	O
)	O
,	O
which	O
is	O
used	O
in	O
our	O
experiments	O
(	O
Section	O
5	O
)	O
.	O

Image	B-TaskName
Caption	I-TaskName
Generation	I-TaskName
.	O

That	O
the	O
numbers	O
of	O
images	O
and	O
captions	O
are	O
large	O
in	O
STAIR	B-DatasetName
Captions	I-DatasetName
is	O
an	O
important	O
point	O
in	O
image	O
caption	O
.	O

In	O
the	O
public	O
part	O
of	O
STAIR	B-DatasetName
Captions	I-DatasetName
,	O
the	O
numbers	O
of	O
images	O
and	O
Japanese	O
captions	O
are	O
4.65x	O
and	O
4.67x	O
greater	O
than	O
those	O
in	O
YJ	B-DatasetName
Captions	I-DatasetName
,	O
respectively	O
.	O

Compared	O
with	O
YJ	B-DatasetName
Captions	I-DatasetName
,	O
overall	O
,	O
the	O
numbers	O
of	O
Japanese	O
captions	O
and	O
images	O
in	O
STAIR	B-DatasetName
Captions	I-DatasetName
are	O
6.23x	O
and	O
6.19x	O
,	O
respectively	O
.	O

In	O
addition	O
,	O
we	O
compare	O
it	O
to	O
YJ	B-DatasetName
Captions	I-DatasetName
(	O
Miyazaki	O
and	O
Shimizu	O
,	O
2016	O
)	O
,	O
a	O
dataset	O
with	O
Japanese	O
captions	O
for	O
the	O
images	O
in	O
MS	B-DatasetName
-	I-DatasetName
COCO	I-DatasetName
like	O
in	O
STAIR	B-DatasetName
Captions	I-DatasetName
.	O

This	O
section	O
introduces	O
the	O
quantitative	O
characteristics	O
of	O
STAIR	B-DatasetName
Captions	I-DatasetName
.	O

To	O
concurrently	O
and	O
inexpensively	O
annotate	O
captions	O
by	O
using	O
the	O
above	O
web	O
system	O
,	O
we	O
asked	O
part	O
-	O
time	O
job	O
workers	O
and	O
crowd	O
-	O
sourcing	O
workers	O
to	O
perform	O
the	O
caption	O
annotation	O
.	O

Following	O
the	O
rules	O
for	O
publishing	O
datasets	O
created	O
based	O
on	O
MS	B-DatasetName
-	I-DatasetName
COCO	I-DatasetName
,	O
the	O
Japanese	O
captions	O
we	O
created	O
for	O
the	O
test	O
images	O
are	O
excluded	O
from	O
the	O
public	O
part	O
of	O
STAIR	B-DatasetName
Captions	I-DatasetName
.	O

We	O
annotated	O
all	O
images	O
(	O
164,062	O
images	O
)	O
in	O
the	O
2014	O
edition	O
of	O
MS	B-DatasetName
-	I-DatasetName
COCO	I-DatasetName
.	O

This	O
section	O
explains	O
how	O
we	O
constructed	O
STAIR	B-DatasetName
Captions	I-DatasetName
.	O

STAIR	B-DatasetName
Captions	I-DatasetName
.	O

In	O
Section	O
3	O
,	O
we	O
highlight	O
this	O
difference	O
by	O
comparing	O
the	O
statistics	O
of	O
STAIR	B-DatasetName
Captions	I-DatasetName
and	O
YJ	B-DatasetName
Captions	I-DatasetName
.	O

The	O
main	O
difference	O
between	O
STAIR	B-DatasetName
Captions	I-DatasetName
and	O
YJ	B-DatasetName
Captions	I-DatasetName
is	O
that	O
STAIR	B-DatasetName
Captions	I-DatasetName
provides	O
Japanese	O
captions	O
for	O
a	O
greater	O
number	O
of	O
images	O
.	O

As	O
in	O
our	O
study	O
,	O
they	O
constructed	O
a	O
Japanese	O
caption	O
dataset	O
called	O
YJ	B-DatasetName
Captions	I-DatasetName
.	O

In	O
addition	O
,	O
many	O
studies	O
have	O
extended	O
MS	B-DatasetName
-	I-DatasetName
COCO	I-DatasetName
by	O
annotating	O
additional	O
information	O
about	O
the	O
images	O
in	O
MS	B-DatasetName
-	I-DatasetName
COCO2	I-DatasetName
.	O

Since	O
its	O
release	O
,	O
MS	B-DatasetName
-	I-DatasetName
COCO	I-DatasetName
has	O
been	O
used	O
as	O
a	O
benchmark	O
dataset	O
for	O
image	B-TaskName
classification	I-TaskName
and	O
caption	B-TaskName
generation	I-TaskName
.	O

MS	B-DatasetName
-	I-DatasetName
COCO	I-DatasetName
is	O
a	O
dataset	O
constructed	O
for	O
research	O
on	O
image	B-TaskName
classification	I-TaskName
,	O
object	B-TaskName
recognition	I-TaskName
,	O
and	O
English	O
caption	B-TaskName
generation	I-TaskName
.	O

Note	O
that	O
when	O
annotating	O
the	O
Japanese	O
captions	O
,	O
we	O
did	O
not	O
refer	O
to	O
the	O
original	O
English	O
captions	O
in	O
MS	B-DatasetName
-	I-DatasetName
COCO	I-DatasetName
.	O

As	O
detailed	O
in	O
Section	O
3	O
,	O
we	O
annotate	O
Japanese	O
captions	O
for	O
the	O
images	O
in	O
MS	B-DatasetName
-	I-DatasetName
COCO	I-DatasetName
.	O

STAIR	B-DatasetName
Captions	I-DatasetName
is	O
available	O
for	O
download	O
from	O
http://captions.stair.center	O
.	O

•	O
We	O
confirmed	O
that	O
quantitatively	O
and	O
qualitatively	O
better	O
Japanese	O
captions	O
than	O
the	O
ones	O
translated	O
from	O
English	O
captions	O
can	O
be	O
generated	O
by	O
applying	O
a	O
neural	O
network	O
-	O
based	O
image	O
caption	O
generation	O
model	O
learned	O
on	O
STAIR	B-DatasetName
Captions	I-DatasetName
(	O
Section	O
5	O
)	O
.	O

The	O
contributions	O
of	O
this	O
paper	O
are	O
as	O
follows	O
:	O
•	O
We	O
constructed	O
a	O
large	O
-	O
scale	O
Japanese	O
image	O
caption	O
dataset	O
,	O
STAIR	B-DatasetName
Captions	I-DatasetName
,	O
which	O
consists	O
of	O
Japanese	O
captions	O
for	O
all	O
the	O
images	O
in	O
MS	B-DatasetName
-	I-DatasetName
COCO	I-DatasetName
(	O
Lin	O
et	O
al	O
.	O
,	O
2014	O
)	O
(	O
Section	O
3	O
)	O
.	O

By	O
improving	O
the	O
quality	O
of	O
image	B-TaskName
captioning	I-TaskName
,	O
image	B-TaskName
search	I-TaskName
using	O
natural	O
sentences	O
and	O
image	B-TaskName
recognition	I-TaskName
support	O
for	O
1In	O
recent	O
years	O
it	O
has	O
been	O
held	O
as	O
a	O
joint	O
workshop	O
such	O
as	O
EMNLP	O
and	O
ACL	O
;	O
https://vision.cs.hacettepe	O
.	O

Image	B-TaskName
captioning	I-TaskName
is	O
to	O
automatically	O
generate	O
a	O
caption	O
for	O
a	O
given	O
image	O
.	O

In	O
this	O
research	O
area	O
,	O
methods	O
to	O
automatically	O
generate	O
image	O
descriptions	O
(	O
captions	O
)	O
,	O
that	O
is	O
,	O
image	B-TaskName
captioning	I-TaskName
,	O
have	O
attracted	O
a	O
great	O
deal	O
of	O
attention	O
(	O
Karpathy	O
and	O
Fei	O
-	O
Fei	O
,	O
2015;Donahue	O
et	O
al	O
.	O
,	O
2015;Vinyals	O
et	O
al	O
.	O
,	O
2015;Mao	O
et	O
al	O
.	O
,	O
2015	O
)	O
.	O

In	O
the	O
experiment	O
,	O
we	O
show	O
that	O
a	O
neural	O
network	O
trained	O
using	O
STAIR	B-DatasetName
Captions	I-DatasetName
can	O
generate	O
more	O
natural	O
and	O
better	O
Japanese	O
captions	O
,	O
compared	O
to	O
those	O
generated	O
using	O
English	O
-	O
Japanese	O
machine	B-TaskName
translation	I-TaskName
after	O
generating	O
English	O
captions	O
.	O

STAIR	B-DatasetName
Captions	I-DatasetName
consists	O
of	O
820,310	O
Japanese	O
captions	O
for	O
164,062	O
images	O
.	O

To	O
tackle	O
this	O
problem	O
,	O
we	O
construct	O
a	O
large	O
-	O
scale	O
Japanese	O
image	O
caption	O
dataset	O
based	O
on	O
images	O
from	O
MS	B-DatasetName
-	I-DatasetName
COCO	I-DatasetName
,	O
which	O
is	O
called	O
STAIR	B-DatasetName
Captions	I-DatasetName
.	O

In	O
recent	O
years	O
,	O
automatic	O
generation	O
of	O
image	O
descriptions	O
(	O
captions	O
)	O
,	O
that	O
is	O
,	O
image	B-TaskName
captioning	I-TaskName
,	O
has	O
attracted	O
a	O
great	O
deal	O
of	O
attention	O
.	O

STAIR	B-DatasetName
Captions	I-DatasetName
:	O
Constructing	O
a	O
Large	O
-	O
Scale	O
Japanese	O
Image	O
Caption	O
Dataset	O
.	O

In	O
future	O
work	O
,	O
we	O
will	O
analyze	O
the	O
experimental	O
results	O
in	O
greater	O
detail	O
.	O

The	O
total	O
number	O
of	O
Japanese	O
captions	O
is	O
820,310	O
.	O

Conclusion	O
.	O

Table	O
2	O
summarizes	O
the	O
experimental	O
results	O
.	O

Results	O
.	O

As	O
preprocessing	O
,	O
we	O
applied	O
morphological	O
analysis	O
to	O
the	O
Japanese	O
captions	O
using	O
MeCab6	O
.	O

We	O
divided	O
the	O
dataset	O
into	O
three	O
parts	O
,	O
i.e.	O
,	O
113,287	O
images	O
for	O
the	O
training	O
set	O
,	O
5,000	O
images	O
for	O
the	O
validation	O
set	O
,	O
and	O
5,000	O
images	O
for	O
the	O
test	O
set	O
.	O

Following	O
the	O
experimental	O
setting	O
in	O
the	O
previous	O
studies	O
(	O
Chen	O
et	O
al	O
.	O
,	O
2015;Karpathy	O
and	O
Fei	O
-	O
Fei	O
,	O
2015	O
)	O
,	O
we	O
used	O
123,287	O
images	O
included	O
in	O
the	O
MS	O
-	O
COCO	O
training	O
and	O
validation	O
sets	O
and	O
their	O
corresponding	O
Japanese	O
captions	O
.	O

Dataset	O
Separation	O
.	O

ドーナツがたくさん並んでいる	O
。	O
.	O

A	O
bunch	O
of	O
food	O
that	O
are	O
on	O
a	O
table	O
.	O

二階建てのバスが道路を走っている	O
。	O
.	O

A	O
double	O
decker	O
bus	O
driving	O
down	O
a	O
street	O
.	O

Examples	O
of	O
generated	O
image	O
captions	O
.	O

This	O
method	O
is	O
the	O
baseline	O
.	O

Here	O
,	O
we	O
use	O
Google	O
translate4	O
for	O
machine	O
translation	O
.	O

In	O
this	O
experiment	O
,	O
we	O
evaluate	O
the	O
following	O
caption	O
generation	O
methods	O
.	O

Comparison	O
Methods	O
.	O

Evaluation	O
Measure	O
.	O

Experimental	O
Setup	O
.	O

The	O
aim	O
of	O
this	O
experiment	O
is	O
to	O
show	O
the	O
necessity	O
of	O
a	O
Japanese	O
caption	O
dataset	O
.	O

Experiments	O
.	O

The	O
input	O
x	O
t	O
at	O
time	O
t	O
is	O
substituted	O
by	O
a	O
word	O
embedding	O
vector	O
corresponding	O
to	O
the	O
previous	O
output	O
,	O
that	O
is	O
,	O
y	O
t−1	O
.	O

Let	O
I	O
be	O
an	O
image	O
,	O
and	O
the	O
corresponding	O
caption	O
be	O
Y	O
=	O
(	O
y	O
1	O
,	O
y	O
2	O
,	O
•	O
•	O
•	O
,	O
y	O
n	O
)	O
.	O

Table	O
1	O
summarizes	O
the	O
statistics	O
of	O
the	O
datasets	O
.	O

Statistics	O
.	O

The	O
entire	O
annotation	O
work	O
was	O
completed	O
by	O
about	O
2,100	O
workers	O
in	O
about	O
half	O
a	O
year	O
.	O

To	O
guarantee	O
the	O
quality	O
of	O
the	O
captions	O
created	O
in	O
this	O
manner	O
,	O
we	O
conducted	O
sampling	O
inspection	O
of	O
the	O
annotated	O
captions	O
,	O
and	O
the	O
captions	O
not	O
in	O
line	O
with	O
the	O
guidelines	O
were	O
removed	O
.	O

(	O
5	O
)	O
A	O
caption	O
must	O
not	O
include	O
emotions	O
or	O
opinions	O
about	O
the	O
image	O
.	O

(	O
4	O
)	O
A	O
caption	O
must	O
be	O
a	O
single	O
sentence	O
.	O

(	O
3	O
)	O
A	O
caption	O
must	O
describe	O
only	O
what	O
is	O
happening	O
in	O
an	O
image	O
and	O
the	O
things	O
displayed	O
therein	O
.	O

(	O
2	O
)	O
A	O
caption	O
must	O
follow	O
the	O
da	O
/	O
dearu	O
style	O
(	O
one	O
of	O
writing	O
styles	O
in	O
Japanese	O
)	O
.	O

(	O
1	O
)	O
A	O
caption	O
must	O
contain	O
more	O
than	O
15	O
letters	O
.	O

The	O
workers	O
annotated	O
the	O
images	O
based	O
on	O
the	O
following	O
guidelines	O
.	O

By	O
pressing	O
the	O
send	O
(	O
送信	O
)	O
button	O
,	O
a	O
single	O
task	O
is	O
completed	O
and	O
the	O
next	O
task	O
is	O
started	O
.	O

Each	O
annotator	O
looks	O
at	O
the	O
displayed	O
image	O
and	O
writes	O
the	O
corresponding	O
Japanese	O
description	O
in	O
the	O
text	O
box	O
under	O
the	O
image	O
.	O

Figure	O
1	O
shows	O
the	O
example	O
of	O
the	O
annotation	O
screen	O
in	O
the	O
web	O
system	O
.	O

To	O
annotate	O
captions	O
efficiently	O
,	O
we	O
first	O
developed	O
a	O
web	O
system	O
for	O
caption	O
annotation	O
.	O

Therefore	O
,	O
the	O
total	O
number	O
of	O
captions	O
was	O
820,310	O
.	O

For	O
each	O
image	O
,	O
we	O
provided	O
five	O
Japanese	O
captions	O
.	O

Annotation	O
Procedure	O
.	O

In	O
particular	O
,	O
the	O
study	O
of	O
Miyazaki	O
and	O
Shimizu	O
(	O
2016	O
)	O
is	O
closest	O
to	O
the	O
present	O
study	O
.	O

Recently	O
,	O
a	O
few	O
caption	O
datasets	O
in	O
languages	O
other	O
than	O
English	O
have	O
been	O
constructed	O
(	O
Miyazaki	O
and	O
Shimizu	O
,	O
2016;Grubinger	O
et	O
al	O
.	O
,	O
2006;Elliott	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

Representative	O
examples	O
are	O
PASCAL	O
(	O
Rashtchian	O
et	O
al	O
.	O
,	O
2010	O
)	O
,	O
Flickr3k	O
(	O
Rashtchian	O
et	O
al	O
.	O
,	O
2010;Hodosh	O
et	O
al	O
.	O
,	O
2013	O
)	O
,	O
Flickr30k	O
(	O
Young	O
et	O
al	O
.	O
,	O
2014	O
)	O
-an	O
extension	O
of	O
Flickr3k-	O
,	O
and	O
MS	O
-	O
COCO	O
(	O
Microsoft	O
Common	O
Objects	O
in	O
Context	O
)	O
(	O
Lin	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

Some	O
English	O
image	O
caption	O
datasets	O
have	O
been	O
proposed	O
(	O
Krishna	O
et	O
al	O
.	O
,	O
2016;Kuznetsova	O
et	O
al	O
.	O
,	O
2013;Ordonez	O
et	O
al	O
.	O
,	O
2011;Vedantam	O
et	O
al	O
.	O
,	O
2015;Isola	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

Related	O
Work	O
.	O

Therefore	O
,	O
in	O
this	O
study	O
,	O
we	O
construct	O
a	O
Japanese	O
image	O
caption	O
dataset	O
,	O
and	O
for	O
given	O
images	O
,	O
we	O
aim	O
to	O
generate	O
more	O
natural	O
Japanese	O
captions	O
than	O
translating	O
the	O
generated	O
English	O
captions	O
into	O
the	O
Japanese	O
ones	O
.	O

However	O
,	O
the	O
translated	O
captions	O
may	O
be	O
literal	O
and	O
unnatural	O
because	O
image	O
information	O
can	O
not	O
be	O
reflected	O
in	O
the	O
translation	O
.	O

A	O
straightforward	O
solution	O
is	O
to	O
translate	O
English	O
captions	O
into	O
Japanese	O
ones	O
by	O
using	O
machine	O
translation	O
such	O
as	O
Google	O
Translate	O
.	O

Since	O
most	O
available	O
caption	O
datasets	O
have	O
been	O
constructed	O
for	O
English	O
language	O
,	O
there	O
are	O
few	O
datasets	O
for	O
Japanese	O
.	O

In	O
this	O
study	O
,	O
we	O
consider	O
generating	O
image	O
captions	O
in	O
Japanese	O
.	O

Recognizing	O
various	O
images	O
and	O
generating	O
appropriate	O
captions	O
for	O
the	O
images	O
necessitates	O
the	O
compilation	O
of	O
a	O
large	O
number	O
of	O
image	O
and	O
caption	O
pairs	O
.	O

edu.tr/vl2017/	O
visually	O
impaired	O
people	O
by	O
outputting	O
captions	O
as	O
sounds	O
can	O
be	O
made	O
available	O
.	O

The	O
Workshop	O
on	O
Vision	O
and	O
Language	O
held	O
in	O
2011	O
has	O
since	O
become	O
an	O
annual	O
event1	O
.	O

Integrated	O
processing	O
of	O
natural	O
language	O
and	O
images	O
has	O
attracted	O
attention	O
in	O
recent	O
years	O
.	O

Introduction	O
.	O

Besides	O
,	O
we	O
can	O
see	O
the	O
results	O
without	O
absolute	O
value	O
operation	O
for	O
symmetry	O
is	O
worse	O
,	O
demonstrating	O
absolute	O
value	O
operation	O
is	O
necessary	O
.	O

We	O
can	O
know	O
weight	O
for	O
meaningless	O
tokens	O
is	O
effective	O
.	O

And	O
the	O
results	O
without	O
weight	O
operation	O
for	O
word	O
embeddings	O
perform	O
worse	O
.	O

We	O
can	O
see	O
the	O
results	O
without	O
"	O
Delete	O
Sequence	O
"	O
and	O
"	O
Insert	O
Sequence	O
"	O
performs	O
a	O
little	O
worse	O
,	O
proving	O
its	O
necessity	O
.	O

It	O
demonstrates	O
that	O
subtraction	O
has	O
better	O
ability	O
to	O
capture	O
the	O
difference	O
between	O
two	O
sentences	O
,	O
and	O
provides	O
better	O
instance	O
representation	O
for	O
diversity	O
rank	O
.	O

We	O
can	O
see	O
subtraction	O
operation	O
is	O
better	O
than	O
sum	O
operation	O
.	O

Table	O
4	O
and	O
Figure	O
6	O
report	O
accuracy	B-MetricName
and	O
learning	B-MetricName
curves	I-MetricName
respectively	O
.	O

Table	O
4	O
:	O
Accuracy	B-MetricName
of	O
subtraction	O
operation	O
on	O
Levenshtein	O
Distance	O
.	O

the	O
two	O
sentences	O
without	O
"	O
Delete	O
Sequence	O
"	O
and	O
"	O
Insert	O
Sequence	O
"	O
(	O
Sub	O
)	O
;	O
(	O
c	O
)	O
without	O
weight	O
for	O
word	O
embeddings	O
(	O
Nowei	O
)	O
;	O
(	O
d	O
)	O
without	O
absolute	O
value	O
operation	O
for	O
symmetry	O
(	O
Noabs	O
)	O
.	O

This	O
research	O
work	O
was	O
also	O
supported	O
by	O
the	O
independent	O
research	O
project	O
of	O
National	O
Laboratory	O
of	O
Pattern	O
Recognition	O
and	O
the	O
Youth	O
Innovation	O
Promotion	O
Association	O
CAS	O
.	O

The	O
work	O
is	O
supported	O
by	O
the	O
National	O
Natural	O
Science	O
Foundation	O
of	O
China	O
under	O
Grant	O
Nos.61533018	O
,	O
U1936207	O
,	O
61976211	O
,	O
and	O
61702512	O
.	O

Acknowledgements	O
.	O

We	O
compare	O
it	O
with	O
4	O
baselines	O
:	O
(	O
a	O
)	O
using	O
the	O
sum	O
of	O
word	O
embeddings	O
of	O
the	O
two	O
sentences	O
(	O
Sum	O
)	O
;	O
(	O
b	O
)	O
directly	O
using	O
the	O
subtraction	O
of	O
word	O
embeddings	O
of	O
.	O

(	O
2)Effectiveness	O
of	O
subtraction	O
operation	O
on	O
Levenshtein	O
Distance	O
:	O
Here	O
we	O
validate	O
the	O
effectiveness	O
of	O
the	O
operation	O
that	O
uses	O
the	O
subtraction	O
of	O
word	O
embeddings	O
between	O
"	O
Delete	O
Sequence	O
"	O
and	O
"	O
Insert	O
Sequence	O
"	O
in	O
diversity	O
criterion	O
on	O
SNLI	B-DatasetName
dataset	O
.	O

It	O
is	O
possibly	O
because	O
BERT	O
used	O
more	O
data	O
to	O
learn	O
language	O
representations	O
.	O

In	O
intuition	O
,	O
contextual	O
representations	O
are	O
more	O
exact	O
especially	O
when	O
dealing	O
with	O
polysemy	O
.	O

We	O
can	O
see	O
contextual	O
representations	O
are	O
better	O
than	O
context	O
-	O
dependent	O
representations	O
.	O

Table	O
3	O
and	O
Figure	O
5	O
report	O
accuracy	B-MetricName
and	O
learning	B-MetricName
curves	I-MetricName
respectively	O
.	O

We	O
evaluated	O
performance	O
by	O
calculating	O
accuracy	B-MetricName
and	O
learning	B-MetricName
curves	I-MetricName
on	O
a	O
held	O
-	O
out	O
test	O
set	O
(	O
classes	O
are	O
fairly	O
balanced	O
in	O
datasets	O
)	O
after	O
all	O
rounds	O
.	O

Batch	B-HyperparameterName
size	I-HyperparameterName
is	O
16	B-HyperparameterValue
for	O
English	O
and	O
32	B-HyperparameterValue
for	O
Chinese	O
,	O
Adam	B-HyperparameterValue
is	O
used	O
for	O
optimization	O
.	O

(	O
Configuration	O
:	O
The	O
number	B-HyperparameterName
of	I-HyperparameterName
instances	I-HyperparameterName
to	O
select	O
n	B-HyperparameterName
is	O
100	B-HyperparameterValue
at	O
every	O
round	O
and	O
we	O
perform	O
25	B-HyperparameterValue
rounds	B-HyperparameterName
of	O
active	O
learning	O
,	O
that	O
is	O
there	O
are	O
total	O
of	O
2500	O
labeled	O
instances	O
for	O
training	O
in	O
the	O
end	O
.	O

(	O
4)LCQMC	B-DatasetName
:	O
an	O
open	O
-	O
domain	O
Chinese	O
question	O
matching	O
corpus	O
from	O
the	O
community	O
question	O
answering	O
website	O
Baidu	O
Knows	O
.	O

(	O
3)Quora	B-DatasetName
:	O
an	O
English	O
question	O
matching	O
corpus	O
from	O
the	O
online	O
question	O
answering	O
forum	O
Quora	O
.	O

(	O
2)MultiNLI	B-DatasetName
:	O
an	O
English	O
natural	O
language	O
inference	O
corpus	O
with	O
greater	O
linguistic	O
difficulty	O
and	O
diversity	O
.	O

(	O
1)SNLI	B-DatasetName
:	O
an	O
English	O
natural	O
language	O
inference	O
corpus	O
based	O
on	O
image	O
captioning	O
.	O

We	O
illustrate	O
it	O
in	O
2	O
provides	O
statistics	O
of	O
these	O
datasets	O
.	O

Visualization	O
of	O
Delete	O
Sequence	O
and	O
Insert	O
Sequence	O
:	O
To	O
model	O
the	O
difference	O
between	O
two	O
sentences	O
,	O
we	O
employ	O
the	O
subtraction	O
of	O
word	O
embeddings	O
between	O
"	O
Delete	O
Sequence	O
"	O
and	O
"	O
Insert	O
Sequence	O
"	O
from	O
Levenshtein	O
Distance	O
(	O
when	O
we	O
transform	O
sentence	O
A	O
to	O
sentence	O
B	O
by	O
deleting	O
and	O
inserting	O
tokens	O
,	O
these	O
tokens	O
are	O
added	O
into	O
"	O
Delete	O
Sequence	O
"	O
and	O
"	O
Insert	O
Sequence	O
"	O
espectively	O
)	O
.	O

A	O
general	O
uncertainty	O
criterion	O
uses	O
entropy	O
,	O
which	O
is	O
defined	O
as	O
follows	O
:	O
Ent(x	O
i	O
)	O
=	O
−	O
k	O
P	O
(	O
y	O
i	O
=	O
k|x	O
i	O
)	O
log	O
P	O
(	O
y	O
i	O
=	O
k|x	O
i	O
)	O
(	O
9	O
)	O
where	O
k	O
indexes	O
all	O
possible	O
labels	O
,	O
x	O
i	O
denotes	O
a	O
candidate	O
instance	O
that	O
is	O
made	O
up	O
of	O
a	O
pair	O
of	O
sentences	O
A	O
and	O
B	O
in	O
available	O
unlabeled	O
data	O
Q.	O

Commonly	O
,	O
the	O
criteria	O
is	O
mainly	O
based	O
on	O
uncertainty	O
criterion	O
(	O
uncertainty	B-MethodName
sampling	I-MethodName
)	O
,	O
in	O
which	O
ones	O
near	O
decision	O
boundaries	O
have	O
priority	O
to	O
be	O
selected	O
.	O

Train	O
and	O
update	O
classifier	O
M	O
based	O
on	O
P	O
6	O
:	O
until	O
The	O
annotation	O
budget	O
is	O
exhausted	O
With	O
the	O
same	O
amount	O
of	O
labeled	O
data	O
P	O
,	O
criteria	O
for	O
instance	O
selection	O
in	O
active	O
learning	O
determine	O
the	O
classifier	O
performance	O
.	O

At	O
every	O
round	O
,	O
there	O
are	O
n	O
instances	O
to	O
be	O
selected	O
and	O
labeled	O
.	O

The	O
instance	O
selection	O
process	O
is	O
iterative	O
,	O
and	O
the	O
process	O
will	O
repeat	O
until	O
a	O
fixed	O
annotation	O
budget	O
is	O
reached	O
.	O

The	O
process	O
is	O
illustrated	O
in	O
Algorithm	O
1	O
.	O

In	O
the	O
selection	O
criteria	O
,	O
a	O
measure	O
is	O
used	O
to	O
score	O
all	O
candidate	O
instances	O
in	O
Q	O
,	O
and	O
instances	O
maximizing	O
this	O
measure	O
are	O
selected	O
into	O
P	O
.	O

The	O
task	O
for	O
the	O
active	O
learning	O
is	O
to	O
select	O
instances	O
in	O
Q	O
based	O
on	O
some	O
criteria	O
,	O
and	O
then	O
label	O
them	O
and	O
add	O
them	O
into	O
P	O
,	O
so	O
as	O
to	O
maximize	O
classifier	O
performance	O
and	O
minimize	O
annotation	O
cost	O
.	O

P	O
is	O
for	O
training	O
a	O
classifier	O
and	O
can	O
absorb	O
new	O
instances	O
from	O
Q.	O

Standard	O
Active	O
Learning	O
:	O
In	O
a	O
general	O
active	O
learning	O
scenario	O
,	O
there	O
exists	O
a	O
small	O
set	O
of	O
labeled	O
data	O
P	O
and	O
a	O
large	O
pool	O
of	O
available	O
unlabeled	O
data	O
Q.	O

When	O
training	O
,	O
the	O
model	O
M	O
is	O
optimized	O
by	O
minimizing	O
cross	O
entropy	O
:	O
Loss	O
=	O
−P	O
(	O
y|a	O
,	O
b	O
;	O
θ	O
M	O
)	O
log	O
P	O
(	O
y|a	O
,	O
b	O
;	O
θ	O
M	O
)	O
(	O
8)	O
where	O
y	O
denotes	O
the	O
golden	O
label	O
.	O

When	O
testing	O
,	O
we	O
choose	O
the	O
label	O
with	O
the	O
highest	O
probability	O
in	O
prediction	O
distribution	O
P	O
(	O
y	O
i	O
|a	O
,	O
b	O
;	O
θ	O
M	O
)	O
as	O
output	O
,	O
where	O
θ	O
M	O
denotes	O
parameters	O
of	O
the	O
model	O
M	O
and	O
y	O
i	O
denotes	O
a	O
possible	O
label	O
.	O

And	O
there	O
is	O
a	O
sentence	O
matching	O
model	O
M	O
to	O
predict	O
a	O
label	O
ŷ	O
based	O
on	O
a	O
and	O
b.	O

,	O
e(b	O
l	O
B	O
)	O
]	O
,	O
where	O
n	O
e	O
denotes	O
the	O
vocabulary	O
size	O
,	O
d	B-HyperparameterName
denotes	O
the	O
embedding	B-HyperparameterName
size	I-HyperparameterName
and	O
e(a	O
i	O
)	O
and	O
e(b	O
j	O
)	O
denote	O
the	O
word	O
embedding	O
of	O
the	O
i	O
-	O
th	O
and	O
j	O
-	O
th	O
word	O
respectively	O
in	O
corresponding	O
sentences	O
.	O

,	O
e(a	O
l	O
A	O
)	O
]	O
and	O
b=[e(b	O
1	O
)	O
,	O
e(b	O
2	O
)	O
,	O
.	O

Through	O
a	O
shared	O
word	O
embedding	O
matrix	O
W	O
e	O
∈	O
R	O
ne×d	O
,	O
we	O
can	O
obtain	O
word	O
embeddings	O
of	O
input	O
sentences	O
a=[e(a	O
1	O
)	O
,	O
e(a	O
2	O
)	O
,	O
.	O

,	O
b	O
l	O
B	O
]	O
,	O
where	O
a	O
i	O
and	O
b	O
j	O
denote	O
the	O
i	O
-	O
th	O
and	O
j	O
-	O
th	O
word	O
respectively	O
in	O
corresponding	O
sentences	O
,	O
and	O
l	O
A	O
and	O
l	O
B	O
denote	O
the	O
length	O
of	O
corresponding	O
sentences	O
.	O

,	O
a	O
l	O
A	O
]	O
and	O
B=[b	O
1	O
,	O
b	O
2	O
,	O
.	O

In	O
formal	O
,	O
we	O
have	O
two	O
sentences	O
A=[a	O
1	O
,	O
a	O
2	O
,	O
.	O

Appendix	O
A	O
:	O
More	O
Details	O
and	O
Discussions	O
Sentence	B-TaskName
Matching	I-TaskName
Task	O
:	O
Given	O
a	O
pair	O
of	O
sentences	O
as	O
input	O
,	O
the	O
goal	O
of	O
the	O
task	O
is	O
to	O
judge	O
the	O
relation	O
between	O
them	O
,	O
such	O
as	O
whether	O
they	O
express	O
the	O
same	O
meaning	O
.	O

Experiments	O
show	O
that	O
our	O
proposed	O
active	O
learning	O
approach	O
obtains	O
better	O
performance	O
.	O

We	O
devise	O
extra	O
linguistic	O
criteria	O
from	O
a	O
pre	O
-	O
trained	O
language	O
model	O
,	O
which	O
can	O
capture	O
language	O
characteristics	O
and	O
enhance	O
active	O
learning	O
.	O

In	O
this	O
paper	O
,	O
we	O
combine	B-MethodName
active	I-MethodName
learning	I-MethodName
with	I-MethodName
a	I-MethodName
pre	I-MethodName
-	I-MethodName
trained	I-MethodName
language	I-MethodName
model	I-MethodName
.	O

Conclusion	O
.	O

More	O
ablation	O
discussions	O
are	O
shown	O
in	O
the	O
Appendix	O
.	O

It	O
demonstrates	O
that	O
each	O
linguistic	O
criterion	O
from	O
a	O
pre	O
-	O
trained	O
language	O
model	O
helps	O
capture	O
language	O
characteristics	O
and	O
enhances	O
selection	O
of	O
instances	O
.	O

We	O
can	O
see	O
each	O
combined	O
criterion	O
performs	O
better	O
than	O
a	O
single	O
uncertainty	O
criterion	O
.	O

Curves	O
are	O
also	O
illustrated	O
in	O
the	O
Appendix	O
.	O

Table	O
1	O
reports	O
the	O
accuracy	B-MetricName
.	O

"	O
Ent	O
"	O
denotes	O
the	O
standard	O
uncertainty	O
criterion	O
,	O
"	O
E+Noi	O
/	O
E+Cov	O
/	O
E+Div	O
/	O
E+All	O
"	O
denotes	O
combining	O
uncertainty	O
with	O
noise	O
/	O
coverage	O
/	O
diversity	O
/	O
all	O
criteria	O
.	O

To	O
validate	O
the	O
effectiveness	O
of	O
extra	O
linguistic	O
criteria	O
,	O
we	O
separately	O
combining	O
them	O
with	O
standard	O
uncertainty	O
criterion	O
.	O

Ablation	O
Study	O
.	O

Moreover	O
,	O
we	O
show	O
the	O
relation	O
between	O
the	O
size	O
of	O
unlabeled	O
data	O
and	O
accuracy	B-MetricName
in	O
Figure	O
2	O
(	O
6	O
)	O
,	O
we	O
can	O
see	O
the	O
superiority	O
of	O
the	O
pre	O
-	O
trained	O
model	O
based	O
approach	O
is	O
more	O
significant	O
for	O
larger	O
data	O
size	O
.	O

In	O
fact	O
,	O
sentence	B-TaskName
matching	I-TaskName
needs	O
to	O
capture	O
the	O
difference	O
between	O
sentences	O
and	O
gradients	O
of	O
a	O
single	O
token	O
ca	O
n't	O
reflect	O
the	O
relation	O
.	O

And	O
EGL	B-MethodName
performs	O
worse	O
than	O
the	O
standard	O
approach	O
active	B-MethodName
learning	I-MethodName
,	O
maybe	O
gradient	O
based	O
active	O
learning	O
is	O
not	O
suitable	O
for	O
sentence	O
matching	O
.	O

It	O
demonstrates	O
that	O
the	O
amount	O
of	O
labeled	O
data	O
for	O
sentence	O
matching	O
can	O
be	O
substantially	O
reduced	O
by	O
active	O
learning	O
.	O

Besides	O
,	O
active	B-MethodName
learning	I-MethodName
approaches	O
always	O
obtain	O
better	O
performance	O
than	O
random	B-MethodName
sampling	I-MethodName
.	O

We	O
can	O
know	O
that	O
extra	O
linguistic	O
criteria	O
are	O
effective	O
,	O
demonstrating	O
that	O
a	O
pre	O
-	O
trained	O
language	O
model	O
can	O
substantially	O
capture	O
language	O
characteristics	O
and	O
provide	O
more	O
efficient	O
instances	O
for	O
training	O
.	O

Overall	O
,	O
our	O
approach	O
obtains	O
better	O
performance	O
on	O
both	O
English	O
and	O
Chinese	O
datasets	O
.	O

Table	O
1	O
and	O
Figure	O
2	O
(	O
1	O
-	O
5	O
)	O
report	O
accuracy	B-MetricName
and	O
learning	O
curves	O
of	O
each	O
approach	O
on	O
the	O
five	O
datasets	O
.	O

Results	O
.	O

(	O
4)Pre	B-MethodName
-	I-MethodName
trained	I-MethodName
language	I-MethodName
model	I-MethodName
(	O
LM	O
)	O
is	O
our	O
proposed	O
active	O
learning	O
approach	O
.	O

(	O
3)Expected	B-MethodName
Gradient	I-MethodName
Length	I-MethodName
(	O
EGL	B-MethodName
)	O
aims	O
to	O
select	O
instances	O
expected	O
to	O
result	O
in	O
the	O
greatest	O
change	O
to	O
the	O
gradients	O
of	O
tokens	O
(	O
Settles	O
and	O
Craven	O
,	O
2008;Zhang	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

(	O
2)Uncertainty	B-MethodName
sampling	I-MethodName
(	O
Entropy	B-MethodName
)	O
is	O
the	O
standard	O
entropy	O
criterion	O
(	O
Tong	O
and	O
Koller	O
,	O
2001;Zhu	O
et	O
al	O
.	O
,	O
2008	O
)	O
.	O

We	O
compare	O
the	O
following	O
active	O
learning	O
approaches	O
:	O
(	O
1)Random	B-MethodName
sampling	I-MethodName
(	O
Random	B-MethodName
)	O
randomly	O
selects	O
instances	O
for	O
annotation	O
and	O
training	O
at	O
each	O
round	O
.	O

There	O
is	O
a	O
held	O
-	O
out	O
test	O
set	O
for	O
evaluation	O
after	O
all	O
rounds	O
.	O

We	O
choose	O
(	O
Devlin	O
et	O
al	O
.	O
,	O
2018	O
)	O
as	O
classifier	O
M	O
and	O
perform	O
25	B-HyperparameterValue
rounds	B-HyperparameterName
of	O
active	O
learning	O
.	O

The	O
number	O
of	O
instances	O
to	O
select	O
at	O
every	O
round	O
is	O
n	B-HyperparameterName
=	O
100	B-HyperparameterValue
.	O

In	O
practice	O
,	O
according	O
to	O
different	O
effectiveness	O
of	O
criteria	O
,	O
we	O
combine	O
ranks	O
of	O
criteria	O
and	O
select	O
the	O
top	O
n	B-HyperparameterName
candidate	O
instances	O
in	O
unlabeled	O
data	O
Q.	O

We	O
conduct	O
experiments	O
on	O
Both	O
English	O
and	O
Chinese	O
datasets	O
,	O
including	O
SNLI	B-DatasetName
(	O
Bowman	O
et	O
al	O
.	O
,	O
2015	O
)	O
,	O
MultiNLI	B-DatasetName
(	O
Williams	O
et	O
al	O
.	O
,	O
2017	O
)	O
,	O
Quora	B-DatasetName
(	O
Iyer	O
et	O
al	O
.	O
,	O
2017	O
)	O
,	O
LCQMC	B-DatasetName
(	O
Liu	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
BQ	B-DatasetName
(	O
Chen	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

Settings	O
and	O
Comparisons	O
.	O

3	O
Experiments	O
.	O

Specifically	O
,	O
we	O
sequentially	O
use	O
rank	O
uncer	O
,	O
rank	O
diver	O
,	O
rank	O
cover	O
,	O
rank	O
noise	O
to	O
select	O
top	O
8n	O
,	O
4n	O
,	O
2n	O
,	O
n	O
candidate	O
instances	O
,	O
and	O
add	O
the	O
final	O
n	O
instances	O
into	O
labeled	O
data	O
P	O
for	O
training	O
at	O
every	O
round	O
.	O

Instance	O
Selection	O
.	O

•	O
denotes	O
multiplication	O
on	O
element	O
.	O

Specifically	O
,	O
we	O
employ	O
k	O
-	O
means	O
clustering	O
algorithm	O
for	O
diversity	O
rank	O
as	O
follows	O
:	O
rank	O
diver	O
(	O
x	O
i	O
)	O
=	O
0	O
if	O
v	O
i	O
•	O
v	O
i	O
∈	O
O	O
diver	O
n	O
others(7	O
)	O
where	O
O	O
diver	O
are	O
the	O
centers	O
of	O
n	O
clusters	O
of	O
{	O
v	O
i	O
•	O
v	O
i	O
}	O
.	O

With	O
instance	O
representation	O
,	O
we	O
want	O
to	O
select	O
diverse	O
ones	O
that	O
are	O
representative	O
and	O
different	O
from	O
each	O
other	O
.	O

w	O
a	O
i	O
/w	O
b	O
j	O
denotes	O
the	O
weight	O
for	O
tokens	O
.	O

e(a	O
j	O
)	O
/e(b	O
j	O
)	O
denotes	O
word	O
embdeddings	O
.	O

Formally	O
,	O
v	O
i	O
=	O
j∈L	O
I	O
w	O
b	O
j	O
e(b	O
j	O
)	O
−	O
j∈L	O
D	O
w	O
a	O
j	O
e(a	O
j	O
)	O
(	O
5	O
)	O
w	O
a	O
j	O
=	O
s	O
a	O
j	O
k∈l	O
A	O
s	O
a	O
k	O
,	O
w	O
b	O
j	O
=	O
s	O
b	O
j	O
k∈l	O
B	O
s	O
b	O
k	O
(	O
6	O
)	O
where	O
s	O
a	O
i	O
/s	O
b	O
j	O
is	O
the	O
reconstruction	O
loss	O
of	O
the	O
i	O
/	O
j	O
-	O
th	O
word	O
of	O
sentence	O
A	O
/	O
B.	O

Intuitively	O
,	O
meaningless	O
tokens	O
such	O
as	O
preposition	O
should	O
have	O
less	O
weight	O
,	O
and	O
they	O
are	O
usually	O
easier	O
to	O
predict	O
with	O
lower	O
reconstruction	O
losses	O
.	O

Besides	O
,	O
the	O
word	O
embeddings	O
in	O
the	O
subtraction	O
are	O
weighted	O
by	O
reconstruction	O
losses	O
.	O

It	O
is	O
illustrated	O
in	O
the	O
Appendix	O
.	O

To	O
model	O
the	O
difference	O
between	O
two	O
sentences	O
,	O
we	O
employ	O
the	O
subtraction	O
of	O
word	O
embeddings	O
between	O
"	O
Delete	O
Sequence	O
"	O
L	O
D	O
and	O
"	O
Insert	O
Sequence	O
"	O
L	O
I	O
from	O
Levenshtein	O
Distance	O
(	O
when	O
we	O
transform	O
sentence	O
A	O
to	O
sentence	O
B	O
by	O
deleting	O
and	O
inserting	O
tokens	O
,	O
these	O
tokens	O
are	O
added	O
into	O
L	O
D	O
and	O
L	O
I	O
respectively	O
)	O
.	O

First	O
,	O
we	O
use	O
a	O
vector	O
v	O
i	O
for	O
instance	O
representation	O
of	O
a	O
sentence	O
pair	O
instance	O
x	O
i	O
.	O

In	O
contrast	O
,	O
diverse	O
ones	O
can	O
help	O
learn	O
more	O
various	O
language	O
expressions	O
and	O
matching	O
patterns	O
.	O

Redundant	O
instances	O
are	O
inefficient	O
and	O
waste	O
annotation	O
resources	O
.	O

Thus	O
,	O
we	O
can	O
employ	O
reconstruction	O
losses	O
to	O
capture	O
the	O
low	O
coverage	O
ones	O
as	O
follows	O
:	O
rank	O
cover	O
(	O
x	O
i	O
)	O
∝	O
−	O
j∈l	O
A	O
c	O
a	O
j	O
s	O
a	O
j	O
j∈l	O
A	O
c	O
a	O
j	O
−	O
j∈l	O
B	O
c	O
b	O
j	O
s	O
b	O
j	O
j∈l	O
B	O
c	O
b	O
j	O
(	O
3	O
)	O
c	O
a	O
j	O
=	O
0	O
if	O
s	O
a	O
j	O
>	O
β	O
1	O
others	O
,	O
c	O
b	O
j	O
=	O
0	O
if	O
s	O
b	O
j	O
>	O
β	O
1	O
others	O
(	O
4	O
)	O
where	O
β	O
denotes	O
a	O
hyperparameter	O
to	O
distinguish	O
noise	O
and	O
is	O
set	O
as	O
10.0	O
.	O
(	O
4	O
)	O
Diversity	O
:	O
The	O
diversity	O
criterion	O
indicates	O
the	O
diversity	O
of	O
instances	O
.	O

These	O
fresh	O
instances	O
like	O
relatively	O
low	O
-	O
frequency	O
professional	O
expressions	O
usually	O
have	O
lower	O
generating	O
probabilities	O
than	O
common	O
ones	O
.	O

On	O
the	O
other	O
hand	O
,	O
the	O
classifier	O
needs	O
fresh	O
instances	O
(	O
low	O
coverage	O
)	O
to	O
enrich	O
representation	O
learning	O
.	O

On	O
the	O
one	O
hand	O
,	O
some	O
tokens	O
like	O
stop	O
words	O
are	O
meaningless	O
and	O
easy	O
to	O
model	O
(	O
high	O
coverage	O
)	O
.	O

(	O
3	O
)	O
Coverage	O
:	O
The	O
coverage	O
criterion	O
indicates	O
whether	O
the	O
language	O
expression	O
of	O
the	O
current	O
instance	O
can	O
enrich	O
representation	O
learning	O
.	O

rank	O
noise	O
(	O
x	O
i	O
)	O
denotes	O
noise	O
rank	O
of	O
the	O
i	O
-	O
th	O
instance	O
in	O
Q	O
,	O
s	O
a	O
i	O
/s	O
b	O
i	O
is	O
the	O
reconstruction	O
loss	O
of	O
the	O
i	O
-	O
th	O
word	O
a	O
i	O
/b	O
i	O
in	O
sentence	O
A	O
/	O
B	O
from	O
the	O
pre	O
-	O
trained	O
language	O
model	O
.	O

P	O
(	O
B	O
)	O
is	O
similar	O
.	O

a	O
l	O
A	O
)	O
∝	O
l	O
A	O
i∈l	O
A	O
sa	O
i	O
.	O

Based	O
on	O
this	O
assumption	O
,	O
noise	O
criterion	O
is	O
formulated	O
about	O
losses	O
of	O
reconstructing	O
masked	O
tokens	O
:	O
rank	O
noise	O
(	O
x	O
i	O
)	O
∝	O
−P	O
(	O
A	O
)	O
−	O
P	O
(	O
B)(2	O
)	O
where	O
P	O
(	O
A	O
)	O
=	O
P	O
(	O
a	O
1	O
a	O
2	O
.	O

Thus	O
,	O
tokens	O
in	O
noisy	O
instances	O
may	O
be	O
hard	O
to	O
be	O
reconstructed	O
with	O
context	O
by	O
the	O
pre	O
-	O
trained	O
language	O
model	O
.	O

Noisy	O
instances	O
usually	O
have	O
rare	O
expression	O
with	O
low	O
generating	O
probability	O
.	O

Intuitively	O
,	O
instances	O
with	O
noise	O
may	O
degrade	O
the	O
labeled	O
data	O
P	O
,	O
and	O
we	O
want	O
to	O
select	O
noiseless	O
instances	O
.	O

(	O
2	O
)	O
Noise	O
:	O
The	O
noise	O
criterion	O
indicates	O
how	O
much	O
potential	O
noise	O
there	O
is	O
in	O
an	O
instance	O
.	O

Formally	O
,	O
rank	O
uncer	O
(	O
x	O
i	O
)	O
∝	O
−Ent(x	O
i	O
)	O
(	O
1	O
)	O
where	O
Ent(x	O
i	O
)	O
=	O
−	O
k	O
P	O
(	O
y	O
i	O
=	O
k|x	O
i	O
)	O
log	O
P	O
(	O
y	O
i	O
=	O
k|x	O
i	O
)	O
.	O

The	O
uncertainty	O
is	O
computed	O
as	O
the	O
entropy	O
,	O
and	O
we	O
can	O
obtain	O
uncertainty	O
rank	O
rank	O
uncer	O
(	O
x	O
i	O
)	O
for	O
the	O
i	O
-	O
th	O
instance	O
in	O
Q	O
based	O
on	O
the	O
entropy	O
.	O

Instances	O
with	O
high	O
uncertainty	O
are	O
more	O
helpful	O
to	O
optimize	O
the	O
classifier	O
and	O
thus	O
are	O
worthier	O
to	O
be	O
selected	O
.	O

(	O
1	O
)	O
Uncertainty	O
:	O
The	O
uncertainty	O
criterion	O
indicates	O
classification	O
uncertainty	O
of	O
an	O
instance	O
and	O
is	O
the	O
standard	O
criterion	O
in	O
active	O
learning	O
.	O

Criteria	O
for	O
Instance	O
Selection	O
.	O

,	O
e(a	O
l	O
A	O
)	O
]	O
in	O
the	O
sentence	O
,	O
where	O
l	O
A	O
is	O
the	O
length	O
of	O
sentence	O
A.	O

The	O
other	O
is	O
word	O
embeddings	O
(	O
contextual	O
representations	O
of	O
the	O
last	O
layer	O
)	O
a=[e(a	O
1	O
)	O
,	O
e(a	O
2	O
)	O
,	O
.	O

One	O
is	O
the	O
cross	O
entropy	O
loss	O
s	O
a	O
i	O
of	O
reconstructing	O
of	O
the	O
i	O
-	O
th	O
word	O
a	O
i	O
in	O
sentence	O
A	O
(	O
the	O
same	O
with	O
another	O
B	O
)	O
by	O
masking	O
only	O
a	O
i	O
and	O
predicting	O
a	O
i	O
again	O
.	O

From	O
BERT	O
,	O
we	O
can	O
obtain	O
two	O
kinds	O
of	O
information	O
to	O
provide	O
linguistic	O
criteria	O
.	O

We	O
choose	O
the	O
widely	O
used	O
language	O
model	O
BERT	O
(	O
Devlin	O
et	O
al	O
.	O
,	O
2018	O
)	O
as	O
the	O
pre	O
-	O
trained	O
language	O
model	O
.	O

Pre	O
-	O
trained	O
Language	O
Model	O
.	O

More	O
details	O
of	O
preliminaries	O
about	O
sentence	O
matching	O
and	O
active	O
learning	O
are	O
in	O
the	O
Appendix	O
.	O

Active	O
learning	O
is	O
to	O
select	O
instances	O
in	O
Q	O
according	O
to	O
some	O
criteria	O
,	O
and	O
then	O
label	O
them	O
and	O
add	O
them	O
into	O
P	O
,	O
so	O
as	O
to	O
maximize	O
classifier	O
M	O
performance	O
and	O
minimize	O
annotation	O
cost	O
.	O

In	O
a	O
general	O
active	O
learning	O
scenario	O
,	O
there	O
is	O
a	O
small	O
set	O
of	O
labeled	O
training	O
data	O
P	O
and	O
a	O
large	O
pool	O
of	O
available	O
unlabeled	O
data	O
Q.	O

Methodology	O
.	O

Experiments	O
on	O
both	O
English	O
and	O
Chinese	O
sentence	O
matching	O
datasets	O
demonstrate	O
the	O
pre	O
-	O
trained	O
language	O
model	O
can	O
enhance	O
active	O
learning	O
.	O

It	O
is	O
shown	O
in	O
Figure	O
1	O
.	O

In	O
this	O
paper	O
,	O
we	O
devise	O
linguistic	O
criteria	O
from	O
a	O
pre	O
-	O
trained	O
language	O
model	O
to	O
capture	O
language	O
characteristics	O
,	O
and	O
then	O
utilize	O
these	O
extra	O
linguistic	O
criteria	O
(	O
noise	O
,	O
coverage	O
and	O
diversity	O
)	O
to	O
enhance	O
active	O
learning	O
.	O

Accordingly	O
,	O
pre	O
-	O
trained	O
language	O
models	O
may	O
provide	O
a	O
reliable	O
way	O
to	O
help	O
capture	O
language	O
characteristics	O
.	O

Recently	O
,	O
pre	O
-	O
trained	O
language	O
models	O
(	O
Peters	O
et	O
al	O
.	O
,	O
2018;Radford	O
et	O
al	O
.	O
,	O
2018;Devlin	O
et	O
al	O
.	O
,	O
2018;Yang	O
et	O
al	O
.	O
,	O
2019	O
)	O
have	O
been	O
shown	O
to	O
be	O
powerful	O
for	O
learning	O
language	O
representation	O
.	O

Thus	O
,	O
how	O
to	O
devise	O
linguistic	O
criteria	O
to	O
measure	O
candidate	O
instances	O
is	O
an	O
important	O
challenge	O
.	O

To	O
be	O
more	O
specific	O
,	O
if	O
we	O
ignore	O
the	O
linguistic	O
similarity	O
,	O
we	O
may	O
select	O
redundant	O
instances	O
and	O
waste	O
many	O
annotation	O
resources	O
.	O

However	O
,	O
previous	O
active	O
learning	O
approaches	O
in	O
natural	O
language	O
processing	O
mainly	O
depend	O
on	O
the	O
entropy	O
-	O
based	O
uncertainty	O
criterion	O
(	O
Settles	O
,	O
2009	O
)	O
,	O
and	O
ignore	O
the	O
characteristics	O
of	O
natural	O
language	O
.	O

Instead	O
of	O
randomly	O
selecting	O
instances	O
,	O
active	O
learning	O
can	O
measure	O
the	O
whole	O
candidate	O
instances	O
according	O
to	O
some	O
criteria	O
,	O
and	O
then	O
select	O
more	O
efficient	O
instances	O
for	O
annotation	O
(	O
Zhang	O
et	O
al	O
.	O
,	O
2017;Shen	O
et	O
al	O
.	O
,	O
2017;Erdmann	O
et	O
al	O
.	O
,	O
;	O
Kasai	O
et	O
al	O
.	O
,	O
2019;Xu	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

To	O
alleviate	O
this	O
problem	O
,	O
active	O
learning	O
is	O
proposed	O
to	O
achieve	O
better	O
performance	O
with	O
fewer	O
labeled	O
training	O
instances	O
(	O
Settles	O
,	O
2009	O
)	O
.	O

If	O
large	O
labeled	O
data	O
ca	O
n't	O
be	O
obtained	O
,	O
the	O
advantages	O
of	O
deep	O
learning	O
will	O
significantly	O
diminish	O
.	O

However	O
,	O
this	O
data	O
-	O
driven	O
technique	O
typically	O
requires	O
large	O
amounts	O
of	O
manual	O
annotation	O
and	O
brings	O
much	O
cost	O
.	O

Over	O
the	O
past	O
few	O
years	O
,	O
deep	O
learning	O
as	O
a	O
data	O
-	O
driven	O
technique	O
has	O
yielded	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
sentence	B-TaskName
matching	I-TaskName
(	O
Wang	O
et	O
al	O
.	O
,	O
2017;Chen	O
et	O
al	O
.	O
,	O
2016;Gong	O
et	O
al	O
.	O
,	O
2017;Yang	O
et	O
al	O
.	O
,	O
2016;Parikh	O
et	O
al	O
.	O
,	O
2016;Gong	O
et	O
al	O
.	O
,	O
2017;Kim	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

Sentence	B-TaskName
matching	I-TaskName
is	O
a	O
fundamental	O
technology	O
in	O
natural	O
language	O
processing	O
.	O

Introduction	O
.	O

Experiments	O
demonstrate	O
our	O
approach	O
can	O
achieve	O
greater	O
accuracy	O
with	O
fewer	O
labeled	O
training	O
instances	O
.	O

Differing	O
from	O
previous	O
active	O
learning	O
,	O
it	O
can	O
provide	O
linguistic	O
criteria	O
from	O
the	O
pre	O
-	O
trained	O
language	O
model	O
to	O
measure	O
instances	O
and	O
help	O
select	O
more	O
effective	O
instances	O
for	O
annotation	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
pre	B-MethodName
-	I-MethodName
trained	I-MethodName
language	I-MethodName
model	I-MethodName
based	I-MethodName
active	I-MethodName
learning	I-MethodName
approach	I-MethodName
for	O
sentence	O
matching	O
.	O

However	O
,	O
previous	O
active	O
learning	O
approaches	O
for	O
natural	O
language	O
processing	O
mainly	O
depend	O
on	O
the	O
entropy	O
-	O
based	O
uncertainty	O
criterion	O
,	O
and	O
ignore	O
the	O
characteristics	O
of	O
natural	O
language	O
.	O

Active	O
learning	O
is	O
able	O
to	O
significantly	O
reduce	O
the	O
annotation	O
cost	O
for	O
data	O
-	O
driven	O
techniques	O
.	O

Pre	B-MethodName
-	I-MethodName
trained	I-MethodName
Language	I-MethodName
Model	I-MethodName
Based	I-MethodName
Active	I-MethodName
Learning	I-MethodName
for	O
Sentence	B-TaskName
Matching	I-TaskName
.	O

Next	O
,	O
we	O
find	O
our	O
proposed	O
method	O
outperforms	O
sentence	O
vector	O
based	O
methods	O
(	O
Topic	O
,	O
AE	O
,	O
and	O
Skip	O
)	O
.	O

In	O
summary	O
,	O
we	O
can	O
report	O
that	O
ensemble	B-MethodName
methods	I-MethodName
turned	O
out	O
to	O
be	O
fruitful	O
when	O
applied	O
to	O
CompLex	B-DatasetName
.	O

Our	O
systems	O
ware	O
ranked	B-MetricName
15/54	B-MetricValue
and	O
19/37	B-MetricValue
during	O
shared	O
task	O
evaluations	O
according	O
to	O
Pearson	B-MetricName
's	I-MetricName
correlation	I-MetricName
coefficient	I-MetricName
.	O

We	O
presented	O
both	O
our	O
systems	O
submitted	O
to	O
SemEval-2021	O
Task	O
1	O
combining	O
a	O
heterogeneous	B-MethodName
feature	I-MethodName
set	I-MethodName
with	I-MethodName
gradient	I-MethodName
boosting	I-MethodName
as	O
regression	O
algorithm	O
.	O

In	O
order	O
to	O
compare	O
the	O
predicted	O
values	O
of	O
our	O
models	O
to	O
the	O
ground	O
truth	O
data	O
,	O
we	O
scatterplotted	O
the	O
relationship	O
between	O
ground	O
truth	O
labels	O
and	O
the	O
scores	O
predicted	O
by	O
our	O
systems	O
(	O
see	O
Figures	O
1	O
and	O
2	O
)	O
using	O
the	O
CompLex	B-DatasetName
evaluation	O
set	O
.	O

This	O
could	O
be	O
explained	O
by	O
the	O
fact	O
that	O
most	O
multiword	O
expressions	O
within	O
the	O
CompLex	B-DatasetName
corpus	O
follow	O
the	O
structure	O
of	O
a	O
semantic	O
head	O
in	O
combination	O
with	O
a	O
modifier	O
as	O
most	O
of	O
them	O
are	O
either	O
multi	O
token	O
compounds	O
or	O
single	O
token	O
nouns	O
modified	O
by	O
adjectives	O
.	O

In	O
the	O
case	O
of	O
our	O
model	O
dealing	O
with	O
multiword	O
expressions	O
,	O
the	O
ten	O
most	O
important	O
features	O
all	O
stem	O
from	O
the	O
flair	B-MethodName
-	I-MethodName
mix	I-MethodName
-	I-MethodName
backward	I-MethodName
embedding	O
of	O
the	O
second	O
word	O
.	O

A	O
few	O
single	O
dimension	O
from	O
the	O
embeddings	O
provided	O
by	O
flair	B-MethodName
-	I-MethodName
mix	I-MethodName
-	I-MethodName
backward	I-MethodName
seem	O
to	O
play	O
the	O
major	O
role	O
here	O
.	O

Within	O
this	O
category	O
,	O
the	O
most	O
dominant	O
features	O
for	O
both	O
models	O
came	O
from	O
the	O
flair	B-MethodName
-	I-MethodName
mix	I-MethodName
-	I-MethodName
backward	I-MethodName
and	O
flair	B-MethodName
-	I-MethodName
mix	I-MethodName
-	I-MethodName
forward	I-MethodName
models	O
(	O
see	O
Tables	O
3	O
and	O
4	O
)	O
.	O

Inspecting	O
the	O
results	O
of	O
these	O
calculations	O
,	O
we	O
noticed	O
that	O
our	O
systems	O
did	O
not	O
use	O
the	O
character	O
bigram	O
frequencies	O
derived	O
from	O
the	O
Google	B-DatasetName
Books	I-DatasetName
Corpus	I-DatasetName
,	O
nor	O
the	O
frequencies	O
from	O
EFLLex	B-DatasetName
or	O
the	O
word	O
list	O
inclusion	O
features	O
.	O

Feature	O
importance	O
was	O
calculated	O
using	O
the	O
evaluation	O
set	O
of	O
CompLex	B-DatasetName
.	O

To	O
determine	O
which	O
features	O
were	O
used	O
by	O
our	O
models	O
to	O
predict	O
lexical	O
complexity	O
,	O
we	O
rely	O
on	O
the	O
functionality	O
provided	O
by	O
CatBoost	B-MethodName
which	O
scores	O
each	O
feature	O
for	O
its	O
influence	O
on	O
a	O
given	O
final	O
prediction	O
.	O

According	O
to	O
this	O
,	O
our	O
systems	O
achieved	O
the	O
15th	B-MetricValue
and	O
19th	B-MetricValue
rank	B-MetricName
respectively	O
.	O

Throughout	O
the	O
shared	O
task	O
,	O
the	O
systems	O
were	O
evaluated	O
with	O
regard	O
to	O
Pearson	B-MetricName
's	I-MetricName
correlation	I-MetricName
coefficient	I-MetricName
,	O
Spearman	B-MetricName
's	I-MetricName
rank	I-MetricName
correlation	I-MetricName
coefficient	I-MetricName
,	O
mean	B-MetricName
average	I-MetricName
error	I-MetricName
,	O
mean	B-MetricName
squared	I-MetricName
error	I-MetricName
and	O
R2	B-MetricName
with	O
Pearson	B-MetricName
's	I-MetricName
correlation	I-MetricName
coefficient	I-MetricName
determining	O
the	O
main	O
ranking	O
.	O

The	O
other	O
one	O
was	O
the	O
Academic	B-DatasetName
Word	I-DatasetName
List	I-DatasetName
as	O
described	O
in	O
Coxhead	B-DatasetName
(	O
2011	O
)	O
,	O
a	O
structured	O
lexicon	O
of	O
terms	O
used	O
primarily	O
in	O
academic	O
discourse	O
which	O
we	O
believed	O
to	O
contain	O
more	O
complex	O
words	O
.	O

Here	O
,	O
our	O
idea	O
was	O
that	O
this	O
could	O
help	O
to	O
identify	O
simple	O
words	O
within	O
CompLex	B-DatasetName
.	O

The	O
first	O
one	O
is	O
Ogden	B-DatasetName
's	I-DatasetName
Basic	I-DatasetName
English	I-DatasetName
Vocabulary	I-DatasetName
10	O
,	O
a	O
list	O
of	O
simple	O
words	O
used	O
for	O
writing	O
simple	O
English	O
as	O
described	O
in	O
Ogden	O
(	O
1932	O
)	O
.	O

We	O
included	O
this	O
set	O
as	O
we	O
deemed	O
that	O
CEFR	B-MethodName
as	O
a	O
framework	O
for	O
rating	O
language	O
competence	O
could	O
also	O
function	O
as	O
an	O
according	O
proxy	O
.	O

The	O
third	O
set	O
we	O
used	O
was	O
EFLLex	B-DatasetName
(	O
Dürlich	O
and	O
Franc	O
¸ois	O
,	O
2018	O
)	O
which	O
lists	O
the	O
frequencies	O
of	O
words	O
within	O
several	O
pieces	O
of	O
English	O
literature	O
appropriate	O
for	O
different	O
CEFR	O
9	O
levels	O
.	O

Besides	O
SUBTLEXus	B-DatasetName
,	O
we	O
utilised	O
the	O
character	O
bigram	O
frequencies	O
from	O
Norvig	B-DatasetName
(	O
2013	O
)	O
which	O
were	O
extracted	O
from	O
the	O
Google	B-DatasetName
Books	I-DatasetName
Corpus	I-DatasetName
.	O

The	O
first	O
of	O
these	O
data	O
sets	O
was	O
the	O
frequency	O
list	O
extracted	O
from	O
the	O
SUB	B-DatasetName
-	I-DatasetName
TLEXus	I-DatasetName
corpus	I-DatasetName
(	O
Brysbaert	O
and	O
New	O
,	O
2009	O
)	O
consisting	O
of	O
various	O
movie	O
subtitles	O
from	O
which	O
we	O
used	O
the	O
log	O
-	O
normalised	O
term	O
frequency	O
and	O
the	O
log	O
-	O
normalised	O
document	O
frequency	O
as	O
features	O
.	O

As	O
we	O
switched	O
to	O
using	O
gradient	B-MethodName
boosting	I-MethodName
for	O
our	O
final	O
systems	O
,	O
we	O
decided	O
to	O
use	O
the	O
fine	O
-	O
tuned	O
variants	O
of	O
the	O
transformer	O
embedding	O
models	O
as	O
using	O
them	O
led	O
to	O
small	O
improvements	O
when	O
testing	O
our	O
models	O
on	O
the	O
shared	O
task	O
trial	O
sets	O
compared	O
to	O
using	O
the	O
non	O
-	O
fine	O
-	O
tuned	O
variants	O
.	O

While	O
we	O
deemed	O
this	O
an	O
okay	O
result	O
,	O
we	O
decided	O
to	O
stick	O
with	O
gradient	B-MethodName
boosting	I-MethodName
for	O
our	O
final	O
systems	O
as	O
early	O
experiments	O
with	O
this	O
algorithm	O
yielded	O
results	O
superior	O
to	O
the	O
purely	O
neural	O
approach	O
when	O
evaluated	O
on	O
the	O
same	O
set	O
.	O

This	O
model	O
achieved	O
a	O
Pearson	B-MetricName
's	I-MetricName
correlation	I-MetricName
coefficient	I-MetricName
score	O
of	O
0.7103	B-MetricValue
when	O
evaluated	O
on	O
the	O
trial	O
set	O
.	O

This	O
network	O
was	O
then	O
trained	O
for	O
5	B-HyperparameterValue
epochs	B-HyperparameterName
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.000001	B-HyperparameterValue
,	O
mean	B-HyperparameterValue
squared	I-HyperparameterValue
error	I-HyperparameterValue
as	O
loss	B-HyperparameterName
function	I-HyperparameterName
and	O
Adam	B-HyperparameterValue
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
as	O
optimizer	B-HyperparameterName
on	O
the	O
training	O
set	O
part	O
of	O
CompLex	B-DatasetName
.	O

This	O
collection	O
of	O
embeddings	O
was	O
derived	O
from	O
previous	O
experiments	O
on	O
the	O
CompLex	B-DatasetName
corpus	O
where	O
we	O
tried	O
to	O
fine	O
-	O
tune	O
a	O
purely	O
neural	O
model	O
using	O
the	O
approach	O
of	O
stacking	O
different	O
embedding	O
models	O
in	O
combination	O
with	O
an	O
attached	O
prediction	O
head	O
central	O
to	O
flairNLP	O
8	O
(	O
Akbik	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

This	O
includes	O
the	O
transformer	O
-	O
based	O
(	O
Devlin	O
et	O
al	O
.	O
,	O
2019	O
)	O
BiomedNLP	B-MethodName
-	I-MethodName
PubMedBERTbase	I-MethodName
-	I-MethodName
uncased	I-MethodName
-	I-MethodName
abstract	I-MethodName
(	O
Gu	O
et	O
al	O
.	O
,	O
2020	O
)	O
,	O
distilgpt2	B-MethodName
4	O
(	O
Radford	O
et	O
al	O
.	O
,	O
2018	O
)	O
and	O
distilbert	B-MethodName
-	I-MethodName
base	I-MethodName
-	I-MethodName
uncased	I-MethodName
(	O
Sanh	O
et	O
al	O
.	O
,	O
2019	O
)	O
,	O
the	O
contextual	O
string	O
embed	O
-	O
ding	O
models	O
mix	B-MethodName
-	I-MethodName
forward	I-MethodName
and	O
mix	B-MethodName
-	I-MethodName
backward	I-MethodName
5	O
(	O
Akbik	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
and	O
the	O
static	O
GloVe	B-MethodName
6	O
(	O
Pennington	O
et	O
al	O
.	O
,	O
2014	O
)	O
and	O
English	O
fastText	B-MethodName
7	O
(	O
Bojanowski	O
et	O
al	O
.	O
,	O
2017	O
)	O
embeddings	O
.	O

All	O
of	O
these	O
features	O
are	O
encoded	O
as	O
one-	O
,	O
respectively	O
n	O
-	O
hot	O
vectors	O
using	O
the	O
LabelBinarizer	B-MethodName
and	O
Mul	B-MethodName
-	I-MethodName
tiLabelBinarizer	I-MethodName
classes	O
provided	O
by	O
Scikit	O
-	O
learn	O
(	O
Pedregosa	O
et	O
al	O
.	O
,	O
2011	O
)	O
.	O

Syntactic	O
features	O
:	O
This	O
category	O
of	O
features	O
includes	O
XPOS-	O
,	O
UPOS-	O
,	O
dependencyand	O
named	O
entity	O
tags	O
as	O
well	O
as	O
universal	O
features	O
2	O
inferred	O
using	O
the	O
English	B-MethodName
Stanza	I-MethodName
3	I-MethodName
(	O
Qi	O
et	O
al	O
.	O
,	O
2020	O
)	O
model	O
fit	O
to	O
the	O
version	O
of	O
the	O
English	O
Web	O
Treebank	O
following	O
the	O
Universal	O
Dependencies	O
formalism	O
(	O
Silveira	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

While	O
the	O
problem	O
presented	O
in	O
their	O
paper	O
is	O
formulated	O
as	O
a	O
binary	O
classification	O
task	O
using	O
different	O
data	O
sets	O
,	O
we	O
wanted	O
to	O
test	O
if	O
their	O
findings	O
would	O
still	O
translate	O
to	O
a	O
regression	O
task	O
on	O
CompLex	B-DatasetName
.	O

(	O
2018	O
)	O
that	O
ensemble	B-MethodName
-	I-MethodName
based	I-MethodName
learners	I-MethodName
perform	O
best	O
for	O
complex	O
word	O
identification	O
contributed	O
to	O
this	O
decision	O
,	O
as	O
well	O
.	O

Additionally	O
,	O
we	O
set	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
maximum	I-HyperparameterName
iterations	I-HyperparameterName
to	O
5000	B-HyperparameterValue
and	O
then	O
used	O
the	O
trial	O
set	O
to	O
perform	O
early	O
stopping	O
during	O
training	O
in	O
order	O
to	O
determine	O
the	O
exact	O
number	O
of	O
required	O
iterations	O
.	O

We	O
set	O
the	O
growing	B-HyperparameterName
policy	I-HyperparameterName
to	O
lossguide	B-HyperparameterValue
,	O
the	O
L2	B-HyperparameterName
leaf	I-HyperparameterName
regularisation	I-HyperparameterName
to	O
15	B-HyperparameterValue
,	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
to	I-HyperparameterName
0.01	B-HyperparameterValue
,	O
tree	B-HyperparameterName
depth	I-HyperparameterName
to	O
6	B-HyperparameterValue
and	O
the	O
maximum	B-HyperparameterName
number	I-HyperparameterName
of	I-HyperparameterName
leaves	I-HyperparameterName
to	O
15	B-HyperparameterValue
.	O

Our	O
models	O
are	O
based	O
on	O
the	O
implementation	O
of	O
gradient	O
boosting	O
provided	O
by	O
CatBoost	B-MethodName
1	O
(	O
Dorogush	O
et	O
al	O
.	O
,	O
2018;Prokhorenkova	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

We	O
trained	O
one	O
model	O
to	O
predict	B-TaskName
single	I-TaskName
word	I-TaskName
lexical	I-TaskName
complexity	I-TaskName
scores	O
and	O
another	O
one	O
to	O
predict	B-TaskName
bigram	I-TaskName
multiword	I-TaskName
expression	I-TaskName
complexity	I-TaskName
scores	O
.	O

Our	O
systems	O
rely	O
on	O
gradient	B-MethodName
-	I-MethodName
boosted	I-MethodName
regression	I-MethodName
tree	I-MethodName
ensembles	I-MethodName
(	O
Mason	O
et	O
al	O
.	O
,	O
1999	O
)	O
for	O
predicting	B-TaskName
lexical	I-TaskName
complexity	I-TaskName
scores	O
.	O

This	O
shared	O
task	O
focused	O
on	O
predicting	B-TaskName
lexical	I-TaskName
complexity	I-TaskName
for	O
English	O
,	O
German	O
,	O
Spanish	O
and	O
a	O
multi	O
-	O
lingual	O
data	O
set	O
with	O
a	O
French	O
test	O
set	O
.	O

The	O
most	O
important	O
findings	O
for	O
this	O
shared	O
task	O
were	O
that	O
ensemble	B-MethodName
methods	I-MethodName
performed	O
best	O
in	O
predicting	B-TaskName
lexical	I-TaskName
complexity	I-TaskName
with	O
word	O
frequency	O
being	O
the	O
best	O
indicator	O
.	O

CompLex	B-DatasetName
is	O
divided	O
into	O
two	O
sub	O
-	O
corpora	O
,	O
one	O
dealing	O
with	O
the	O
complexity	O
of	O
single	O
words	O
and	O
the	O
other	O
one	O
with	O
the	O
complexity	O
of	O
bigram	O
multiword	O
expressions	O
.	O

For	O
the	O
shared	O
task	O
,	O
CompLex	B-DatasetName
corpus	I-DatasetName
(	O
Shardlow	O
et	O
al	O
.	O
,	O
2020(Shardlow	O
et	O
al	O
.	O
,	O
,	O
2021b	O
)	O
)	O
was	O
used	O
as	O
data	O
set	O
.	O

The	O
code	O
and	O
our	O
full	O
results	O
can	O
be	O
found	O
at	O
https://github.com/SGombert/	O
tudacclsemeval	O
.	O

Out	O
of	O
all	O
participants	O
,	O
our	O
systems	O
were	O
ranked	B-MetricName
15/54	B-MetricValue
in	O
the	O
single	O
word	O
-	O
and	O
19/37	B-MetricValue
in	O
the	O
multiword	O
category	O
during	O
the	O
official	O
shared	O
task	O
evaluations	O
according	O
to	O
Pearson	B-MetricName
's	I-MetricName
correlation	I-MetricName
coefficient	I-MetricName
.	O

We	O
assumed	O
that	O
lexical	O
complexity	O
could	O
be	O
correlated	O
with	O
a	O
wide	O
range	O
of	O
features	O
,	O
neural	O
ones	O
as	O
much	O
as	O
distributional	O
or	O
psycholinguistic	O
ones	O
,	O
which	O
is	O
why	O
we	O
chose	O
to	O
use	O
an	O
ensemble	B-MethodName
-	I-MethodName
based	I-MethodName
method	O
in	O
the	O
form	O
of	O
gradient	B-MethodName
boosting	I-MethodName
(	O
Mason	O
et	O
al	O
.	O
,	O
1999	O
)	O
for	O
our	O
system	O
as	O
it	O
usually	O
performs	O
best	O
for	O
tasks	O
where	O
such	O
a	O
feature	O
set	O
is	O
needed	O
compared	O
to	O
solely	O
neural	O
models	O
which	O
need	O
dense	O
,	O
homogeneous	O
input	O
data	O
to	O
perform	O
well	O
.	O

Our	O
approach	O
to	O
solve	O
this	O
problem	O
relies	O
on	O
gradient	B-MethodName
-	I-MethodName
boosted	I-MethodName
regression	I-MethodName
tree	I-MethodName
ensembles	I-MethodName
which	O
we	O
fit	O
on	O
a	O
heterogeneous	O
feature	O
set	O
including	O
different	O
word	O
embedding	O
models	O
,	O
linguistic	O
features	O
,	O
WordNet	O
features	O
,	O
psycholinguistic	O
lexica	O
,	O
corpus	O
-	O
based	O
word	O
frequencies	O
and	O
word	O
lists	O
.	O

The	O
term	O
lexical	B-TaskName
complexity	I-TaskName
prediction	I-TaskName
describes	O
the	O
task	O
of	O
assigning	O
a	O
word	O
or	O
multiword	O
expression	O
a	O
continuous	O
or	O
discrete	O
score	O
signifying	O
its	O
likeliness	O
of	O
being	O
understood	O
well	O
within	O
a	O
given	O
context	O
,	O
especially	O
by	O
a	O
non	O
-	O
native	O
speaker	O
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
our	O
contribution	O
to	O
SemEval-2021	O
Shared	O
Task	O
1	O
(	O
Shardlow	O
et	O
al	O
.	O
,	O
2021a	O
)	O
,	O
a	O
shared	O
task	O
focused	O
on	O
the	O
topic	O
of	O
lexical	B-TaskName
complexity	I-TaskName
prediction	I-TaskName
.	O

We	O
can	O
show	O
that	O
especially	O
contextualised	O
string	O
embeddings	O
(	O
Akbik	O
et	O
al	O
.	O
,	O
2018	O
)	O
can	O
help	O
with	O
predicting	B-TaskName
lexical	I-TaskName
complexity	I-TaskName
.	O

Our	O
approach	O
relies	O
on	O
gradient	B-MethodName
boosted	I-MethodName
regression	I-MethodName
tree	I-MethodName
ensembles	I-MethodName
fitted	O
using	O
a	O
heterogeneous	O
feature	O
set	O
combining	O
linguistic	O
features	O
,	O
static	O
and	O
contextualized	O
word	O
embeddings	O
,	O
psycholinguistic	O
norm	O
lexica	O
,	O
WordNet	O
,	O
word	O
-	O
and	O
character	O
bigram	O
frequencies	O
and	O
inclusion	O
in	O
word	O
lists	O
to	O
create	O
a	O
model	O
able	O
to	O
assign	O
a	O
word	O
or	O
multiword	O
expression	O
a	O
context	O
-	O
dependent	O
complexity	B-MetricName
score	O
.	O

The	O
aim	O
of	O
this	O
shared	O
task	O
was	O
to	O
create	O
systems	O
able	O
to	O
predict	B-TaskName
the	I-TaskName
lexical	I-TaskName
complexity	I-TaskName
of	O
word	O
tokens	O
and	O
bigram	O
multiword	O
expressions	O
within	O
a	O
given	O
sentence	O
context	O
,	O
a	O
continuous	O
value	O
indicating	O
the	O
difficulty	O
in	O
understanding	O
a	O
respective	O
utterance	O
.	O

TUDA	O
-	O
CCL	O
at	O
SemEval-2021	O
Task	O
1	O
:	O
Using	O
Gradient	B-MethodName
-	I-MethodName
boosted	I-MethodName
Regression	I-MethodName
Tree	I-MethodName
Ensembles	I-MethodName
Trained	O
on	O
a	O
Heterogeneous	O
Feature	O
Set	O
for	O
Predicting	B-TaskName
Lexical	I-TaskName
Complexity	I-TaskName
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
our	O
systems	O
submitted	O
to	O
SemEval-2021	O
Task	O
1	O
on	O
lexical	B-TaskName
complexity	I-TaskName
prediction	I-TaskName
(	O
Shardlow	O
et	O
al	O
.	O
,	O
2021a	O
)	O
.	O

Moreover	O
,	O
our	O
systems	O
rarely	O
assign	O
scores	O
below	O
0.2	O
.	O
It	O
must	O
be	O
explored	O
further	O
if	O
there	O
are	O
features	O
which	O
could	O
improve	O
our	O
systems	O
in	O
this	O
respect	O
.	O

We	O
attribute	O
this	O
to	O
a	O
relationship	O
between	O
lexical	O
complexity	O
and	O
the	O
distribution	O
of	O
characters	O
throughout	O
words	O
and	O
sentences	O
,	O
but	O
this	O
needs	O
further	O
clarification	O
which	O
could	O
be	O
the	O
objective	O
of	O
future	O
work	O
.	O

The	O
type	O
of	O
feature	O
playing	O
the	O
most	O
important	O
role	O
for	O
our	O
models	O
are	O
contextual	O
string	O
embeddings	O
as	O
they	O
influenced	O
the	O
outcome	O
the	O
most	O
.	O

However	O
,	O
the	O
results	O
achieved	O
by	O
our	O
systems	O
were	O
still	O
close	O
to	O
the	O
best	O
results	O
,	O
especially	O
in	O
the	O
case	O
of	O
the	O
system	O
dealing	O
with	O
single	O
word	O
complexity	O
.	O

Conclusion	O
.	O

This	O
indicates	O
that	O
our	O
feature	O
set	O
does	O
not	O
contain	O
features	O
which	O
could	O
help	O
our	O
models	O
to	O
identify	O
very	O
simple	O
words	O
.	O

The	O
system	O
dealing	O
with	O
multiword	O
expressions	O
does	O
not	O
assign	O
any	O
value	O
below	O
0.2	O
at	O
all	O
and	O
the	O
one	O
dealing	O
with	O
single	O
word	O
complexity	O
rarely	O
does	O
so	O
.	O

It	O
can	O
be	O
observed	O
that	O
both	O
systems	O
,	O
especially	O
the	O
one	O
dealing	O
with	O
single	O
word	O
complexity	O
,	O
show	O
the	O
tendency	O
to	O
assign	O
slightly	O
higher	O
scores	O
than	O
given	O
in	O
the	O
ground	O
truth	O
for	O
simple	O
words	O
and	O
slightly	O
lower	O
scores	O
for	O
complex	O
words	O
.	O

Ground	O
Truth	O
.	O

Predictions	O
vs.	O

However	O
,	O
without	O
further	O
research	O
,	O
this	O
currently	O
remains	O
pure	O
speculation	O
.	O

This	O
links	O
each	O
input	O
dimension	O
also	O
to	O
a	O
larger	O
variety	O
of	O
latently	O
encoded	O
distributional	O
knowledge	O
which	O
could	O
then	O
contain	O
certain	O
regularities	O
strongly	O
correlated	O
with	O
lexical	O
complexity	O
.	O

As	O
a	O
consequence	O
,	O
such	O
models	O
use	O
fewer	O
input	O
dimensions	O
and	O
each	O
of	O
the	O
dimensions	O
present	O
is	O
in	O
turn	O
involved	O
in	O
the	O
encoding	O
of	O
more	O
different	O
words	O
.	O

While	O
the	O
exact	O
reason	O
for	O
the	O
strong	O
influence	O
of	O
the	O
contextualised	O
string	O
embeddings	O
is	O
hard	O
to	O
determine	O
due	O
to	O
the	O
fact	O
that	O
embeddings	O
lack	O
the	O
property	O
of	O
being	O
easily	O
interpretable	O
,	O
we	O
assume	O
that	O
the	O
dominant	O
role	O
they	O
play	O
for	O
the	O
results	O
could	O
be	O
determined	O
by	O
them	O
being	O
calculated	O
on	O
the	O
character	O
level	O
(	O
Akbik	O
et	O
al	O
.	O
,	O
2018	O
)	O
instead	O
of	O
the	O
level	O
of	O
fixed	O
words	O
or	O
subword	O
units	O
such	O
as	O
morphemes	O
.	O

Each	O
entry	O
refers	O
to	O
a	O
single	O
dimension	O
of	O
the	O
feature	O
vector	O
.	O

Table	O
4	O
:	O
The	O
10	O
most	O
important	O
features	O
observed	O
for	O
our	O
system	O
dealing	O
with	O
multiword	O
expression	O
complexity	O
and	O
their	O
categories	O
.	O

It	O
is	O
intuitive	O
from	O
a	O
linguistic	O
point	O
of	O
view	O
that	O
in	O
such	O
cases	O
,	O
the	O
semantic	O
head	O
,	O
which	O
comes	O
as	O
second	O
element	O
,	O
should	O
play	O
the	O
dominant	O
semantic	O
role	O
resulting	O
in	O
it	O
being	O
more	O
influential	O
in	O
the	O
overall	O
results	O
.	O

While	O
features	O
from	O
all	O
other	O
categories	O
were	O
utilised	O
,	O
the	O
most	O
dominant	O
features	O
by	O
far	O
are	O
contained	O
in	O
the	O
word	O
embedding	O
category	O
.	O

The	O
outputs	O
of	O
this	O
method	O
are	O
normalised	O
so	O
that	O
the	O
sum	O
of	O
the	O
importance	O
values	O
of	O
all	O
features	O
equals	O
100	O
.	O

This	O
is	O
achieved	O
by	O
changing	O
a	O
respective	O
feature	O
values	O
and	O
observing	O
the	O
resulting	O
change	O
in	O
the	O
model	O
prediction	O
(	O
see	O
11	O
for	O
further	O
information	O
on	O
the	O
exact	O
method	O
)	O
.	O

Most	O
Important	O
Features	O
.	O

(	O
2021a	O
)	O
.	O

The	O
full	O
results	O
for	O
all	O
submitted	O
systems	O
are	O
presented	O
in	O
Shardlow	O
et	O
al	O
.	O

Further	O
hyperparameter	O
tuning	O
and	O
the	O
addition	O
of	O
more	O
features	O
could	O
likely	O
close	O
this	O
gap	O
.	O

The	O
results	O
show	O
that	O
our	O
systems	O
,	O
while	O
only	O
achieving	O
upper	O
mid	O
-	O
table	O
results	O
on	O
average	O
,	O
come	O
close	O
to	O
the	O
best	O
systems	O
performance	O
-	O
wise	O
which	O
speaks	O
for	O
our	O
approach	O
.	O

Table	O
1	O
shows	O
the	O
results	O
achieved	O
by	O
our	O
system	O
dealing	O
with	O
single	O
words	O
and	O
Table	O
2	O
the	O
results	O
achieved	O
by	O
our	O
system	O
dealing	O
with	O
multiword	O
expressions	O
.	O

Results	O
.	O

Metric	O
.	O

In	O
both	O
cases	O
,	O
we	O
encoded	O
the	O
inclusion	O
of	O
a	O
word	O
within	O
a	O
respective	O
word	O
list	O
binarily	O
.	O

Word	O
Lists	O
:	O
We	O
used	O
two	O
different	O
word	O
lists	O
as	O
features	O
.	O

In	O
the	O
case	O
of	O
both	O
sets	O
,	O
our	O
intuition	O
was	O
that	O
lower	O
frequency	O
would	O
likely	O
function	O
as	O
a	O
proxy	O
for	O
complexity	O
.	O

Here	O
,	O
to	O
represent	O
a	O
word	O
,	O
we	O
calculated	O
the	O
mean	O
of	O
all	O
frequencies	O
of	O
the	O
bigrams	O
consituting	O
the	O
same	O
and	O
used	O
this	O
as	O
feature	O
.	O

Word	O
frequencies	O
:	O
We	O
utilised	O
three	O
resources	O
containing	O
corpus	O
-	O
based	O
word	O
respectively	O
character	O
bigram	O
frequencies	O
.	O

In	O
both	O
cases	O
,	O
the	O
inclusion	O
of	O
these	O
features	O
was	O
mainly	O
motivated	O
by	O
our	O
general	O
intuition	O
that	O
the	O
perceived	O
complexity	O
of	O
words	O
could	O
be	O
linked	O
to	O
different	O
psycholinguistic	O
variables	O
.	O

The	O
ratings	O
within	O
this	O
lexicon	O
were	O
derived	O
algorithmically	O
from	O
smaller	O
lexicons	O
using	O
linear	O
combinations	O
and	O
semantic	O
similarity	O
scores	O
to	O
approximate	O
the	O
ratings	O
for	O
words	O
not	O
included	O
in	O
the	O
source	O
lexica	O
.	O

The	O
second	O
lexicon	O
is	O
described	O
in	O
Malandrakis	O
and	O
Narayanan	O
(	O
2015	O
)	O
and	O
includes	O
ratings	O
for	O
arousal	O
,	O
dominance	O
,	O
valence	O
,	O
pleasantness	O
,	O
concreteness	O
,	O
imagability	O
,	O
age	O
of	O
acquisition	O
,	O
familarity	O
,	O
pronouncability	O
,	O
context	O
availability	O
and	O
gender	O
ladenness	O
.	O

These	O
ratings	O
were	O
acquired	O
from	O
annotators	O
on	O
the	O
Amazon	O
Mechanical	O
Turk	O
platform	O
.	O

(	O
2013	O
)	O
and	O
scores	O
words	O
with	O
empirical	O
ratings	O
for	O
pleasantness	O
,	O
arousal	O
and	O
dominance	O
using	O
the	O
SAM	O
score	O
(	O
Bradley	O
and	O
Lang	O
,	O
1994	O
)	O
.	O

The	O
first	O
one	O
is	O
described	O
in	O
Warriner	O
et	O
al	O
.	O

Psycholinguistic	O
norm	O
lexica	O
:	O
Our	O
feature	O
set	O
includes	O
two	O
psycholinguistic	O
norm	O
lexica	O
.	O

During	O
this	O
training	O
,	O
fine	O
-	O
tuning	O
was	O
active	O
for	O
all	O
transformer	O
-	O
based	O
language	O
models	O
so	O
that	O
their	O
weights	O
were	O
adjusted	O
during	O
the	O
process	O
and	O
scalar	O
mixing	O
(	O
Liu	O
et	O
al	O
.	O
,	O
2019	O
)	O
was	O
used	O
for	O
the	O
transformer	O
-	O
based	O
language	O
models	O
as	O
it	O
was	O
not	O
foreseeable	O
which	O
layers	O
of	O
the	O
transformer	O
models	O
would	O
influence	O
results	O
the	O
most	O
.	O

More	O
precisely	O
,	O
in	O
the	O
setup	O
we	O
chose	O
,	O
the	O
outputs	O
of	O
all	O
language	O
models	O
were	O
fed	O
to	O
a	O
feed	O
-	O
forward	O
layer	O
responsible	O
for	O
calculating	O
the	O
final	O
complexity	O
scores	O
.	O

Word	O
embeddings	O
:	O
We	O
used	O
multiple	O
static	O
and	O
contextual	O
word	O
embedding	O
models	O
for	O
our	O
feature	O
set	O
.	O

The	O
main	O
intuition	O
behind	O
using	O
this	O
resource	O
was	O
that	O
the	O
length	O
of	O
the	O
shortest	O
hypernym	O
path	O
and	O
the	O
count	O
for	O
the	O
different	O
lexico	O
-	O
semantic	O
relations	O
could	O
be	O
a	O
good	O
indicator	O
for	O
lexical	O
complexity	O
.	O

We	O
accessed	O
WordNet	O
using	O
NLTK	O
(	O
Bird	O
et	O
al	O
.	O
,	O
2009	O
)	O
.	O

In	O
cases	O
where	O
multiple	O
synsets	O
were	O
given	O
for	O
a	O
word	O
,	O
we	O
calculated	O
the	O
respective	O
means	O
and	O
in	O
cases	O
where	O
a	O
given	O
word	O
was	O
not	O
included	O
in	O
the	O
resource	O
,	O
we	O
set	O
all	O
respective	O
feature	O
values	O
to	O
0	O
.	O

WordNet	O
features	O
:	O
Here	O
,	O
we	O
included	O
the	O
numbers	O
of	O
hypernyms	O
,	O
root	O
hypernyms	O
,	O
hyponyms	O
,	O
member	O
holonyms	O
,	O
part	O
meronyms	O
and	O
member	O
meronyms	O
of	O
the	O
respective	O
word(s	O
)	O
as	O
well	O
as	O
the	O
number	O
of	O
given	O
examples	O
and	O
the	O
length	O
of	O
the	O
shortest	O
hypernym	O
path	O
from	O
WordNet	O
(	O
Miller	O
,	O
1995	O
)	O
.	O

In	O
addition	O
to	O
the	O
tags	O
assigned	O
to	O
the	O
word(s	O
)	O
whose	O
score	O
was	O
to	O
be	O
predicted	O
,	O
we	O
included	O
the	O
XPOSand	O
UPOS	O
tags	O
of	O
the	O
two	O
neighbouring	O
words	O
to	O
the	O
left	O
and	O
to	O
the	O
right	O
as	O
well	O
as	O
the	O
dependency	O
tags	O
of	O
the	O
siblings	O
,	O
direct	O
children	O
and	O
the	O
parent	O
of	O
the	O
word(s	O
)	O
within	O
the	O
dependency	O
structure	O
of	O
a	O
given	O
sentence	O
.	O

Thus	O
,	O
the	O
exact	O
number	O
of	O
input	O
features	O
was	O
7424	O
for	O
our	O
system	O
dealing	O
with	O
single	O
words	O
and	O
14848	O
for	O
our	O
system	O
dealing	O
with	O
multiword	O
expressions	O
.	O

In	O
case	O
of	O
our	O
system	O
dealing	O
with	O
bigram	O
multiword	O
expressions	O
,	O
we	O
calculated	O
such	O
a	O
vector	O
for	O
each	O
of	O
both	O
words	O
and	O
then	O
concatenated	O
them	O
to	O
acquire	O
the	O
final	O
input	O
vectors	O
.	O

The	O
following	O
paragraphs	O
describe	O
the	O
features	O
we	O
used	O
to	O
create	O
the	O
feature	O
vectors	O
used	O
to	O
represent	O
words	O
.	O

Feature	O
Engineering	O
.	O

Moreover	O
,	O
the	O
reportings	O
of	O
Paetzold	O
and	O
Specia	O
(	O
2016	O
)	O
and	O
Yimam	O
et	O
al	O
.	O

The	O
motivation	O
behind	O
using	O
this	O
algorithm	O
was	O
its	O
general	O
ability	O
to	O
perform	O
well	O
on	O
heterogeneous	O
and	O
sparse	O
feature	O
sets	O
which	O
allowed	O
us	O
to	O
mix	O
regular	O
linguistic	O
features	O
,	O
WordNet	O
features	O
,	O
word	O
embeddings	O
,	O
psycho	O
-	O
linguistic	O
norm	O
lexica	O
,	O
corpusbased	O
word	O
frequencies	O
and	O
selected	O
word	O
lists	O
as	O
all	O
of	O
these	O
were	O
features	O
we	O
assumed	O
to	O
possibly	O
correlate	O
with	O
lexical	O
complexity	O
.	O

System	O
Overview	O
.	O

The	O
findings	O
of	O
this	O
shared	O
task	O
confirmed	O
the	O
finding	O
of	O
the	O
previous	O
one	O
that	O
using	O
ensemble	O
methods	O
yield	O
best	O
results	O
for	O
complex	O
word	O
identification	O
with	O
a	O
system	O
submitted	O
by	O
Gooding	O
and	O
Kochmar	O
(	O
2018	O
)	O
relying	O
on	O
decision	O
tree	O
ensembles	O
.	O

The	O
data	O
for	O
this	O
was	O
acquired	O
by	O
presenting	O
annotators	O
on	O
Amazon	O
Mechanical	O
Turk	O
with	O
paragraphs	O
of	O
text	O
and	O
letting	O
them	O
mark	O
words	O
which	O
according	O
to	O
their	O
perception	O
could	O
hinder	O
the	O
same	O
paragraph	O
from	O
being	O
understood	O
by	O
a	O
less	O
proficient	O
reader	O
.	O

(	O
2018	O
)	O
.	O

In	O
2018	O
,	O
a	O
second	O
shared	O
task	O
was	O
conducted	O
on	O
the	O
same	O
topic	O
as	O
described	O
in	O
Yimam	O
et	O
al	O
.	O

In	O
the	O
first	O
one	O
,	O
a	O
word	O
was	O
considered	O
complex	O
if	O
at	O
least	O
one	O
of	O
the	O
annotators	O
had	O
judged	O
it	O
as	O
such	O
,	O
and	O
in	O
the	O
second	O
one	O
,	O
each	O
word	O
was	O
given	O
20	O
different	O
labels	O
,	O
one	O
per	O
annotator	O
.	O

From	O
these	O
judgements	O
,	O
two	O
different	O
data	O
sets	O
were	O
derived	O
.	O

The	O
data	O
set	O
used	O
for	O
this	O
task	O
was	O
created	O
by	O
presenting	O
20	O
nonnative	O
speakers	O
with	O
sentences	O
and	O
letting	O
them	O
judge	O
whether	O
the	O
words	O
contained	O
within	O
these	O
sentences	O
were	O
rated	O
as	O
complex	O
or	O
not	O
.	O

Here	O
,	O
the	O
problem	O
of	O
determining	O
the	O
complexity	O
of	O
a	O
word	O
was	O
formulated	O
as	O
a	O
classification	O
task	O
designed	O
to	O
determine	O
whether	O
a	O
word	O
could	O
be	O
considered	O
as	O
being	O
complex	O
or	O
not	O
.	O

The	O
first	O
approaches	O
to	O
the	O
systematic	O
prediction	O
of	O
lexical	O
complexity	O
were	O
made	O
during	O
SemEval-2016	O
Task	O
11	O
(	O
Paetzold	O
and	O
Specia	O
,	O
2016	O
)	O
.	O

Related	O
Work	O
.	O

The	O
assigned	O
scores	O
were	O
then	O
projected	O
onto	O
values	O
between	O
0	O
and	O
1	O
and	O
averaged	O
between	O
all	O
annotators	O
to	O
calculate	O
the	O
final	O
scores	O
.	O

The	O
scores	O
given	O
for	O
simple	O
words	O
,	O
respectively	O
multiword	O
expressions	O
,	O
were	O
derived	O
from	O
letting	O
annotators	O
subjectively	O
judge	O
the	O
difficulty	O
of	O
understanding	O
words	O
respectively	O
word	O
bigrams	O
on	O
a	O
Likert	O
scale	O
ranging	O
from	O
1	O
to	O
5	O
with	O
1	O
indicating	O
a	O
very	O
simple	O
and	O
5	O
a	O
very	O
complex	O
word	O
.	O

For	O
the	O
task	O
,	O
both	O
subcorpora	O
were	O
partitioned	O
into	O
training	O
,	O
test	O
and	O
trial	O
sets	O
.	O

Within	O
both	O
CompLex	O
sub	O
-	O
corpora	O
,	O
the	O
sentences	O
are	O
organised	O
into	O
quadruples	O
consisting	O
of	O
a	O
given	O
sentence	O
,	O
a	O
reference	O
to	O
its	O
original	O
corpus	O
,	O
a	O
selected	O
word	O
,	O
respectively	O
a	O
multiword	O
expression	O
from	O
this	O
sentence	O
,	O
and	O
a	O
continuous	O
complexity	O
score	O
denoting	O
the	O
difficulty	O
of	O
this	O
selected	O
word	O
or	O
bigram	O
which	O
is	O
to	O
be	O
predicted	O
by	O
systems	O
submitted	O
to	O
the	O
shared	O
task	O
.	O

Accordingly	O
,	O
the	O
shared	O
task	O
was	O
divided	O
into	O
two	O
sub	O
-	O
tasks	O
,	O
one	O
dedicated	O
to	O
each	O
sub	O
-	O
corpus	O
.	O

This	O
English	O
corpus	O
consists	O
of	O
sentences	O
extracted	O
from	O
the	O
World	O
English	O
Bible	O
of	O
the	O
multilingual	O
corpus	O
consisting	O
of	O
bible	O
translations	O
published	O
by	O
Christodoulopoulos	O
and	O
Steedman	O
(	O
2015	O
)	O
,	O
the	O
English	O
version	O
of	O
Europarl	O
(	O
Koehn	O
,	O
2005	O
)	O
,	O
a	O
corpus	O
containing	O
various	O
texts	O
concerned	O
with	O
European	O
policy	O
,	O
and	O
CRAFT	O
(	O
Bada	O
et	O
al	O
.	O
,	O
2012	O
)	O
,	O
a	O
corpus	O
consisting	O
of	O
biomedical	O
articles	O
.	O

Task	O
Setup	O
.	O

Background	O
.	O

Our	O
key	O
discovery	O
is	O
that	O
while	O
features	O
from	O
nearly	O
all	O
categories	O
provided	O
by	O
us	O
were	O
used	O
by	O
our	O
systems	O
,	O
contextual	O
string	O
embeddings	O
(	O
Akbik	O
et	O
al	O
.	O
,	O
2018	O
)	O
were	O
the	O
by	O
far	O
most	O
important	O
category	O
of	O
features	O
to	O
determine	O
lexical	O
complexity	O
for	O
both	O
systems	O
.	O

Predicting	O
these	O
scores	O
can	O
be	O
formulated	O
as	O
a	O
regression	O
problem	O
.	O

One	O
could	O
imagine	O
using	O
such	O
scores	O
to	O
extract	O
vocabulary	O
lists	O
appropriate	O
for	O
a	O
learner	O
level	O
from	O
corpora	O
and	O
literature	O
(	O
Alfter	O
and	O
Volodina	O
,	O
2018	O
)	O
,	O
to	O
judge	O
if	O
a	O
given	O
piece	O
of	O
literature	O
fits	O
a	O
learner	O
's	O
skill	O
or	O
to	O
assist	O
authors	O
of	O
textbooks	O
in	O
finding	O
a	O
level	O
of	O
textual	O
difficulty	O
appropriate	O
for	O
a	O
target	O
audience	O
.	O

Solving	O
this	O
task	O
could	O
benefit	O
second	O
-	O
language	O
learners	O
and	O
non	O
-	O
native	O
speakers	O
in	O
various	O
ways	O
.	O

Introduction	O
.	O

We	O
also	O
thank	O
Takeru	O
Miyato	O
,	O
who	O
gave	O
us	O
valuable	O
comments	O
about	O
AdvT	B-MethodName
/	O
VAT	B-MethodName
.	O

We	O
believe	O
that	O
adversarial	B-MethodName
regularization	I-MethodName
can	O
be	O
one	O
of	O
the	O
common	O
and	O
fundamental	O
technologies	O
to	O
further	O
improve	O
the	O
translation	O
quality	O
,	O
such	O
as	O
model	B-MethodName
ensemble	I-MethodName
,	O
byte	B-MethodName
-	I-MethodName
pair	I-MethodName
encoding	I-MethodName
,	O
and	O
back	B-MethodName
-	I-MethodName
translation	I-MethodName
.	O

Additionally	O
,	O
we	O
confirmed	O
that	O
adversarial	B-MethodName
regularization	I-MethodName
techniques	O
effectively	O
worked	O
even	O
if	O
we	O
performed	O
them	O
with	O
the	O
training	O
data	O
increased	O
by	O
a	O
back	B-MethodName
-	I-MethodName
translation	I-MethodName
method	O
.	O

Our	O
experimental	O
results	O
demonstrated	O
that	O
applying	O
VAT	B-MethodName
to	O
both	O
encoder	O
and	O
decoder	O
embeddings	O
consistently	O
outperformed	O
other	O
configurations	O
.	O

This	O
paper	O
discussed	O
the	O
practical	O
usage	O
and	O
benefit	O
of	O
adversarial	B-MethodName
regularization	I-MethodName
based	O
on	O
adversarial	B-MethodName
perturbation	I-MethodName
in	O
the	O
current	O
NMT	B-TaskName
models	O
.	O

We	O
observe	O
that	O
Transformer+VAT	B-MethodName
with	O
using	O
training	O
data	O
increased	O
by	O
the	O
backtranslation	B-MethodName
method	O
seems	O
to	O
generate	O
higher	O
qual	O
-	O
ity	O
translations	O
compared	O
with	O
those	O
of	O
the	O
baseline	B-MethodName
Transformer	I-MethodName
.	O

In	O
addition	O
,	O
the	O
rows	O
+	O
VAT+AdvT	O
show	O
the	O
performance	O
obtained	O
by	O
applying	O
both	O
AdvT	B-MethodName
and	O
VAT	B-MethodName
simultaneously	O
.	O

Therefore	O
,	O
we	O
can	O
expect	O
that	O
VAT	B-MethodName
can	O
improve	O
the	O
translation	O
performance	O
on	O
other	O
datasets	O
and	O
settings	O
with	O
relatively	O
highconfidence	O
.	O

We	O
report	O
that	O
VAT	B-MethodName
did	O
not	O
require	O
us	O
to	O
perform	O
additional	O
heavy	O
hyper	O
-	O
parameter	O
search	O
(	O
excluding	O
the	O
hyper	O
-	O
parameter	O
search	O
in	O
base	O
models	O
)	O
.	O

We	O
observe	O
that	O
Transformer+VAT	B-MethodName
consistently	O
outperformed	O
the	O
baseline	B-MethodName
Transformer	I-MethodName
results	O
in	O
both	O
standard	B-MethodName
(	O
a	O
)	O
and	O
back	B-MethodName
-	I-MethodName
translation	I-MethodName
(	O
b	O
)	O
settings	O
.	O

Furthermore	O
,	O
the	O
row	O
(	O
b	O
)	O
shows	O
the	O
results	O
obtained	O
when	O
we	O
incorporated	O
pseudo	O
-	O
parallel	O
corpora	O
generated	O
using	O
the	O
back	O
-	O
translation	O
method	O
(	O
Sennrich	O
et	O
al	O
.	O
,	O
2016a	O
)	O
generating	O
the	O
pseudo	O
-	O
parallel	O
corpora	O
,	O
we	O
used	O
the	O
WMT14	B-DatasetName
news	I-DatasetName
translation	I-DatasetName
corpus	I-DatasetName
.	O

Results	O
on	O
four	O
language	O
pairs	O
Table	O
3	O
shows	O
the	O
BLEU	B-MetricName
scores	O
of	O
averaged	O
over	O
five	O
models	O
on	O
four	O
different	O
language	O
pairs	O
(	O
directions	O
)	O
,	O
namely	O
German!English	O
,	O
French!English	O
,	O
English!German	O
,	O
and	O
English!French	O
.	O

They	O
referred	O
to	O
this	O
phenomenon	O
of	O
AdvT	B-MethodName
as	O
label	O
leaking	O
.	O

(	O
2017	O
)	O
,	O
AdvT	B-MethodName
generates	O
the	O
adversarial	O
examples	O
from	O
correct	O
examples	O
,	O
and	O
thus	O
,	O
the	O
models	O
trained	O
by	O
AdvT	B-MethodName
tend	O
to	O
overfit	O
to	O
training	O
data	O
rather	O
than	O
those	O
trained	O
by	O
VAT	B-MethodName
.	O

Furthermore	O
,	O
the	O
results	O
of	O
VAT	B-MethodName
was	O
consistently	O
better	O
than	O
those	O
of	O
AdvT.	B-MethodName

Moreover	O
,	O
we	O
achieved	O
better	O
performance	O
when	O
we	O
added	O
perturbation	O
to	O
the	O
encoder	B-HyperparameterValue
-	I-HyperparameterValue
side	I-HyperparameterValue
(	O
encemb	B-HyperparameterValue
)	O
rather	O
than	O
the	O
decoder	B-HyperparameterValue
-	I-HyperparameterValue
side	I-HyperparameterValue
(	O
dec	B-HyperparameterValue
-	I-HyperparameterValue
emb	I-HyperparameterValue
)	O
.	O

Investigation	O
of	O
effective	O
configuration	O
Table	O
2	O
shows	O
the	O
experimental	O
results	O
with	O
configurations	B-HyperparameterName
of	I-HyperparameterName
perturbation	I-HyperparameterName
positions	I-HyperparameterName
(	O
enc	B-HyperparameterValue
-	I-HyperparameterValue
emb	I-HyperparameterValue
,	O
decemb	B-HyperparameterValue
,	O
or	O
enc	B-HyperparameterValue
-	I-HyperparameterValue
dec	I-HyperparameterValue
-	I-HyperparameterValue
emb	I-HyperparameterValue
)	O
and	O
adversarial	B-MethodName
regularization	I-MethodName
techniques	O
(	O
AdvT	B-MethodName
or	O
VAT	B-MethodName
)	O
.	O

Firstly	O
,	O
in	O
terms	O
of	O
the	O
effective	B-HyperparameterName
perturbation	I-HyperparameterName
position	I-HyperparameterName
,	O
enc	B-HyperparameterValue
-	I-HyperparameterValue
dec	I-HyperparameterValue
-	I-HyperparameterValue
emb	I-HyperparameterValue
configurations	O
,	O
which	O
add	O
perturbations	O
to	O
both	O
encoder	O
and	O
decoder	O
embeddings	O
,	O
consistently	O
outperformed	O
other	O
configurations	O
,	O
which	O
used	O
either	O
encoder	O
or	O
decoder	O
only	O
.	O

Note	O
that	O
all	O
reported	O
BLEU	B-MetricName
scores	O
are	O
averaged	O
over	O
five	O
models	O
.	O

As	O
evaluation	O
metrics	O
,	O
we	O
used	O
BLEU	B-MetricName
scores	O
(	O
Papineni	O
et	O
al	O
.	O
,	O
2002	O
)	O
6	O
.	O

We	O
set	O
=	O
1	B-HyperparameterValue
and	O
✏	B-HyperparameterName
=	O
1	B-HyperparameterValue
for	O
all	O
AdvT	B-MethodName
and	O
VAT	B-MethodName
experiments	O
.	O

Hereafter	O
,	O
we	O
refer	O
to	O
the	O
model	O
trained	O
with	O
the	O
adversarial	B-MethodName
regularization	I-MethodName
(	O
`	O
in	O
Eq	O
.	O
7	O
)	O
as	O
AdvT	B-MethodName
,	O
and	O
similarly	O
,	O
with	O
the	O
virtual	B-MethodName
adversarial	I-MethodName
training	I-MethodName
(	O
`	O
K	O
L	O
in	O
Eq	O
.	O
11	O
)	O
as	O
VAT	B-MethodName
.	O

(	O
2015	O
)	O
and	O
self	B-MethodName
-	I-MethodName
attentionbased	I-MethodName
encoder	I-MethodName
-	I-MethodName
decoder	I-MethodName
,	O
the	O
so	O
-	O
called	O
Transformer	B-MethodName
(	O
Vaswani	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

We	O
selected	O
two	O
widely	O
used	O
model	O
architectures	O
,	O
namely	O
,	O
LSTM	B-MethodName
-	I-MethodName
based	I-MethodName
encoder	I-MethodName
-	I-MethodName
decoder	I-MethodName
2	O
https://github.com/moses-smt/	O
mosesdecoder	O
/	O
blob	O
/	O
master	O
/	O
scripts/	O
tokenizer	O
/	O
tokenizer.perl	O
3	O
https://github.com/moses-smt/	O
mosesdecoder	O
/	O
blob	O
/	O
master	O
/	O
scripts/	O
recaser	O
/	O
truecase.perl	O
4	O
https://github.com/rsennrich/	O
subword	O
-	O
nmt	O
used	O
in	O
Luong	O
et	O
al	O
.	O

For	O
preprocessing	O
of	O
our	O
experimental	O
datasets	O
,	O
we	O
used	O
the	O
Moses	B-MethodName
tokenizer	I-MethodName
2	O
and	O
the	O
truecaser	B-MethodName
3	O
.	O

We	O
used	O
the	O
IWSLT	B-DatasetName
2016	I-DatasetName
training	O
set	O
for	O
training	O
models	O
,	O
2012	B-DatasetName
test	O
set	O
(	O
test2012	O
)	O
as	O
the	O
development	O
set	O
,	O
and	O
2013	B-DatasetName
and	O
2014	B-DatasetName
test	O
sets	O
(	O
test2013	O
and	O
test2014	O
)	O
as	O
our	O
test	O
sets	O
.	O

We	O
conducted	O
experiments	O
on	O
the	O
IWSLT	B-DatasetName
evaluation	I-DatasetName
campaign	I-DatasetName
dataset	O
(	O
Cettolo	O
et	O
al	O
.	O
,	O
2012	O
)	O
.	O

Finally	O
,	O
we	O
have	O
three	O
options	O
for	O
applying	O
the	O
perturbation	O
into	O
typical	O
NMT	B-TaskName
models	O
,	O
namely	O
,	O
applying	O
the	O
perturbation	O
into	O
embeddings	O
in	O
the	O
(	O
1	O
)	O
encoder	O
-	O
side	O
only	O
,	O
(	O
2	O
)	O
decoder	O
-	O
side	O
only	O
,	O
and	O
(	O
3	O
)	O
both	O
encoder	O
and	O
decoder	O
sides	O
.	O

For	O
example	O
,	O
let	O
r0	O
j	O
2	O
R	O
D	O
be	O
an	O
adversarial	B-MethodName
perturbation	I-MethodName
vector	O
for	O
the	O
j	O
-	O
th	O
word	O
in	O
output	O
Y	O
.	O

This	O
fact	O
immediately	O
offers	O
us	O
also	O
to	O
consider	O
applying	O
the	O
adversarial	B-MethodName
perturbation	I-MethodName
into	O
the	O
decoder	O
-	O
side	O
embeddings	O
f	O
j	O
.	O

However	O
,	O
NMT	B-TaskName
models	O
generally	O
have	O
another	O
embedding	O
layer	O
in	O
the	O
decoder	O
-	O
side	O
,	O
as	O
we	O
explained	O
in	O
Eq	O
.	O
2	O
.	O

Adversarial	B-MethodName
Regularization	I-MethodName
in	O
NMT	B-TaskName
.	O

It	O
is	O
worth	O
noting	O
here	O
that	O
,	O
in	O
our	O
experiments	O
,	O
we	O
never	O
applied	O
the	O
semi	O
-	O
supervised	O
learning	O
,	O
but	O
used	O
the	O
above	O
equation	O
for	O
calculating	O
perturbation	O
as	O
the	O
replacement	O
of	O
standard	O
adversarial	B-MethodName
regularization	I-MethodName
.	O

`	O
KL	O
(	O
X	O
,	O
r	O
,	O
•	O
,	O
⇥	O
)	O
=	O
KL	O
p(•	O
|X	O
,	O
⇥	O
)	O
||p(•	O
|X	O
,	O
r	O
,	O
⇥	O
)	O
,	O
(	O
11	O
)	O
where	O
KL(•||•	O
)	O
denotes	O
the	O
KL	B-MetricName
divergence	I-MetricName
.	O

This	O
section	O
briefly	O
describes	O
the	O
adversarial	B-MethodName
regularization	I-MethodName
technique	O
applied	O
to	O
the	O
text	B-TaskName
classification	I-TaskName
tasks	O
proposed	O
in	O
Miyato	O
et	O
al	O
.	O

Adversarial	B-MethodName
Regularization	I-MethodName
.	O

We	O
generally	O
use	O
a	O
K	B-MethodName
-	I-MethodName
best	I-MethodName
beam	I-MethodName
search	I-MethodName
to	O
generate	O
an	O
output	O
sentence	O
with	O
the	O
(	O
approximated	O
)	O
K	O
-	O
highest	O
probability	O
given	O
input	O
sentence	O
X	O
in	O
the	O
generation	O
(	O
test	O
)	O
phase	O
.	O

For	O
training	O
,	O
we	O
generally	O
seek	O
the	O
optimal	O
parameters	O
⇥	O
that	O
can	O
minimize	O
the	O
following	O
optimization	O
problem	O
:	O
⇥	O
=	O
argmin	O
⇥	O
J	O
(	O
D	O
,	O
⇥	O
)	O
,	O
(	O
3	O
)	O
J	O
(	O
D	O
,	O
⇥	O
)	O
=	O
1	O
|D|	O
X	O
(	O
X	O
,	O
Y	O
)	O
2D	O
`	O
(	O
X	O
,	O
Y	O
,	O
⇥	O
)	O
,	O
(	O
4	O
)	O
`	O
(	O
X	O
,	O
Y	O
,	O
⇥	O
)	O
=	O
log	O
p(Y	O
|X	O
,	O
⇥	O
)	O
,	O
(	O
5	O
)	O
where	O
⇥	O
represents	O
a	O
set	O
of	O
trainable	O
parameters	O
in	O
the	O
NMT	B-TaskName
model	O
.	O

Thus	O
,	O
the	O
NMT	B-TaskName
model	O
approximates	O
the	O
following	O
conditional	O
probability	O
:	O
p(Y	O
|X	O
)	O
=	O
Y	O
J+1	O
j=1	O
p(y	O
j	O
|y	O
0	O
:	O
j	O
1	O
,	O
X),(1	O
)	O
where	O
y	O
0	O
and	O
y	O
J+1	O
represent	O
one	O
-	O
hot	O
vectors	O
of	O
special	O
beginning	O
-	O
of	O
-	O
sentence	O
(	O
BOS	O
)	O
and	O
end	O
-	O
ofsentence	O
(	O
EOS	O
)	O
tokens	O
,	O
respectively	O
,	O
and	O
X	O
=	O
x	O
1	O
:	O
I	O
and	O
Y	O
=	O
y	O
1	O
:	O
J+1	O
.	O

To	O
explain	O
the	O
NMT	B-TaskName
model	O
concisely	O
,	O
we	O
assume	O
that	O
its	O
input	O
and	O
output	O
are	O
both	O
sequences	O
of	O
one	O
-	O
hot	O
vectors	O
x	O
1	O
:	O
I	O
and	O
y	O
1	O
:	O
J	O
that	O
correspond	O
to	O
input	O
and	O
output	O
sentences	O
whose	O
lengths	O
are	O
I	O
and	O
J	O
,	O
respectively	O
.	O

Model	O
Definition	O
In	O
general	O
,	O
an	O
NMT	B-TaskName
model	O
receives	O
a	O
sentence	O
as	O
input	O
and	O
returns	O
a	O
corresponding	O
(	O
translated	O
)	O
sentence	O
as	O
output	O
.	O

Neural	B-TaskName
Machine	I-TaskName
Translation	I-TaskName
Model	O
.	O

(	O
2018	O
)	O
used	O
virtual	B-MethodName
adversarial	I-MethodName
training	I-MethodName
(	O
VAT	B-MethodName
)	O
,	O
which	O
is	O
a	O
semi	O
-	O
supervised	O
extension	O
of	O
the	O
adversarial	B-MethodName
regularization	I-MethodName
technique	O
originally	O
proposed	O
in	O
Miyato	O
et	O
al	O
.	O

We	O
investigate	O
the	O
effectiveness	O
of	O
the	O
several	O
practical	O
configurations	O
that	O
have	O
not	O
been	O
examined	O
in	O
their	O
paper	O
,	O
such	O
as	O
the	O
combinations	O
with	O
VAT	B-MethodName
and	O
back	B-MethodName
-	I-MethodName
translation	I-MethodName
.	O

They	O
also	O
demonstrated	O
the	O
impacts	O
of	O
the	O
adversarial	B-MethodName
regularization	I-MethodName
technique	O
in	O
NMT	B-TaskName
models	O
.	O

(	O
2019	O
)	O
also	O
investigated	O
the	O
effectiveness	O
of	O
the	O
adversarial	B-MethodName
regularization	I-MethodName
technique	O
in	O
neural	B-TaskName
language	I-TaskName
modeling	I-TaskName
and	O
NMT	B-TaskName
.	O

Namely	O
,	O
they	O
focused	O
on	O
sequential	O
labeling	O
,	O
whereas	O
we	O
discuss	O
NMT	B-TaskName
models	O
.	O

However	O
,	O
the	O
main	O
focus	O
of	O
these	O
methods	O
is	O
the	O
incorporation	O
of	O
adversarial	O
examples	O
in	O
the	O
training	O
phase	O
,	O
which	O
is	O
orthogonal	O
to	O
our	O
attention	O
,	O
adversarial	B-MethodName
regularization	I-MethodName
,	O
as	O
described	O
in	O
Section	O
1	O
.	O

We	O
investigate	O
the	O
effectiveness	O
of	O
several	O
possible	O
configurations	O
that	O
can	O
significantly	O
and	O
consistently	O
improve	O
the	O
performance	O
of	O
typical	O
baseline	O
NMT	B-TaskName
models	O
,	O
such	O
as	O
LSTM	O
-	O
based	O
and	O
Transformer	O
-	O
based	O
models	O
,	O
.	O

Therefore	O
,	O
the	O
goal	O
of	O
this	O
paper	O
is	O
to	O
re	O
-	O
veal	O
the	O
effectiveness	O
of	O
the	O
adversarial	B-MethodName
regularization	I-MethodName
in	O
NMT	B-TaskName
models	O
and	O
encourage	O
researchers	O
/	O
developers	O
to	O
apply	O
the	O
adversarial	B-MethodName
regularization	I-MethodName
as	O
a	O
common	O
technique	O
for	O
further	O
improving	O
the	O
performance	O
of	O
their	O
NMT	B-TaskName
models	O
.	O

Figure	O
1	O
illustrates	O
the	O
model	O
architecture	O
of	O
NMT	B-TaskName
models	O
with	O
adversarial	O
perturbation	O
.	O

Unfortunately	O
,	O
this	O
application	O
is	O
not	O
fully	O
trivial	O
since	O
we	O
potentially	O
have	O
several	O
configurations	O
for	O
applying	O
adversarial	O
perturbations	O
into	O
NMT	B-TaskName
models	O
(	O
see	O
details	O
in	O
Section	O
5	O
)	O
.	O

We	O
aim	O
to	O
further	O
leverage	O
this	O
promising	O
methodology	O
into	O
more	O
sophisticated	O
and	O
critical	O
neural	O
models	O
,	O
i.e.	O
,	O
neural	B-TaskName
machine	I-TaskName
translation	I-TaskName
(	O
NMT	B-TaskName
)	O
models	O
,	O
since	O
NMT	B-TaskName
models	O
recently	O
play	O
one	O
of	O
the	O
central	O
roles	O
in	O
the	O
NLP	O
research	O
community	O
;	O
NMT	B-TaskName
models	O
have	O
been	O
widely	O
utilized	O
for	O
not	O
only	O
NMT	B-TaskName
but	O
also	O
many	O
other	O
NLP	O
tasks	O
,	O
such	O
as	O
text	B-TaskName
summarization	I-TaskName
(	O
Rush	O
et	O
al	O
.	O
,	O
2015;Chopra	O
et	O
al	O
.	O
,	O
2016	O
)	O
,	O
grammatical	B-TaskName
error	I-TaskName
correction	I-TaskName
(	O
Ji	O
et	O
al	O
.	O
,	O
2017	O
)	O
,	O
dialog	B-TaskName
generation	I-TaskName
(	O
Shang	O
et	O
al	O
.	O
,	O
2015	O
)	O
,	O
and	O
parsing	B-TaskName
(	O
Vinyals	O
et	O
al	O
.	O
,	O
2015;Suzuki	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

We	O
refer	O
to	O
this	O
regularization	O
technique	O
as	O
adversarial	B-MethodName
regularization	I-MethodName
.	O

The	O
key	O
idea	O
of	O
their	O
success	O
is	O
to	O
apply	O
adversarial	O
perturbations	O
into	O
the	O
input	O
embedding	O
layer	O
instead	O
of	O
the	O
inputs	O
themselves	O
as	O
used	O
in	O
image	B-TaskName
processing	I-TaskName
tasks	O
.	O

and	O
reported	O
excellent	O
performance	O
improvements	O
on	O
multiple	O
benchmark	O
datasets	O
of	O
text	B-TaskName
classification	I-TaskName
task	O
.	O

(	O
2017	O
)	O
overcame	O
this	O
problem	O
1	O
Our	O
code	O
for	O
replicating	O
the	O
experiments	O
in	O
this	O
paper	O
is	O
available	O
at	O
the	O
following	O
URL	O
:	O
https://github.com/	O
pfnet	O
-	O
research	O
/	O
vat_nmt	O
Encoder	O
Decoder	O
!	O
"	O
#	O
$	O
"	O
!	O
%	O
#	O
$	O
%	O
!	O
&	O
#	O
$	O
&	O
'	O
(	O
#	O
$	O
(	O
)	O
'	O
"	O
#	O
$	O
"	O
)	O
'	O
*	O
#	O
$	O
+	O
)	O
,	O
"	O
,	O
%	O
,	O
+	O
-	O
"	O
Figure	O
1	O
:	O
An	O
intuitive	O
sketch	O
that	O
explains	O
how	O
we	O
add	O
adversarial	O
perturbations	O
to	O
a	O
typical	O
NMT	B-TaskName
model	O
structure	O
for	O
adversarial	B-MethodName
regularization	I-MethodName
.	O

Thus	O
,	O
this	O
paper	O
investigates	O
the	O
effectiveness	O
of	O
several	O
possible	O
configurations	O
of	O
applying	O
the	O
adversarial	O
perturbation	O
and	O
reveals	O
that	O
the	O
adversarial	B-MethodName
regularization	I-MethodName
technique	O
can	O
significantly	O
and	O
consistently	O
improve	O
the	O
performance	O
of	O
widely	O
used	O
NMT	B-TaskName
models	O
,	O
such	O
as	O
LSTMbased	O
and	O
Transformer	O
-	O
based	O
models	O
.	O

We	O
aim	O
to	O
further	O
leverage	O
this	O
promising	O
methodology	O
into	O
more	O
sophisticated	O
and	O
critical	O
neural	O
models	O
in	O
the	O
natural	O
language	O
processing	O
field	O
,	O
i.e.	O
,	O
neural	B-TaskName
machine	I-TaskName
translation	I-TaskName
(	O
NMT	B-TaskName
)	O
models	O
.	O

A	O
regularization	O
technique	O
based	O
on	O
adversarial	B-MethodName
perturbation	I-MethodName
,	O
which	O
was	O
initially	O
developed	O
in	O
the	O
field	O
of	O
image	O
processing	O
,	O
has	O
been	O
successfully	O
applied	O
to	O
text	B-TaskName
classification	I-TaskName
tasks	O
and	O
has	O
yielded	O
attractive	O
improvements	O
.	O

Effective	O
Adversarial	B-MethodName
Regularization	I-MethodName
for	O
Neural	B-TaskName
Machine	I-TaskName
Translation	I-TaskName
.	O

We	O
thank	O
three	O
anonymous	O
reviewers	O
for	O
their	O
helpful	O
comments	O
.	O

Acknowledgments	O
.	O

Conclusion	O
.	O

Actual	O
Translation	O
Examples	O
Table	O
.	O

4	O
shows	O
actual	O
translation	O
examples	O
generated	O
by	O
the	O
models	O
compared	O
in	O
our	O
German!English	O
translation	O
setting	O
.	O

We	O
can	O
further	O
improve	O
the	O
performance	O
in	O
some	O
cases	O
,	O
but	O
the	O
improvement	O
is	O
not	O
consistent	O
among	O
the	O
datasets	O
.	O

As	O
discussed	O
in	O
Kurakin	O
et	O
al	O
.	O

(	O
2016	O
)	O
.	O

This	O
tendency	O
was	O
also	O
observed	O
in	O
the	O
results	O
reported	O
by	O
Miyato	O
et	O
al	O
.	O

Results	O
.	O

We	O
adapted	O
the	O
hyper	O
-	O
parameters	O
based	O
on	O
the	O
several	O
recent	O
previous	O
papers	O
5	O
.	O

Model	O
Configurations	O
.	O

We	O
also	O
applied	O
the	O
byte	O
-	O
pair	O
encoding	O
(	O
BPE	O
)	O
based	O
subword	O
splitting	O
script	O
4	O
with	O
16,000	O
merge	O
operations	O
(	O
Sennrich	O
et	O
al	O
.	O
,	O
2016b	O
)	O
.	O

We	O
removed	O
sentences	O
over	O
50	O
words	O
from	O
the	O
training	O
set	O
.	O

Table	O
1	O
shows	O
the	O
statistics	O
of	O
datasets	O
used	O
in	O
our	O
experiments	O
.	O

Datasets	O
.	O

6	O
Experiments	O
.	O

In	O
addition	O
,	O
we	O
need	O
to	O
slightly	O
modify	O
the	O
definition	O
of	O
r	O
,	O
which	O
is	O
originally	O
the	O
concatenation	O
vector	O
of	O
all	O
r	O
i	O
for	O
all	O
i	O
,	O
to	O
the	O
concatenation	O
vector	O
of	O
all	O
r	O
i	O
and	O
r	O
0	O
j	O
for	O
all	O
i	O
and	O
j.	O

The	O
perturbed	O
embedding	O
f	O
0	O
j	O
2	O
R	O
D	O
is	O
computed	O
for	O
each	O
decoder	O
time	O
-	O
step	O
j	O
as	O
follows	O
:	O
f	O
0	O
j	O
=	O
F	O
y	O
j	O
1	O
+	O
r0	O
j	O
.(12	O
)	O
Then	O
similar	O
to	O
Eq	O
.	O
8	O
,	O
we	O
can	O
calculate	O
r0	O
as	O
:	O
r0	O
j	O
=	O
✏	O
b	O
j	O
||b||	O
2	O
,	O
b	O
j	O
=	O
r	O
f	O
j	O
`	O
(	O
X	O
,	O
Y	O
,	O
⇥	O
)	O
,	O
(	O
13	O
)	O
where	O
b	O
is	O
a	O
concatenated	O
vector	O
of	O
b	O
j	O
for	O
all	O
j.	O

As	O
strictly	O
following	O
the	O
original	O
definition	O
of	O
the	O
conventional	O
adversarial	O
training	O
,	O
the	O
straightforward	O
approach	O
to	O
applying	O
the	O
adversarial	O
perturbation	O
is	O
to	O
add	O
the	O
perturbation	O
into	O
the	O
encoderside	O
embeddings	O
e	O
i	O
as	O
described	O
in	O
Eq	O
.	O
6	O
.	O

This	O
means	O
that	O
the	O
training	O
data	O
is	O
identical	O
in	O
both	O
settings	O
.	O

(	O
9	O
)	O
Finally	O
,	O
we	O
jointly	O
minimize	O
the	O
objective	O
functions	O
J	O
(	O
D	O
,	O
⇥	O
)	O
and	O
A(D	O
,	O
⇥	O
):	O
⇥	O
=	O
argmin	O
⇥	O
n	O
J	O
(	O
D	O
,	O
⇥	O
)	O
+	O
A(D	O
,	O
⇥	O
)	O
o	O
,	O
(	O
10	O
)	O
where	O
is	O
a	O
scalar	O
hyper	O
-	O
parameter	O
that	O
controls	O
the	O
balance	O
of	O
the	O
two	O
loss	O
functions	O
.	O

(	O
8)	O
Thus	O
,	O
based	O
on	O
adversarial	O
perturbation	O
r	O
,	O
the	O
loss	O
function	O
can	O
be	O
defined	O
as	O
:	O
A(D	O
,	O
⇥	O
)	O
=	O
1	O
|D|	O
X	O
(	O
X	O
,	O
Y	O
)	O
2D	O
`	O
(	O
X	O
,	O
r	O
,	O
Y	O
,	O
⇥	O
)	O
.	O

This	O
approximation	O
method	O
induces	O
the	O
following	O
non	O
-	O
iterative	O
solution	O
for	O
calculating	O
ri	O
for	O
all	O
encoder	O
time	O
-	O
step	O
i	O
:	O
ri	O
=	O
✏	O
a	O
i	O
||a||	O
2	O
,	O
a	O
i	O
=	O
r	O
e	O
i	O
`	O
(	O
X	O
,	O
Y	O
,	O
⇥	O
)	O
.	O

(	O
2015	O
)	O
,	O
where	O
`	O
(	O
X	O
,	O
Y	O
,	O
r	O
,	O
⇥	O
)	O
is	O
linearized	O
around	O
X.	O

As	O
a	O
solution	O
,	O
an	O
approximation	O
method	O
was	O
proposed	O
by	O
Goodfellow	O
et	O
al	O
.	O

However	O
,	O
it	O
is	O
generally	O
infeasible	O
to	O
exactly	O
estimate	O
r	O
in	O
Eq	O
.	O
7	O
for	O
deep	O
neural	O
models	O
.	O

Here	O
,	O
`	O
(	O
X	O
,	O
r	O
,	O
Y	O
,	O
⇥	O
)	O
represents	O
an	O
extension	O
of	O
Eq	O
.	O
5	O
,	O
where	O
the	O
perturbation	O
r	O
i	O
in	O
r	O
is	O
applied	O
to	O
the	O
position	O
of	O
ri	O
as	O
described	O
in	O
Eq	O
.	O
6	O
.	O

To	O
obtain	O
the	O
worst	O
case	O
perturbations	O
as	O
an	O
adversarial	O
perturbation	O
in	O
terms	O
of	O
minimizing	O
the	O
log	O
-	O
likelihood	O
of	O
given	O
X	O
,	O
we	O
seek	O
the	O
optimal	O
solution	O
r	O
by	O
maximizing	O
the	O
following	O
equation	O
:	O
r	O
=	O
argmax	O
r,||r||	O
✏	O
n	O
`	O
(	O
X	O
,	O
r	O
,	O
Y	O
,	O
⇥	O
)	O
o	O
,	O
(	O
7	O
)	O
where	O
✏	O
is	O
a	O
scalar	O
hyper	O
-	O
parameter	O
that	O
controls	O
the	O
norm	O
of	O
the	O
perturbation	O
,	O
and	O
r	O
represents	O
a	O
concatenated	O
vector	O
of	O
r	O
i	O
for	O
all	O
i.	O

Adversarial	O
Training	O
(	O
AdvT	O
)	O
.	O

(	O
6	O
)	O
.	O

The	O
perturbed	O
input	O
embedding	O
e	O
0	O
i	O
2	O
R	O
D	O
is	O
computed	O
for	O
each	O
encoder	O
time	O
-	O
step	O
i	O
as	O
follows	O
:	O
e	O
0	O
i	O
=	O
Ex	O
i	O
+	O
ri	O
.	O

Let	O
ri	O
2	O
R	O
D	O
be	O
an	O
adversarial	O
perturbation	O
vector	O
for	O
the	O
i	O
-	O
th	O
word	O
in	O
input	O
X.	O

(	O
2017	O
)	O
.	O

We	O
omit	O
to	O
explain	O
this	O
part	O
in	O
detail	O
as	O
our	O
focus	O
is	O
a	O
regularization	O
technique	O
that	O
is	O
independent	O
of	O
the	O
generation	O
phase	O
.	O

Generation	O
Phase	O
.	O

Training	O
Phase	O
Let	O
D	O
be	O
the	O
training	O
data	O
consisting	O
of	O
a	O
set	O
of	O
pairs	O
of	O
X	O
n	O
and	O
Y	O
n	O
,	O
namely	O
,	O
D	O
=	O
{	O
(	O
X	O
n	O
,	O
Y	O
n	O
)	O
}	O
N	O
n=1	O
,	O
where	O
N	O
represents	O
the	O
amount	O
of	O
training	O
data	O
.	O

Thus	O
,	O
p(y	O
j	O
|y	O
0	O
:	O
j	O
1	O
,	O
X	O
)	O
in	O
Eq	O
.	O
1	O
is	O
calculated	O
as	O
follows	O
:	O
p(y	O
j	O
|y	O
0	O
:	O
j	O
1	O
,	O
X	O
)	O
=	O
AttDec	O
f	O
j	O
,	O
h	O
1	O
:	O
I	O
,	O
h	O
1	O
:	O
I	O
=	O
Enc(e	O
1	O
:	O
I	O
)	O
,	O
f	O
j	O
=	O
F	O
y	O
j	O
1	O
,	O
e	O
i	O
=	O
Ex	O
i	O
,	O
(	O
2	O
)	O
where	O
Enc(•	O
)	O
and	O
AttDec(•	O
)	O
represent	O
functions	O
that	O
abstract	O
the	O
entire	O
encoder	O
and	O
decoder	O
(	O
with	O
an	O
attention	O
mechanism	O
)	O
procedures	O
,	O
respectively	O
.	O

Let	O
E	O
2	O
R	O
D	O
⇥	O
|Vs|	O
and	O
F	O
2	O
R	O
D	O
⇥	O
|Vt|	O
be	O
the	O
encoder	O
and	O
decoder	O
embedding	O
matrices	O
,	O
respectively	O
,	O
where	O
D	O
is	O
the	O
dimension	O
of	O
the	O
embedding	O
vectors	O
.	O

,	O
x	O
j	O
)	O
.	O

Here	O
,	O
we	O
introduce	O
a	O
short	O
notation	O
x	O
i	O
:	O
j	O
for	O
representing	O
a	O
sequence	O
of	O
vectors	O
(	O
x	O
i	O
,	O
.	O

x	O
i	O
and	O
y	O
j	O
denote	O
the	O
one	O
-	O
hot	O
vectors	O
of	O
the	O
i	O
-	O
th	O
and	O
j	O
-	O
th	O
to	O
-	O
kens	O
in	O
input	O
and	O
output	O
sentences	O
,	O
respectively	O
,	O
i.e.	O
x	O
i	O
2	O
{	O
0	O
,	O
1	O
}	O
|Vs|	O
and	O
y	O
j	O
2	O
{	O
0	O
,	O
1	O
}	O
|Vt|	O
.	O

Let	O
V	O
s	O
and	O
V	O
t	O
represent	O
the	O
vocabularies	O
of	O
the	O
input	O
and	O
output	O
sentences	O
,	O
respectively	O
.	O

In	O
parallel	O
to	O
our	O
work	O
,	O
Wang	O
et	O
al	O
.	O

Therefore	O
,	O
the	O
focus	O
of	O
the	O
neural	O
models	O
differs	O
from	O
this	O
paper	O
.	O

(	O
2016	O
)	O
,	O
in	O
their	O
experiments	O
to	O
compare	O
the	O
results	O
with	O
those	O
of	O
their	O
proposed	O
method	O
.	O

Clark	O
et	O
al	O
.	O

They	O
utilized	O
the	O
generated	O
(	O
input	O
)	O
sentences	O
as	O
additional	O
training	O
data	O
.	O

(	O
2017	O
)	O
proposed	O
methods	O
that	O
generate	O
input	O
sentences	O
with	O
random	O
character	O
swaps	O
.	O

For	O
example	O
,	O
Belinkov	O
and	O
Bisk	O
(	O
2018	O
)	O
;	O
Hosseini	O
et	O
al	O
.	O

Several	O
studies	O
have	O
recently	O
applied	O
adversarial	O
training	O
to	O
NLP	O
tasks	O
,	O
e.g.	O
,	O
(	O
Jia	O
and	O
Liang	O
,	O
2017;Belinkov	O
and	O
Bisk	O
,	O
2018;Hosseini	O
et	O
al	O
.	O
,	O
2017;Samanta	O
and	O
Mehta	O
,	O
2017;Miyato	O
et	O
al	O
.	O
,	O
2017;Sato	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

Related	O
Work	O
.	O

An	O
important	O
implication	O
of	O
their	O
study	O
is	O
that	O
their	O
method	O
can	O
be	O
interpreted	O
as	O
a	O
regularization	O
method	O
,	O
and	O
thus	O
,	O
they	O
do	O
not	O
focus	O
on	O
generating	O
adversarial	O
examples	O
.	O

Moreover	O
,	O
those	O
of	O
ri	O
and	O
r0	O
j	O
are	O
in	O
Eq	O
.	O
8	O
and	O
13	O
,	O
respectively	O
.	O

The	O
definitions	O
of	O
e	O
i	O
and	O
f	O
j	O
can	O
be	O
found	O
in	O
Eq	O
.	O
2	O
.	O

Recently	O
,	O
Miyato	O
et	O
al	O
.	O

Since	O
it	O
is	O
unreasonable	O
to	O
add	O
a	O
small	O
perturbation	O
to	O
the	O
symbols	O
,	O
applying	O
the	O
idea	O
of	O
adversarial	O
training	O
to	O
NLP	O
tasks	O
has	O
been	O
recognized	O
as	O
a	O
challenging	O
problem	O
.	O

In	O
the	O
field	O
of	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
,	O
the	O
input	O
is	O
a	O
sequence	O
of	O
discrete	O
symbols	O
,	O
such	O
as	O
words	O
or	O
sentences	O
.	O

This	O
learning	O
framework	O
is	O
referred	O
to	O
as	O
adversarial	O
training	O
.	O

(	O
2015	O
)	O
proposed	O
a	O
learning	O
framework	O
that	O
simultaneously	O
leverages	O
adversarial	O
examples	O
as	O
additional	O
training	O
data	O
for	O
reducing	O
the	O
prediction	O
errors	O
.	O

Subsequently	O
,	O
Goodfellow	O
et	O
al	O
.	O

Such	O
perturbed	O
inputs	O
are	O
often	O
referred	O
to	O
as	O
adversarial	O
examples	O
in	O
the	O
literature	O
.	O

The	O
existence	O
of	O
(	O
small	O
)	O
perturbations	O
that	O
induce	O
a	O
critical	O
prediction	O
error	O
in	O
machine	O
learning	O
models	O
was	O
first	O
discovered	O
and	O
discussed	O
in	O
the	O
field	O
of	O
image	O
processing	O
(	O
Szegedy	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

Introduction	O
.	O

1	O
.	O

However	O
,	O
it	O
is	O
not	O
trivial	O
to	O
apply	O
this	O
methodology	O
to	O
such	O
models	O
.	O

Considering	O
the	O
instability	O
of	O
the	O
few	O
-	O
shot	O
learning	O
,	O
we	O
run	O
each	O
experiment	O
5	O
times	O
on	O
the	O
random	O
seed	O
[	O
10,20,30,40,50	O
]	O
and	O
report	O
the	O
averaged	O
performance	O
as	O
well	O
as	O
the	O
standard	O
deviation	O
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
PPT	B-MethodName
,	O
a	O
framework	O
that	O
improves	O
prompt	B-MethodName
tuning	I-MethodName
for	O
few	O
-	O
shot	O
learning	O
.	O

In	O
this	O
paper	O
,	O
we	O
study	O
few	O
-	O
shot	O
learning	O
on	O
large	O
-	O
scale	O
11B	O
PLMs	O
.	O
Conclusion	O
and	O
Future	O
Work	O
.	O

There	O
is	O
also	O
work	O
(	O
IV	O
et	O
al	O
.	O
,	O
2021	O
)	O
pointing	O
out	O
the	O
low	O
performance	O
of	O
PT	B-MethodName
for	O
few	O
-	O
shot	O
learning	O
.	O

Few	O
-	O
shot	O
Learning	O
with	O
PLMs	O
Since	O
long	O
-	O
tail	O
distribution	O
is	O
common	O
in	O
real	O
-	O
world	O
applications	O
,	O
few	O
-	O
shot	O
learning	O
is	O
quite	O
meaningful	O
for	O
the	O
stable	O
and	O
effective	O
use	O
of	O
PLMs	O
,	O
thereby	O
attracts	O
much	O
attention	O
recently	O
.	O

Few	O
-	O
shot	O
learning	O
is	O
notorious	O
for	O
its	O
instability	O
,	O
which	O
becomes	O
very	O
obvious	O
in	O
Vanilla	B-MethodName
PT	I-MethodName
.	O

In	O
this	O
section	O
,	O
we	O
present	O
pilot	O
experiments	O
of	O
PT	B-MethodName
for	O
few	O
-	O
shot	O
learning	O
.	O

Experiments	O
show	O
that	O
PPT	B-MethodName
can	O
not	O
only	O
improve	O
PT	B-MethodName
by	O
a	O
large	O
margin	O
,	O
reaching	O
or	O
even	O
outperforming	O
FT	B-MethodName
methods	O
,	O
but	O
also	O
reduce	O
the	O
variance	O
of	O
few	O
-	O
shot	O
learning	O
.	O

Hence	O
,	O
in	O
this	O
paper	O
,	O
we	O
explore	O
how	O
to	O
use	O
PLMs	O
for	O
few	O
-	O
shot	O
learning	O
in	O
an	O
efficient	O
and	O
effective	O
manner	O
through	O
PT	B-MethodName
.	O

PPT	B-MethodName
:	O
Pre	B-MethodName
-	I-MethodName
trained	I-MethodName
Prompt	I-MethodName
Tuning	I-MethodName
for	O
Few	O
-	O
shot	O
Learning	O
.	O

Results	O
have	O
shown	O
that	O
task	B-MethodName
-	I-MethodName
oriented	I-MethodName
fine	I-MethodName
-	I-MethodName
tuning	I-MethodName
can	O
outperform	O
models	O
trained	O
from	O
scratch	O
on	O
a	O
series	O
of	O
NLP	O
tasks	O
.	O

Then	O
,	O
all	O
parameters	O
of	O
both	O
PLMs	O
and	O
additional	O
heads	O
are	O
tuned	O
using	O
task	O
-	O
specific	O
data	O
.	O

To	O
adapt	O
these	O
PLMs	O
to	O
downstream	O
NLP	O
tasks	O
,	O
task	B-TaskName
-	I-TaskName
oriented	I-TaskName
fine	I-TaskName
-	I-TaskName
tuning	I-TaskName
has	O
been	O
proposed	O
,	O
where	O
researchers	O
use	O
PLMs	O
as	O
the	O
backbone	O
and	O
add	O
some	O
task	O
-	O
specific	O
heads	O
to	O
optimize	O
task	O
-	O
specific	O
objectives	O
.	O

Given	O
the	O
input	O
x	O
=	O
(	O
s	O
)	O
,	O
we	O
have	O
:	O
Based	O
on	O
the	O
PVP	O
pre	O
i	O
,	O
the	O
design	O
of	O
PVP	O
k	O
i	O
is	O
similar	O
to	O
that	O
of	O
English	O
tasks	O
.	O

Sentence	B-TaskName
-	I-TaskName
Pair	I-TaskName
Classification	I-TaskName
Given	O
the	O
input	O
x	O
=	O
(	O
s	O
1	O
,	O
s	O
2	O
)	O
,	O
the	O
label	O
list	O
Y	O
=	O
[	O
0	O
,	O
1	O
,	O
2	O
]	O
,	O
we	O
have	O
:	O
Multiple	B-TaskName
-	I-TaskName
Choice	I-TaskName
Classification	I-TaskName
Given	O
a	O
input	O
x	O
consisting	O
of	O
a	O
query	O
and	O
six	O
candidates	O
:	O
x	O
=	O
(	O
s	O
q	O
,	O
s	O
1	O
,	O
s	O
2	O
,	O
•	O
•	O
•	O
,	O
s	O
6	O
)	O
,	O
we	O
convert	O
x	O
to	O
a	O
language	O
sequence	O
by	O
defining	O
the	O
PVP	O
pre	O
i	O
as	O
follows	O
:	O
Single	B-TaskName
-	I-TaskName
Sentence	I-TaskName
Classification	I-TaskName
Similar	O
to	O
the	O
English	O
scenario	O
,	O
we	O
take	O
sentiment	O
classification	O
as	O
an	O
example	O
.	O

Just	O
like	O
English	O
scenarios	O
,	O
all	O
these	O
PVPs	O
are	O
simple	O
and	O
intuitive	O
.	O

We	O
describe	O
the	O
PVP	O
pre	O
i	O
for	O
Chinese	O
datasets	O
in	O
this	O
section	O
.	O

B	O
PVPs	O
for	O
Chinese	O
Tasks	O
.	O

For	O
Chinese	O
experiments	O
,	O
we	O
use	O
four	O
datasets	O
from	O
CLUE	B-DatasetName
(	O
Xu	O
et	O
al	O
.	O
,	O
2020	O
)	O
(	O
CMNLI	B-DatasetName
3	I-DatasetName
,	O
OCNLI	B-DatasetName
(	O
Hu	O
et	O
al	O
.	O
,	O
2020	O
)	O
,	O
TNews	B-DatasetName
3	I-DatasetName
,	O
C	B-DatasetName
3	I-DatasetName
(	O
Sun	O
et	O
al	O
.	O
,	O
2020	O
)	O
)	O
,	O
two	O
sentiment	B-TaskName
analysis	I-TaskName
datasets	O
(	O
ChnSent	B-DatasetName
4	I-DatasetName
and	O
Amazon	B-DatasetName
Reviews	I-DatasetName
4	I-DatasetName
)	O
,	O
and	O
one	O
extra	O
natural	B-TaskName
language	I-TaskName
inference	I-TaskName
dataset	O
LCQMC	B-DatasetName
(	O
Liu	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

(	O
2021	O
)	O
to	O
use	O
original	O
validation	O
sets	O
for	O
testing	O
.	O

(	O
2021	O
)	O
and	O
Gao	O
et	O
al	O
.	O

Since	O
some	O
of	O
the	O
test	O
sets	O
of	O
the	O
datasets	O
we	O
used	O
is	O
not	O
publicly	O
available	O
,	O
we	O
follow	O
Zhang	O
et	O
al	O
.	O

A	O
Dataset	O
Information	O
.	O

Appendices	O
.	O

This	O
work	O
was	O
also	O
supported	O
by	O
the	O
Guoqiang	O
Institute	O
of	O
Tsinghua	O
University	O
,	O
with	O
Grant	O
No	O
.	O
2019GQG1	O
and	O
2020GQG0005	O
.	O

This	O
work	O
was	O
supported	O
by	O
the	O
National	O
Science	O
Foundation	O
for	O
Distinguished	O
Young	O
Scholars	O
(	O
with	O
No	O
.	O
62125604	O
)	O
and	O
the	O
NSFC	O
projects	O
(	O
Key	O
project	O
with	O
No	O
.	O
61936010	O
and	O
regular	O
project	O
with	O
No	O
.	O
61876096	O
)	O
.	O

Acknowledgements	O
.	O

E	O
Training	O
Consumption	O
.	O

The	O
hard	O
prompts	O
corresponding	O
to	O
each	O
task	O
format	O
are	O
shown	O
in	O
Table	O
7	O
.	O

For	O
simplicity	O
,	O
we	O
choose	O
the	O
best	O
hard	O
prompts	O
for	O
each	O
task	O
format	O
(	O
e.g.	O
sentence	B-TaskName
-	I-TaskName
pair	I-TaskName
classification	I-TaskName
,	O
multiple	B-TaskName
-	I-TaskName
choice	I-TaskName
classification	I-TaskName
,	O
and	O
single	O
-	O
sentence	O
classification	O
)	O
based	O
on	O
PT	B-MethodName
in	O
pilot	O
experiments	O
and	O
directly	O
use	O
them	O
in	O
Hybrid	B-MethodName
PPT	I-MethodName
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
the	O
hard	O
prompts	O
we	O
use	O
in	O
Hybrid	B-MethodName
PT	I-MethodName
and	O
Hybrid	B-MethodName
PPT	I-MethodName
.	O

D	O
Hard	O
Prompts	O
.	O

The	O
thresholds	O
of	O
the	O
label	O
0	O
∼	O
4	O
are	O
[	O
0.95	O
,	O
0.50	O
,	O
0.50	O
,	O
0.50	O
,	O
0.70	O
]	O
.	O

We	O
set	O
different	O
minimal	O
classification	O
confidence	O
thresholds	O
for	O
the	O
5	O
labels	O
to	O
control	O
annotation	O
quality	O
and	O
balance	O
the	O
label	O
.	O

We	O
choose	O
the	O
checkpoint	O
with	O
the	O
highest	O
accuracy	O
on	O
the	O
validation	O
set	O
,	O
which	O
is	O
70.53	O
at	O
the	O
5	O
-	O
th	O
epoch	O
,	O
to	O
annotate	O
the	O
label	O
.	O

We	O
use	O
learning	B-HyperparameterName
rate	I-HyperparameterName
1e-4	B-HyperparameterValue
,	O
batch	B-HyperparameterName
size	I-HyperparameterName
16	B-HyperparameterValue
,	O
warm	B-HyperparameterName
-	I-HyperparameterName
up	I-HyperparameterName
rate	I-HyperparameterName
0.01	B-HyperparameterValue
,	O
and	O
train	O
the	O
model	O
for	O
10	B-HyperparameterValue
epochs	B-HyperparameterName
.	O

Single	B-TaskName
-	I-TaskName
Sentence	I-TaskName
Classification	I-TaskName
We	O
use	O
the	O
RoBERTa	B-MethodName
BASE	I-MethodName
model	O
trained	O
on	O
the	O
Yelp-5	B-DatasetName
dataset	O
to	O
annotate	O
pseudo	O
labels	O
on	O
the	O
unlabeled	O
data	O
.	O

The	O
input	O
configurations	O
of	O
different	O
option	O
numbers	O
is	O
shown	O
in	O
Table	O
9	O
.	O

To	O
fit	O
in	O
the	O
max	O
input	O
length	O
,	O
we	O
truncate	O
the	O
query	O
sentence	O
to	O
389	O
tokens	O
and	O
the	O
options	O
to	O
86	O
tokens	O
.	O

We	O
also	O
filter	O
out	O
the	O
sentences	O
with	O
less	O
than	O
5	O
tokens	O
.	O

Sentence	B-TaskName
-	I-TaskName
Pair	I-TaskName
Classification	I-TaskName
In	O
the	O
next	O
sentence	O
prediction	O
task	O
,	O
we	O
set	O
the	O
two	O
sentences	O
next	O
to	O
each	O
other	O
as	O
label	O
2	O
,	O
those	O
from	O
the	O
same	O
document	O
but	O
not	O
true	O
next	O
sentence	O
as	O
1	O
,	O
and	O
those	O
from	O
different	O
documents	O
as	O
0	O
.	O

The	O
details	O
of	O
constructing	O
the	O
pre	O
-	O
training	O
data	O
for	O
each	O
task	O
are	O
as	O
follows	O
.	O

We	O
evaluate	O
the	O
performance	O
on	O
the	O
validation	O
set	O
every	O
2,000	O
steps	O
and	O
choose	O
the	O
prompt	O
with	O
the	O
lowest	O
validation	O
loss	O
.	O

We	O
split	B-HyperparameterName
5	B-HyperparameterValue
%	I-HyperparameterValue
data	O
for	O
validation	O
and	O
the	O
rest	O
for	O
pre	O
-	O
training	O
.	O

We	O
set	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
as	O
256	B-HyperparameterValue
,	O
the	O
max	B-HyperparameterName
input	I-HyperparameterName
length	I-HyperparameterName
as	O
512	B-HyperparameterValue
,	O
and	O
train	O
the	O
prompts	O
for	O
at	O
most	O
200,000	B-HyperparameterValue
steps	B-HyperparameterName
.	O

Across	O
all	O
tasks	O
,	O
we	O
use	O
the	O
"	O
inverse	O
square	O
root	O
"	O
learning	O
rate	O
scheduler	O
(	O
Raffel	O
et	O
al	O
.	O
,	O
2020	O
)	O
and	O
set	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
in	O
this	O
scheduler	O
as	O
0.1	B-HyperparameterValue
with	O
no	O
warmup	O
steps	O
.	O

We	O
use	O
the	O
sampled	O
10	O
GB	O
data	O
to	O
construct	O
the	O
pre	O
-	O
training	O
data	O
for	O
each	O
task	O
format	O
for	O
prompt	B-MethodName
pre	I-MethodName
-	I-MethodName
training	I-MethodName
.	O

C.3	O
Prompt	O
Pre	O
-	O
Training	O
.	O

Therefore	O
,	O
we	O
search	O
for	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
in	O
[	O
5e-3	B-HyperparameterValue
,	O
1e-2	B-HyperparameterValue
,	O
2e-2	B-HyperparameterValue
,	O
5e-2	B-HyperparameterValue
]	O
and	O
choose	O
the	O
model	O
with	O
the	O
best	O
performance	O
on	O
the	O
validation	O
set	O
.	O

steps	O
.	O

Generally	O
,	O
small	O
models	O
prefer	O
large	O
learning	B-HyperparameterName
rates	I-HyperparameterName
.	O

Similar	O
to	O
FT	B-MethodName
,	O
we	O
fix	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
as	O
16	B-HyperparameterValue
and	O
train	O
the	O
model	O
for	O
50	B-HyperparameterValue
epochs	B-HyperparameterName
,	O
while	O
evaluating	O
the	O
model	O
every	O
6	O
Model	O
Size	O
Searching	O
Interval	O
Small	O
2e-4	B-HyperparameterValue
,	O
5e-4	B-HyperparameterValue
,	O
1e-3	B-HyperparameterValue
Base	O
2e-4	B-HyperparameterValue
,	O
5e-4	B-HyperparameterValue
,	O
1e-3	B-HyperparameterValue
Large	O
5e-5	B-HyperparameterValue
,	O
1e-4	B-HyperparameterValue
,	O
2e-4	B-HyperparameterValue
XL	O
3e-5	B-HyperparameterValue
,	O
5e-5	B-HyperparameterValue
,	O
1e-4	B-HyperparameterValue
XXL	O
3e-6	B-HyperparameterValue
,	O
5e-6	B-HyperparameterValue
,	O
1e-5	B-HyperparameterValue
Table	O
8	O
:	O
The	O
searching	O
intervals	O
of	O
learning	B-HyperparameterName
rates	I-HyperparameterName
for	O
the	O
models	O
with	O
different	O
sizes	O
.	O

When	O
adapting	O
the	O
model	O
to	O
downstream	O
tasks	O
,	O
we	O
only	O
tune	O
the	O
soft	O
prompts	O
with	O
the	O
entire	O
model	O
fixed	O
.	O

We	O
choose	O
the	O
model	O
performing	O
the	O
best	O
on	O
the	O
validation	O
set	O
and	O
evaluate	O
it	O
on	O
the	O
test	O
set	O
.	O

We	O
train	O
the	O
model	O
for	O
50	B-HyperparameterValue
epochs	B-HyperparameterName
and	O
do	O
evaluation	O
every	O
6	O
optimization	O
steps	O
.	O

Therefore	O
,	O
we	O
search	O
for	O
the	O
learning	B-HyperparameterName
rates	I-HyperparameterName
in	O
varied	O
intervals	O
and	O
show	O
each	O
model	O
size	O
and	O
its	O
corresponding	O
searching	O
interval	O
in	O
Table	O
8	O
.	O

In	O
this	O
way	O
,	O
we	O
train	O
the	O
largest	O
11B	O
model	O
with	O
16	O
NVIDIA	O
V100	O
32	O
G	O
GPUs	O
.	O
We	O
find	O
that	O
different	O
sized	O
models	O
prefer	O
significantly	O
different	O
learning	B-HyperparameterName
rates	I-HyperparameterName
.	O

For	O
all	O
models	O
,	O
we	O
fix	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
as	O
16	B-HyperparameterValue
.	O

We	O
describe	O
the	O
details	O
of	O
the	O
training	O
hyper	O
-	O
parameters	O
in	O
the	O
following	O
sections	O
.	O

For	O
models	O
in	O
other	O
sizes	O
,	O
we	O
all	O
use	O
full	O
-	O
precision	O
training	O
.	O

We	O
also	O
use	O
mixedprecision	O
training	O
(	O
Micikevicius	O
et	O
al	O
.	O
,	O
2018	O
)	O
and	O
ZeRO	O
(	O
Rajbhandari	O
et	O
al	O
.	O
,	O
2020	O
)	O
stage-1	O
provided	O
in	O
DeepSpeed	O
(	O
Rasley	O
et	O
al	O
.	O
,	O
2020	O
)	O
to	O
reduce	O
GPU	O
memory	O
usage	O
.	O

Due	O
to	O
the	O
resource	O
limit	O
,	O
for	O
11B	O
models	O
,	O
we	O
adopt	O
model	O
parallelism	O
(	O
Shoeybi	O
et	O
al	O
.	O
,	O
2019	O
)	O
and	O
store	O
a	O
model	O
with	O
4	O
GPU	O
devices	O
.	O

C	O
Training	O
Details	O
.	O

(	O
3	O
)	O
Beyond	O
the	O
soft	O
prompt	O
,	O
studying	O
whether	O
unified	O
task	O
pre	O
-	O
training	O
helps	O
the	O
pre	O
-	O
trained	O
language	O
models	O
itself	O
.	O

(	O
2	O
)	O
Evaluating	O
the	O
few	O
-	O
shot	O
performance	O
of	O
other	O
parameter	B-MethodName
-	I-MethodName
efficient	I-MethodName
tuning	I-MethodName
approaches	O
(	O
He	O
et	O
al	O
.	O
,	O
2022	O
)	O
and	O
adapting	O
unified	O
task	O
pre	O
-	O
training	O
to	O
them	O
.	O

There	O
are	O
three	O
important	O
directions	O
for	O
future	O
work	O
:	O
(	O
1	O
)	O
Designing	O
unified	O
task	O
formats	O
and	O
the	O
corresponding	O
pre	O
-	O
training	O
objectives	O
for	O
other	O
kinds	O
of	O
tasks	O
such	O
as	O
language	O
generation	O
and	O
relation	O
extraction	O
.	O

Then	O
,	O
we	O
design	O
self	O
-	O
supervised	O
pre	O
-	O
training	O
tasks	O
for	O
each	O
format	O
and	O
pre	O
-	O
train	O
prompts	O
on	O
these	O
tasks	O
.	O

We	O
propose	O
to	O
firstly	O
unify	O
downstream	O
tasks	O
to	O
several	O
formats	O
.	O

But	O
they	O
mostly	O
focus	O
on	O
PLMs	O
with	O
fewer	O
than	O
400	O
M	O
parameters	O
.	O

(	O
2021	O
)	O
also	O
discuss	O
reasonable	O
fewshot	O
settings	O
by	O
restricting	O
the	O
size	O
of	O
validation	O
set	O
and	O
proposing	O
a	O
unified	O
framework	O
to	O
evaluate	O
few	O
-	O
shot	O
performance	O
.	O

(	O
2021	O
)	O
;	O
Bragg	O
et	O
al	O
.	O

Apart	O
from	O
GPT-3	B-MethodName
(	O
Brown	O
et	O
al	O
.	O
,	O
2020	O
)	O
and	O
PET	B-MethodName
(	O
Schick	O
and	O
Schütze	O
,	O
2021a	O
)	O
which	O
demonstrates	O
the	O
superiority	O
of	O
PLMs	O
in	O
few	O
-	O
shot	O
scenarios	O
,	O
some	O
later	O
works	O
Perez	O
et	O
al	O
.	O

To	O
step	O
forward	O
,	O
some	O
works	O
(	O
Li	O
and	O
Liang	O
,	O
2021;Qin	O
and	O
Eisner	O
,	O
2021;Lester	O
et	O
al	O
.	O
,	O
2021	O
)	O
propose	O
to	O
only	O
tune	O
soft	O
prompts	O
and	O
fix	O
the	O
entire	O
PLM	O
parameters	O
.	O

Different	O
from	O
hard	O
prompts	O
using	O
concrete	O
and	O
discrete	O
tokens	O
,	O
soft	O
prompts	O
are	O
composed	O
of	O
several	O
continuous	O
learnable	O
embeddings	O
,	O
and	O
these	O
embeddings	O
are	O
randomly	O
initialized	O
.	O

(	O
2021b	O
)	O
explore	O
to	O
combine	O
hard	O
prompts	O
and	O
soft	O
prompts	O
.	O

(	O
2021	O
)	O
;	O
Zhong	O
et	O
al	O
.	O

(	O
2021b	O
)	O
;	O
Hambardzumyan	O
et	O
al	O
.	O

(	O
2021	O
)	O
;	O
Han	O
et	O
al	O
.	O

To	O
overcome	O
the	O
shortcomings	O
of	O
discrete	O
spaces	O
,	O
Li	O
and	O
Liang	O
(	O
2021	O
)	O
;	O
Liu	O
et	O
al	O
.	O

However	O
,	O
these	O
works	O
still	O
restrict	O
auto	O
-	O
generated	O
prompts	O
to	O
discrete	O
spaces	O
which	O
are	O
usually	O
sub	O
-	O
optimal	O
.	O

Considering	O
manually	O
designing	O
prompts	O
is	O
both	O
time	O
-	O
consuming	O
and	O
difficult	O
to	O
find	O
the	O
best	O
choice	O
,	O
later	O
works	O
(	O
Gao	O
et	O
al	O
.	O
,	O
2021;Jiang	O
et	O
al	O
.	O
,	O
2020;Shin	O
et	O
al	O
.	O
,	O
2020	O
)	O
proposed	O
to	O
generate	O
prompts	O
automatically	O
.	O

These	O
pioneering	O
works	O
demonstrate	O
that	O
language	O
prompts	O
can	O
effectively	O
stimulate	O
the	O
knowledge	O
from	O
PLMs	O
.	O
Encouraged	O
by	O
this	O
,	O
manually	O
designing	O
hard	O
prompts	O
consisting	O
of	O
discrete	O
words	O
is	O
first	O
used	O
in	O
prompt	B-MethodName
-	I-MethodName
oriented	I-MethodName
fine	I-MethodName
-	I-MethodName
tuning	I-MethodName
Schick	O
and	O
Schütze	O
(	O
2021a	O
,	O
b	O
)	O
.	O

In	O
knowledge	O
probing	O
,	O
language	O
triggers	O
are	O
widely	O
used	O
to	O
induce	O
PLMs	O
to	O
generate	O
relational	O
facts	O
.	O

Knowledge	O
probing	O
(	O
Petroni	O
et	O
al	O
.	O
,	O
2019;Trinh	O
and	O
Le	O
,	O
2018;Davison	O
et	O
al	O
.	O
,	O
2019	O
)	O
is	O
the	O
seminal	O
work	O
that	O
stimulates	O
the	O
development	O
of	O
prompts	O
.	O

In	O
prompt	B-MethodName
-	I-MethodName
oriented	I-MethodName
finetuning	I-MethodName
,	O
downstream	O
tasks	O
are	O
also	O
formalized	O
as	O
language	O
modeling	O
problems	O
by	O
inserting	O
language	O
prompts	O
,	O
and	O
the	O
results	O
of	O
language	O
modeling	O
can	O
correspond	O
to	O
the	O
solutions	O
of	O
downstream	O
tasks	O
.	O

To	O
overcome	O
the	O
gap	O
between	O
pretraining	O
and	O
downstream	O
tasks	O
,	O
prompt	B-MethodName
-	I-MethodName
oriented	I-MethodName
fine	I-MethodName
-	I-MethodName
tuning	I-MethodName
is	O
introduced	O
.	O

Prompt	B-MethodName
-	I-MethodName
oriented	I-MethodName
Fine	I-MethodName
-	I-MethodName
tuning	I-MethodName
Most	O
existing	O
PLMs	O
are	O
pre	O
-	O
trained	O
with	O
language	O
modeling	O
objectives	O
,	O
yet	O
the	O
objectives	O
of	O
downstream	O
tasks	O
are	O
quite	O
different	O
.	O

PLMs	O
and	O
Task	O
-	O
oriented	O
Fine	O
-	O
tuning	O
Recently	O
,	O
various	O
powerful	O
PLMs	O
have	O
been	O
proposed	O
,	O
such	O
as	O
GPT	B-MethodName
(	O
Radford	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O
2019	O
)	O
,	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
.	O
,	O
2019	O
)	O
and	O
T5	B-MethodName
(	O
Raffel	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

Since	O
PPT	B-MethodName
still	O
converges	O
a	O
bit	O
slower	O
than	O
FT	B-MethodName
,	O
how	O
to	O
further	O
accelerate	O
the	O
convergence	O
of	O
PT	B-MethodName
is	O
worth	O
studying	O
in	O
future	O
work	O
.	O

Related	O
Works	O
.	O

We	O
give	O
a	O
more	O
detailed	O
analysis	O
of	O
the	O
training	O
consumption	O
in	O
the	O
Appendix	O
E.	O

As	O
shown	O
in	O
Figure	O
5	O
,	O
with	O
the	O
pre	O
-	O
trained	O
initialization	O
,	O
PPT	B-MethodName
speeds	O
up	O
the	O
convergence	O
of	O
Vanilla	B-MethodName
PT	I-MethodName
on	O
both	O
RACE	B-DatasetName
-	I-DatasetName
m	I-DatasetName
and	O
CB	B-DatasetName
datasets	O
.	O

We	O
also	O
compare	O
different	O
tuning	O
approaches	O
given	O
the	O
full	O
training	O
data	O
.	O

For	O
32	B-HyperparameterValue
to	O
128	B-HyperparameterValue
samples	B-HyperparameterName
,	O
PPT	B-MethodName
is	O
consistently	O
better	O
than	O
PT	B-MethodName
,	O
and	O
the	O
performances	O
of	O
the	O
three	O
methods	O
gradually	O
converge	O
when	O
the	O
number	O
grows	O
to	O
256	B-HyperparameterValue
.	O

In	O
Figure	O
4	O
,	O
we	O
show	O
the	O
trend	O
of	O
these	O
methods	O
on	O
the	O
RACE	B-DatasetName
-	I-DatasetName
m	I-DatasetName
and	O
CB	B-DatasetName
datasets	O
.	O

Sample	O
Efficiency	O
.	O

We	O
report	O
the	O
mean	O
and	O
the	O
standard	O
deviation	O
over	O
3	O
random	O
seeds	O
on	O
the	O
validation	O
set	O
.	O

When	O
the	O
number	O
grows	O
,	O
the	O
performance	O
of	O
these	O
methods	O
becomes	O
closer	O
.	O

Based	O
on	O
this	O
observation	O
,	O
an	O
intuitive	O
extension	O
of	O
our	O
method	O
is	O
to	O
further	O
pre	O
-	O
train	O
the	O
entire	O
model	O
with	O
PVP	O
pre	O
i	O
and	O
fine	O
-	O
tune	O
the	O
model	O
to	O
the	O
corresponding	O
downstream	O
tasks	O
.	O

Prompt	O
pre	O
-	O
training	O
bridges	O
this	O
gap	O
to	O
some	O
extend	O
.	O

This	O
indicates	O
that	O
there	O
still	O
remains	O
a	O
gap	O
between	O
masked	O
language	O
modeling	O
and	O
downstream	O
tasks	O
.	O

The	O
second	O
is	O
the	O
hybrid	O
strategy	O
in	O
Section	O
2	O
.	O

This	O
means	O
pre	O
-	O
training	O
soft	O
prompts	O
and	O
using	O
hybrid	O
prompts	O
are	O
complementary	O
.	O

First	O
,	O
larger	O
models	O
achieve	O
better	O
overall	O
performance	O
,	O
which	O
means	O
increasing	O
the	O
model	O
size	O
still	O
helps	O
under	O
the	O
few	O
-	O
shot	O
setting	O
.	O

Effectiveness	O
From	O
the	O
Table	O
4	O
we	O
have	O
four	O
observations	O
.	O

The	O
main	O
results	O
of	O
English	O
and	O
Chinese	O
datasets	O
are	O
shown	O
in	O
Table	O
4	O
.	O

Main	O
Results	O
.	O

We	O
use	O
the	O
Yelp-5	B-DatasetName
(	O
Zhang	O
et	O
al	O
.	O
,	O
2015a	O
)	O
dataset	O
to	O
train	O
the	O
RoBERTa	B-MethodName
BASE	I-MethodName
model	O
mentioned	O
in	O
Section	O
3.2.3	O
.	O
More	O
details	O
of	O
the	O
training	O
hyper	O
-	O
parameters	O
can	O
be	O
found	O
in	O
the	O
Appendix	O
C.	O

For	O
prompt	O
pre	O
-	O
training	O
,	O
we	O
sample	O
10	O
GB	O
data	O
from	O
OpenWebText	B-DatasetName
(	O
Gokaslan	O
et	O
al	O
.	O
,	O
2019	O
)	O
for	O
English	O
tasks	O
and	O
10	O
GB	O
data	O
from	O
WuDaoCorpora	B-DatasetName
(	O
Yuan	O
et	O
al	O
.	O
,	O
2021	O
)	O
for	O
Chinese	O
tasks	O
.	O

As	O
a	O
result	O
,	O
the	O
tunable	O
parameters	O
is	O
only	O
100×4096	O
=	O
4.1	O
×	O
10	O
5	O
=	O
410K.	O

Since	O
CPM-2	B-MethodName
does	O
not	O
provide	O
other	O
size	O
models	O
,	O
we	O
compare	O
it	O
with	O
mT5	B-MethodName
(	O
Xue	O
et	O
al	O
.	O
,	O
2021	O
)	O
of	O
various	O
sizes	O
.	O

Therefore	O
,	O
we	O
randomly	O
select	O
8	O
samples	O
for	O
each	O
label	O
.	O

For	O
tasks	O
with	O
more	O
than	O
5	O
labels	O
like	O
TNews	O
and	O
YahooAnswer	O
,	O
it	O
is	O
hard	O
to	O
compose	O
a	O
dataset	O
with	O
label	O
-	O
balanced	O
samples	O
.	O

As	O
described	O
in	O
Section	O
2	O
,	O
for	O
tasks	O
with	O
fewer	O
than	O
5	O
labels	O
,	O
we	O
construct	O
D	O
train	O
and	O
D	O
dev	O
with	O
32	O
samples	O
from	O
the	O
original	O
training	O
data	O
and	O
ensure	O
the	O
number	O
of	O
labels	O
is	O
balanced	O
.	O

We	O
conduct	O
experiments	O
on	O
both	O
Chinese	O
and	O
English	O
tasks	O
(	O
see	O
Table	O
3	O
)	O
.	O

Setup	O
.	O

Experiments	O
.	O

We	O
use	O
the	O
PVP	O
in	O
Section	O
3.2.2	O
for	O
pre	O
-	O
training	O
,	O
and	O
then	O
apply	O
pre	O
-	O
trained	O
soft	O
prompts	O
to	O
cover	O
the	O
above	O
mentioned	O
three	O
classification	O
tasks	O
.	O

Since	O
different	O
tasks	O
may	O
have	O
different	O
candidate	O
numbers	O
and	O
lengths	O
,	O
we	O
construct	O
pretraining	O
samples	O
with	O
option	O
numbers	O
varying	O
from	O
2	O
to	O
16	O
2	O
and	O
option	O
lengths	O
from	O
50	O
to	O
20	O
.	O

They	O
tune	O
the	O
entire	O
model	O
with	O
this	O
meta	O
task	O
on	O
a	O
collection	O
of	O
QA	O
datasets	O
and	O
then	O
transfer	O
to	O
other	O
classification	O
tasks	O
under	O
low	O
-	O
resource	O
settings	O
.	O

(	O
2021a	O
)	O
use	O
some	O
hard	O
prompts	O
to	O
unify	O
several	O
tasks	O
as	O
a	O
meta	O
question	O
answering	O
task	O
.	O

Recently	O
,	O
Zhong	O
et	O
al	O
.	O

Constructing	O
a	O
unified	O
PVP	O
is	O
similar	O
to	O
the	O
idea	O
of	O
MultiQA	B-DatasetName
(	O
Talmor	O
and	O
Berant	O
,	O
2019	O
)	O
and	O
Uni	B-DatasetName
-	I-DatasetName
fiedQA	I-DatasetName
(	O
Khashabi	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

Note	O
that	O
in	O
this	O
way	O
,	O
the	O
pre	O
-	O
trained	O
PVPs	O
can	O
be	O
used	O
in	O
single	B-TaskName
text	I-TaskName
classification	I-TaskName
tasks	O
from	O
arbitrary	O
domains	O
and	O
with	O
much	O
more	O
labels	O
.	O

For	O
single	B-TaskName
-	I-TaskName
sentence	I-TaskName
classification	I-TaskName
,	O
the	O
query	O
is	O
the	O
input	O
sentence	O
and	O
the	O
options	O
are	O
the	O
concrete	O
labels	O
.	O

Unifying	O
Task	O
Formats	O
.	O

Therefore	O
,	O
the	O
method	O
described	O
in	O
the	O
following	O
section	O
is	O
proposed	O
to	O
solve	O
this	O
problem	O
.	O

Although	O
the	O
above	O
method	O
improves	O
the	O
model	O
performance	O
,	O
we	O
have	O
to	O
point	O
out	O
that	O
it	O
is	O
still	O
limited	O
to	O
generalize	O
to	O
other	O
single	B-TaskName
-	I-TaskName
text	I-TaskName
classifications	I-TaskName
in	O
different	O
domains	O
and	O
with	O
different	O
numbers	O
of	O
labels	O
.	O

For	O
those	O
with	O
fewer	O
than	O
5	O
labels	O
,	O
we	O
choose	O
a	O
subset	O
from	O
v	O
pre	O
i	O
(	O
Y	O
)	O
as	O
labels	O
.	O

(	O
5	O
)	O
For	O
sentiment	O
classification	O
tasks	O
with	O
5	O
labels	O
,	O
we	O
can	O
use	O
PVP	O
k	O
i	O
=	O
PVP	O
pre	O
i	O
.	O

X	O
.	O
"	O
,	O
v	O
pre	O
i	O
(	O
Y	O
)	O
=	O
[	O
terrible	O
,	O
bad	O
,	O
maybe	O
,	O
good	O
,	O
great	O
]	O
.	O

(	O
f	O
pre	O
i	O
,	O
v	O
pre	O
i	O
)	O
is	O
given	O
as	O
f	O
pre	O
i	O
(	O
x	O
)	O
=	O
"	O
s.	O

Then	O
with	O
a	O
sentence	O
s	O
from	O
the	O
corpus	O
,	O
we	O
have	O
the	O
input	O
x	O
=	O
(	O
s	O
)	O
and	O
the	O
label	O
set	O
Y	O
=	O
{	O
1	O
,	O
2	O
,	O
3	O
,	O
4	O
,	O
5	O
}	O
.	O

Taking	O
sentiment	O
classification	O
as	O
an	O
example	O
,	O
we	O
use	O
another	O
small	O
model	O
to	O
annotate	O
sentiment	O
labels	O
for	O
the	O
sentences	O
from	O
the	O
pre	O
-	O
training	O
corpus	O
and	O
filter	O
out	O
those	O
with	O
low	O
classification	O
probability	O
.	O

For	O
single	B-TaskName
-	I-TaskName
sentence	I-TaskName
classification	I-TaskName
,	O
we	O
create	O
pseudo	O
labels	O
for	O
prompt	O
pre	O
-	O
training	O
.	O

Single	B-TaskName
-	I-TaskName
Sentence	I-TaskName
Classification	I-TaskName
.	O

We	O
concatenate	O
them	O
to	O
form	O
the	O
query	O
.	O

A.s1	O
•	O
•	O
•	O
F.s6.Answer	O
is	O
X	O
.	O
"	O
,	O
v	O
pre	O
i	O
(	O
Y	O
)	O
=	O
[	O
A	O
,	O
B	O
,	O
C	O
,	O
D	O
,	O
E	O
,	O
F].(4	O
)	O
Most	O
multiple	B-TaskName
-	I-TaskName
choice	I-TaskName
tasks	O
can	O
use	O
{	O
f	O
pre	O
i	O
,	O
v	O
pre	O
i	O
}	O
directly	O
as	O
their	O
PVPs	O
.	O
For	O
tasks	O
like	O
reading	B-TaskName
comprehension	I-TaskName
,	O
the	O
input	O
may	O
contain	O
a	O
passage	O
and	O
a	O
question	O
.	O

For	O
x	O
=	O
(	O
s	O
q	O
,	O
s	O
1	O
,	O
s	O
2	O
,	O
•	O
•	O
•	O
,	O
s	O
6	O
)	O
,	O
(	O
f	O
pre	O
i	O
,	O
v	O
pre	O
i	O
)	O
is	O
given	O
as	O
f	O
pre	O
i	O
(	O
x	O
)	O
=	O
"	O
sq	O
?	O

These	O
candidates	O
consist	O
of	O
the	O
right	O
answer	O
,	O
one	O
sentence	O
from	O
the	O
same	O
document	O
but	O
is	O
not	O
adjacent	O
to	O
the	O
query	O
,	O
and	O
four	O
sentences	O
from	O
other	O
documents	O
.	O

Given	O
a	O
sentence	O
as	O
the	O
query	O
s	O
q	O
,	O
the	O
model	O
is	O
trained	O
to	O
select	O
the	O
adjacent	O
sentence	O
from	O
six	O
candidates	O
,	O
denoted	O
as	O
s	O
1	O
∼	O
s	O
6	O
and	O
thus	O
the	O
label	O
set	O
is	O
Y	O
=	O
{	O
1	O
,	O
2	O
,	O
3	O
,	O
4	O
,	O
5	O
,	O
6	O
}	O
.	O

We	O
design	O
a	O
next	O
sentence	O
selection	O
task	O
to	O
pre	O
-	O
train	O
the	O
prompt	O
.	O

If	O
a	O
task	O
requires	O
to	O
measure	O
the	O
similarity	O
between	O
two	O
sentences	O
,	O
the	O
probability	O
over	O
{	O
no	O
,	O
yes	O
}	O
can	O
serve	O
for	O
this	O
task	O
.	O

(	O
3	O
)	O
Designing	O
PVP	O
k	O
i	O
=	O
(	O
f	O
k	O
i	O
,	O
v	O
k	O
i	O
)	O
according	O
k	O
i	O
=	O
v	O
pre	O
i	O
.	O

PVP	O
pre	O
i	O
=	O
(	O
f	O
pre	O
i	O
,	O
v	O
pre	O
i	O
)	O
is	O
given	O
as	O
f	O
pre	O
i	O
(	O
x	O
)	O
=	O
"	O
s1	O
X	O
.s2	O
"	O
,	O
v	O
pre	O
i	O
(	O
Y	O
)	O
=	O
[	O
no	O
,	O
maybe	O
,	O
yes	O
]	O
.	O

We	O
consider	O
the	O
label	O
set	O
|Y|	O
≤	O
3	O
because	O
this	O
covers	O
most	O
sentence	O
pair	O
tasks	O
.	O

To	O
construct	O
signal	O
from	O
unlabeled	O
documents	O
,	O
we	O
set	O
the	O
two	O
sentences	O
next	O
to	O
each	O
other	O
as	O
label	O
2	O
,	O
those	O
from	O
the	O
same	O
document	O
but	O
not	O
true	O
next	O
sentences	O
as	O
1	O
,	O
and	O
those	O
from	O
different	O
documents	O
as	O
0	O
.	O

These	O
labels	O
in	O
Y	O
can	O
respectively	O
indicate	O
that	O
the	O
semantic	O
relation	O
between	O
two	O
sentences	O
is	O
coherent	O
(	O
with	O
label	O
2	O
)	O
,	O
similar	O
(	O
1	O
)	O
and	O
irrelevant	O
(	O
0	O
)	O
.	O

(	O
2019	O
)	O
to	O
a	O
3	O
-	O
class	O
classification	O
with	O
labels	O
Y	O
=	O
{	O
0	O
,	O
1	O
,	O
2	O
}	O
as	O
the	O
pretraining	O
task	O
.	O

To	O
design	O
a	O
PVP	O
for	O
these	O
tasks	O
,	O
we	O
extend	O
the	O
next	O
sentence	O
prediction	O
in	O
Devlin	O
et	O
al	O
.	O

Sentence	B-TaskName
-	I-TaskName
pair	I-TaskName
classification	I-TaskName
tasks	O
such	O
as	O
natural	O
language	O
inference	O
and	O
sentence	O
similarity	O
take	O
two	O
sentences	O
x	O
=	O
(	O
s	O
1	O
,	O
s	O
2	O
)	O
as	O
the	O
input	O
.	O

In	O
this	O
section	O
,	O
we	O
take	O
three	O
typical	O
classification	O
tasks	O
as	O
examples	O
to	O
describe	O
the	O
design	O
of	O
patternverbalizer	O
pairs	O
PVP	O
pre	O
i	O
for	O
prompt	O
pre	O
-	O
training	O
.	O

Designing	O
Pattern	O
-	O
Verbalizer	O
Pairs	O
for	O
Pre	O
-	O
training	O
.	O

Then	O
,	O
for	O
each	O
task	O
PVP	O
k	O
i	O
in	O
T	O
i	O
,	O
we	O
continue	O
to	O
optimize	O
Eq	O
.	O
(	O
2	O
)	O
by	O
using	O
P	O
i	O
as	O
the	O
soft	O
prompts	O
initialization	O
.	O

After	O
pre	O
-	O
training	O
soft	O
prompts	O
on	O
these	O
tasks	O
with	O
all	O
model	O
parameters	O
fixed	O
,	O
we	O
get	O
m	O
pre	O
-	O
trained	O
prompts	O
{	O
P	O
1	O
,	O
P	O
2	O
,	O
...	O
,	O
P	O
m	O
}	O
.	O

For	O
each	O
group	O
,	O
we	O
design	O
a	O
corresponding	O
pre	O
-	O
training	O
task	O
PVP	O
pre	O
i	O
=	O
(	O
f	O
pre	O
i	O
,	O
v	O
pre	O
i	O
)	O
.	O

Formally	O
,	O
suppose	O
we	O
can	O
divide	O
downstream	O
tasks	O
into	O
m	O
groups	O
{	O
T	O
1	O
,	O
T	O
2	O
,	O
...	O
,	O
T	O
m	O
}	O
,	O
where	O
T	O
i	O
is	O
the	O
set	O
containing	O
n	O
i	O
downstream	O
tasks	O
:	O
{	O
PVP	O
1	O
i	O
,	O
PVP	O
2	O
i	O
,	O
...	O
,	O
PVP	O
n	O
i	O
i	O
}	O
,	O
where	O
PVP	O
k	O
i	O
=	O
(	O
f	O
k	O
i	O
,	O
v	O
k	O
i	O
)	O
.	O

Therefore	O
,	O
soft	O
prompts	O
pre	O
-	O
trained	O
by	O
NSP	O
can	O
be	O
a	O
good	O
initialization	O
for	O
these	O
sentence	O
-	O
pair	O
tasks	O
.	O

As	O
shown	O
in	O
Figure	O
3	O
,	O
these	O
tasks	O
all	O
take	O
two	O
sentences	O
as	O
input	O
and	O
compare	O
their	O
semantic	O
meanings	O
.	O

We	O
notice	O
that	O
some	O
groups	O
of	O
downstream	O
tasks	O
are	O
related	O
to	O
certain	O
self	O
-	O
supervised	O
tasks	O
built	O
on	O
unlabeled	O
pre	O
-	O
training	O
corpora	O
.	O

Inspired	O
by	O
this	O
,	O
we	O
propose	O
to	O
pre	O
-	O
train	O
soft	O
prompts	O
.	O

Recently	O
,	O
pre	O
-	O
training	O
has	O
been	O
proven	O
to	O
be	O
an	O
effective	O
method	O
to	O
find	O
a	O
good	O
model	O
initialization	O
.	O

However	O
,	O
we	O
find	O
it	O
hard	O
to	O
learn	O
effective	O
soft	O
prompts	O
,	O
which	O
may	O
result	O
in	O
low	O
performance	O
in	O
various	O
few	O
-	O
shot	O
scenarios	O
.	O

With	O
f	O
(	O
•	O
)	O
and	O
v(•	O
)	O
,	O
a	O
classification	O
task	O
can	O
be	O
represented	O
by	O
a	O
pattern	O
-	O
verbalizer	O
pair	O
(	O
f	O
,	O
v	O
):	O
arg	O
max	O
θ	O
x	O
log	O
p	O
y|x	O
;	O
θ	O
=	O
arg	O
max	O
θ	O
x	O
log	O
p	O
X	O
=	O
v(y)|f	O
(	O
x	O
)	O
;	O
θ	O
,	O
(	O
1	O
)	O
where	O
θ	O
indicates	O
all	O
tunable	O
parameters	O
,	O
especially	O
the	O
parameters	O
of	O
PLMs	O
.	O
For	O
convenience	O
,	O
we	O
use	O
"	O
PVP	O
"	O
to	O
denote	O
this	O
pattern	O
-	O
verbalizer	O
pair	O
(	O
Schick	O
and	O
Schütze	O
,	O
2021a	O
)	O
.	O

Then	O
,	O
a	O
verbalizer	O
v	O
:	O
Y	O
→	O
V	O
*	O
is	O
used	O
to	O
map	O
y	O
to	O
some	O
label	O
tokens	O
v(y	O
)	O
.	O

Taking	O
classification	O
for	O
example	O
,	O
given	O
an	O
input	O
sentence	O
x	O
∈	O
V	O
*	O
and	O
its	O
label	O
y	O
∈	O
Y	O
,	O
a	O
pattern	O
mapping	O
f	O
:	O
V	O
*	O
→	O
V	O
*	O
is	O
first	O
applied	O
to	O
convert	O
x	O
into	O
a	O
new	O
sequence	O
f	O
(	O
x	O
)	O
,	O
where	O
V	O
is	O
the	O
vocabulary	O
of	O
PLMs	O
.	O
f	O
(	O
x	O
)	O
not	O
only	O
adds	O
some	O
prompt	O
tokens	O
as	O
hints	O
,	O
but	O
also	O
preserves	O
the	O
mask	O
token	O
X	O
to	O
let	O
PLMs	O
predict	O
tokens	O
at	O
the	O
masked	O
positions	O
.	O

As	O
shown	O
in	O
Figure	O
1	O
(	O
c	O
)	O
,	O
to	O
reduce	O
the	O
objective	O
gap	O
between	O
pre	O
-	O
training	O
and	O
downstream	O
tasks	O
,	O
promptoriented	O
fine	O
-	O
tuning	O
converts	O
downstream	O
tasks	O
into	O
cloze	O
-	O
style	O
objectives	O
.	O

Overview	O
.	O

use	O
these	O
pre	O
-	O
trained	O
prompts	O
for	O
specific	O
tasks	O
.	O

This	O
suggests	O
that	O
observations	O
on	O
small	O
models	O
can	O
not	O
be	O
directly	O
adapted	O
to	O
large	O
models	O
and	O
finding	O
a	O
good	O
initialization	O
for	O
soft	O
prompts	O
is	O
yet	O
to	O
be	O
explored	O
.	O

However	O
,	O
from	O
the	O
experiments	O
on	O
SST-2	B-DatasetName
(	O
Socher	O
et	O
al	O
.	O
,	O
2013	O
)	O
and	O
BoolQ	B-DatasetName
(	O
Clark	O
et	O
al	O
.	O
,	O
2019	O
)	O
(	O
Table	O
2	O
)	O
,	O
we	O
find	O
that	O
for	O
the	O
11B	O
model	O
,	O
real	O
word	O
initialization	O
has	O
little	O
or	O
even	O
negative	O
impact	O
on	O
the	O
performance	O
in	O
few	O
-	O
shot	O
scenarios	O
.	O

The	O
effectiveness	O
of	O
this	O
approach	O
has	O
been	O
verified	O
on	O
small	O
PLMs	O
(	O
fewer	O
than	O
3B	O
parameters	O
)	O
in	O
previous	O
works	O
(	O
Lester	O
et	O
al	O
.	O
,	O
2021	O
)	O
.	O

In	O
real	O
word	O
initialization	O
,	O
we	O
use	O
the	O
embeddings	O
of	O
concrete	O
words	O
to	O
initialize	O
the	O
soft	O
prompt	O
and	O
test	O
four	O
initialization	O
strategies	O
.	O

Real	O
Word	O
Initialization	O
.	O

In	O
general	O
,	O
common	O
words	O
that	O
explain	O
the	O
meaning	O
of	O
corresponding	O
labels	O
work	O
well	O
.	O

From	O
Table	O
1	O
we	O
can	O
see	O
that	O
the	O
choices	O
of	O
verbalizers	O
influence	O
the	O
performance	O
remarkably	O
.	O

(	O
2021	O
in	O
Figure	O
1	O
(	O
c	O
)	O
and	O
(	O
d	O
)	O
,	O
the	O
verbalizer	O
maps	O
the	O
label	O
"	O
Positive	O
"	O
to	O
"	O
great	O
"	O
.	O

Verbalizer	O
Selection	O
Verbalizer	O
maps	O
taskspecific	O
labels	O
to	O
concrete	O
tokens	O
.	O

Furthermore	O
,	O
different	O
hard	O
prompts	O
affect	O
the	O
performance	O
remarkably	O
,	O
therefore	O
much	O
human	O
labor	O
for	O
prompt	O
design	O
and	O
selection	O
is	O
needed	O
.	O

In	O
Table	O
1	O
,	O
we	O
show	O
the	O
results	O
of	O
combining	O
soft	O
prompts	O
P	O
with	O
three	O
manually	O
designed	O
hard	O
prompts	O
and	O
two	O
auto	O
-	O
generated	O
hard	O
prompts	O
(	O
Gao	O
et	O
al	O
.	O
,	O
2021	O
)	O
on	O
a	O
sentiment	B-TaskName
classification	I-TaskName
task	O
(	O
Socher	O
et	O
al	O
.	O
,	O
2013	O
)	O
.	O

However	O
,	O
previous	O
works	O
train	O
soft	O
prompts	O
jointly	O
with	O
the	O
entire	O
model	O
.	O

Hybrid	B-MethodName
Prompt	I-MethodName
Tuning	I-MethodName
In	O
hybrid	B-MethodName
prompt	I-MethodName
tuning	I-MethodName
,	O
both	O
soft	O
and	O
hard	O
prompts	O
are	O
used	O
(	O
Liu	O
et	O
al	O
.	O
,	O
2021;Han	O
et	O
al	O
.	O
,	O
2021b	O
)	O
.	O

(	O
2021	O
)	O
to	O
use	O
the	O
original	O
validation	O
set	O
as	O
the	O
test	O
set	O
D	O
test	O
,	O
which	O
means	O
|D	O
test	O
|	O
|D	O
train	O
|	O
=	O
|D	O
dev	O
|	O
.	O

(	O
2021	O
)	O
and	O
Gao	O
et	O
al	O
.	O

We	O
analyze	O
three	O
strategies	O
including	O
hybrid	B-MethodName
prompt	I-MethodName
tuning	I-MethodName
,	O
verbalizer	O
selec-	O
(	O
Perez	O
et	O
al	O
.	O
,	O
2021	O
)	O
.	O

To	O
ensure	O
the	O
generalization	O
of	O
pre	O
-	O
trained	O
prompts	O
,	O
we	O
group	O
typical	O
classification	O
tasks	O
into	O
three	O
formats	O
:	O
sentencepair	B-TaskName
classification	I-TaskName
,	O
multiple	B-TaskName
-	I-TaskName
choice	I-TaskName
classification	I-TaskName
,	O
and	O
single	B-TaskName
-	I-TaskName
text	I-TaskName
classification	I-TaskName
,	O
each	O
format	O
corresponding	O
to	O
one	O
self	O
-	O
supervised	O
pre	O
-	O
training	O
task	O
.	O

We	O
follow	O
Zhang	O
et	O
al	O
.	O

To	O
help	O
the	O
model	O
find	O
suitable	O
prompts	O
,	O
we	O
pretrain	O
these	O
tokens	O
with	O
self	O
-	O
supervised	O
tasks	O
on	O
large	O
-	O
scale	O
unlabeled	O
corpora	O
.	O

The	O
above	O
observations	O
reveal	O
that	O
prompt	O
searching	O
for	O
PLMs	O
is	O
not	O
trivial	O
,	O
and	O
carefully	O
initialized	O
soft	O
prompt	O
tokens	O
is	O
crucial	O
.	O

First	O
,	O
soft	O
prompts	O
can	O
be	O
learned	O
end	O
-	O
to	O
-	O
end	O
in	O
comparison	O
to	O
hard	O
prompts	O
.	O

These	O
continuous	O
prompts	O
are	O
generally	O
randomly	O
initialized	O
and	O
learned	O
end	O
-	O
toend	O
.	O

To	O
address	O
this	O
challenge	O
,	O
Lester	O
et	O
al	O
.	O

As	O
shown	O
in	O
Figure	O
1	O
,	O
compared	O
to	O
task	O
-	O
oriented	O
finetuning	O
,	O
prompt	O
-	O
oriented	O
fine	O
-	O
tuning	O
is	O
more	O
similar	O
to	O
the	O
pre	O
-	O
training	O
objectives	O
(	O
masked	O
language	O
modeling	O
)	O
,	O
thereby	O
helping	O
to	O
better	O
use	O
knowledge	O
in	O
PLMs	O
and	O
often	O
obtaining	O
better	O
performance	O
.	O

As	O
shown	O
in	O
Figure	O
1	O
(	O
c	O
)	O
,	O
by	O
adding	O
the	O
prompt	O
"	O
It	O
was	O
X	O
.	O
"	O
to	O
a	O
sentence	O
,	O
we	O
can	O
determine	O
its	O
sentiment	O
polarity	O
with	O
PLMs	O
by	O
predicting	O
"	O
great	O
"	O
or	O
"	O
terrible	O
"	O
at	O
the	O
mask	O
position	O
.	O

In	O
promptoriented	O
fine	O
-	O
tuning	O
,	O
data	O
samples	O
are	O
converted	O
to	O
sequences	O
containing	O
prompt	O
tokens	O
,	O
and	O
downstream	O
tasks	O
are	O
formalized	O
as	O
language	O
modeling	O
problems	O
.	O

The	O
second	O
one	O
is	O
prompt	O
-	O
oriented	O
finetuning	O
(	O
Schick	O
and	O
Schütze	O
,	O
2021a	O
)	O
,	O
which	O
is	O
inspired	O
by	O
the	O
recent	O
works	O
utilizing	O
language	O
prompts	O
to	O
probe	O
the	O
knowledge	O
in	O
PLMs	O
(	O
Petroni	O
et	O
al	O
.	O
,	O
2019;Brown	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

The	O
first	O
one	O
is	O
task	O
-	O
oriented	O
fine	O
-	O
tuning	O
,	O
where	O
a	O
task	O
-	O
specific	O
head	O
is	O
added	O
on	O
top	O
of	O
PLMs	O
,	O
and	O
the	O
entire	O
model	O
is	O
then	O
fine	O
-	O
tuned	O
by	O
optimizing	O
task	O
-	O
specific	O
objectives	O
on	O
corresponding	O
training	O
data	O
.	O

As	O
shown	O
in	O
Figure	O
1	O
(	O
b	O
)	O
and	O
(	O
c	O
)	O
,	O
there	O
are	O
two	O
mainstream	O
FT	B-MethodName
approaches	O
.	O

For	O
simplicity	O
,	O
we	O
name	O
this	O
full	B-MethodName
-	I-MethodName
model	I-MethodName
tuning	I-MethodName
as	O
"	O
FT	B-MethodName
"	O
.	O

various	O
NLP	O
tasks	O
and	O
outperform	O
the	O
approach	O
of	O
learning	O
models	O
from	O
scratch	O
(	O
Han	O
et	O
al	O
.	O
,	O
2021a	O
)	O
.	O

*	O
indicates	O
equal	O
contribution	O
.	O

By	O
tuning	O
the	O
entire	O
model	O
parameters	O
,	O
the	O
versatile	O
knowledge	O
acquired	O
from	O
large	O
-	O
scale	O
unlabeled	O
corpora	O
can	O
be	O
adapted	O
to	O
handling	O
†	O
Corresponding	O
author	O
.	O

Fine	O
-	O
tuning	O
pre	O
-	O
trained	O
language	O
models	O
(	O
PLMs	O
)	O
(	O
Devlin	O
et	O
al	O
.	O
,	O
2019;Radford	O
et	O
al	O
.	O
,	O
2019;Raffel	O
et	O
al	O
.	O
,	O
2020	O
)	O
has	O
made	O
great	O
progress	O
in	O
recent	O
years	O
.	O

Introduction	O
.	O

Our	O
approach	O
is	O
effective	O
and	O
efficient	O
for	O
using	O
large	O
-	O
scale	O
PLMs	O
in	O
practice	O
.	O

Extensive	O
experiments	O
show	O
that	O
tuning	B-MethodName
pre	I-MethodName
-	I-MethodName
trained	I-MethodName
prompts	I-MethodName
for	O
downstream	O
tasks	O
can	O
reach	O
or	O
even	O
outperform	O
full	B-MethodName
-	I-MethodName
model	I-MethodName
fine	I-MethodName
-	I-MethodName
tuning	I-MethodName
under	O
both	O
full	O
-	O
data	O
and	O
few	O
-	O
shot	O
settings	O
.	O

Therefore	O
,	O
in	O
this	O
work	O
,	O
we	O
propose	O
to	O
pre	O
-	O
train	O
prompts	O
by	O
adding	O
soft	O
prompts	O
into	O
the	O
pre	O
-	O
training	O
stage	O
to	O
obtain	O
a	O
better	O
initialization	O
.	O

We	O
attribute	O
this	O
low	O
performance	O
to	O
the	O
manner	O
of	O
initializing	O
soft	O
prompts	O
.	O

Prompts	O
for	O
pre	O
-	O
trained	O
language	O
models	O
(	O
PLMs	O
)	O
have	O
shown	O
remarkable	O
performance	O
by	O
bridging	O
the	O
gap	O
between	O
pre	O
-	O
training	O
tasks	O
and	O
various	O
downstream	O
tasks	O
.	O

For	O
English	O
experiments	O
,	O
we	O
use	O
a	O
dataset	O
from	O
GLUE	O
(	O
Wang	O
et	O
al	O
.	O
,	O
2019b	O
)	O
(	O
SST-2	B-DatasetName
(	O
Socher	O
et	O
al	O
.	O
,	O
2013	O
)	O
)	O
,	O
datasets	O
from	O
Su	O
-	O
perGLUE	O
(	O
Wang	O
et	O
al	O
.	O
,	O
2019a	O
)	O
,	O
(	O
BoolQ	B-DatasetName
(	O
Clark	O
et	O
al	O
.	O
,	O
2019	O
)	O
,	O
CB	O
(	O
De	O
Marneffe	O
et	O
al	O
.	O
,	O
2019	O
)	O
,	O
andRTE	O
(	O
Dagan	O
et	O
al	O
.	O
,	O
2006	O
)	O
)	O
,	O
two	O
extra	O
single	O
-	O
text	O
classification	O
datasets	O
(	O
SST-5	O
(	O
Socher	O
et	O
al	O
.	O
,	O
2013	O
)	O
and	O
YahooAnswers	O
(	O
Zhang	O
et	O
al	O
.	O
,	O
2015b	O
)	O
)	O
,	O
and	O
two	O
standard	O
question	O
answering	O
datasets	O
(	O
RACEmiddle	O
and	O
RACE	O
-	O
high	O
)	O
(	O
Lai	O
et	O
al	O
.	O
,	O
2017	O
)	O
for	O
multiple	B-TaskName
-	I-TaskName
choice	I-TaskName
classification	I-TaskName
.	O

For	O
PPT	B-MethodName
,	O
the	O
consump-	O
.	O

We	O
analyze	O
the	O
time	O
and	O
memory	O
consumption	O
of	O
FT	B-MethodName
and	O
PT	B-MethodName
in	O
this	O
section	O
.	O

For	O
Unified	B-MethodName
PPT	I-MethodName
,	O
we	O
uniformly	O
sample	O
the	O
option	O
numbers	O
from	O
2	O
to	O
16	O
to	O
cover	O
more	O
downstream	O
circumstances	O
.	O

Multiple	B-TaskName
-	I-TaskName
Choice	I-TaskName
Classification	I-TaskName
In	O
the	O
next	O
sentence	O
selection	O
task	O
,	O
giving	O
a	O
query	O
sentence	O
,	O
the	O
options	O
contain	O
one	O
adjacent	O
sentence	O
,	O
one	O
sentence	O
from	O
the	O
same	O
document	O
as	O
the	O
query	O
,	O
and	O
four	O
from	O
the	O
different	O
documents	O
.	O

We	O
filter	O
out	O
the	O
sentences	O
with	O
less	O
than	O
5	O
tokens	O
and	O
the	O
pairs	O
in	O
which	O
the	O
two	O
sentences	O
'	O
length	O
ratios	O
are	O
larger	O
than	O
100	B-HyperparameterValue
.	O

This	O
observation	O
also	O
implies	O
that	O
PT	B-MethodName
is	O
much	O
harder	O
to	O
train	O
than	O
FT	B-MethodName
,	O
which	O
is	O
consistent	O
with	O
the	O
experiment	O
results	O
in	O
the	O
main	O
paper	O
.	O

We	O
find	O
PT	B-MethodName
requires	O
a	O
much	O
larger	O
learning	O
rate	O
than	O
FT	B-MethodName
.	O

Since	O
the	O
tunable	O
parameters	O
are	O
much	O
less	O
in	O
PT	B-MethodName
,	O
8	O
NVIDIA	O
V100	O
32	O
G	O
GPUs	O
are	O
enough	O
for	O
the	O
training	O
.	O

For	O
Prompt	B-MethodName
Tuning	I-MethodName
(	O
PT	B-MethodName
)	O
,	O
we	O
add	O
a	O
set	O
of	O
soft	O
prompts	O
before	O
the	O
input	O
text	O
.	O

C.2	O
Prompt	B-MethodName
Tuning	I-MethodName
.	O

For	O
Full	B-MethodName
-	I-MethodName
Model	I-MethodName
Tuning	I-MethodName
(	O
FT	B-MethodName
)	O
,	O
we	O
tune	O
the	O
entire	O
parameters	O
of	O
the	O
model	O
without	O
concatenating	O
soft	O
prompts	O
.	O

C.1	O
Full	B-MethodName
-	I-MethodName
Model	I-MethodName
Tuning	I-MethodName
.	O

Extensive	O
experiments	O
show	O
that	O
our	O
method	O
significantly	O
outperforms	O
other	O
prompt	B-MethodName
tuning	I-MethodName
baselines	O
,	O
performing	O
comparable	O
or	O
even	O
better	O
than	O
full	B-MethodName
-	I-MethodName
model	I-MethodName
tuning	I-MethodName
.	O

Finally	O
,	O
we	O
do	O
prompt	B-MethodName
tuning	I-MethodName
on	O
downstream	O
tasks	O
based	O
on	O
the	O
pre	O
-	O
trained	O
initialization	O
.	O

When	O
models	O
are	O
large	O
enough	O
,	O
this	O
method	O
can	O
be	O
comparable	O
to	O
full	B-MethodName
-	I-MethodName
model	I-MethodName
tuning	I-MethodName
.	O

We	O
argue	O
that	O
PPT	B-MethodName
can	O
be	O
an	O
effective	O
solution	O
to	O
this	O
problem	O
.	O

In	O
addition	O
,	O
we	O
observe	O
that	O
although	O
PT	B-MethodName
is	O
faster	O
than	O
FT	B-MethodName
in	O
a	O
single	O
optimization	O
step	O
,	O
it	O
converges	O
much	O
slower	O
,	O
which	O
results	O
in	O
an	O
even	O
longer	O
training	O
time	O
.	O

From	O
Table	O
6	O
,	O
we	O
can	O
see	O
that	O
PPT	B-MethodName
and	O
Unified	B-MethodName
PPT	I-MethodName
still	O
outperform	O
the	O
Vanilla	B-MethodName
PT	I-MethodName
on	O
most	O
datasets	O
.	O

We	O
discuss	O
how	O
the	O
performance	O
of	O
FT	B-MethodName
,	O
PT	B-MethodName
,	O
and	O
PPT	B-MethodName
varies	O
when	O
the	O
number	O
of	O
training	O
samples	O
increases	O
.	O

Table	O
6	O
:	O
The	O
performance	O
of	O
FT	B-MethodName
,	O
PT	B-MethodName
,	O
PPT	B-MethodName
,	O
and	O
Unified	B-MethodName
PPT	I-MethodName
when	O
the	O
full	O
training	O
datasets	O
are	O
available	O
.	O

For	O
the	O
small	O
number	O
of	O
samples	O
,	O
PPT	B-MethodName
is	O
consistently	O
better	O
than	O
Vanilla	B-MethodName
PT	I-MethodName
.	O

However	O
,	O
we	O
can	O
see	O
that	O
Unified	B-MethodName
PPT	I-MethodName
still	O
achieves	O
the	O
best	O
performance	O
,	O
even	O
exceeding	O
FT	B-MethodName
by	O
a	O
large	O
margin	O
.	O

We	O
do	O
not	O
use	O
PPT	B-MethodName
for	O
singlesentence	O
classification	O
discussed	O
in	O
Section	O
3.2.3	O
because	O
it	O
is	O
hard	O
to	O
find	O
other	O
suitable	O
datasets	O
to	O
train	O
the	O
pseudo	O
label	O
annotator	O
.	O

PT	B-MethodName
(	O
MC	B-TaskName
)	O
means	O
we	O
solve	O
the	O
task	O
in	O
a	O
multiple	B-TaskName
-	I-TaskName
choice	I-TaskName
classification	I-TaskName
format	O
without	O
prompt	O
pre	O
-	O
training	O
.	O

For	O
some	O
datasets	O
like	O
SST-2	B-DatasetName
,	O
the	O
variance	O
reaches	O
15.5	O
which	O
means	O
the	O
model	O
does	O
not	O
perform	O
better	O
than	O
random	O
guesses	O
under	O
some	O
a	O
verbalizer	O
to	O
map	O
the	O
labels	O
to	O
the	O
intuitively	O
selected	O
words	O
.	O

Fourth	O
,	O
PPT	B-MethodName
results	O
in	O
lower	O
variances	O
on	O
most	O
of	O
the	O
datasets	O
.	O

However	O
,	O
since	O
we	O
focus	O
on	O
PT	B-MethodName
in	O
this	O
paper	O
,	O
we	O
leave	O
this	O
as	O
future	O
work	O
.	O

Third	O
,	O
PPT	B-MethodName
outperforms	O
FT	B-MethodName
on	O
all	O
Chinese	O
datasets	O
and	O
most	O
English	O
datasets	O
.	O

Similar	O
phenomenons	O
are	O
observed	O
on	O
other	O
datasets	O
like	O
RACE	B-DatasetName
-	I-DatasetName
m	I-DatasetName
,	O
LCQMC	B-DatasetName
,	O
and	O
C	B-DatasetName
3	I-DatasetName
,	O
where	O
adding	O
hard	O
prompts	O
to	O
PPT	B-MethodName
continues	O
to	O
improve	O
results	O
.	O

Although	O
PPT	B-MethodName
is	O
worse	O
than	O
Hybrid	B-MethodName
PT	I-MethodName
on	O
BoolQ	B-DatasetName
,	O
combining	O
PPT	B-MethodName
and	O
hard	O
prompts	O
(	O
Hybrid	B-MethodName
PPT	I-MethodName
)	O
outperforms	O
all	O
baselines	O
.	O

Second	O
,	O
PPT	B-MethodName
outperforms	O
Vanilla	B-MethodName
PT	I-MethodName
and	O
LM	B-MethodName
Adaption	I-MethodName
on	O
most	O
datasets	O
significantly	O
.	O

Since	O
CPM-2	B-MethodName
outperforms	O
mT5	B-MethodName
-	I-MethodName
XXL	I-MethodName
across	O
all	O
tasks	O
,	O
we	O
use	O
CPM-2	B-MethodName
as	O
the	O
base	O
model	O
.	O

Note	O
that	O
for	O
Chinese	O
experiments	O
,	O
CPM-2	B-MethodName
and	O
mT5	B-MethodName
-	I-MethodName
XXL	I-MethodName
share	O
the	O
same	O
parameter	O
scale	O
.	O

Therefore	O
,	O
we	O
study	O
PT	B-MethodName
on	O
the	O
large	O
-	O
scale	O
pre	O
-	O
trained	O
model	O
.	O

We	O
test	O
two	O
variants	O
of	O
PPT	B-MethodName
:	O
Hybrid	B-MethodName
PPT	I-MethodName
,	O
in	O
which	O
carefully	O
designed	O
hard	O
prompts	O
are	O
combined	O
with	O
pre	O
-	O
trained	O
soft	O
prompt	O
,	O
and	O
Unified	B-MethodName
PPT	I-MethodName
,	O
in	O
which	O
all	O
tasks	O
are	O
unified	O
in	O
the	O
multiple	B-TaskName
-	I-TaskName
choice	I-TaskName
classification	I-TaskName
format	O
.	O

(	O
2021	O
)	O
in	O
which	O
the	O
T5	B-MethodName
model	O
is	O
further	O
pre	O
-	O
trained	O
for	O
10	O
K	O
steps	O
with	O
language	O
modeling	O
to	O
reduce	O
the	O
gap	O
between	O
the	O
pre	O
-	O
training	O
and	O
PT	B-MethodName
.	O

We	O
also	O
consider	O
LM	B-MethodName
Adaption	I-MethodName
used	O
in	O
Lester	O
et	O
al	O
.	O

The	O
first	O
baseline	O
is	O
Vanilla	B-MethodName
PT	I-MethodName
,	O
where	O
the	O
soft	O
prompts	O
are	O
randomly	O
initialized	O
from	O
a	O
normal	O
distribution	O
.	O

In	O
the	O
block	O
PT	B-MethodName
,	O
we	O
show	O
the	O
results	O
of	O
PPT	B-MethodName
and	O
other	O
baselines	O
.	O

In	O
the	O
block	O
FT	B-MethodName
,	O
we	O
present	O
the	O
FT	B-MethodName
results	O
of	O
the	O
T5	B-MethodName
model	O
from	O
the	O
size	O
small	O
to	O
XXL	O
.	O

Compared	O
with	O
the	O
11B	O
(	O
1.1	O
×	O
10	O
10	O
)	O
parameters	O
of	O
FT	B-MethodName
,	O
PT	B-MethodName
only	O
needs	O
to	O
store	O
3000	O
times	O
smaller	O
parameters	O
for	O
each	O
task	O
.	O

Consistently	O
,	O
we	O
use	O
100	B-HyperparameterValue
soft	B-HyperparameterName
tokens	I-HyperparameterName
for	O
PT	B-MethodName
.	O

For	O
Chinese	O
datasets	O
,	O
we	O
do	O
PT	B-MethodName
based	O
on	O
a	O
11B	O
model	O
CPM-2	B-MethodName
.	O

We	O
also	O
evaluate	O
FT	B-MethodName
on	O
various	O
sizes	O
of	O
T5	B-MethodName
to	O
verify	O
that	O
larger	O
models	O
perform	O
better	O
and	O
thus	O
improving	O
PT	B-MethodName
based	O
on	O
T5	B-MethodName
-	I-MethodName
XXL	I-MethodName
is	O
meaningful	O
.	O

For	O
English	O
datasets	O
,	O
we	O
conduct	O
PT	B-MethodName
based	O
on	O
T5	B-MethodName
-	I-MethodName
XXL	I-MethodName
with	O
11B	O
parameters	O
because	O
previous	O
works	O
(	O
Lester	O
et	O
al	O
.	O
,	O
2021;Zhang	O
et	O
al	O
.	O
,	O
2022	O
)	O
have	O
shown	O
that	O
,	O
T5	B-MethodName
-	I-MethodName
XXL	I-MethodName
is	O
comparable	O
with	O
FT	B-MethodName
under	O
the	O
full	O
-	O
data	O
setting	O
.	O

However	O
,	O
our	O
PPT	B-MethodName
focuses	O
on	O
tuning	O
soft	O
prompts	O
with	O
the	O
main	O
body	O
of	O
PLMs	O
fixed	O
and	O
our	O
pretraining	O
is	O
conducted	O
on	O
fully	O
unsupervised	O
data	O
,	O
rather	O
than	O
the	O
collection	O
of	O
supervised	O
datasets	O
.	O

Specifically	O
,	O
for	O
sentence	B-TaskName
-	I-TaskName
pair	I-TaskName
classification	I-TaskName
,	O
the	O
query	O
is	O
the	O
concatenation	O
of	O
the	O
two	O
sentences	O
and	O
there	O
are	O
three	O
options	O
:	O
no	O
,	O
maybe	O
,	O
and	O
yes	O
.	O

The	O
above	O
-	O
mentioned	O
PVPs	O
for	O
pre	O
-	O
training	O
can	O
be	O
unified	O
to	O
a	O
single	O
format	O
:	O
multiple	B-TaskName
-	I-TaskName
choice	I-TaskName
classification	I-TaskName
.	O

In	O
practice	O
,	O
we	O
use	O
a	O
RoBERTa	B-MethodName
BASE	I-MethodName
(	O
Liu	O
et	O
al	O
.	O
,	O
2019	O
)	O
model	O
fine	O
-	O
tuned	O
on	O
a	O
5	O
-	O
class	O
sentiment	O
classification	O
dataset	O
other	O
than	O
the	O
few	O
-	O
shot	O
datasets	O
we	O
evaluate	O
on	O
.	O

Many	O
tasks	O
can	O
be	O
formulated	O
as	O
multiple	B-TaskName
-	I-TaskName
choice	I-TaskName
classification	I-TaskName
,	O
which	O
takes	O
a	O
query	O
and	O
several	O
answer	O
candidates	O
as	O
the	O
input	O
.	O

Multiple	B-TaskName
-	I-TaskName
Choice	I-TaskName
Classification	I-TaskName
.	O

Sentence	B-TaskName
-	I-TaskName
Pair	I-TaskName
Classification	I-TaskName
.	O

For	O
instance	O
,	O
some	O
tasks	O
in	O
the	O
form	O
of	O
sentence	B-TaskName
-	I-TaskName
pair	I-TaskName
classification	I-TaskName
,	O
such	O
as	O
natural	B-TaskName
language	I-TaskName
inference	I-TaskName
and	O
sentence	O
similarity	O
,	O
are	O
similar	O
to	O
the	O
next	O
sentence	O
prediction	O
(	O
NSP	O
)	O
(	O
Devlin	O
et	O
al	O
.	O
,	O
2019	O
)	O
task	O
used	O
in	O
the	O
pre	O
-	O
training	O
stage	O
.	O

The	O
parameter	O
initialization	O
usually	O
has	O
a	O
large	O
impact	O
on	O
the	O
difficulty	O
of	O
the	O
model	O
training	O
and	O
optimization	O
,	O
and	O
our	O
pilot	O
experiments	O
have	O
shown	O
that	O
existing	O
initialization	O
strategies	O
have	O
little	O
or	O
even	O
negative	O
impact	O
on	O
the	O
PT	B-MethodName
performance	O
of	O
large	O
-	O
scale	O
PLMs	O
.	O
We	O
refer	O
more	O
details	O
of	O
these	O
pilot	O
experiments	O
to	O
Section	O
4	O
.	O

By	O
tuning	O
P	O
,	O
Eq	O
.	O
(	O
1	O
)	O
is	O
replaced	O
by	O
arg	O
max	O
P	O
x	O
log	O
p	O
X	O
=	O
v(y	O
)	O
|	O
[	O
P	O
;	O
f	O
(	O
x	O
)	O
]	O
;	O
P	O
.(2	O
)	O
Owing	O
to	O
the	O
power	O
of	O
large	O
-	O
scale	O
PLMs	O
,	O
Eq	O
.	O
(	O
2	O
)	O
is	O
verified	O
to	O
be	O
comparable	O
to	O
these	O
FT	B-MethodName
methods	O
under	O
full	O
-	O
data	O
settings	O
.	O

In	O
PT	B-MethodName
(	O
Lester	O
et	O
al	O
.	O
,	O
2021	O
)	O
,	O
a	O
set	O
of	O
soft	O
prompts	O
P	O
are	O
concatenated	O
to	O
the	O
beginning	O
of	O
the	O
sequence	O
and	O
the	O
model	O
input	O
becomes	O
[	O
P	O
;	O
f	O
(	O
x	O
)	O
]	O
,	O
where	O
[	O
•	O
;	O
•	O
]	O
is	O
the	O
concatenation	O
operation	O
.	O

Following	O
the	O
approach	O
of	O
T5	B-MethodName
(	O
Raffel	O
et	O
al	O
.	O
,	O
2020	O
)	O
and	O
PT	B-MethodName
(	O
Lester	O
et	O
al	O
.	O
,	O
2021	O
)	O
,	O
we	O
solve	O
all	O
downstream	O
tasks	O
in	O
a	O
text	O
-	O
to	O
-	O
text	O
format	O
.	O

In	O
the	O
following	O
sections	O
,	O
we	O
describe	O
our	O
PPT	B-MethodName
framework	O
and	O
show	O
in	O
experiments	O
that	O
PPT	B-MethodName
not	O
only	O
provides	O
a	O
good	O
prompt	O
initialization	O
,	O
but	O
also	O
takes	O
advantage	O
of	O
the	O
good	O
verbalizer	O
,	O
and	O
is	O
complementary	O
to	O
hybrid	O
prompts	O
.	O

To	O
summarize	O
,	O
although	O
the	O
above	O
enhancement	O
strategies	O
can	O
not	O
help	O
PT	B-MethodName
achieve	O
comparable	O
results	O
with	O
FT	B-MethodName
under	O
few	O
-	O
shot	O
settings	O
,	O
they	O
are	O
still	O
the	O
key	O
factors	O
that	O
influence	O
the	O
PT	B-MethodName
performance	O
.	O

This	O
also	O
guides	O
our	O
verbalizer	O
selection	O
for	O
PPT	B-MethodName
in	O
Section	O
3	O
.	O

For	O
instance	O
,	O
1	O
Using	O
100	B-HyperparameterValue
soft	O
prompt	O
tokens	O
achieves	O
the	O
best	O
performance	O
in	O
Lester	O
et	O
al	O
.	O

We	O
can	O
see	O
that	O
hard	O
prompts	O
improve	O
PT	B-MethodName
,	O
but	O
still	O
under	O
-	O
perform	O
FT	B-MethodName
.	O

In	O
PT	B-MethodName
where	O
only	O
prompt	O
tokens	O
are	O
tunable	O
,	O
the	O
effectiveness	O
of	O
hybrid	O
prompts	O
is	O
under	O
-	O
explored	O
.	O

Besides	O
the	O
effectiveness	O
,	O
PPT	B-MethodName
also	O
retains	O
the	O
parameter	O
efficiency	O
of	O
PT	B-MethodName
,	O
which	O
is	O
valuable	O
for	O
future	O
applications	O
on	O
large	O
-	O
scale	O
PLMs	O
.	O
Pilot	O
Experiments	O
.	O

We	O
evaluate	O
PPT	B-MethodName
on	O
several	O
datasets	O
based	O
on	O
three	O
11B	O
PLMs	O
:	O
T5	B-MethodName
-	I-MethodName
XXL	I-MethodName
(	O
Raffel	O
et	O
al	O
.	O
,	O
2020	O
)	O
,	O
mT5	B-MethodName
-	I-MethodName
XXL	I-MethodName
(	O
Xue	O
et	O
al	O
.	O
,	O
2021	O
)	O
and	O
CPM-2	B-MethodName
(	O
Zhang	O
et	O
al	O
.	O
,	O
2022	O
)	O
in	O
few	O
-	O
shot	O
scenarios	O
.	O

We	O
name	O
this	O
Pre	B-MethodName
-	I-MethodName
trained	I-MethodName
Prompt	I-MethodName
Tuning	I-MethodName
framework	O
"	O
PPT	B-MethodName
"	O
.	O

In	O
addition	O
,	O
we	O
find	O
multiple	B-TaskName
-	I-TaskName
choice	I-TaskName
classification	I-TaskName
more	O
general	O
among	O
these	O
formats	O
and	O
we	O
can	O
unify	O
all	O
classification	O
tasks	O
to	O
this	O
format	O
.	O

Our	O
discoveries	O
are	O
as	O
follows	O
:	O
(	O
1	O
)	O
the	O
verbalizer	O
choice	O
has	O
a	O
large	O
impact	O
on	O
the	O
performance	O
;	O
(	O
2	O
)	O
simply	O
initializing	O
soft	O
prompts	O
with	O
concrete	O
word	O
embeddings	O
fails	O
to	O
improve	O
the	O
performance	O
,	O
yet	O
(	O
3	O
)	O
combining	O
soft	O
and	O
hard	O
prompts	O
is	O
helpful	O
;	O
and	O
(	O
4	O
)	O
all	O
these	O
methods	O
can	O
not	O
handle	O
few	O
-	O
shot	O
prompt	B-MethodName
tuning	I-MethodName
problems	O
well	O
.	O

Specifically	O
,	O
we	O
con	O
-	O
duct	O
pilot	O
experiments	O
to	O
empirically	O
analyze	O
the	O
effectiveness	O
of	O
PT	B-MethodName
on	O
PLMs	O
in	O
Section	O
2	O
,	O
which	O
is	O
ignored	O
by	O
most	O
existing	O
works	O
.	O

However	O
,	O
as	O
shown	O
in	O
Figure	O
2(b	O
)	O
,	O
we	O
find	O
that	O
PT	B-MethodName
performs	O
much	O
worse	O
than	O
FT	B-MethodName
under	O
few	O
-	O
shot	O
settings	O
,	O
which	O
may	O
hinder	O
the	O
application	O
of	O
PT	B-MethodName
in	O
various	O
low	O
-	O
resource	O
scenarios	O
.	O

Second	O
,	O
PT	B-MethodName
is	O
an	O
efficient	O
and	O
effective	O
paradigm	O
for	O
the	O
practical	O
use	O
of	O
largescale	O
PLMs	O
,	O
which	O
is	O
comparable	O
to	O
FT	B-MethodName
when	O
downstream	O
data	O
are	O
sufficient	O
(	O
Figure	O
2(a	O
)	O
)	O
.	O

PT	B-MethodName
has	O
two	O
promising	O
advantages	O
.	O

To	O
avoid	O
storing	O
the	O
entire	O
model	O
for	O
each	O
downstream	O
task	O
,	O
PT	B-MethodName
freezes	O
all	O
PLM	O
parameters	O
and	O
merely	O
tunes	O
soft	O
prompts	O
,	O
without	O
adding	O
any	O
intermediate	O
layers	O
and	O
task	O
-	O
specific	O
components	O
.	O

Specifically	O
,	O
PT	B-MethodName
uses	O
soft	O
prompts	O
composed	O
of	O
continuous	O
embeddings	O
instead	O
of	O
hard	O
prompts	O
(	O
discrete	O
language	O
phrases	O
)	O
.	O

(	O
2021	O
)	O
proposes	O
prompt	B-MethodName
tuning	I-MethodName
(	O
PT	B-MethodName
)	O
to	O
adapt	O
large	O
PLMs	O
to	O
downstream	O
tasks	O
cheaply	O
,	O
as	O
shown	O
in	O
Figure	O
1	O
(	O
d	O
)	O
.	O

Although	O
FT	B-MethodName
has	O
shown	O
promising	O
results	O
,	O
with	O
the	O
rapid	O
growth	O
of	O
model	O
scale	O
,	O
fine	O
-	O
tuning	O
and	O
storing	O
the	O
entire	O
large	O
model	O
for	O
each	O
downstream	O
task	O
becomes	O
much	O
more	O
expensive	O
.	O

The	O
code	O
is	O
publicly	O
available	O
at	O
https://	O
github.com/thu-coai/PPT	B-MethodName
.	O

To	O
ensure	O
the	O
generalization	O
of	O
PPT	B-MethodName
,	O
we	O
formulate	O
similar	O
classification	O
tasks	O
into	O
a	O
unified	O
task	O
form	O
and	O
pre	O
-	O
train	O
soft	O
prompts	O
for	O
this	O
unified	O
task	O
.	O

We	O
name	O
this	O
Pretrained	B-MethodName
Prompt	I-MethodName
Tuning	I-MethodName
framework	O
"	O
PPT	B-MethodName
"	O
.	O

In	O
our	O
pilot	O
experiments	O
,	O
we	O
find	O
that	O
prompt	B-MethodName
tuning	I-MethodName
performs	O
comparably	O
with	O
conventional	O
full	B-MethodName
-	I-MethodName
model	I-MethodName
tuning	I-MethodName
when	O
downstream	O
data	O
are	O
sufficient	O
,	O
whereas	O
it	O
is	O
much	O
worse	O
under	O
fewshot	O
learning	O
settings	O
,	O
which	O
may	O
hinder	O
the	O
application	O
of	O
prompt	B-MethodName
tuning	I-MethodName
.	O

However	O
,	O
prompt	B-MethodName
tuning	I-MethodName
is	O
yet	O
to	O
be	O
fully	O
explored	O
.	O

Among	O
these	O
methods	O
,	O
prompt	B-MethodName
tuning	I-MethodName
,	O
which	O
freezes	O
PLMs	O
and	O
only	O
tunes	O
soft	O
prompts	O
,	O
provides	O
an	O
efficient	O
and	O
effective	O
solution	O
for	O
adapting	O
largescale	O
PLMs	O
to	O
downstream	O
tasks	O
.	O

After	O
constructing	O
clusters	O
given	O
a	O
document	O
,	O
a	O
word	O
-	O
graph	O
is	O
created	O
for	O
each	O
cluster	O
to	O
get	O
abstractive	O
fusions	O
from	O
these	O
related	O
sentences	O
.	O

We	O
use	O
the	O
NLTK	O
4	O
and	O
BNLP	O
5	O
to	O
preprocess	O
each	O
sentence	O
and	O
obtain	O
a	O
more	O
accurate	O
representation	O
of	O
the	O
information	O
.	O

BenSumm	B-MethodName
Model	O
.	O

The	O
summary	O
of	O
our	O
contributions	O
:	O
•	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
our	O
Bengali	O
Text	B-TaskName
Summarization	I-TaskName
model	O
(	O
BenSumm	O
)	O
is	O
the	O
very	O
first	O
unsupervised	O
model	O
to	O
generate	O
abstractive	O
summary	O
from	O
Bengali	O
text	O
documents	O
while	O
being	O
simple	O
yet	O
robust	O
.	O

A	O
cluster	O
of	O
sentences	O
uses	O
multi	O
-	O
sentence	O
compression	O
(	O
MSC	O
)	O
to	O
summarize	O
into	O
one	O
single	O
sentence	O
originally	O
called	O
sentence	O
fusion	O
(	O
Barzilay	O
and	O
McKeown	O
,	O
2005;Nayeem	O
and	O
Chali	O
,	O
2017b	O
)	O
.	O

A	O
detailed	O
illustration	O
of	O
our	O
BenSumm	B-MethodName
model	O
with	O
outputs	O
from	O
each	O
step	O
for	O
a	O
sample	O
input	O
document	O
is	O
presented	O
in	O
Figure	O
6	O
.	O

We	O
have	O
implemented	O
a	O
graphbased	B-MethodName
model	I-MethodName
to	O
fuse	O
multiple	O
related	O
sentences	O
,	O
requiring	O
only	O
a	O
POS	O
tagger	O
and	O
a	O
pre	O
-	O
trained	O
language	O
model	O
.	O

In	O
this	O
paper	O
,	O
we	O
have	O
developed	O
an	O
unsupervised	B-TaskName
abstractive	I-TaskName
text	I-TaskName
summarization	I-TaskName
system	O
for	O
Bengali	O
text	O
documents	O
.	O

We	O
get	O
an	O
average	O
score	O
of	O
4.41	B-MetricValue
,	O
3.95	B-MetricValue
,	O
and	O
4.2	B-MetricValue
in	O
content	B-MetricName
,	O
readability	B-MetricName
,	O
and	O
overall	B-MetricName
quality	I-MetricName
respectively	O
.	O

Here	O
,	O
content	B-MetricName
means	O
how	O
well	O
the	O
summary	O
can	O
convey	O
the	O
original	O
input	O
document	O
's	O
meaning	O
,	O
and	O
readability	B-MetricName
represents	O
the	O
grammatical	O
correction	O
and	O
the	O
overall	O
summary	O
sentence	O
coherence	O
.	O

They	O
have	O
evaluated	O
each	O
system	O
generated	O
summary	O
with	O
scores	B-MetricName
ranges	O
from	O
1	B-MetricValue
to	O
5	B-MetricValue
,	O
where	O
1	B-MetricValue
represents	O
very	O
poor	O
performance	O
,	O
and	O
5	B-MetricValue
represents	O
very	O
good	O
performance	O
.	O

Therefore	O
,	O
we	O
assign	O
three	O
different	O
evaluators	O
to	O
each	O
summary	O
generated	O
from	O
our	O
abstractive	O
system	O
(	O
BenSumm	B-MethodName
[	O
Abs	O
]	O
)	O
considering	O
three	O
different	O
aspects	O
,	O
i.e.	O
,	O
Content	B-MetricName
,	O
Readability	B-MetricName
,	O
and	O
Overall	B-MetricName
Quality	I-MetricName
.	O

10	O
Human	O
Evaluation	O
Though	O
ROUGE	B-MetricName
(	O
Lin	O
,	O
2004	O
)	O
has	O
been	O
shown	O
to	O
correlate	O
well	O
with	O
human	O
judgments	O
,	O
it	O
is	O
biased	O
towards	O
surface	O
level	O
lexical	O
similarities	O
,	O
and	O
this	O
makes	O
it	O
inappropriate	O
for	O
the	O
evaluation	O
of	O
abstractive	O
summaries	O
.	O

We	O
get	O
better	O
scores	O
in	O
terms	O
of	O
R1	B-MetricName
and	O
RL	B-MetricName
compared	O
to	O
the	O
baselines	O
.	O

Moreover	O
,	O
we	O
compare	O
our	O
extractive	O
version	O
of	O
our	O
model	O
BenSumm	B-MethodName
without	O
the	O
sentence	O
fusion	O
component	O
.	O

According	O
to	O
Table	O
2	O
,	O
our	O
abstractive	O
summarization	O
model	O
outperforms	O
all	O
the	O
extractive	O
baselines	O
in	O
terms	O
of	O
all	O
the	O
ROUGE	B-MetricName
metrics	O
even	O
though	O
the	O
dataset	O
itself	O
is	O
highly	O
abstractive	B-MetricName
(	O
reference	O
summary	O
contains	O
almost	O
73	B-MetricValue
%	I-MetricValue
new	O
words	O
)	O
.	O

Results	O
We	O
report	O
our	O
model	O
's	O
performance	O
compared	O
with	O
the	O
baselines	O
in	O
terms	O
of	O
F1	B-MetricName
scores	O
of	O
R-1	B-MetricName
,	O
R-2	B-MetricName
,	O
and	O
R	B-MetricName
-	I-MetricName
L	I-MetricName
in	O
Table	O
2	O
.	O

Baseline	O
Systems	O
We	O
compare	O
our	O
system	O
with	O
various	O
well	O
established	O
baseline	O
systems	O
like	O
LexRank	B-MethodName
(	O
Erkan	O
and	O
Radev	O
,	O
2004	O
)	O
,	O
TextRank	B-MethodName
(	O
Mihalcea	O
and	O
Tarau	O
,	O
2004	O
)	O
,	O
GreedyKL	B-MethodName
(	O
Haghighi	O
and	O
Vanderwende	O
,	O
2009	O
)	O
,	O
and	O
SumBasic	B-MethodName
(	O
Nenkova	O
and	O
Vanderwende	O
,	O
2005	O
)	O
.	O

Since	O
ROUGE	B-MetricName
computes	O
scores	O
based	O
on	O
the	O
lexical	O
overlap	O
at	O
the	O
surface	O
level	O
,	O
there	O
is	O
no	O
difference	O
in	O
implementation	O
for	O
summary	O
evaluation	O
of	O
the	O
Bengali	O
language	O
.	O

We	O
report	O
unigram	O
and	O
bigram	O
overlap	O
(	O
ROUGE-1	B-MetricName
and	O
ROUGE-2	B-MetricName
)	O
to	O
measure	O
informativeness	O
and	O
the	O
longest	O
common	O
subsequence	O
(	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
)	O
to	O
measure	O
the	O
summaries	O
'	O
fluency	O
.	O

Automatic	O
Evaluation	O
We	O
evaluate	O
our	O
system	O
(	O
BenSumm	B-MethodName
)	O
using	O
an	O
automatic	O
evaluation	O
metric	O
ROUGE	B-TaskName
F1	O
(	O
Lin	O
,	O
2004	O
)	O
without	O
any	O
limit	O
of	O
words	O
.	O

We	O
remove	O
the	O
abstractive	B-MethodName
sentence	I-MethodName
fusion	I-MethodName
part	O
to	O
compare	O
with	O
the	O
baselines	O
for	O
the	O
extractive	O
evaluation	O
.	O

Moreover	O
,	O
to	O
provide	O
our	O
proposed	O
framework	O
's	O
effectiveness	O
,	O
we	O
also	O
experiment	O
with	O
an	O
extractive	O
dataset	O
BNLPC	B-DatasetName
7	O
(	O
Haque	O
et	O
al	O
.	O
,	O
2015	O
)	O
.	O

From	O
the	O
dataset	O
,	O
we	O
measure	O
the	O
copy	B-MetricName
rate	I-MetricName
between	O
the	O
source	O
document	O
and	O
the	O
human	O
summaries	O
.	O

6	O
We	O
collected	O
the	O
human	O
written	O
document	O
-	O
summary	O
pairs	O
from	O
the	O
several	O
printed	O
copy	O
of	O
NCTB	B-DatasetName
books	I-DatasetName
.	O

We	O
conduct	O
experiments	O
on	O
our	O
dataset	O
which	O
consists	O
of	O
139	O
samples	O
of	O
human	B-TaskName
-	I-TaskName
written	I-TaskName
abstractive	I-TaskName
document	I-TaskName
-	I-TaskName
summary	I-TaskName
pairs	I-TaskName
written	O
professional	O
summary	O
writers	O
of	O
the	O
National	O
Curriculum	O
and	O
Textbook	O
Board	O
(	O
NCTB	O
)	O
.	O

This	O
section	O
presents	O
our	O
experimental	O
details	O
for	O
assessing	O
the	O
performance	O
of	O
the	O
proposed	O
Ben	B-MethodName
-	I-MethodName
Summ	I-MethodName
model	O
.	O

Word	B-MethodName
Graph	I-MethodName
Generation	O
Sentence	B-MethodName
Fusion	I-MethodName
.	O

Word	B-MethodName
Graph	I-MethodName
Generation	O
Sentence	B-MethodName
Fusion	I-MethodName
.	O

We	O
get	O
multiple	O
weighted	O
sentences	O
(	O
see	O
Figure	O
2	O
)	O
form	O
the	O
clusters	O
using	O
the	O
ranking	B-MethodName
strategy	O
(	O
Boudin	O
and	O
Morin	O
,	O
2013	O
)	O
.	O

Word	B-MethodName
Graph	I-MethodName
(	O
WG	B-MethodName
)	O
Construction	O
.	O

We	O
chose	O
to	O
build	O
an	O
abstractive	O
summarizer	O
with	O
a	O
sentence	B-MethodName
fusion	I-MethodName
technique	O
by	O
generating	O
word	O
graphs	O
(	O
Filippova	O
,	O
2010;Boudin	O
and	O
Morin	O
,	O
2013	O
)	O
for	O
the	O
Bengali	O
Language	O
.	O

We	O
measure	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
clusters	I-HyperparameterName
for	O
a	O
given	O
document	O
using	O
the	O
silhouette	O
value	O
.	O

There	O
will	O
be	O
a	O
minimum	B-HyperparameterName
of	O
2	B-HyperparameterValue
and	O
a	O
maximum	B-HyperparameterName
of	O
n	B-HyperparameterValue
−	I-HyperparameterValue
1	I-HyperparameterValue
clusters	B-HyperparameterName
.	O

We	O
use	O
hierarchical	B-MethodName
agglomerative	I-MethodName
clustering	I-MethodName
with	O
the	O
ward	B-MethodName
's	I-MethodName
method	I-MethodName
(	O
Murtagh	O
and	O
Legendre	O
,	O
2014	O
)	O
.	O

Therefore	O
,	O
we	O
calculate	O
the	O
cosine	O
similarity	O
between	O
the	O
sentence	O
vectors	O
obtained	O
from	O
ULMfit	B-MethodName
pre	O
-	O
trained	O
language	O
model	O
(	O
Howard	O
and	O
Ruder	O
,	O
2018	O
)	O
.	O

The	O
Term	B-MethodName
Frequency	I-MethodName
-	I-MethodName
Inverse	I-MethodName
Document	I-MethodName
Frequency	I-MethodName
(	O
TF	B-MethodName
-	I-MethodName
IDF	I-MethodName
)	O
measure	O
does	O
not	O
work	O
well	O
(	O
Aggarwal	O
and	O
Zhai	O
,	O
2012	O
)	O
.	O

We	O
here	O
describe	O
each	O
of	O
the	O
steps	O
involved	O
in	O
our	O
Bengali	B-TaskName
Unsupervised	I-TaskName
Abstractive	I-TaskName
Text	I-TaskName
Summarization	I-TaskName
model	O
(	O
BenSumm	O
)	O
for	O
single	O
document	O
setting	O
.	O

(	O
2018	O
)	O
developed	O
an	O
unsupervised	B-TaskName
abstractive	I-TaskName
summarization	I-TaskName
system	O
that	O
jointly	O
performs	O
sentence	O
fusion	O
and	O
paraphrasing	O
.	O

Clarke	O
and	O
Lapata	O
(	O
2008	O
)	O
;	O
Filippova	O
(	O
2010	O
)	O
showed	O
a	O
first	O
intermediate	O
step	O
towards	O
abstractive	B-TaskName
summarization	I-TaskName
,	O
which	O
compresses	O
original	O
sentences	O
for	O
a	O
summary	O
generation	O
.	O

Potential	O
utility	O
for	O
extractive	O
text	B-TaskName
summarization	I-TaskName
made	O
SC	O
very	O
popular	O
for	O
single	O
or	O
multi	O
-	O
document	O
summarization	O
(	O
Nenkova	O
and	O
McKeown	O
,	O
2012	O
)	O
.	O

Moreover	O
,	O
there	O
is	O
no	O
human	O
-	O
annotated	O
dataset	O
to	O
compare	O
abstractive	B-TaskName
summarization	I-TaskName
methods	O
of	O
this	O
language	O
.	O

(	O
,	O
2015	O
)	O
)	O
worked	O
on	O
extractive	O
Bengali	B-TaskName
text	I-TaskName
summarization	I-TaskName
using	O
pronoun	O
replacement	O
,	O
sentence	O
ranking	O
with	O
term	O
frequency	O
,	O
numerical	O
figures	O
,	O
and	O
overlapping	O
of	O
title	O
words	O
with	O
the	O
document	O
sentences	O
.	O

Nevertheless	O
,	O
very	O
few	O
attempts	O
have	O
been	O
made	O
for	O
Bengali	B-TaskName
Text	I-TaskName
summarization	I-TaskName
despite	O
Bangla	O
being	O
the	O
7	O
th	O
most	O
spoken	O
language	O
.	O

Many	O
researchers	O
have	O
worked	O
on	O
text	B-TaskName
summarization	I-TaskName
and	O
introduced	O
different	O
extractive	O
and	O
abstractive	O
methods	O
.	O

Our	O
model	O
requires	O
only	O
POS	O
tagger	O
and	O
a	O
pre	O
-	O
trained	O
language	O
model	O
,	O
which	O
is	O
easily	O
reproducible	O
.	O

2	O
•	O
We	O
design	O
an	O
unsupervised	B-TaskName
abstractive	I-TaskName
sentence	I-TaskName
generation	I-TaskName
model	O
that	O
performs	O
sentence	B-MethodName
fusion	I-MethodName
on	O
Bengali	B-DatasetName
texts	I-DatasetName
.	O

The	O
success	O
of	O
neural	O
sequence	O
-	O
tosequence	O
(	O
seq2seq	O
)	O
models	O
with	O
attention	O
(	O
Bahdanau	O
et	O
al	O
.	O
,	O
2015;Luong	O
et	O
al	O
.	O
,	O
2015	O
)	O
provides	O
an	O
effective	O
way	O
for	O
text	O
generation	O
which	O
has	O
been	O
extensively	O
applied	O
in	O
the	O
case	O
of	O
abstractive	O
summarization	O
of	O
English	O
language	O
documents	O
(	O
Rush	O
et	O
al	O
.	O
,	O
2015;Chopra	O
et	O
al	O
.	O
,	O
2016;Nallapati	O
et	O
al	O
.	O
,	O
2016;Miao	O
and	O
Blunsom	O
,	O
2016;Paulus	O
et	O
al	O
.	O
,	O
2018;Nayeem	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

Compared	O
to	O
extractive	O
,	O
abstractive	B-TaskName
summary	I-TaskName
generation	I-TaskName
is	O
indeed	O
a	O
challenging	O
task	O
.	O

Extractive	B-TaskName
summarization	I-TaskName
is	O
about	O
ranking	O
important	O
sentences	O
from	O
the	O
original	O
text	O
.	O

The	O
process	O
of	O
shortening	O
a	O
large	O
text	O
document	O
with	O
the	O
most	O
relevant	O
information	O
of	O
the	O
source	O
is	O
known	O
as	O
automatic	B-TaskName
text	I-TaskName
summarization	I-TaskName
.	O

Our	O
unsupervised	B-TaskName
abstractive	I-TaskName
summarization	I-TaskName
model	O
outperforms	O
the	O
baselines	O
without	O
being	O
exposed	O
to	O
any	O
human	O
-	O
annotated	O
reference	O
summaries	O
.	O

We	O
conduct	O
experiments	O
on	O
this	O
dataset	O
and	O
compare	O
our	O
system	O
with	O
several	O
well	O
-	O
established	O
unsupervised	B-TaskName
extractive	I-TaskName
summarization	I-TaskName
systems	O
.	O

We	O
also	O
provide	O
a	O
human	B-MetricName
-	I-MetricName
annotated	I-MetricName
dataset	I-MetricName
with	O
document	O
-	O
summary	O
pairs	O
to	O
evaluate	O
our	O
abstractive	O
model	O
and	O
to	O
support	O
the	O
comparison	O
of	O
future	O
abstractive	B-TaskName
summarization	I-TaskName
systems	O
of	O
the	O
Bengali	O
Language	O
.	O

To	O
overcome	O
this	O
problem	O
,	O
we	O
propose	O
a	O
graph	O
-	O
based	O
unsupervised	B-TaskName
abstractive	I-TaskName
summarization	I-TaskName
system	O
in	O
the	O
single	O
-	O
document	O
setting	O
for	O
Bengali	O
text	O
documents	O
,	O
which	O
requires	O
only	O
a	O
Part	O
-	O
Of	O
-	O
Speech	O
(	O
POS	O
)	O
tagger	O
and	O
a	O
pre	O
-	O
trained	O
language	O
model	O
trained	O
on	O
Bengali	O
texts	O
.	O

Abstractive	B-TaskName
summarization	I-TaskName
systems	O
generally	O
rely	O
on	O
large	O
collections	O
of	O
documentsummary	O
pairs	O
.	O

Unsupervised	B-TaskName
Abstractive	I-TaskName
Summarization	I-TaskName
of	O
Bengali	O
Text	O
Documents	O
.	O

A	O
Appendix	O
.	O

We	O
want	O
to	O
thank	O
all	O
the	O
anonymous	O
reviewers	O
for	O
their	O
thoughtful	O
comments	O
and	O
constructive	O
suggestions	O
for	O
future	O
improvements	O
to	O
this	O
work	O
.	O

Acknowledgments	O
.	O

In	O
the	O
future	O
,	O
we	O
would	O
like	O
to	O
jointly	O
model	O
multi	O
-	O
sentence	O
compression	O
and	O
paraphrasing	O
in	O
our	O
system	O
.	O

One	O
of	O
the	O
limitations	O
of	O
our	O
model	O
is	O
that	O
it	O
can	O
not	O
generate	O
new	O
words	O
.	O

We	O
design	O
a	O
Bengali	O
Document	O
Summarization	O
tool	O
to	O
provide	O
both	O
extractive	O
and	O
abstractive	O
summary	O
of	O
a	O
given	O
document	O
.	O

Experimental	O
results	O
on	O
our	O
proposed	O
dataset	O
demonstrate	O
the	O
superiority	O
of	O
our	O
approach	O
against	O
strong	O
extractive	O
baselines	O
.	O

Conclusion	O
and	O
Future	O
Work	O
.	O

Moreover	O
,	O
We	O
design	O
a	O
Bengali	O
Document	O
Summarization	O
tool	O
(	O
see	O
Figure	O
5	O
)	O
capable	O
of	O
providing	O
both	O
extractive	O
and	O
abtractive	O
summary	O
for	O
an	O
input	O
document	O
.	O

Finally	O
,	O
we	O
present	O
an	O
example	O
of	O
our	O
model	O
output	O
in	O
Figure	O
4	O
.	O

On	O
the	O
other	O
hand	O
,	O
our	O
model	O
is	O
unsupervised	O
and	O
abstractive	O
.	O

It	O
is	O
important	O
to	O
note	O
that	O
these	O
summarizers	O
are	O
completely	O
extractive	O
and	O
designed	O
for	O
English	O
language	O
.	O

We	O
use	O
an	O
open	O
source	O
implementation	O
9	O
of	O
these	O
summarizers	O
and	O
adapted	O
it	O
for	O
Bengali	O
language	O
.	O

8	O
We	O
extract	O
3	O
-	O
best	O
sentences	O
from	O
our	O
system	O
and	O
the	O
systems	O
we	O
compare	O
as	O
baselines	O
.	O

It	O
's	O
clearly	O
visible	O
from	O
the	O
table	O
that	O
our	O
dataset	O
is	O
highly	O
abstractive	O
and	O
will	O
serve	O
as	O
a	O
robust	O
benchmark	O
for	O
this	O
task	O
's	O
future	O
works	O
.	O

The	O
overall	O
statistics	O
of	O
the	O
datasets	O
are	O
presented	O
in	O
Table	O
1	O
.	O

The	O
majority	O
of	O
Bangladeshi	O
schools	O
follow	O
these	O
books	O
.	O

The	O
NCTB	O
is	O
responsible	O
for	O
the	O
development	O
of	O
the	O
curriculum	O
and	O
distribution	O
of	O
textbooks	O
.	O

Dataset	O
.	O

Experiments	O
.	O

Do	O
n't	O
be	O
happy	O
to	O
see	O
the	O
beautiful	O
faces	O
.	O
]	O
Human	O
Reference	O
.	O

We	O
need	O
hard	O
work	O
and	O
pursuit	O
to	O
form	O
the	O
nature	O
,	O
otherwise	O
it	O
is	O
not	O
possible	O
to	O
defeat	O
the	O
devil	O
.	O

People	O
hate	O
his	O
nature	O
,	O
his	O
touch	O
,	O
his	O
customs	O
.	O

[	O
Evil	O
people	O
are	O
fascinated	O
by	O
human	O
form	O
and	O
enjoy	O
its	O
fruits	O
.	O

দ	O
ুঃস্বভাবের	O
মান	O
ষ	O
মান	O
বষর	O
রূপ	O
দদবে	O
ম	O
গ্ধ	O
হয়	O
এেং	O
তার	O
ফল	O
দভাগ	O
কবর	O
।	O
যার	O
স্বভাে	O
,	O
তার	O
স্পর্	O
শ	O
,	O
তার	O
রীততনীততবক	O
মান	O
ষ	O
ঘৃ	O
ণা	O
কবর	O
।	O
স্বভাে	O
গঠবন	O
কঠঠন	O
পতরশ্রম	O
ও	O
সাধনা	O
চাই	O
,	O
নইবল	O
র্য়তানবক	O
পরাজিত	O
করা	O
সম্ভে	O
নয়	O
।	O
তার	O
স	O
ন্দর	O
ম	O
ে	O
দদবে	O
আনজন্দত	O
হবয়া	O
না	O
।	O
.	O

System	O
Summary	O
.	O

Ranking	O
.	O

Ranking	O
.	O

Cluster	O
n	O
Cluster	O
1	O
.	O

Clustering	O
.	O

Preprocessing	O
.	O

Single	O
Document	O
.	O

We	O
also	O
present	O
a	O
detailed	O
illustration	O
of	O
our	O
framework	O
with	O
an	O
example	O
source	O
document	O
in	O
the	O
Appendix	O
.	O

The	O
overall	O
process	O
is	O
presented	O
in	O
Figure	O
3	O
.	O

We	O
generate	O
the	O
final	O
summary	O
by	O
merging	O
all	O
the	O
topranked	O
sentences	O
.	O

We	O
take	O
the	O
top	O
-	O
ranked	O
sentence	O
from	O
each	O
cluster	O
to	O
present	O
the	O
summary	O
.	O

Start	O
.	O

Figure	O
1	O
illustrates	O
an	O
example	O
WG	O
for	O
these	O
two	O
sentences	O
.	O

Figure	O
2	O
presents	O
two	O
sentences	O
,	O
which	O
is	O
one	O
of	O
the	O
source	O
document	O
clusters	O
,	O
and	O
the	O
possible	O
paths	O
with	O
their	O
weighted	O
values	O
are	O
generated	O
using	O
the	O
word	O
-	O
graph	O
approach	O
.	O

After	O
constructing	O
the	O
word	O
-	O
graph	O
,	O
we	O
can	O
generate	O
M	O
-shortest	O
paths	O
from	O
the	O
dummy	O
start	O
node	O
to	O
the	O
end	O
node	O
in	O
the	O
word	O
graph	O
(	O
see	O
Figure	O
1	O
)	O
.	O

Each	O
sentence	O
of	O
the	O
cluster	O
is	O
connected	O
to	O
a	O
dummy	O
start	O
and	O
end	O
node	O
to	O
mark	O
the	O
beginning	O
and	O
ending	O
sentences	O
.	O

After	O
the	O
first	O
sentence	O
is	O
added	O
to	O
the	O
graph	O
as	O
word	O
nodes	O
(	O
punctuation	O
included	O
)	O
,	O
words	O
from	O
the	O
other	O
related	O
sentences	O
are	O
mapped	O
onto	O
a	O
node	O
in	O
the	O
graph	O
with	O
the	O
same	O
POS	O
tag	O
.	O

Directed	O
edges	O
are	O
formed	O
by	O
connecting	O
the	O
adjacent	O
words	O
from	O
the	O
sentences	O
.	O

The	O
words	O
are	O
represented	O
as	O
vertices	O
along	O
with	O
the	O
parts	O
-	O
of	O
-	O
speech	O
(	O
POS	O
)	O
tags	O
.	O

Let	O
,	O
a	O
set	O
of	O
related	O
sentences	O
S	O
=	O
{	O
s	O
1	O
,	O
s	O
2	O
,	O
...	O
,	O
s	O
n	O
}	O
,	O
we	O
construct	O
a	O
graph	O
G	O
=	O
(	O
V	O
,	O
E	O
)	O
by	O
iteratively	O
adding	O
sentences	O
to	O
it	O
.	O

Given	O
a	O
cluster	O
of	O
related	O
sentences	O
,	O
we	O
construct	O
a	O
word	O
-	O
graph	O
following	O
(	O
Filippova	O
,	O
2010;Boudin	O
and	O
Morin	O
,	O
2013	O
)	O
.	O

This	O
method	O
is	O
entirely	O
unsupervised	O
and	O
needs	O
only	O
a	O
POS	O
tagger	O
,	O
which	O
is	O
highly	O
suitable	O
for	O
the	O
low	O
-	O
resource	O
setting	O
.	O

Textual	O
graphs	O
to	O
generate	O
abstractive	O
summaries	O
provide	O
effective	O
results	O
(	O
Ganesan	O
et	O
al	O
.	O
,	O
2010	O
)	O
.	O

The	O
following	O
formula	O
can	O
measure	O
silhouette	O
Score	O
:	O
Silhouette	O
Score	O
=	O
(	O
x	O
−	O
y	O
)	O
max(x	O
,	O
y)(1	O
)	O
where	O
y	O
denotes	O
mean	O
distance	O
to	O
the	O
other	O
instances	O
of	O
intra	O
-	O
cluster	O
and	O
x	O
is	O
the	O
mean	O
distance	O
to	O
the	O
instances	O
of	O
the	O
next	O
closest	O
cluster	O
.	O

The	O
clusters	O
are	O
highly	O
coherent	O
as	O
it	O
has	O
to	O
contain	O
sentences	O
similar	O
to	O
every	O
other	O
sentence	O
in	O
the	O
same	O
cluster	O
even	O
if	O
the	O
clusters	O
are	O
small	O
.	O

Here	O
,	O
n	O
denotes	O
the	O
number	O
of	O
sentences	O
in	O
the	O
document	O
.	O

This	O
step	O
is	O
critical	O
to	O
ensure	O
good	O
coverage	O
of	O
the	O
whole	O
document	O
and	O
avoid	O
redundancy	O
by	O
selecting	O
at	O
most	O
one	O
sentence	O
from	O
each	O
cluster	O
(	O
Nayeem	O
and	O
Chali	O
,	O
2017a	O
)	O
.	O

The	O
clustering	O
step	O
allows	O
us	O
to	O
group	O
similar	O
sentences	O
from	O
a	O
given	O
document	O
.	O

Sentence	O
Clustering	O
.	O

Our	O
preprocessing	O
step	O
includes	O
tokenization	O
,	O
removal	O
of	O
stopwords	O
,	O
Part	O
-	O
Of	O
-	O
Speech	O
(	O
POS	O
)	O
tagging	O
,	O
and	O
filtering	O
of	O
punctuation	O
marks	O
.	O

Nayeem	O
et	O
al	O
.	O

Boudin	O
and	O
Morin	O
(	O
2013	O
)	O
improved	O
Filippova	O
's	O
approach	O
by	O
re	O
-	O
ranking	O
the	O
compression	O
paths	O
according	O
to	O
keyphrases	O
,	O
which	O
resulted	O
in	O
more	O
informative	O
sentences	O
.	O

The	O
Word	O
-	O
Graph	O
based	O
approaches	O
were	O
first	O
proposed	O
by	O
(	O
Filippova	O
,	O
2010	O
)	O
,	O
which	O
require	O
only	O
a	O
POS	O
tagger	O
and	O
a	O
list	O
of	O
stopwords	O
.	O

Tex	O
-	O
tRank	O
(	O
Mihalcea	O
and	O
Tarau	O
,	O
2004	O
)	O
and	O
LexRank	O
(	O
Erkan	O
and	O
Radev	O
,	O
2004	O
)	O
are	O
graph	O
-	O
based	O
methods	O
for	O
extracting	O
important	O
sentences	O
from	O
a	O
document	O
.	O

Jing	O
and	O
McKeown	O
(	O
2000	O
)	O
worked	O
on	O
Sentence	O
Compression	O
(	O
SC	O
)	O
which	O
has	O
received	O
considerable	O
attention	O
in	O
the	O
NLP	O
community	O
.	O

Unfortunately	O
,	O
the	O
methods	O
are	O
limited	O
to	O
extractive	O
summarization	O
,	O
which	O
ranks	O
some	O
important	O
sentences	O
from	O
the	O
document	O
instead	O
of	O
generating	O
new	O
sentences	O
which	O
is	O
challenging	O
for	O
an	O
extremely	O
low	O
resource	O
language	O
like	O
Bengali	O
.	O

(	O
2017Haque	O
et	O
al	O
.	O

Haque	O
et	O
al	O
.	O

3	O
Das	O
and	O
Bandyopadhyay	O
(	O
2010	O
)	O
developed	O
Bengali	O
opinion	O
based	O
text	O
summarizer	O
using	O
given	O
topic	O
which	O
can	O
determine	O
the	O
information	O
on	O
sentiments	O
of	O
the	O
original	O
texts	O
.	O

Related	O
works	O
.	O

•	O
We	O
also	O
introduce	O
a	O
highly	O
abstractive	O
dataset	O
with	O
document	O
-	O
summary	O
pairs	O
to	O
evaluate	O
our	O
model	O
,	O
which	O
is	O
written	O
by	O
professional	O
summary	O
writers	O
of	O
National	O
Curriculum	O
and	O
Textbook	O
Board	O
(	O
NCTB	O
)	O
.	O

Therefore	O
,	O
we	O
choose	O
to	O
create	O
an	O
effective	O
Bengali	O
Text	O
Summarizer	O
with	O
an	O
unsupervised	O
approach	O
.	O

In	O
contrast	O
,	O
the	O
unsupervised	O
approach	O
reduces	O
the	O
human	O
effort	O
and	O
cost	O
for	O
collecting	O
and	O
annotating	O
large	O
amount	O
of	O
paired	O
training	O
data	O
.	O

These	O
models	O
are	O
usually	O
trained	O
with	O
lots	O
of	O
gold	O
summaries	O
,	O
but	O
there	O
is	O
no	O
large	O
-	O
scale	O
human	O
-	O
annotated	O
abstractive	O
summaries	O
available	O
for	O
low	O
-	O
resource	O
language	O
like	O
Bengali	O
.	O

Traditionally	O
used	O
abstractive	O
techniques	O
are	O
sentence	O
compression	O
,	O
syntactic	O
reorganization	O
,	O
sentence	O
fusion	O
,	O
and	O
lexical	O
paraphrasing	O
(	O
Lin	O
and	O
Ng	O
,	O
2019	O
)	O
.	O

The	O
abstractive	O
method	O
generates	O
human	O
-	O
like	O
sentences	O
using	O
natural	O
language	O
generation	O
techniques	O
.	O

There	O
are	O
two	O
types	O
of	O
summarizations	O
:	O
extractive	O
and	O
abstractive	O
.	O

A	O
good	O
summary	O
should	O
be	O
coherent	O
,	O
non	O
-	O
redundant	O
,	O
and	O
grammatically	O
readable	O
while	O
retaining	O
the	O
original	O
document	O
's	O
most	O
important	O
contents	O
(	O
Nenkova	O
and	O
McKeown	O
,	O
2012;Nayeem	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

Introduction	O
.	O

1	O
We	O
make	O
our	O
code	O
&	O
dataset	O
publicly	O
available	O
at	O
https://github.com/tafseer-nayeem/	O
BengaliSummarization	O
for	O
reproduciblity	O
.	O

Listed	O
by	O
alphabetical	O
order	O
.	O

1	O
*	O
Equal	O
contribution	O
.	O

However	O
,	O
the	O
performance	O
of	O
abstractive	O
systems	O
remains	O
a	O
challenge	O
due	O
to	O
the	O
unavailability	O
of	O
the	O
parallel	O
data	O
for	O
low	O
-	O
resource	O
languages	O
like	O
Bengali	O
.	O

Lastly	O
,	O
we	O
provide	O
a	O
web	O
interface	O
to	O
access	O
this	O
service	O
easily	O
.	O

We	O
built	O
machine	O
learning	O
models	O
that	O
can	O
use	O
the	O
data	O
to	O
map	O
user	O
locations	O
to	O
countries	O
more	O
effectively	O
compared	O
to	O
existing	O
resources	O
such	O
as	O
geopy	B-MethodName
Python	O
package	O
or	O
GeoNames	B-DatasetName
geographical	O
database	O
.	O

The	O
user	O
can	O
also	O
test	O
random	O
samples	O
from	O
UL2C	B-DatasetName
dataset	O
to	O
see	O
their	O
mapping	O
.	O

We	O
build	O
an	O
interface	O
for	O
users	O
to	O
map	B-TaskName
user	I-TaskName
locations	I-TaskName
to	I-TaskName
countries	I-TaskName
.	O

However	O
,	O
there	O
is	O
no	O
significant	O
improvement	O
when	O
additional	O
data	O
is	O
used	O
compared	O
to	O
when	O
using	O
only	O
UL2C.	B-DatasetName

From	O
Table	O
3	O
,	O
we	O
see	O
a	O
similar	O
trend	O
where	O
the	O
lower	O
performance	O
of	O
using	O
Top10KLoc	B-DatasetName
only	O
is	O
offset	O
by	O
use	O
of	O
UL2C.	B-DatasetName

UL2C	B-DatasetName
+	O
Top10KLoc	B-DatasetName
Lastly	O
,	O
we	O
modified	O
the	O
cross	O
-	O
validation	O
setting	O
by	O
adding	O
Top10KLoc	B-DatasetName
to	O
each	O
of	O
the	O
folds	O
during	O
training	O
.	O

3	O
that	O
adding	O
our	O
dataset	O
to	O
GeoNames	B-DatasetName
dataset	O
offsets	O
the	O
lower	O
performance	O
when	O
using	O
GeoNames	B-DatasetName
alone	O
and	O
with	O
F1	B-MetricName
score	O
of	O
88.2	B-MetricValue
,	O
improves	O
the	O
results	O
from	O
using	O
UL2C	B-DatasetName
dataset	O
alone	O
by	O
a	O
small	O
margin	O
.	O

The	O
results	O
were	O
seen	O
to	O
improve	O
by	O
a	O
significant	O
margin	O
since	O
the	O
best	O
results	O
were	O
obtained	O
by	O
SVM	B-MethodName
trained	O
with	O
character	O
[	O
2	B-HyperparameterValue
-	O
5	B-HyperparameterValue
]	O
n	B-HyperparameterName
-	I-HyperparameterName
grams	I-HyperparameterName
,	O
a	O
relative	O
improvement	O
of	O
26	B-MetricValue
%	I-MetricValue
in	O
F1	B-MetricName
score	I-MetricName
from	O
previous	O
best	O
(	O
70.5	B-MetricValue
)	O
when	O
trained	O
on	O
Top10KLoc	B-DatasetName
.	O

We	O
performed	O
5	B-HyperparameterValue
-	I-HyperparameterValue
fold	I-HyperparameterValue
crossvalidation	B-HyperparameterName
with	O
same	O
set	O
of	O
classifiers	O
on	O
the	O
dataset	O
presented	O
in	O
this	O
paper	O
.	O

UL2C	B-DatasetName
(	O
Our	O
dataset	O
)	O
.	O

The	O
models	O
were	O
seen	O
to	O
outperform	O
geopy	B-MethodName
by	O
a	O
small	O
margin	O
and	O
GeoNames	B-DatasetName
by	O
a	O
large	O
margin	O
with	O
maximum	B-MetricName
F1	I-MetricName
score	O
of	O
70.5	B-MetricValue
(	O
see	O
Table	O
3	O
)	O
.	O

Top10KLoc	B-DatasetName
We	O
trained	O
similar	O
set	O
of	O
classifiers	O
on	O
Top10KLoc	B-DatasetName
dataset	O
and	O
evaluated	O
on	O
our	O
dataset	O
.	O

We	O
can	O
see	O
that	O
even	O
after	O
rebalancing	O
the	O
data	O
,	O
with	O
maximum	B-MetricName
F1	I-MetricName
score	O
of	O
50.1	B-MetricValue
,	O
the	O
classifiers	O
trained	O
on	O
GeoNames	B-DatasetName
are	O
outperformed	O
by	O
all	O
others	O
.	O

GeoNames	B-DatasetName
We	O
trained	O
several	O
classifiers	O
on	O
GeoNames	B-DatasetName
dataset	O
and	O
evaluated	O
on	O
our	O
UL2C	B-DatasetName
dataset	O
.	O

geopy	B-MethodName
baseline	O
Serving	O
as	O
our	O
baseline	O
model	O
,	O
geopy	B-MethodName
achieves	O
F1	B-MetricName
score	O
of	O
69.2	B-MetricValue
when	O
evaluated	O
on	O
UL2C	B-DatasetName
dataset	O
.	O

We	O
used	O
LibSVM	B-MethodName
implementation	O
with	O
default	O
parameters	O
by	O
scikit	O
-	O
learn	O
7	O
for	O
training	O
.	O

Even	O
in	O
recent	O
Arabic	O
text	O
classification	O
tasks	O
such	O
as	O
offensive	O
language	O
identification	O
(	O
Hassan	O
et	O
al	O
.	O
,	O
2020b	O
,	O
a	O
)	O
,	O
spam	O
detection	O
(	O
Mubarak	O
et	O
al	O
.	O
,	O
2020	O
)	O
,	O
adult	O
content	O
detection	O
(	O
Mubarak	O
et	O
al	O
.	O
,	O
2021	O
)	O
,	O
and	O
dialect	O
identification	O
(	O
Abdelali	O
et	O
al	O
.	O
,	O
2020	O
)	O
,	O
SVMs	B-MethodName
have	O
shown	O
promising	O
results	O
.	O

Support	B-MethodName
Vector	I-MethodName
Machines	I-MethodName
(	O
SVMs	B-MethodName
)	O
SVMs	B-MethodName
have	O
traditionally	O
been	O
used	O
for	O
many	O
classification	O
tasks	O
.	O

The	O
geopy	B-MethodName
library	O
acts	O
as	O
our	O
baseline	O
.	O

Classification	O
Models	O
geopy	B-MethodName
baseline	O
.	O

In	O
this	O
section	O
,	O
we	O
compare	O
effectiveness	O
of	O
mapping	B-TaskName
user	I-TaskName
locations	I-TaskName
to	I-TaskName
countries	I-TaskName
with	O
classifiers	O
trained	O
on	O
different	O
datasets	O
,	O
namely	O
,	O
UL2C	B-DatasetName
,	O
Top10KLoc	B-DatasetName
,	O
GeoNames	B-DatasetName
and	O
combination	O
of	O
these	O
datasets	O
.	O

It	O
's	O
worth	O
mentioning	O
that	O
country	O
distribution	O
obtained	O
from	O
UL2C	B-DatasetName
and	O
Top10KLoc	B-DatasetName
datasets	O
are	O
very	O
similar	O
which	O
indicates	O
that	O
most	O
probably	O
any	O
random	O
big	O
collection	O
of	O
tweets	O
will	O
have	O
similar	O
country	O
distribution	O
.	O

Dataset	O
can	O
be	O
downloaded	O
from	O
:	O
Figure	O
2	O
shows	O
some	O
information	O
about	O
Damascus	O
,	O
the	O
capital	O
of	O
Syria	O
,	O
and	O
its	O
alternate	O
names	O
written	O
in	O
tens	O
of	O
languages	O
as	O
obtained	O
from	O
GeoNames	B-DatasetName
.	O

GeoNames	B-DatasetName
geographical	O
database	O
covers	O
all	O
countries	O
and	O
contains	O
over	O
11	O
M	O
placenames	O
whereof	O
4.8	O
M	O
populated	O
places	O
and	O
13	O
M	O
alternate	O
names	O
.	O

GeoNames	B-DatasetName
Dataset	O
.	O

User	O
locations	O
and	O
their	O
country	O
mapping	O
(	O
UL2C	B-DatasetName
dataset	O
)	O
can	O
be	O
downloaded	O
from	O
this	O
link	O
:	O
https://alt.qcri.org/resources/	O
UL2C-UserLocationsToCountries.tsv	O
.	O

We	O
used	O
geopy	B-MethodName
output	O
as	O
an	O
initial	O
mapping	O
of	O
user	O
locations	O
to	O
countries	O
then	O
all	O
unique	O
user	O
locations	O
were	O
revised	O
manually	O
by	O
an	O
Arabic	O
native	O
speaker	O
.	O

We	O
observed	O
that	O
geopy	B-MethodName
has	O
difficulties	O
in	O
identifying	O
many	O
locations	O
when	O
they	O
are	O
short	O
,	O
have	O
special	O
characters	O
,	O
or	O
unreal	O
locations	O
in	O
addition	O
to	O
many	O
correct	O
locations	O
.	O

Figure	O
1	O
shows	O
the	O
output	O
from	O
geopy	B-MethodName
for	O
an	O
arbitrary	O
location	O
.	O

geopy	B-MethodName
is	O
a	O
Python	O
package	O
that	O
locates	O
the	O
coordinates	O
of	O
addresses	O
,	O
cities	O
,	O
countries	O
,	O
and	O
landmarks	O
across	O
the	O
globe	O
using	O
third	O
-	O
party	O
geocoders	O
and	O
other	O
data	O
sources	O
.	O

We	O
used	O
geopy	B-MethodName
4	O
to	O
map	O
user	O
locations	O
to	O
countries	O
.	O

We	O
used	O
twarc	B-MethodName
search	I-MethodName
API	I-MethodName
3	O
to	O
collect	O
Arabic	O
tweets	O
.	O

To	O
map	O
these	O
locations	O
to	O
countries	O
,	O
the	O
authors	O
first	O
use	O
GeoNames	B-DatasetName
dataset	O
and	O
then	O
manually	O
revise	O
them	O
.	O

Alshutayri	O
and	O
Atwell	O
(	O
2017	O
)	O
As	O
discussed	O
earlier	O
,	O
the	O
closest	O
work	O
to	O
ours	O
that	O
targets	O
converting	B-TaskName
user	I-TaskName
locations	I-TaskName
directly	I-TaskName
to	I-TaskName
countries	I-TaskName
is	O
by	O
Mubarak	O
and	O
Darwish	O
(	O
2014	O
)	O
.	O

(	O
2008	O
)	O
Despite	O
Arabic	O
being	O
one	O
of	O
the	O
most	O
popular	O
languages	O
on	O
Twitter	O
,	O
there	O
has	O
been	O
very	O
few	O
works	O
aimed	O
at	O
mapping	B-TaskName
user	I-TaskName
location	I-TaskName
in	I-TaskName
the	I-TaskName
Arab	I-TaskName
region	I-TaskName
to	I-TaskName
countries	I-TaskName
.	O

There	O
has	O
been	O
a	O
number	O
of	O
works	O
that	O
focus	O
on	O
identifying	B-TaskName
locations	I-TaskName
of	I-TaskName
Twitter	I-TaskName
users	I-TaskName
.	O

•	O
We	O
provide	O
a	O
web	O
interface	O
that	O
users	O
can	O
use	O
to	O
map	B-TaskName
user	I-TaskName
locations	I-TaskName
to	I-TaskName
countries	I-TaskName
.	O

Models	O
trained	O
on	O
our	O
dataset	O
achieve	O
macroaverage	B-MetricName
F1	I-MetricName
score	O
of	O
88.1	B-MetricValue
,	O
which	O
outperforms	O
similar	O
models	O
trained	O
on	O
other	O
datasets	O
with	O
at	O
least	O
26	O
%	O
relative	O
improvement	O
.	O

•	O
We	O
show	O
that	O
by	O
using	O
machine	O
learning	O
models	O
trained	O
on	O
our	O
dataset	O
,	O
we	O
can	O
achieve	O
significantly	O
better	O
results	O
compared	O
to	O
existing	O
libraries	O
(	O
e.g.	O
geopy	B-MethodName
package	O
)	O
or	O
resources	O
(	O
e.g.	O
GeoNames	B-DatasetName
or	O
Top10KLoc	B-DatasetName
2	O
datasets	O
)	O
.	O

The	O
contributions	O
of	O
this	O
paper	O
are	O
summarized	O
below	O
:	O
•	O
We	O
present	O
UL2C	B-DatasetName
;	O
the	O
largest	O
dataset	O
for	O
mapping	O
user	O
locations	O
on	O
Arabic	O
Twitter	O
to	O
countries	O
which	O
contains	O
more	O
than	O
28	O
K	O
unique	O
locations	O
.	O

Our	O
work	O
extends	O
Top10KLoc	B-DatasetName
by	O
increasing	O
unique	O
user	O
locations	O
from	O
most	O
common	O
10	O
K	O
to	O
random	O
28	O
K	O
user	O
locations	O
obtained	O
from	O
160	O
K	O
locations	O
that	O
are	O
self	O
-	O
declared	O
by	O
users	O
1	O
.	O

We	O
refer	O
to	O
this	O
dataset	O
as	O
Top10KLoc	B-DatasetName
.	O

To	O
our	O
knowledge	O
,	O
there	O
has	O
been	O
very	O
few	O
works	O
for	O
Arabic	O
that	O
map	O
noisy	B-TaskName
user	I-TaskName
locations	I-TaskName
to	I-TaskName
countries	I-TaskName
,	O
with	O
work	O
by	O
Mubarak	O
and	O
Darwish	O
(	O
2014	O
)	O
being	O
one	O
of	O
the	O
most	O
notable	O
ones	O
and	O
the	O
closest	O
to	O
our	O
work	O
.	O

Our	O
focus	O
in	O
this	O
work	O
significantly	O
differs	O
from	O
dialect	O
identification	O
since	O
our	O
purpose	O
is	O
to	O
provide	O
a	O
dataset	O
that	O
can	O
be	O
used	O
to	O
map	B-TaskName
Twitter	I-TaskName
user	I-TaskName
locations	I-TaskName
to	I-TaskName
countries	I-TaskName
,	O
which	O
in	O
turn	O
,	O
can	O
aid	O
in	O
NLP	O
tasks	O
such	O
as	O
dialect	O
identification	O
or	O
event	O
detection	O
.	O

Related	O
works	O
for	O
Arabic	O
primarily	O
focus	O
on	O
dialect	B-TaskName
identification	I-TaskName
(	O
e.g.	O
,	O
(	O
Bouamor	O
et	O
al	O
.	O
,	O
2019;Abdelali	O
et	O
al	O
.	O
,	O
2020;Zaidan	O
and	O
Callison	O
-	O
Burch	O
,	O
2011	O
)	O
,	O
many	O
of	O
which	O
involve	O
manual	O
annotation	O
of	O
dialects	O
for	O
sentences	O
.	O

Deducing	B-TaskName
a	I-TaskName
Twitter	I-TaskName
user	I-TaskName
's	I-TaskName
location	I-TaskName
from	O
geotagged	O
tweets	O
is	O
difficult	O
because	O
less	O
than	O
1	O
%	O
of	O
tweets	O
are	O
geotagged	O
(	O
Cheng	O
et	O
al	O
.	O
,	O
2010;Hecht	O
et	O
al	O
.	O
,	O
2011	O
)	O
.	O

Identifying	B-TaskName
geolocation	I-TaskName
of	O
Twitter	O
users	O
can	O
also	O
help	O
in	O
event	O
detection	O
(	O
Agarwal	O
et	O
al	O
.	O
,	O
2012	O
)	O
or	O
disaster	O
management	O
(	O
Earle	O
et	O
al	O
.	O
,	O
2012;Carley	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

Identifying	O
countries	O
of	O
the	O
users	O
can	O
help	O
in	O
various	O
NLP	O
tasks	O
such	O
as	O
dialect	B-TaskName
identification	I-TaskName
,	O
author	B-TaskName
profiling	I-TaskName
,	O
and	O
recommendation	B-TaskName
systems	I-TaskName
.	O

We	O
also	O
show	O
that	O
our	O
dataset	O
is	O
more	O
effective	O
than	O
data	O
extracted	O
from	O
GeoNames	B-DatasetName
geographical	O
database	O
in	O
this	O
task	O
as	O
the	O
latter	O
covers	O
only	O
locations	O
written	O
in	O
formal	O
ways	O
.	O

We	O
build	O
effective	O
machine	O
learning	O
models	O
that	O
can	O
automate	O
this	O
mapping	O
with	O
significantly	O
better	O
efficiency	O
compared	O
to	O
libraries	O
such	O
as	O
geopy	B-MethodName
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
the	O
largest	O
manually	O
labeled	O
dataset	O
for	O
mapping	B-TaskName
user	I-TaskName
locations	I-TaskName
on	O
Arabic	O
Twitter	O
to	O
their	O
corresponding	O
countries	O
.	O

Mapping	B-TaskName
user	I-TaskName
locations	I-TaskName
to	I-TaskName
countries	I-TaskName
can	O
be	O
useful	O
for	O
many	O
applications	O
such	O
as	O
dialect	O
identification	O
,	O
author	O
profiling	O
,	O
recommendation	O
systems	O
,	O
etc	O
.	O

UL2C	O
:	O
Mapping	B-TaskName
User	I-TaskName
Locations	I-TaskName
to	I-TaskName
Countries	I-TaskName
on	O
Arabic	O
Twitter	O
.	O

We	O
analyzed	O
different	O
characteristics	O
of	O
our	O
data	O
such	O
as	O
country	O
distribution	O
and	O
top	O
locations	O
.	O

In	O
this	O
paper	O
,	O
we	O
have	O
presented	O
a	O
large	O
manually	O
annotated	O
and	O
publicly	O
available	O
dataset	O
of	O
Twitter	O
user	O
locations	O
from	O
the	O
Arab	O
region	O
,	O
mapped	O
to	O
their	O
respective	O
countries	O
.	O

Conclusion	O
.	O

To	O
visualize	O
the	O
heatmap	O
,	O
we	O
use	O
Leaflet.js	O
10	O
and	O
Heatmap.js	O
11	O
with	O
Open	O
-	O
StreetMap	O
12	O
map	O
server	O
.	O

We	O
use	O
Flask	O
9	O
,	O
a	O
python	O
-	O
based	O
web	O
development	O
framework	O
,	O
for	O
backend	O
development	O
and	O
javascript	O
for	O
communication	O
between	O
backend	O
and	O
frontend	O
.	O

We	O
use	O
Bootstrap	O
8	O
for	O
our	O
responsive	O
frontend	O
design	O
.	O

Implementation	O
.	O

The	O
user	O
is	O
then	O
able	O
to	O
download	O
a	O
file	O
containing	O
predictions	O
and	O
probabilities	O
of	O
the	O
user	O
locations	O
belonging	O
to	O
different	O
countries	O
.	O

We	O
impose	O
a	O
restriction	O
on	O
file	O
size	O
in	O
order	O
to	O
limit	O
abuse	O
of	O
our	O
system	O
.	O

This	O
allows	O
users	O
to	O
map	O
many	O
user	O
locations	O
at	O
the	O
same	O
time	O
.	O

We	O
also	O
allow	O
the	O
user	O
to	O
upload	O
a	O
file	O
consisting	O
user	O
locations	O
.	O

To	O
help	O
the	O
users	O
visualize	O
distribution	O
of	O
possible	O
countries	O
related	O
to	O
the	O
location	O
,	O
we	O
dis	O
-	O
play	O
a	O
heatmap	O
of	O
the	O
probabilities	O
.	O

The	O
user	O
is	O
then	O
shown	O
probabilities	O
of	O
the	O
location	O
belonging	O
to	O
different	O
countries	O
.	O

This	O
allows	O
the	O
user	O
to	O
easily	O
understand	O
the	O
functionalities	O
of	O
the	O
interface	O
.	O

The	O
user	O
can	O
type	O
user	O
location	O
to	O
be	O
mapped	O
to	O
countries	O
.	O

Design	O
.	O

Figure	O
6	O
shows	O
sample	O
outputs	O
from	O
the	O
website	O
.	O

qcri.org	O
.	O

The	O
web	O
interface	O
is	O
added	O
to	O
Arabic	O
Social	O
media	O
Analytics	O
and	O
unDerstanding	O
(	O
ASAD	O
)	O
(	O
Hassan	O
et	O
al	O
.	O
,	O
2021	O
)	O
at	O
https://asad	O
.	O

Interface	O
.	O

The	O
results	O
are	O
summarized	O
in	O
Table	O
3	O
.	O

This	O
yielded	O
a	O
total	O
of	O
8,632	O
unique	O
instances	O
(	O
4,316	O
from	O
the	O
Arab	O
region	O
and	O
4,316	O
from	O
outside	O
the	O
Arab	O
region	O
)	O
.	O

To	O
address	O
this	O
problem	O
,	O
we	O
rebalanced	O
the	O
dataset	O
to	O
have	O
equal	O
number	O
of	O
locations	O
from	O
within	O
and	O
outside	O
the	O
Arab	O
region	O
.	O

We	O
noticed	O
that	O
many	O
of	O
the	O
locations	O
were	O
outside	O
Arab	O
region	O
which	O
likely	O
caused	O
the	O
poor	O
performance	O
(	O
omitted	O
here	O
for	O
conciseness	O
)	O
.	O

The	O
classifiers	O
yielded	O
very	O
poor	O
results	O
7	O
http://scikit-learn.org/	O
initially	O
.	O

Experiment	O
results	O
.	O

We	O
call	O
library	O
with	O
Twitter	O
user	O
locations	O
and	O
extract	O
countries	O
they	O
are	O
mapped	O
to	O
by	O
the	O
library	O
.	O

Similar	O
to	O
word	O
n	O
-	O
gram	O
,	O
we	O
used	O
tf	O
-	O
idf	O
weighting	O
.	O

Character	O
n	O
-	O
gram	O
We	O
experimented	O
with	O
character	O
n	O
-	O
grams	O
ranging	O
from	O
[	O
2	O
-	O
3	O
]	O
to	O
[	O
2	O
-	O
5	O
]	O
,	O
beyond	O
which	O
,	O
we	O
did	O
not	O
see	O
any	O
further	O
improvement	O
.	O

We	O
use	O
term	O
frequency	O
-	O
inverse	O
term	O
document	O
frequency	O
(	O
tf	O
-	O
idf	O
)	O
for	O
weighting	O
the	O
n	O
-	O
gram	O
vectors	O
.	O

Word	O
n	O
-	O
gram	O
Since	O
our	O
input	O
text	O
is	O
name	O
of	O
a	O
location	O
and	O
therefore	O
,	O
typically	O
short	O
,	O
we	O
limit	O
range	O
of	O
word	O
n	O
-	O
gram	O
to	O
[	O
1	O
-	O
2	O
]	O
.	O

Features	O
.	O

In	O
summary	O
,	O
a	O
location	O
like	O
"	O
αβHA1	O
:)	O
*	O
,	O
"	O
is	O
converted	O
to	O
"	O
abha	O
"	O
(	O
city	O
in	O
SA	O
)	O
after	O
decorated	O
letters	O
mapping	O
and	O
applying	O
other	O
normalization	O
steps	O
.	O

In	O
order	O
to	O
reduce	O
noise	O
in	O
user	O
-	O
declared	O
locations	O
,	O
we	O
perform	O
the	O
following	O
preprocessing	O
steps	O
:	O
•	O
Convert	O
English	O
and	O
Arabic	O
decorated	O
letters	O
(	O
e.g.	O
some	O
Farsi	O
letters	O
)	O
to	O
original	O
letters	O
using	O
the	O
mapping	O
list	O
shared	O
by	O
(	O
Mubarak	O
and	O
Abdelali	O
,	O
2016).For	O
example	O
,	O
we	O
map	O
"	O
α	O
,	O
β	O
"	O
to	O
"	O
a	O
,	O
B	O
"	O
and	O
"	O
"	O
to	O
"	O
"	O
in	O
order	O
.	O

Preprocessing	O
text	O
.	O

In	O
our	O
experiments	O
,	O
we	O
merge	O
all	O
countries	O
that	O
are	O
outside	O
Arab	O
region	O
to	O
"	O
UNK	O
/	O
OTHER	O
"	O
class	O
,	O
yielding	O
a	O
total	O
of	O
21	O
classes	O
(	O
20	O
Arab	O
countries	O
+	O
Unknown	O
/	O
Other	O
)	O
.	O

Experiments	O
and	O
Results	O
.	O

This	O
gives	O
an	O
indication	O
about	O
the	O
popularity	O
of	O
language	O
usage	O
across	O
different	O
regions	O
in	O
the	O
Arab	O
World	O
.	O

Arabic	O
and	O
English	O
names	O
are	O
widely	O
used	O
in	O
EG	O
and	O
SY	O
.	O

For	O
example	O
,	O
while	O
majority	O
of	O
names	O
are	O
written	O
in	O
Arabic	O
in	O
SA	O
(	O
Gulf	O
region	O
)	O
,	O
they	O
are	O
written	O
in	O
Arabic	O
,	O
English	O
and	O
French	O
in	O
DZ	O
(	O
Maghreb	O
region	O
)	O
.	O

We	O
can	O
see	O
country	O
and	O
major	O
city	O
names	O
are	O
written	O
in	O
bigger	O
font	O
in	O
different	O
languages	O
.	O

This	O
can	O
give	O
an	O
estimation	O
about	O
countries	O
that	O
Arabs	O
live	O
in	O
Figure	O
5	O
shows	O
the	O
most	O
common	O
words	O
used	O
in	O
user	O
locations	O
for	O
4	O
countries	O
,	O
namely	O
SA	O
(	O
Saudi	O
Arabia	O
)	O
,	O
EG	O
(	O
Egypt	O
)	O
,	O
SY	O
(	O
Syria	O
)	O
and	O
DZ	O
(	O
Algeria	O
)	O
which	O
represent	O
major	O
regions	O
in	O
the	O
Arab	O
World	O
(	O
Gulf	O
,	O
Levant	O
,	O
Nile	O
Basin	O
and	O
Maghreb	O
regions	O
respectively	O
)	O
.	O

The	O
top	O
5	O
countries	O
outside	O
the	O
Arab	O
World	O
where	O
Arabic	O
tweets	O
come	O
from	O
are	O
:	O
US	O
(	O
United	O
States	O
)	O
,	O
GB	O
(	O
United	O
Kingdom	O
)	O
,	O
TR	O
(	O
Turkey	O
)	O
,	O
DE	O
(	O
Germany	O
)	O
and	O
FR	O
(	O
France	O
)	O
in	O
order	O
.	O

From	O
the	O
geographical	O
map	O
of	O
all	O
Twitter	O
users	O
shown	O
in	O
Figure	O
4	O
,	O
we	O
can	O
see	O
that	O
Arabic	O
tweets	O
come	O
from	O
almost	O
all	O
world	O
countries	O
.	O

Around	O
70	O
%	O
of	O
Twitter	O
users	O
are	O
from	O
Gulf	O
region	O
(	O
countries	O
:	O
SA	O
,	O
KW	O
,	O
OM	O
,	O
AE	O
,	O
QA	O
and	O
BH	O
)	O
,	O
4	O
%	O
of	O
users	O
are	O
from	O
Levant	O
region	O
(	O
JO	O
,	O
PS	O
,	O
LB	O
and	O
SY	O
)	O
,	O
3	O
%	O
of	O
users	O
are	O
from	O
Maghreb	O
region	O
(	O
DZ	O
,	O
LY	O
,	O
MA	O
and	O
TN	O
)	O
,	O
and	O
users	O
from	O
other	O
regions	O
(	O
EG	O
,	O
YE	O
and	O
IQ	O
)	O
are	O
in	O
between	O
.	O

We	O
observe	O
that	O
users	O
from	O
Saudi	O
Arabia	O
(	O
SA	O
)	O
represent	O
more	O
than	O
half	O
of	O
Arab	O
Twitter	O
users	O
.	O

Although	O
there	O
are	O
22	O
Arab	O
countries	O
,	O
in	O
our	O
collection	O
we	O
did	O
n't	O
find	O
locations	O
from	O
two	O
countries	O
,	O
namely	O
Djibouti	O
and	O
Comoros	O
.	O

Distribution	O
of	O
user	O
countries	O
is	O
shown	O
in	O
Figure	O
3	O
.	O

Many	O
of	O
user	O
locations	O
were	O
either	O
empty	O
(	O
38	O
%	O
)	O
or	O
can	O
not	O
be	O
mapped	O
to	O
a	O
specific	O
country	O
(	O
6	O
%	O
)	O
.	O

By	O
assigning	O
countries	O
to	O
unique	O
user	O
locations	O
,	O
we	O
could	O
map	O
locations	O
of	O
≈	O
90	O
K	O
users	O
to	O
countries	O
which	O
represent	O
56	O
%	O
of	O
the	O
160	O
K	O
users	O
in	O
our	O
dataset	O
.	O

Data	O
Analysis	O
.	O

In	O
the	O
experiments	O
section	O
,	O
we	O
will	O
examine	O
the	O
efficiency	O
of	O
using	O
GeoNames	O
to	O
identify	O
countries	O
of	O
Twitter	O
users	O
.	O

We	O
ended	O
up	O
with	O
having	O
a	O
list	O
of	O
66	O
K	O
English	O
location	O
names	O
(	O
ASCII	O
name	O
field	O
)	O
and	O
a	O
shorter	O
list	O
of	O
13	O
K	O
Arabic	O
names	O
for	O
some	O
of	O
them	O
.	O

The	O
figure	O
shows	O
also	O
an	O
example	O
of	O
the	O
excluded	O
locations	O
that	O
we	O
anticipate	O
users	O
will	O
not	O
use	O
to	O
describe	O
their	O
locations	O
.	O

We	O
extracted	O
Arabic	O
and	O
English	O
names	O
of	O
all	O
places	O
with	O
population	O
of	O
10	O
K	O
or	O
more	O
6	O
.	O

Users	O
can	O
manually	O
edit	O
,	O
correct	O
and	O
add	O
new	O
names	O
using	O
a	O
user	O
friendly	O
interface	O
.	O

We	O
agreed	O
with	O
the	O
manual	O
annotations	O
in	O
98	O
%	O
of	O
the	O
cases	O
which	O
indicates	O
that	O
annotation	O
quality	O
is	O
very	O
high	O
.	O

We	O
randomly	O
selected	O
500	O
unique	O
user	O
locations	O
and	O
checked	O
annotation	O
quality	O
.	O

Some	O
common	O
examples	O
and	O
special	O
annotation	O
cases	O
are	O
shown	O
in	O
Table	O
2	O
.	O

In	O
addition	O
to	O
mapping	O
clear	O
locations	O
to	O
countries	O
,	O
the	O
annotator	O
was	O
asked	O
to	O
consider	O
any	O
clues	O
in	O
user	O
location	O
string	O
that	O
indicate	O
belonging	O
to	O
a	O
specific	O
country	O
.	O

Table	O
1	O
shows	O
some	O
examples	O
for	O
these	O
errors	O
.	O

In	O
our	O
study	O
,	O
we	O
focus	O
on	O
mapping	O
user	O
locations	O
to	O
countries	O
and	O
we	O
use	O
ISO	O
3166	O
-	O
1	O
alpha-2	O
for	O
country	O
codes	O
5	O
.	O

It	O
includes	O
geocoder	O
classes	O
for	O
the	O
OpenStreetMap	O
Nominatim	O
,	O
Google	O
Geocoding	O
API	O
(	O
V3	O
)	O
,	O
and	O
many	O
other	O
geocoding	O
services	O
.	O

Data	O
annotation	O
.	O

Around	O
60	O
%	O
of	O
the	O
non	O
-	O
empty	O
locations	O
are	O
written	O
in	O
Arabic	O
and	O
40	O
%	O
are	O
written	O
in	O
other	O
languages	O
,	O
mainly	O
in	O
English	O
.	O

In	O
our	O
data	O
collection	O
,	O
we	O
found	O
that	O
62	O
%	O
of	O
users	O
pro	O
-	O
vided	O
non	O
-	O
empty	O
locations	O
.	O

The	O
user	O
locations	O
are	O
information	O
provided	O
by	O
Twitter	O
users	O
,	O
and	O
they	O
can	O
be	O
real	O
locations	O
(	O
e.g.	O
,	O
country	O
and	O
city	O
names	O
written	O
in	O
formal	O
,	O
informal	O
ways	O
or	O
nicknames	O
)	O
,	O
landmarks	O
,	O
or	O
unreal	O
locations	O
,	O
and	O
can	O
be	O
written	O
in	O
any	O
language	O
.	O

From	O
unique	O
users	O
who	O
authored	O
those	O
tweets	O
,	O
we	O
randomly	O
selected	O
160	O
K	O
unique	O
users	O
for	O
which	O
we	O
obtained	O
28	O
K	O
unique	O
user	O
locations	O
.	O

During	O
the	O
years	O
2018	O
,	O
2019	O
and	O
2020	O
,	O
we	O
collected	O
88	O
M	O
tweets	O
with	O
language	O
filter	O
set	O
to	O
Arabic	O
(	O
lang	O
:	O
ar	O
)	O
.	O

Data	O
Collection	O
.	O

Further	O
,	O
the	O
authors	O
show	O
that	O
after	O
filtering	O
out	O
non	O
-	O
dialectal	O
tweets	O
,	O
the	O
countries	O
obtained	O
from	O
the	O
user	O
locations	O
can	O
be	O
strong	O
indicator	O
of	O
dialects	O
for	O
the	O
remaining	O
tweets	O
.	O

The	O
authors	O
collect	O
10	O
K	O
top	O
user	O
locations	O
from	O
92	O
M	O
tweets	O
.	O

Zaidan	O
and	O
Callison	O
-	O
Burch	O
(	O
2011	O
)	O
collected	O
a	O
52	O
M	O
word	O
dataset	O
from	O
newspapers	O
and	O
annotated	O
them	O
for	O
dialects	O
of	O
5	O
Arab	O
regions	O
,	O
namely	O
,	O
Maghrebi	O
,	O
Egyptian	O
,	O
Levantine	O
,	O
Gulf	O
,	O
and	O
Iraqi	O
.	O

Some	O
works	O
targeted	O
region	O
level	O
classification	O
of	O
Arabic	O
dialects	O
.	O

(	O
2019	O
)	O
manually	O
labeled	O
about	O
3	O
K	O
users	O
for	O
their	O
countries	O
of	O
origin	O
.	O

(	O
2020	O
)	O
automatically	O
labeled	O
500K+	O
tweets	O
for	O
their	O
country	O
-	O
level	O
dialects	O
,	O
Bouamor	O
et	O
al	O
.	O

While	O
Abdelali	O
et	O
al	O
.	O

Some	O
works	O
identify	O
country	O
-	O
level	O
dialects	O
of	O
Arabic	O
tweets	O
(	O
e.g.	O
,	O
(	O
Abdelali	O
et	O
al	O
.	O
,	O
2020	O
)	O
and	O
some	O
focus	O
on	O
dialects	O
at	O
user	O
-	O
level	O
(	O
Bouamor	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

The	O
related	O
field	O
of	O
dialect	O
identification	O
has	O
received	O
significant	O
attention	O
recently	O
.	O

Krishnamurthy	O
et	O
al	O
.	O

Related	O
Work	O
.	O

•	O
We	O
perform	O
analysis	O
of	O
the	O
data	O
collected	O
,	O
identifying	O
key	O
characteristics	O
.	O

Mubarak	O
and	O
Darwish	O
(	O
2014	O
)	O
map	O
the	O
most	O
common	O
10	O
K	O
locations	O
to	O
Arab	O
countries	O
in	O
order	O
to	O
build	O
a	O
multidialectal	O
Arabic	O
corpus	O
.	O

(	O
2019	O
)	O
since	O
they	O
both	O
contain	O
around	O
3	O
K	O
manually	O
annotated	O
users	O
.	O

(	O
2019	O
)	O
and	O
Charfi	O
et	O
al	O
.	O

This	O
is	O
evident	O
in	O
the	O
datasets	O
by	O
Bouamor	O
et	O
al	O
.	O

Being	O
expensive	O
,	O
manual	O
annotation	O
often	O
limits	O
size	O
of	O
datasets	O
.	O

Many	O
works	O
therefore	O
,	O
(	O
e.g.	O
,	O
(	O
Bouamor	O
et	O
al	O
.	O
,	O
2019;Charfi	O
et	O
al	O
.	O
,	O
2019	O
)	O
)	O
,	O
manually	O
annotate	O
user	O
profiles	O
for	O
their	O
locations	O
.	O

This	O
complication	O
often	O
prompts	O
researchers	O
to	O
manually	O
annotate	O
user	O
profiles	O
for	O
their	O
countries	O
.	O

This	O
makes	O
it	O
difficult	O
to	O
automatically	O
infer	O
the	O
country	O
of	O
many	O
users	O
.	O

Some	O
of	O
these	O
names	O
also	O
contain	O
emojis	O
and	O
special	O
symbols	O
.	O

Many	O
of	O
these	O
names	O
are	O
written	O
in	O
informal	O
way	O
and	O
sometimes	O
in	O
mixed	O
languages	O
.	O

Users	O
can	O
choose	O
to	O
specify	O
their	O
location	O
at	O
the	O
level	O
of	O
countries	O
,	O
regions	O
,	O
cities	O
or	O
towns	O
.	O

Although	O
Twitter	O
provides	O
an	O
option	O
to	O
users	O
to	O
declare	O
their	O
location	O
in	O
their	O
profile	O
,	O
this	O
is	O
often	O
noisy	O
.	O

Arabic	O
Twittersphere	O
presents	O
us	O
with	O
an	O
audience	O
of	O
diverse	O
demographics	O
.	O

Spoken	O
across	O
more	O
than	O
20	O
countries	O
,	O
Arabic	O
is	O
one	O
of	O
the	O
most	O
dominant	O
languages	O
on	O
Twitter	O
.	O

Twitter	O
is	O
one	O
of	O
the	O
most	O
popular	O
social	O
media	O
platforms	O
in	O
the	O
Arab	O
region	O
.	O

Introduction	O
.	O

Twitter	O
allows	O
users	O
to	O
declare	O
their	O
locations	O
as	O
free	O
text	O
,	O
and	O
these	O
userdeclared	O
locations	O
are	O
often	O
noisy	O
and	O
hard	O
to	O
decipher	O
automatically	O
.	O

We	O
also	O
plan	O
to	O
extend	O
our	O
approach	O
to	O
cope	O
with	O
multitable	O
text	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
SQL	I-TaskName
task	O
Spider	B-DatasetName
.	O

Equipped	O
with	O
automatic	O
-	O
generated	O
labels	O
and	O
AGG	O
enhancement	O
method	O
,	O
our	O
model	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
the	O
WikiSQL	B-TaskName
benchmark	O
.	O

Thanks	O
to	O
the	O
simple	O
,	O
unified	O
model	O
for	O
mention	B-MethodName
and	I-MethodName
relation	I-MethodName
extraction	I-MethodName
and	O
its	O
capacity	O
for	O
capturing	O
inter	O
mention	O
dependencies	O
,	O
the	O
proposed	O
method	O
proves	O
to	O
be	O
a	O
promising	O
approach	O
to	O
textto	B-TaskName
-	I-TaskName
SQL	I-TaskName
task	I-TaskName
.	O

We	O
can	O
extend	O
our	O
method	O
to	O
the	O
Spider	B-DatasetName
task	O
by	O
following	O
existing	O
sketch	O
construction	O
methods	O
as	O
in	O
RYANSQL	B-MethodName
,	O
while	O
replacing	O
their	O
slot	O
classification	O
modules	O
with	O
our	O
extractor	O
-	O
linker	O
methods	O
.	O

At	O
a	O
high	O
level	O
,	O
our	O
method	O
is	O
along	O
the	O
same	O
line	O
of	O
SQLNet	B-MethodName
-	O
RYANSQL	B-MethodName
,	O
yet	O
differs	O
with	O
them	O
,	O
as	O
our	O
method	O
extracts	O
slots	O
in	O
a	O
unified	O
way	O
rather	O
than	O
using	O
dedicated	O
modules	O
to	O
predict	O
each	O
slot	O
type	O
.	O

Recent	O
trend	O
(	O
Krishnamurthy	O
et	O
al	O
.	O
,	O
2017;Guo	O
et	O
al	O
.	O
,	O
2019;Wang	O
et	O
al	O
.	O
,	O
2020;Choi	O
et	O
al	O
.	O
,	O
2020	O
)	O
in	O
academia	O
starts	O
to	O
shift	O
to	O
multi	O
-	O
table	O
and	O
complex	O
queries	O
setting	O
of	O
text	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
SQL	I-TaskName
,	O
as	O
in	O
the	O
Spider	B-DatasetName
task	O
(	O
Yu	O
et	O
al	O
.	O
,	O
2018b	O
)	O
.	O

State	O
-	O
of	O
-	O
the	O
art	O
methods	O
on	O
Spider	B-DatasetName
typically	O
fall	O
into	O
two	O
categories	O
:	O
grammarbased	B-MethodName
approach	I-MethodName
(	O
Guo	O
et	O
al	O
.	O
,	O
2019;Wang	O
et	O
al	O
.	O
,	O
2020	O
)	O
,	O
and	O
sketch	B-MethodName
-	I-MethodName
based	I-MethodName
approach	I-MethodName
,	O
such	O
as	O
RYAN	B-MethodName
-	I-MethodName
SQL	I-MethodName
(	O
Choi	O
et	O
al	O
.	O
,	O
2020	O
)	O
and	O
RECPARSER	B-MethodName
(	O
Zeng	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

The	O
latter	O
ones	O
have	O
slot	O
prediction	O
modules	O
similar	O
to	O
SQLNet	B-MethodName
for	O
the	O
WikiSQL	B-DatasetName
,	O
while	O
recursion	O
modules	O
are	O
introduced	O
to	O
handle	O
the	O
generation	O
of	O
complex	O
SQL	O
sketches	O
,	O
a	O
characteristic	O
in	O
Spider	B-DatasetName
but	O
absent	O
in	O
WikiSQL	B-DatasetName
.	O

Furthermore	O
,	O
our	O
extractor	B-MethodName
-	I-MethodName
linker	I-MethodName
architecture	I-MethodName
is	O
also	O
much	O
simpler	O
than	O
sketch	B-MethodName
-	I-MethodName
based	I-MethodName
methods	I-MethodName
.	O

Recent	O
advances	O
(	O
Yu	O
et	O
al	O
.	O
,	O
2018a;Dong	O
and	O
Lapata	O
,	O
2018;Hwang	O
et	O
al	O
.	O
,	O
2019;He	O
et	O
al	O
.	O
,	O
2019	O
)	O
follow	O
this	O
approach	O
and	O
achieve	O
comparative	O
results	O
on	O
Wik	B-DatasetName
-	I-DatasetName
iSQL	I-DatasetName
,	O
mostly	O
by	O
using	O
pre	O
-	O
trained	O
language	O
models	O
as	O
the	O
encoder	O
.	O

SQLNet	B-MethodName
(	O
Xu	O
et	O
al	O
.	O
,	O
2018	O
)	O
introduces	O
sketchbased	O
method	O
,	O
which	O
decomposes	O
the	O
SQL	O
synthesis	O
into	O
several	O
independent	O
classification	O
sub	O
-	O
tasks	O
,	O
including	O
select	O
-	O
aggregation	O
/	O
column	O
and	O
wherenumber	O
/	O
column	O
/	O
operator	O
/	O
value	O
.	O

These	O
sequence	B-MethodName
-	I-MethodName
to	I-MethodName
-	I-MethodName
sequence	I-MethodName
approaches	O
often	O
suffer	O
the	O
"	O
ordering	O
issue	O
"	O
since	O
they	O
are	O
designed	O
to	O
fit	O
an	O
ordered	O
sequence	O
,	O
while	O
the	O
conditions	O
in	O
WHEREclause	O
are	O
unordered	O
in	O
nature	O
.	O

Pointer	B-MethodName
networks	I-MethodName
(	O
Vinyals	O
et	O
al	O
.	O
,	O
2015	O
)	O
are	O
also	O
commonly	O
adopted	O
.	O

Earlier	O
work	O
(	O
Dong	O
and	O
Lapata	O
,	O
2016;Krishnamurthy	O
et	O
al	O
.	O
,	O
2017;Zhong	O
et	O
al	O
.	O
,	O
2018;Sun	O
et	O
al	O
.	O
,	O
2018;Wang	O
et	O
al	O
.	O
,	O
2018	O
)	O
follow	O
a	O
neural	B-MethodName
sequence	I-MethodName
-	I-MethodName
to	I-MethodName
-	I-MethodName
sequence	I-MethodName
paradigm	O
(	O
Sutskever	O
et	O
al	O
.	O
,	O
2014	O
)	O
with	O
attention	O
mechanism	O
(	O
Bahdanau	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

Text	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
SQL	I-TaskName
is	O
a	O
sub	O
-	O
area	O
of	O
semantic	B-TaskName
parsing	I-TaskName
,	O
which	O
is	O
widely	O
studied	O
in	O
recent	O
years	O
.	O

Semantic	B-TaskName
parsing	I-TaskName
(	O
Berant	O
et	O
al	O
.	O
,	O
2013	O
)	O
is	O
to	O
map	O
natural	O
language	O
utterances	O
to	O
machine	O
-	O
interpretable	O
representations	O
,	O
such	O
as	O
logic	O
forms	O
(	O
Dong	O
and	O
Lapata	O
,	O
2016	O
)	O
,	O
program	O
codes	O
(	O
Yin	O
and	O
Neubig	O
,	O
2017	O
)	O
,	O
and	O
SQL	O
queries	O
(	O
Zhong	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

In	O
this	O
setting	O
,	O
the	O
logic	B-MetricName
form	I-MetricName
and	O
execution	B-MetricName
accuracy	I-MetricName
on	O
the	O
dev	O
set	O
reaches	O
92.8	B-MetricValue
%	I-MetricValue
and	O
94.2	B-MetricValue
%	I-MetricValue
,	O
respectively	O
,	O
which	O
are	O
the	O
ceiling	O
for	O
our	O
approach	O
.	O

The	O
improved	O
AGG	B-MetricName
accuracy	I-MetricName
also	O
leads	O
to	O
the	O
new	O
stateof	O
-	O
the	O
-	O
art	O
for	O
the	O
overall	O
SQL	O
results	O
.	O

shows	O
the	O
slot	O
type	O
-	O
wise	O
results	O
,	O
where	O
our	O
method	O
achieves	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
the	O
W	B-MetricName
col	I-MetricName
,	O
W	B-MetricName
val	I-MetricName
and	O
W	B-MetricName
op	I-MetricName
accuracies	I-MetricName
.	O

For	O
the	O
results	O
with	O
the	O
EG	O
in	O
Table	O
2	O
,	O
our	O
method	O
outperforms	O
all	O
the	O
existing	O
methods	O
,	O
including	O
SQLova	B-MethodName
,	O
X	B-MethodName
-	I-MethodName
SQL	I-MethodName
and	O
HydraNet	B-MethodName
,	O
leading	O
to	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
the	O
SQL	O
accuracies	B-MetricName
in	O
terms	O
of	O
both	O
logic	O
form	O
and	O
execution	O
.	O

(	O
2019a	O
)	O
shows	O
MT	B-MethodName
-	I-MethodName
DNN	I-MethodName
also	O
outperforms	O
BERT	B-MethodName
in	O
many	O
tasks	O
.	O

(	O
2020	O
)	O
shows	O
that	O
RoBERTa	B-MethodName
large	I-MethodName
outperform	O
BERT	B-MethodName
large	I-MethodName
in	O
their	O
setting	O
and	O
Liu	O
et	O
al	O
.	O

Without	O
EG	O
,	O
our	O
method	O
with	O
BERT	B-MethodName
-	I-MethodName
base	I-MethodName
outperforms	O
most	O
of	O
existing	O
methods	O
,	O
including	O
SQLova	B-MethodName
with	O
BERTlarge	O
and	O
MT	O
-	O
DNN	O
(	O
Liu	O
et	O
al	O
.	O
,	O
2019a)-based	O
X	B-MethodName
-	I-MethodName
SQL	I-MethodName
,	O
and	O
ranks	O
right	O
after	O
HydraNet	B-MethodName
,	O
which	O
is	O
based	O
on	O
RoBerTa	O
(	O
Liu	O
et	O
al	O
.	O
,	O
2019b	O
)	O
large	O
.	O

We	O
compare	O
our	O
method	O
with	O
notable	O
models	O
that	O
have	O
reported	O
results	O
on	O
WikiSQL	B-DatasetName
task	O
,	O
including	O
Seq2SQL	B-MethodName
(	O
Zhong	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
SQLNet	B-MethodName
(	O
Xu	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
TypeSQL	B-MethodName
(	O
Yu	O
et	O
al	O
.	O
,	O
2018a	O
)	O
,	O
Coarseto	B-MethodName
-	I-MethodName
Fine	I-MethodName
(	O
Dong	O
and	O
Lapata	O
,	O
2018	O
)	O
,	O
SQLova	B-MethodName
(	O
Hwang	O
et	O
al	O
.	O
,	O
2019	O
)	O
,	O
X	B-MethodName
-	I-MethodName
SQL	I-MethodName
(	O
He	O
et	O
al	O
.	O
,	O
2019	O
)	O
and	O
Hy	B-MethodName
-	I-MethodName
draNet	I-MethodName
(	O
Lyu	O
et	O
al	O
.	O
,	O
2020	O
)	O
in	O
Table	O
2	O
.	O

We	O
choose	O
uncased	B-MethodName
BERT	I-MethodName
-	I-MethodName
base	I-MethodName
pre	I-MethodName
-	I-MethodName
trained	I-MethodName
model	O
with	O
default	O
settings	O
due	O
to	O
resource	O
limitations	O
.	O

As	O
statistical	O
aligner	O
can	O
occasionally	O
yield	O
null	O
-	O
alignment	O
for	O
a	O
few	O
tokens	O
,	O
we	O
use	O
another	O
unsupervised	B-MethodName
word	I-MethodName
and	I-MethodName
semantic	I-MethodName
similarity	I-MethodName
-	I-MethodName
based	I-MethodName
algorithm	I-MethodName
(	O
Perez	O
et	O
al	O
.	O
,	O
2020	O
)	O
to	O
complement	O
the	O
missing	O
alignments	O
.	O

Given	O
the	O
characteristic	O
of	O
the	O
data	O
and	O
the	O
possible	O
limitation	O
of	O
the	O
information	B-MethodName
extraction	I-MethodName
-	I-MethodName
based	I-MethodName
model	I-MethodName
,	O
we	O
improve	O
the	O
AGG	O
results	O
over	O
the	O
original	O
model	O
,	O
using	O
only	O
simple	O
association	O
signals	O
in	O
the	O
training	O
data	O
.	O

To	O
this	O
end	O
,	O
we	O
adopt	O
transformation	B-MethodName
-	I-MethodName
based	I-MethodName
learning	I-MethodName
algorithm	I-MethodName
(	O
Brill	O
,	O
1995	O
)	O
to	O
update	O
the	O
AGG	O
predictions	O
based	O
on	O
association	O
rules	O
in	O
the	O
form	O
of	O
"	O
change	O
AGG	O
from	O
x	O
to	O
x	O
,	O
given	O
certain	O
word	O
tuple	O
occurrences	O
.	O
"	O
Such	O
rules	O
are	O
mined	O
and	O
ranked	O
from	O
the	O
training	O
data	O
by	O
the	O
algorithm	O
.	O

(	O
2019	O
)	O
that	O
AGG	O
annotations	O
in	O
WikiSQL	B-DatasetName
have	O
up	O
to	O
10	O
%	O
of	O
errors	O
.	O

v	O
CLS	O
i	O
=	O
BERT([span	B-MethodName
;	O
C	O
i	O
]	O
)	O
P	O
(	O
i	O
)	O
=	O
Sigmoid(W	O
v	O
CLS	O
)	O
(	O
2	O
)	O
.	O

Again	O
,	O
BERT	B-MethodName
is	O
used	O
as	O
the	O
underlying	O
model	O
for	O
its	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
text	O
matching	O
.	O

To	O
convert	O
mention	B-MethodName
and	I-MethodName
relation	I-MethodName
extraction	I-MethodName
results	O
to	O
SQL	O
,	O
we	O
need	O
a	O
schema	O
linking	O
module	O
to	O
link	O
explicit	O
and	O
implicit	O
column	O
mentions	O
to	O
its	O
canonical	O
column	O
names	O
in	O
the	O
table	O
schema	O
.	O

As	O
the	O
two	O
labeling	O
tasks	O
can	O
benefit	O
each	O
other	O
,	O
we	O
fine	O
-	O
tune	O
BERT	B-MethodName
in	O
a	O
multi	O
-	O
task	O
learning	O
way	O
.	O

Finally	O
,	O
the	O
resulting	O
token	O
representations	O
are	O
fed	O
to	O
the	O
CRF	B-MethodName
layer	O
,	O
which	O
yields	O
the	O
label	O
sequence	O
.	O

Q	O
B	O
;	O
C	O
B	O
=	O
BERT([Q	B-MethodName
;	O
C	O
]	O
)	O
Q	O
att	O
=	O
Attention(Q	O
B	O
,	O
C	O
B	O
,	O
C	O
B	O
)	O
+	O
Q	O
B	O
L	O
=	O
CRF(W	O
L	O
Q	O
att	O
)	O
(	O
1	O
)	O
Before	O
the	O
BERT	B-MethodName
representations	O
are	O
fed	O
to	O
the	O
CRF	B-MethodName
layer	O
,	O
they	O
first	O
go	O
through	O
an	O
attention	O
layer	O
(	O
Bahdanau	O
et	O
al	O
.	O
,	O
2014	O
)	O
,	O
which	O
encodes	O
the	O
question	O
tokens	O
with	O
columns	O
in	O
the	O
schema	O
.	O

The	O
full	O
model	O
is	O
described	O
as	O
in	O
equation	O
(	O
1	O
)	O
,	O
where	O
BERT	B-MethodName
denotes	O
the	O
BERT	B-MethodName
model	O
while	O
CRF	B-MethodName
denotes	O
a	O
CRF	B-MethodName
layer	O
.	O

As	O
the	O
labeling	O
is	O
w.r.t	O
.	O
the	O
question	O
sentence	O
,	O
the	B-MethodName
conditional	I-MethodName
random	I-MethodName
filed	I-MethodName
(	O
CRF	B-MethodName
)	O
(	O
Lafferty	O
et	O
al	O
.	O
,	O
2001	O
)	O
layer	O
only	O
is	O
applied	O
to	O
the	O
question	O
segment	O
.	O

Similar	O
to	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
for	O
text	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
SQL	I-TaskName
such	O
as	O
SQLova	B-MethodName
(	O
Hwang	O
et	O
al	O
.	O
,	O
2019	O
)	O
,	O
we	O
concatenate	O
the	O
question	O
text	O
along	O
with	O
the	O
table	O
header	O
as	O
input	O
for	O
BERT	B-MethodName
,	O
in	O
the	O
form	O
of	O
q	O
1	O
,	O
q	O
2	O
,	O
..	O
,	O
q	O
L	O
,	O
[	O
SEP	O
]	O
,	O
c	O
1,1	O
,	O
c	O
1,2	O
,	O
...	O
,	O
[	O
SEP	O
]	O
,	O
c	O
2,1	O
,	O
...	O
,	O
c	O
M,1	O
...	O
,	O
where	O
Q	O
(	O
|Q|	O
=	O
L	O
)	O
is	O
the	O
question	O
while	O
C	O
=	O
c	O
1	O
,	O
..	O
,	O
c	O
M	O
(	O
|C|	O
=	O
M	O
)	O
are	O
the	O
table	O
headers	O
.	O

As	O
pre	O
-	O
trained	O
language	O
models	O
such	O
as	O
BERT	B-MethodName
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
various	O
NLP	O
tasks	O
including	O
sequence	O
labeling	O
,	O
we	O
adopt	O
BERT	B-MethodName
to	O
get	O
contextualized	O
representations	O
for	O
both	O
role	O
and	O
span	O
labeling	O
.	O

Such	O
annotations	O
can	O
be	O
useful	O
for	O
developing	O
novel	O
methods	O
for	O
text	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
SQL	I-TaskName
,	O
such	O
as	O
question	O
decomposition	O
-	O
based	O
approaches	O
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
this	O
is	O
the	O
first	O
work	O
that	O
formulates	O
the	O
task	O
as	O
sequence	B-MethodName
labeling	I-MethodName
-	I-MethodName
based	I-MethodName
extraction	I-MethodName
plus	I-MethodName
linking	I-MethodName
,	O
which	O
enjoys	O
the	O
advantage	O
of	O
structural	O
simplicity	O
and	O
interdependency	O
awareness	O
.	O

The	O
main	O
contribution	O
of	O
this	O
paper	O
is	O
the	O
mention	B-MethodName
and	I-MethodName
relation	I-MethodName
extraction	I-MethodName
-	I-MethodName
based	I-MethodName
approach	O
to	O
textto	B-TaskName
-	I-TaskName
SQL	I-TaskName
task	I-TaskName
.	O

Trained	O
with	O
such	O
annotations	O
and	O
applied	O
AE	O
method	O
,	O
the	O
proposed	O
method	O
can	O
already	O
achieves	O
the	O
first	O
place	O
on	O
the	O
WikiSQL	B-DatasetName
benchmark	O
.	O

The	O
output	O
of	O
the	O
extractor	O
can	O
be	O
deterministically	O
translated	O
into	O
pseudo	O
SQLs	O
,	O
before	O
a	O
BERT	B-MethodName
-	I-MethodName
based	I-MethodName
linker	I-MethodName
(	O
Section	O
2.2	O
)	O
maps	O
the	O
column	O
mentions	O
to	O
the	O
table	O
headers	O
to	O
get	O
executable	O
SQL	O
queries	O
.	O

To	O
this	O
end	O
,	O
we	O
leverage	O
one	O
unified	O
BERT	B-MethodName
-	I-MethodName
based	I-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O
2019	O
)	O
extractor	B-MethodName
(	O
Section	O
2.1	O
)	O
to	O
recognize	O
the	O
slot	O
mentions	O
as	O
well	O
as	O
their	O
relations	O
,	O
from	O
the	O
natural	O
language	O
questions	O
.	O

To	O
deal	O
with	O
these	O
drawbacks	O
,	O
this	O
paper	O
formulates	O
text	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
SQL	I-TaskName
as	O
mention	B-MethodName
extraction	I-MethodName
and	I-MethodName
linking	I-MethodName
problems	O
in	O
a	O
sequence	O
labeling	O
manner	O
(	O
Section	O
2	O
)	O
.	O

On	O
the	O
WikiSQL	B-DatasetName
(	O
Zhong	O
et	O
al	O
.	O
,	O
2018	O
)	O
benchmark	O
for	O
multi	O
-	O
domain	O
,	O
single	O
table	O
text	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
SQL	I-TaskName
,	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
systems	O
(	O
Hwang	O
et	O
al	O
.	O
,	O
2019;He	O
et	O
al	O
.	O
,	O
2019	O
)	O
can	O
predict	O
more	O
than	O
80	O
%	O
of	O
entire	O
SQL	O
queries	O
correctly	O
.	O

Recently	O
,	O
by	O
the	O
development	O
of	O
deep	O
learning	O
,	O
significant	O
advances	O
have	O
been	O
made	O
in	O
text	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
SQL	I-TaskName
.	O

As	O
a	O
subarea	O
of	O
semantic	O
parsing	O
(	O
Berant	O
et	O
al	O
.	O
,	O
2013	O
)	O
,	O
text	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
SQL	I-TaskName
is	O
known	O
to	O
be	O
difficult	O
due	O
to	O
the	O
flexibility	O
in	O
natural	O
language	O
.	O

Textto	B-TaskName
-	I-TaskName
SQL	I-TaskName
technology	O
is	O
very	O
useful	O
as	O
it	O
can	O
empower	O
humans	O
to	O
naturally	O
interact	O
with	O
relational	O
databases	O
,	O
which	O
serve	O
as	O
foundations	O
for	O
the	O
digital	O
world	O
today	O
.	O

Text	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
SQL	I-TaskName
systems	O
generate	O
SQL	O
queries	O
according	O
to	O
given	O
natural	O
language	O
questions	O
.	O

Trained	O
with	O
automatically	O
generated	O
annotations	O
,	O
the	O
proposed	O
method	O
achieves	O
the	O
first	O
place	O
on	O
the	O
WikiSQL	B-DatasetName
benchmark	O
.	O

To	O
solve	O
these	O
problems	O
,	O
this	O
paper	O
proposes	O
a	O
novel	O
extraction	B-MethodName
-	I-MethodName
linking	I-MethodName
approach	O
,	O
where	O
a	O
unified	O
extractor	O
recognizes	O
all	O
types	O
of	O
slot	O
mentions	O
appearing	O
in	O
the	O
question	O
sentence	O
before	O
a	O
linker	O
maps	O
the	O
recognized	O
columns	O
to	O
the	O
table	O
schema	O
to	O
generate	O
executable	O
SQL	O
queries	O
.	O

On	O
the	O
WikiSQL	B-DatasetName
benchmark	O
,	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
text	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
SQL	I-TaskName
systems	O
typically	O
take	O
a	O
slotfilling	O
approach	O
by	O
building	O
several	O
dedicated	O
models	O
for	O
each	O
type	O
of	O
slots	O
.	O

Mention	B-MethodName
Extraction	I-MethodName
and	I-MethodName
Linking	I-MethodName
for	O
SQL	B-TaskName
Query	I-TaskName
Generation	I-TaskName
.	O

We	O
thank	O
Jun	O
Xu	O
,	O
Muhua	O
Zhu	O
,	O
Wanxiang	O
Che	O
and	O
Longxu	O
Dou	O
as	O
well	O
as	O
all	O
the	O
anonymous	O
reviewers	O
for	O
their	O
invaluable	O
comments	O
and	O
suggestions	O
.	O

Acknowledgements	O
.	O

Since	O
the	O
current	O
automatic	O
-	O
generated	O
annotations	O
are	O
still	O
noisy	O
,	O
it	O
is	O
useful	O
to	O
further	O
improve	O
the	O
automatic	O
annotation	O
procedure	O
.	O

Conclusion	O
and	O
Future	O
Work	O
.	O

The	O
mentions	O
for	O
values	O
,	O
operators	O
and	O
corresponding	O
columns	O
often	O
appear	O
in	O
proximity	O
in	O
the	O
question	O
,	O
thus	O
the	O
sequence	O
labeling	O
model	O
can	O
better	O
capture	O
their	O
dependencies	O
and	O
benefits	O
the	O
recognition	O
for	O
all	O
of	O
them	O
,	O
as	O
experiment	O
results	O
suggest	O
.	O

While	O
our	O
sequence	O
labeling	O
method	O
is	O
also	O
based	O
on	O
pre	O
-	O
trained	O
language	O
model	O
,	O
it	O
differs	O
from	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
in	O
that	O
it	O
explicitly	O
ex	O
-	O
tracts	O
mentions	O
from	O
the	O
questions	O
and	O
can	O
benefit	O
from	O
inter	O
-	O
dependency	O
modeling	O
between	O
extracted	O
mentions	O
.	O

Moreover	O
,	O
each	O
submodule	O
solves	O
its	O
own	O
classification	O
problem	O
,	O
without	O
considering	O
the	O
dependencies	O
with	O
SQL	O
elements	O
modeled	O
by	O
other	O
sub	O
-	O
modules	O
.	O

These	O
sketch	O
-	O
based	O
models	O
raise	O
challenges	O
in	O
training	O
,	O
deployment	O
and	O
maintenance	O
.	O

Except	O
wherevalue	O
,	O
which	O
is	O
usually	O
predicted	O
by	O
a	O
pointer	O
network	O
,	O
all	O
the	O
other	O
sub	O
-	O
tasks	O
use	O
their	O
own	O
dedicated	O
classifiers	O
to	O
make	O
predictions	O
.	O

Related	O
Work	O
.	O

(	O
2019	O
)	O
,	O
suggesting	O
that	O
the	O
quality	O
of	O
the	O
automatic	O
annotation	O
is	O
reasonably	O
good	O
.	O

Note	O
that	O
such	O
ceiling	O
is	O
above	O
the	O
human	O
-	O
level	O
accuracy	O
reported	O
in	O
Hwang	O
et	O
al	O
.	O

The	O
quality	O
of	O
automatic	O
annotation	O
can	O
be	O
estimated	O
in	O
an	O
oracle	O
extractor	O
setting	O
,	O
where	O
the	O
automatically	O
annotated	O
labels	O
,	O
instead	O
of	O
the	O
extractor	O
prediction	O
,	O
are	O
fed	O
to	O
the	O
linker	O
.	O

A	O
limitation	O
of	O
our	O
sequence	O
labeling	O
-	O
based	O
approach	O
is	O
that	O
it	O
performs	O
passably	O
on	O
some	O
questions	O
with	O
nested	O
span	O
structures	O
,	O
as	O
in	O
the	O
question	O
"	O
When	O
does	O
the	O
train	O
[	O
arriving	O
at	O
[	O
Bourne	O
]	O
Estimating	O
Annotation	O
Quality	O
.	O

We	O
close	O
such	O
gap	O
with	O
AE	O
using	O
only	O
word	O
co	O
-	O
occurrence	O
features	O
.	O

Before	O
applying	O
AGG	O
enhancement	O
(	O
AE	O
)	O
,	O
the	O
bottleneck	O
of	O
our	O
method	O
is	O
on	O
AGG	O
prediction	O
.	O

Since	O
the	O
operators	O
and	O
values	O
are	O
directly	O
derived	O
from	O
the	O
extractor	O
,	O
such	O
results	O
are	O
evidence	O
for	O
the	O
effectiveness	O
of	O
our	O
extraction	O
-	O
based	O
approach	O
.	O

Despite	O
disadvantage	O
in	O
underlying	O
pre	O
-	O
trained	O
language	O
model	O
,	O
our	O
model	O
achieves	O
competitive	O
results	O
.	O

Lyu	O
et	O
al	O
.	O

Results	O
.	O

Codes	O
are	O
implemented	O
in	O
Pytorch	O
1.3	O
and	O
will	O
be	O
made	O
publicly	O
available	O
1	O
.	O

(	O
2019	O
)	O
.	O

The	O
training	O
procedures	O
follows	O
Hwang	O
et	O
al	O
.	O

3	O
Experiment	O
and	O
Ba	O
,	O
2014	O
)	O
to	O
optimize	O
the	O
model	O
with	O
default	O
hyper	O
-	O
parameters	O
.	O

The	O
second	O
step	O
is	O
label	O
generation	O
,	O
where	O
the	O
roles	O
are	O
generated	O
according	O
to	O
aligned	O
elements	O
,	O
while	O
the	O
span	O
labels	O
are	O
assigned	O
by	O
considering	O
minimal	O
text	O
span	O
that	O
covers	O
all	O
the	O
elements	O
in	O
a	O
SELECT	O
/	O
WHERE	O
clause	O
.	O

For	O
this	O
purpose	O
,	O
we	O
choose	O
Berkeley	O
aligner	O
(	O
Liang	O
et	O
al	O
.	O
,	O
2006	O
)	O
,	O
which	O
works	O
by	O
estimating	O
the	O
co	O
-	O
occurrence	O
of	O
tokens	O
in	O
the	O
parallel	O
corpora	O
,	O
which	O
are	O
the	O
question	O
-	O
SQL	O
pairs	O
in	O
our	O
case	O
.	O

The	O
first	O
pass	O
conducts	O
exact	O
and	O
partial	O
string	O
match	O
to	O
recognize	O
values	O
and	O
some	O
of	O
the	O
columns	O
,	O
while	O
the	O
second	O
pass	O
aligns	O
the	O
remaining	O
SQL	O
slots	O
,	O
by	O
training	O
a	O
statistical	O
aligner	O
with	O
the	O
training	O
set	O
of	O
the	O
data	O
.	O

The	O
first	O
step	O
is	O
alignment	O
,	O
which	O
runs	O
two	O
pass	O
of	O
aligning	O
.	O

Specifically	O
,	O
the	O
proposed	O
method	O
is	O
a	O
two	O
-	O
step	O
procedure	O
.	O

Figure	O
1	O
depicts	O
such	O
alignments	O
with	O
arrows	O
and	O
colors	O
.	O

The	O
idea	O
is	O
to	O
annotate	O
mentions	O
by	O
aligning	O
the	O
SQL	O
slots	O
in	O
the	O
query	O
to	O
tokens	O
in	O
the	O
question	O
.	O

Since	O
manual	O
annotations	O
are	O
costly	O
,	O
we	O
resort	O
to	O
automatic	O
ways	O
.	O

A	O
challenge	O
for	O
training	O
the	O
extractor	O
is	O
that	O
benchmark	O
datasets	O
have	O
no	O
role	O
or	O
span	O
annotations	O
.	O

Automatic	O
Annotation	O
via	O
Alignment	O
.	O

Another	O
reason	O
may	O
be	O
that	O
no	O
all	O
the	O
aggregation	O
functions	O
are	O
grounded	O
to	O
particular	O
tokens	O
.	O

In	O
such	O
case	O
,	O
as	O
our	O
extractor	O
model	O
has	O
to	O
take	O
care	O
of	O
other	O
types	O
of	O
slots	O
,	O
these	O
extra	O
constraints	O
make	O
it	O
more	O
challenging	O
for	O
our	O
model	O
to	O
fit	O
flawed	O
data	O
,	O
compared	O
with	O
a	O
dedicated	O
AGG	O
classifier	O
,	O
as	O
in	O
most	O
SOTA	O
methods	O
.	O

Analysis	O
of	O
preliminary	O
results	O
suggests	O
that	O
aggregation	O
function	O
(	O
AGG	O
)	O
prediction	O
is	O
a	O
bottleneck	O
for	O
our	O
system	O
,	O
which	O
is	O
partly	O
attributed	O
to	O
the	O
findings	O
by	O
Hwang	O
et	O
al	O
.	O

AGG	O
prediction	O
enhancement	O
.	O

The	O
matching	O
procedure	O
can	O
be	O
described	O
as	O
in	O
equation	O
(	O
2	O
)	O
.	O

Special	O
tokens	O
of	O
[	O
W	O
]	O
and	O
[	O
S	O
]	O
are	O
used	O
to	O
distinguish	O
SELECT	O
spans	O
from	O
FIL	O
-	O
TER	O
spans	O
.	O

Formally	O
,	O
we	O
define	O
the	O
linker	O
as	O
a	O
text	O
matching	O
model	O
,	O
i.e.	O
estimating	O
a	O
function	O
f	O
(	O
[	O
C	O
i	O
;	O
span	O
;	O
Q	O
]	O
)	O
→	O
{	O
0	O
,	O
1	O
}	O
,	O
where	O
C	O
i	O
is	O
a	O
header	O
in	O
the	O
table	O
schema	O
,	O
span	O
is	O
the	O
either	O
an	O
extracted	O
column	O
mention	O
(	O
for	O
linking	O
explicit	O
column	O
mention	O
)	O
or	O
an	O
extracted	O
value	O
v	O
(	O
for	O
linking	O
implicit	O
column	O
mention	O
)	O
.	O

Such	O
case	O
is	O
challenging	O
yet	O
not	O
uncommon	O
.	O

The	O
latter	O
case	O
is	O
implicit	O
mention	O
of	O
column	O
,	O
as	O
only	O
the	O
value	O
for	O
the	O
column	O
,	O
Josefin	O
Lillhage	O
,	O
appears	O
in	O
the	O
question	O
.	O

The	O
column	O
mentions	O
in	O
the	O
question	O
sentence	O
often	O
differ	O
with	O
the	O
the	O
canonical	O
column	O
names	O
in	O
the	O
table	O
schema	O
in	O
terms	O
of	O
string	O
forms	O
,	O
as	O
shown	O
in	O
Figure	O
1	O
,	O
where	O
SPLIT	O
(	O
50	O
M	O
)	O
is	O
mentioned	O
as	O
50	O
m	O
splits	O
and	O
NAME	O
is	O
not	O
mentioned	O
at	O
all	O
.	O

Schema	O
Linking	O
as	O
Matching	O
.	O

The	O
resulting	O
representation	O
is	O
added	O
to	O
the	O
original	O
token	O
representation	O
in	O
an	O
element	O
-	O
wise	O
manner	O
.	O

Special	O
SEP	O
token	O
is	O
inserted	O
between	O
different	O
headers	O
c	O
i	O
as	O
well	O
as	O
between	O
the	O
question	O
sentence	O
Q	O
and	O
the	O
first	O
header	O
c	O
1	O
.	O

Each	O
header	O
c	O
i	O
may	O
have	O
multiple	O
tokens	O
,	O
thus	O
the	O
2	O
-	O
d	O
indexes	O
of	O
c	O
i	O
,	O
j	O
being	O
used	O
.	O

Extractor	O
Model	O
The	O
model	O
first	O
encodes	O
the	O
question	O
text	O
and	O
the	O
table	O
headers	O
.	O

With	O
these	O
defined	O
label	O
set	O
,	O
the	O
recognition	O
of	O
both	O
slot	O
mentions	O
and	O
slot	O
relations	O
are	O
formulated	O
as	O
sequence	O
labeling	O
.	O

For	O
our	O
task	O
,	O
we	O
define	O
two	O
sets	O
of	O
labels	O
:	O
(	O
a	O
)	O
the	O
SQL	O
role	O
labels	O
representing	O
the	O
slot	O
mentions	O
;	O
(	O
b	O
)	O
the	O
span	O
labels	O
representing	O
the	O
slot	O
relations	O
,	O
both	O
of	O
which	O
are	O
shown	O
in	O
Table	O
1	O
.	O

The	O
standing	O
alone	O
O	O
label	O
is	O
assigned	O
to	O
tokens	O
that	O
are	O
outside	O
of	O
any	O
type	O
of	O
annotation	O
of	O
interest	O
.	O

As	O
shown	O
in	O
bel	O
l	O
∈	O
{	O
T	O
×	O
{	O
B	O
,	O
I	O
}	O
,	O
O	O
}	O
,	O
where	O
×	O
denotes	O
the	O
Cartesian	O
product	O
of	O
T	O
,	O
the	O
set	O
of	O
functional	O
labels	O
,	O
and	O
the	O
set	O
of	O
positional	O
label	O
of	O
{	O
B	O
,	O
I	O
}	O
,	O
where	O
B	O
and	O
I	O
means	O
the	O
beginning	O
and	O
the	O
continuation	O
of	O
a	O
particular	O
annotation	O
t	O
∈	O
T	O
,	O
respectively	O
.	O

Thus	O
,	O
the	O
extraction	O
of	O
the	O
relations	O
is	O
equivalent	O
to	O
the	O
labeling	O
of	O
the	O
corresponding	O
text	O
span	O
.	O

As	O
for	O
the	O
slot	O
relations	O
,	O
note	O
that	O
the	O
column	O
,	O
value	O
and	O
operator	O
that	O
form	O
a	O
filter	O
condition	O
relation	O
usually	O
appear	O
in	O
adjacency	O
in	O
the	O
question	O
,	O
such	O
as	O
in	O
lanes	O
above	O
8	O
in	O
the	O
example	O
.	O

Most	O
of	O
the	O
SQL	O
slots	O
are	O
mentioned	O
in	O
the	O
question	O
,	O
as	O
shown	O
in	O
Figure	O
1(a	O
)	O
.	O

The	O
extractor	O
recognizes	O
(	O
1	O
)	O
slot	O
mentions	O
,	O
including	O
the	O
SELECT	O
column	O
with	O
aggregation	O
function	O
,	O
WHERE	O
columns	O
with	O
corresponding	O
values	O
and	O
operators	O
;	O
and	O
(	O
2	O
)	O
slot	O
relations	O
,	O
namely	O
associating	O
each	O
WHERE	O
column	O
with	O
its	O
operator	O
and	O
value	O
.	O

Extractor	O
.	O

Method	O
.	O

In	O
addition	O
,	O
we	O
also	O
propose	O
an	O
automatic	O
method	O
to	O
generate	O
annotations	O
.	O

Also	O
,	O
preliminary	O
results	O
show	O
that	O
the	O
prediction	O
of	O
aggregation	O
function	O
(	O
AGG	O
)	O
restricts	O
model	O
performance	O
,	O
which	O
induces	O
us	O
to	O
put	O
forward	O
AGG	O
prediction	O
enhancement	O
(	O
AE	O
)	O
method	O
inspired	O
by	O
Brill	O
(	O
1995	O
)	O
.	O

Thus	O
we	O
propose	O
an	O
automatic	O
annotation	O
method	O
(	O
Section	O
2.4	O
)	O
based	O
on	O
aligning	O
tokens	O
in	O
a	O
SQL	O
with	O
corresponding	O
question	O
.	O

A	O
major	O
challenge	O
to	O
the	O
proposed	O
method	O
is	O
the	O
absence	O
of	O
manual	O
annotation	O
of	O
mentions	O
and	O
relations	O
.	O

Thus	O
,	O
the	O
recognition	O
of	O
the	O
mentions	O
and	O
their	O
relations	O
would	O
mostly	O
reconstruct	O
the	O
intended	O
SQL	O
query	O
from	O
natural	O
language	O
question	O
.	O

Moreover	O
,	O
the	O
relations	O
between	O
the	O
slot	O
mentions	O
,	O
such	O
as	O
<	O
lanes	O
,	O
above	O
,	O
8	O
>	O
forming	O
a	O
filter	O
condition	O
,	O
are	O
represented	O
by	O
proximity	O
in	O
linear	O
order	O
or	O
other	O
linguistic	O
cues	O
.	O

SQL	O
:	O
SELECT	O
SUM	O
(	O
Split	O
(	O
50	O
m	O
)	O
)	O
FROM	O
some	O
table	O
WHERE	O
Name	O
=	O
'	O
Josefin	O
Lillhage	O
'	O
AND	O
Lane	O
>	O
8	O
We	O
can	O
see	O
that	O
many	O
SQL	O
elements	O
,	O
or	O
slots	O
,	O
such	O
as	O
column	O
names	O
of	O
SPLIT	O
(	O
50	O
M	O
)	O
and	O
LANE	O
,	O
values	O
like	O
"	O
Josefin	O
Lillhage	O
"	O
and	O
8	O
,	O
as	O
well	O
as	O
operators	O
>	O
are	O
mentioned	O
with	O
words	O
similar	O
in	O
form	O
and/or	O
meaning	O
.	O

b.	O

Question	O
:	O
What	O
is	O
the	O
total	O
sum	O
of	O
50	O
m	O
splits	O
for	O
Josefin	O
Lillhage	O
in	O
lanes	O
above	O
8	O
?	O

(	O
1	O
)	O
a.	O

Consider	O
the	O
question	O
and	O
its	O
corresponding	O
SQL	O
query	O
in	O
example	O
(	O
1	O
)	O
,	O
with	O
the	O
headers	O
in	O
the	O
schema	O
being	O
{	O
LANE	O
,	O
NAME	O
,	O
NATIONALITY	O
,	O
SPLIT	O
(	O
50	O
M	O
)	O
,	O
TIME	O
}	O
.	O

In	O
this	O
new	O
formulation	O
,	O
the	O
key	O
to	O
synthesizing	O
SQL	O
is	O
to	O
extract	O
the	O
mentions	O
of	O
SQL	O
slots	O
and	O
the	O
relations	O
between	O
them	O
.	O

among	O
SQL	O
sub	O
-	O
clauses	O
,	O
as	O
each	O
type	O
of	O
slots	O
is	O
modeled	O
separately	O
.	O

Such	O
dedicated	O
modules	O
are	O
complex	O
and	O
often	O
fall	O
short	O
of	O
capturing	O
inter	O
-	O
dependencies	O
*	O
Equal	O
contributions	O
.	O

Most	O
of	O
such	O
systems	O
take	O
a	O
sketch	O
-	O
based	O
approach	O
(	O
Xu	O
et	O
al	O
.	O
,	O
2018	O
)	O
that	O
builds	O
several	O
specialized	O
modules	O
,	O
each	O
of	O
which	O
is	O
dedicated	O
to	O
predicting	O
a	O
particular	O
type	O
of	O
slots	O
,	O
such	O
as	O
the	O
column	O
in	O
SELECT	O
,	O
or	O
the	O
filter	O
value	O
in	O
WHERE	O
.	O

Introduction	O
.	O

Such	O
modularized	O
systems	O
are	O
not	O
only	O
complex	O
but	O
also	O
of	O
limited	O
capacity	O
for	O
capturing	O
interdependencies	O
among	O
SQL	O
clauses	O
.	O

Conclusion	O
and	O
Future	O
Work	O
.	O

We	O
addressed	O
the	O
problem	O
of	O
utilizing	O
GNNs	B-MethodName
to	O
perform	O
relational	B-TaskName
reasoning	I-TaskName
with	I-TaskName
natural	I-TaskName
languages	I-TaskName
.	O

Our	O
proposed	O
model	O
,	O
GP	B-MethodName
-	I-MethodName
GNN	I-MethodName
,	O
solves	O
the	O
relational	B-TaskName
message	I-TaskName
-	I-TaskName
passing	I-TaskName
task	O
by	O
encoding	O
natural	O
language	O
as	O
parameters	O
and	O
performing	O
propagation	O
from	O
layer	O
to	O
layer	O
.	O

Our	O
model	O
can	O
also	O
be	O
considered	O
as	O
a	O
more	O
generic	O
framework	O
for	O
graph	B-TaskName
generation	I-TaskName
problem	I-TaskName
with	I-TaskName
unstructured	I-TaskName
input	I-TaskName
other	O
than	O
text	O
,	O
e.g.	O
image	O
,	O
video	O
,	O
audio	O
.	O

In	O
this	O
work	O
,	O
we	O
demonstrate	O
its	O
effectiveness	O
in	O
predicting	B-TaskName
the	I-TaskName
relationship	I-TaskName
between	I-TaskName
entities	I-TaskName
in	I-TaskName
natural	I-TaskName
language	I-TaskName
and	O
bag	B-TaskName
-	I-TaskName
level	I-TaskName
and	O
show	O
that	O
by	O
considering	O
more	O
hops	O
in	O
reasoning	O
the	O
performance	O
of	O
relation	B-TaskName
extraction	I-TaskName
could	O
be	O
significantly	O
improved	O
.	O

Consequently	O
,	O
Context	B-MethodName
-	I-MethodName
Aware	I-MethodName
RE	I-MethodName
makes	O
a	O
mistake	O
by	O
predicting	O
(	O
Kentucky	O
,	O
share	O
boarder	O
with	O
,	O
Ohio	O
)	O
.	O

Ground	O
truth	O
graphs	O
are	O
the	O
subgraph	O
in	O
Wikidata	B-DatasetName
knowledge	O
graph	O
induced	O
by	O
the	O
sets	O
of	O
entities	O
in	O
the	O
sentences	O
.	O

Table	O
4	O
:	O
Sample	O
predictions	O
from	O
the	O
baseline	O
models	O
and	O
our	O
GP	B-MethodName
-	I-MethodName
GNN	I-MethodName
model	O
.	O

We	O
also	O
find	O
that	O
Context	B-MethodName
-	I-MethodName
Aware	I-MethodName
RE	I-MethodName
tends	O
to	O
predict	O
relations	O
with	O
similar	O
topics	O
.	O

Note	O
that	O
(	O
BankUnited	O
Center	O
,	O
located	O
in	O
,	O
English	O
)	O
is	O
even	O
not	O
in	O
Wikidata	B-DatasetName
,	O
but	O
our	O
model	O
could	O
identify	O
this	O
fact	O
through	O
reasoning	O
.	O

In	O
the	O
first	O
case	O
,	O
GP	B-MethodName
-	I-MethodName
GNN	I-MethodName
implicitly	O
learns	O
a	O
logic	O
rule	O
∃y	O
,	O
x	O
(	O
BankUnited	O
Center	O
,	O
located	O
in	O
,	O
English	O
)	O
.	O

The	O
results	O
show	O
that	O
GP	B-MethodName
-	I-MethodName
GNN	I-MethodName
has	O
the	O
ability	O
to	O
infer	O
the	O
relationship	O
between	O
two	O
entities	O
with	O
reasoning	O
.	O

4	O
shows	O
qualitative	O
results	O
that	O
compare	O
our	O
GP	B-MethodName
-	I-MethodName
GNN	I-MethodName
model	O
and	O
the	O
baseline	O
models	O
.	O

It	O
is	O
probably	O
due	O
to	O
the	O
reason	O
that	O
bag	B-TaskName
-	I-TaskName
level	I-TaskName
relation	I-TaskName
extraction	I-TaskName
is	O
much	O
easier	O
.	O

We	O
could	O
also	O
see	O
that	O
on	O
the	O
human	O
annotated	O
test	O
set	O
3layer	B-HyperparameterValue
version	O
to	O
have	O
a	O
greater	O
improvement	O
over	O
2	B-HyperparameterValue
-	O
layer	B-HyperparameterName
version	O
as	O
compared	O
with	O
2	B-HyperparameterValue
-	O
layer	B-HyperparameterName
version	O
over	O
1	B-HyperparameterValue
-	O
layer	B-HyperparameterName
version	O
.	O

3	O
that	O
as	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
layers	I-HyperparameterName
grows	O
,	O
the	O
curves	O
get	O
higher	O
and	O
higher	O
precision	O
,	O
indicating	O
considering	O
more	O
hops	O
in	O
reasoning	O
leads	O
to	O
better	O
performance	O
.	O

From	O
Table	O
2	O
and	O
Table	O
3	O
,	O
we	O
could	O
see	O
that	O
on	O
all	O
three	O
datasets	O
,	O
3	B-HyperparameterValue
-	O
layer	B-HyperparameterName
version	O
achieves	O
the	O
best	O
.	O

To	O
demonstrate	O
the	O
effects	O
of	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
layers	I-HyperparameterName
,	O
we	O
also	O
compare	O
our	O
models	O
with	O
different	O
numbers	B-HyperparameterName
of	I-HyperparameterName
lay	I-HyperparameterName
-	I-HyperparameterName
ers	I-HyperparameterName
.	O

A	O
K	B-HyperparameterName
-	O
layer	O
version	O
has	O
the	O
ability	O
to	O
infer	O
K	B-HyperparameterName
-	O
hop	O
relations	O
.	O

The	O
number	B-HyperparameterName
of	I-HyperparameterName
layers	I-HyperparameterName
represents	O
the	O
reasoning	O
ability	O
of	O
our	O
models	O
.	O

The	O
Effectiveness	O
of	O
the	O
Number	B-HyperparameterName
of	I-HyperparameterName
Layers	I-HyperparameterName
.	O

(	O
2015	O
)	O
formalize	O
the	O
bag	B-TaskName
-	I-TaskName
level	I-TaskName
relation	I-TaskName
extraction	I-TaskName
as	O
multi	O
-	O
instance	O
learning	O
.	O

To	O
evaluate	O
our	O
models	O
and	O
baseline	O
models	O
in	O
bag	B-TaskName
-	I-TaskName
level	I-TaskName
,	O
we	O
utilize	O
a	O
bag	O
of	O
sentences	O
with	O
a	O
given	O
entity	O
pair	O
to	O
score	O
the	O
relations	O
between	O
them	O
.	O

So	O
far	O
,	O
we	O
have	O
only	O
talked	O
about	O
the	O
way	O
to	O
implement	O
sentence	B-TaskName
-	I-TaskName
level	I-TaskName
relation	I-TaskName
extraction	I-TaskName
.	O

We	O
have	O
also	O
tried	O
two	O
forms	O
of	O
adjacent	B-HyperparameterName
matrices	I-HyperparameterName
:	O
tied	B-HyperparameterValue
-	I-HyperparameterValue
weights	I-HyperparameterValue
(	O
set	O
A	O
(	O
n	O
)	O
=	O
A	O
(	O
n+1	O
)	O
)	O
and	O
untied	B-HyperparameterValue
-	I-HyperparameterValue
weights	I-HyperparameterValue
.	O

We	O
select	O
non	B-HyperparameterName
-	I-HyperparameterName
linear	I-HyperparameterName
activation	I-HyperparameterName
functions	I-HyperparameterName
between	O
relu	B-HyperparameterValue
and	O
tanh	B-HyperparameterValue
,	O
and	O
select	O
d	B-HyperparameterName
n	I-HyperparameterName
among	O
{	O
2	B-HyperparameterValue
,	O
4	B-HyperparameterValue
,	O
8	B-HyperparameterValue
,	O
12	B-HyperparameterValue
,	O
16	B-HyperparameterValue
}	O
9	O
.	O

GP	B-MethodName
-	I-MethodName
GNN	I-MethodName
with	O
K	B-HyperparameterName
=	O
2	B-HyperparameterValue
or	O
K	B-HyperparameterName
=	O
3	B-HyperparameterValue
layers	B-HyperparameterName
.	O

Bidirectional	B-MethodName
LSTM	I-MethodName
(	O
Schuster	O
and	O
Paliwal	O
,	O
1997	O
)	O
could	O
be	O
seen	O
as	O
an	O
1	B-HyperparameterValue
-	O
layer	B-HyperparameterName
variant	O
of	O
our	O
model	O
.	O

LSTM	B-MethodName
or	O
GP	B-MethodName
-	I-MethodName
GNN	I-MethodName
with	O
K	B-HyperparameterName
=	O
1	B-HyperparameterValue
layer	B-HyperparameterName
.	O

For	O
CNN	B-MethodName
and	O
following	O
PCNN	B-MethodName
,	O
the	O
entity	O
markers	O
are	O
the	O
same	O
as	O
originally	O
proposed	O
in	O
Zeng	O
et	O
al	O
.	O

PCNN	B-MethodName
,	O
proposed	O
by	O
Zeng	O
et	O
al	O
.	O

(	O
2014	O
)	O
,	O
our	O
implementation	O
,	O
follows	O
Nguyen	O
and	O
Grishman	O
(	O
2015	O
)	O
,	O
concatenates	O
features	O
extracted	O
by	O
three	O
different	O
window	B-HyperparameterName
sizes	I-HyperparameterName
:	O
3	B-HyperparameterValue
,	O
5	B-HyperparameterValue
,	O
7	B-HyperparameterValue
.	O

Different	O
from	O
the	O
original	O
version	O
of	O
CNN	B-MethodName
proposed	O
in	O
Zeng	O
et	O
al	O
.	O

Multi	B-MethodName
-	I-MethodName
Window	I-MethodName
CNN	I-MethodName
.	O

It	O
was	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
on	O
Wikipedia	B-DatasetName
dataset	I-DatasetName
.	O

Context	B-MethodName
-	I-MethodName
Aware	I-MethodName
RE	I-MethodName
,	O
proposed	O
by	O
Sorokin	O
and	O
Gurevych	O
(	O
2017	O
)	O
.	O

We	O
further	O
split	O
a	O
dense	O
test	O
set	O
from	O
the	O
distantly	B-DatasetName
labeled	I-DatasetName
test	O
set	O
.	O

Dense	O
distantly	B-DatasetName
labeled	I-DatasetName
test	O
set	O
.	O

Human	B-DatasetName
annotated	I-DatasetName
test	O
set	O
Based	O
on	O
the	O
test	O
set	O
provided	O
by	O
(	O
Sorokin	O
and	O
Gurevych	O
,	O
2017	O
)	O
,	O
5	O
annotators	O
6	O
are	O
asked	O
to	O
label	O
the	O
dataset	O
.	O

There	O
is	O
a	O
small	O
difference	O
between	O
our	O
task	O
and	O
theirs	O
:	O
our	O
task	O
is	O
to	O
extract	B-TaskName
the	I-TaskName
relationship	I-TaskName
between	I-TaskName
every	I-TaskName
pair	I-TaskName
of	I-TaskName
entities	I-TaskName
in	I-TaskName
the	I-TaskName
sentence	I-TaskName
,	O
whereas	O
their	O
task	O
is	O
to	O
extract	O
the	O
relationship	O
between	O
the	O
given	O
entity	O
pair	O
and	O
the	O
context	O
entity	O
pairs	O
.	O

Distantly	O
labeled	O
set	O
Sorokin	O
and	O
Gurevych	O
(	O
2017	O
)	O
have	O
proposed	O
a	O
dataset	O
with	O
Wikipedia	B-DatasetName
corpora	I-DatasetName
.	O

In	O
both	O
part	O
(	O
1	O
)	O
and	O
part	O
(	O
2	O
)	O
,	O
we	O
do	O
three	O
subparts	O
of	O
experiments	O
:	O
(	O
i	O
)	O
we	O
will	O
first	O
show	O
that	O
our	O
models	O
could	O
improve	O
instance	B-TaskName
-	I-TaskName
level	I-TaskName
relation	I-TaskName
extraction	I-TaskName
on	O
a	O
human	B-DatasetName
annotated	I-DatasetName
test	O
set	O
,	O
and	O
(	O
ii	O
)	O
then	O
we	O
will	O
show	O
that	O
our	O
models	O
could	O
also	O
help	O
enhance	O
the	O
performance	O
of	O
bag	B-TaskName
-	I-TaskName
level	I-TaskName
relation	I-TaskName
extraction	I-TaskName
on	O
a	O
distantly	B-DatasetName
labeled	I-DatasetName
test	O
set	O
4	O
,	O
and	O
(	O
iii	O
)	O
we	O
also	O
split	O
a	O
subset	O
of	O
distantly	B-DatasetName
labeled	I-DatasetName
test	O
set	O
,	O
where	O
the	O
number	O
of	O
entities	O
and	O
edges	O
is	O
large	O
.	O

Our	O
experiments	O
mainly	O
aim	O
at	O
:	O
(	O
1	O
)	O
showing	O
that	O
our	O
best	O
models	O
could	O
improve	O
the	O
performance	O
of	O
relation	B-TaskName
extraction	I-TaskName
under	O
a	O
variety	O
of	O
settings	O
;	O
(	O
2	O
)	O
illustrating	O
that	O
how	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
layers	I-HyperparameterName
affect	O
the	O
performance	O
of	O
our	O
model	O
;	O
and	O
(	O
3	O
)	O
performing	O
a	O
qualitative	O
investigation	O
to	O
highlight	O
the	O
difference	O
between	O
our	O
models	O
and	O
baseline	O
models	O
.	O

We	O
treat	O
K	B-HyperparameterName
as	O
a	O
hyperparameter	O
,	O
the	O
effectiveness	O
of	O
which	O
will	O
be	O
discussed	O
in	O
detail	O
(	O
Sect	O
.	O

In	O
general	O
graphs	O
,	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
layers	I-HyperparameterName
K	O
is	O
chosen	O
to	O
be	O
of	O
the	O
order	O
of	O
the	O
graph	O
diameter	O
so	O
that	O
all	O
nodes	O
obtain	O
information	O
from	O
the	O
entire	O
graph	O
.	O

Number	B-HyperparameterName
of	I-HyperparameterName
Layers	I-HyperparameterName
.	O

In	O
our	O
experiments	O
,	O
we	O
generalize	O
the	O
idea	O
of	O
Gated	B-MethodName
Graph	I-MethodName
Neural	I-MethodName
Networks	I-MethodName
(	O
Li	O
et	O
al	O
.	O
,	O
2016	O
)	O
by	O
setting	O
a	O
subject	B-HyperparameterName
=	O
[	B-HyperparameterValue
1	I-HyperparameterValue
;	I-HyperparameterValue
0	I-HyperparameterValue
]	I-HyperparameterValue
and	O
a	O
object	B-HyperparameterName
=	O
[	B-HyperparameterValue
0	I-HyperparameterValue
;	I-HyperparameterValue
1	I-HyperparameterValue
]	I-HyperparameterValue
3	O
.	O

To	O
encode	O
the	O
context	O
of	O
entity	O
pairs	O
(	O
or	O
edges	O
in	O
the	O
graph	O
)	O
,	O
we	O
first	O
concatenate	O
the	O
position	O
embeddings	O
with	O
word	O
embeddings	O
in	O
the	O
sentence	O
:	O
E(x	O
i	O
,	O
j	O
t	O
)	O
=	O
[	O
x	O
t	O
;	O
p	O
i	O
,	O
j	O
t	O
]	O
,	O
(	O
4	O
)	O
where	O
x	O
t	O
denotes	O
the	O
word	O
embedding	O
of	O
word	O
x	O
t	O
and	O
p	O
i	O
,	O
j	O
t	O
denotes	O
the	O
position	O
embedding	O
of	O
word	O
position	O
t	O
relative	O
to	O
the	O
entity	O
pair	O
's	O
position	O
i	O
,	O
j	O
(	O
Details	O
of	O
these	O
two	O
embeddings	O
are	O
introduced	O
in	O
the	O
next	O
two	O
paragraphs	O
.	O
)	O
After	O
that	O
,	O
we	O
feed	O
the	O
representations	O
of	O
entity	O
pairs	O
into	O
encoder	O
f	O
(	O
•	O
)	O
which	O
contains	O
a	O
bi	B-MethodName
-	I-MethodName
directional	I-MethodName
LSTM	I-MethodName
and	O
a	O
multilayer	B-MethodName
perceptron	I-MethodName
:	O
A	O
(	O
n	O
)	O
i	O
,	O
j	O
=	O
[	O
MLPn(BiLSTMn((E(x	O
i	O
,	O
j	O
0	O
)	O
,	O
E(x	O
i	O
,	O
j	O
1	O
)	O
,	O
•	O
•	O
•	O
,	O
E(x	O
i	O
,	O
j	O
l−1	O
)	O
)	O
]	O
,	O
(	O
5	O
)	O
where	O
n	O
denotes	O
the	O
index	O
of	O
layer	O
1	O
,	O
[	O
•	O
]	O
means	O
reshaping	O
a	O
vector	O
as	O
a	O
matrix	O
,	O
BiLSTM	B-MethodName
encodes	O
a	O
sequence	O
by	O
concatenating	O
tail	O
hidden	O
states	O
of	O
the	O
forward	O
LSTM	B-MethodName
and	O
head	O
hidden	O
states	O
of	O
the	O
backward	O
LSTM	B-MethodName
together	O
and	O
MLP	B-MethodName
denotes	O
a	O
multilayer	B-MethodName
perceptron	I-MethodName
with	O
non	O
-	O
linear	O
activation	O
σ	O
.	O

In	O
this	O
section	O
,	O
we	O
will	O
introduce	O
how	O
to	O
apply	O
GP	B-MethodName
-	I-MethodName
GNNs	I-MethodName
to	O
relation	B-TaskName
extraction	I-TaskName
.	O

,	O
v	O
|Vs|	O
}	O
,	O
where	O
each	O
v	O
i	O
consists	O
of	O
one	O
or	O
a	O
sequence	O
of	O
tokens	O
,	O
relation	B-TaskName
extraction	I-TaskName
from	O
text	O
is	O
to	O
identify	O
the	O
pairwise	O
relationship	O
r	O
v	O
i	O
,	O
v	O
j	O
∈	O
R	O
between	O
each	O
entity	O
pair	O
(	O
v	O
i	O
,	O
v	O
j	O
)	O
.	O

Relation	B-TaskName
Extraction	I-TaskName
with	O
GP	B-MethodName
-	I-MethodName
GNNs	I-MethodName
.	O
Relation	B-TaskName
extraction	I-TaskName
from	I-TaskName
text	I-TaskName
is	O
a	O
classic	O
natural	B-TaskName
language	I-TaskName
relational	I-TaskName
reasoning	I-TaskName
task	O
.	O

The	O
parameters	O
in	O
GP	B-MethodName
-	I-MethodName
GNNs	I-MethodName
are	O
trained	O
by	O
gradient	O
descent	O
methods	O
.	O

Therefore	O
,	O
the	O
loss	O
of	O
GP	B-MethodName
-	I-MethodName
GNNs	I-MethodName
could	O
be	O
calculated	O
as	O
L	O
=	O
g(h	O
0	O
0:|V|−1	O
,	O
h	O
1	O
0:|V|−1	O
,	O
.	O

The	O
encoding	O
module	O
converts	O
sequences	O
into	O
transition	O
matrices	O
corresponding	O
to	O
edges	O
,	O
i.e.	O
the	O
parameters	O
of	O
the	O
propagation	O
module	O
,	O
by	O
A	O
(	O
n	O
)	O
i	O
,	O
j	O
=	O
f	O
(	O
E(x	O
i	O
,	O
j	O
0	O
)	O
,	O
E(x	O
i	O
,	O
j	O
1	O
)	O
,	O
•	O
•	O
•	O
,	O
E(x	O
i	O
,	O
j	O
l−1	O
)	O
;	O
θ	O
n	O
e	O
)	O
,	O
(	O
1	O
)	O
where	O
f	O
(	O
•	O
)	O
could	O
be	O
any	O
model	O
that	O
could	O
encode	O
sequential	O
data	O
,	O
such	O
as	O
LSTMs	B-MethodName
,	O
GRUs	B-MethodName
,	O
CNNs	B-MethodName
,	O
E(•	O
)	O
indicates	O
an	O
embedding	O
function	O
,	O
and	O
θ	O
n	O
e	O
denotes	O
the	O
parameters	O
of	O
the	O
encoding	O
module	O
of	O
n	O
-	O
th	O
layer	O
.	O

After	O
that	O
,	O
GP	B-MethodName
-	I-MethodName
GNNs	I-MethodName
employ	O
three	O
modules	O
including	O
(	O
1	O
)	O
encoding	O
module	O
,	O
(	O
2	O
)	O
propagation	O
module	O
and	O
(	O
3	O
)	O
classification	O
module	O
to	O
process	O
relational	O
reasoning	O
,	O
as	O
shown	O
in	O
Fig	O
.	O

In	O
this	O
section	O
,	O
we	O
will	O
introduce	O
the	O
general	O
framework	O
of	O
GP	B-MethodName
-	I-MethodName
GNNs	I-MethodName
.	O
GP	B-MethodName
-	I-MethodName
GNNs	I-MethodName
first	O
build	O
a	O
fully	O
-	O
connected	O
graph	O
G	O
=	O
(	O
V	O
,	O
E	O
)	O
,	O
where	O
V	O
is	O
the	O
set	O
of	O
entities	O
,	O
and	O
each	O
edge	O
(	O
v	O
i	O
,	O
v	O
j	O
)	O
∈	O
E	O
,	O
v	O
i	O
,	O
v	O
j	O
∈	O
V	O
corresponds	O
to	O
a	O
sequence	O
s	O
=	O
x	O
i	O
,	O
j	O
0	O
,	O
x	O
i	O
,	O
j	O
1	O
,	O
.	O

Generated	O
Parameters	O
(	O
GP	B-MethodName
-	I-MethodName
GNNs	I-MethodName
)	O
We	O
first	O
define	O
the	O
task	O
of	O
natural	O
language	O
relational	O
reasoning	O
.	O

Graph	B-MethodName
Neural	I-MethodName
Network	I-MethodName
with	O
.	O

(	O
2018	O
)	O
proposed	O
a	O
walk	O
-	O
based	O
model	O
to	O
do	O
relation	B-TaskName
extraction	I-TaskName
.	O

Miwa	O
and	O
Bansal	O
(	O
2016	O
)	O
show	O
the	O
effectiveness	O
of	O
LSTMs	B-MethodName
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
in	O
relation	B-TaskName
extraction	I-TaskName
.	O

(	O
2017	O
)	O
show	O
that	O
the	O
relation	O
path	O
has	O
an	O
important	O
role	O
in	O
relation	B-TaskName
extraction	I-TaskName
.	O

(	O
2017	O
)	O
predict	O
n	O
-	O
ary	O
relations	O
of	O
entities	O
in	O
different	O
sentences	O
with	O
Graph	B-MethodName
LSTMs	I-MethodName
.	O
Le	O
and	O
Titov	O
(	O
2018	O
)	O
treat	O
relations	O
as	O
latent	O
variables	O
which	O
are	O
capable	O
of	O
inducing	O
the	O
relations	O
without	O
any	O
supervision	O
signals	O
.	O

(	O
2016	O
)	O
study	O
an	O
attention	O
mechanism	O
for	O
relation	B-TaskName
extraction	I-TaskName
tasks	O
.	O

Nguyen	O
and	O
Grishman	O
(	O
2015	O
)	O
propose	O
a	O
multi	O
-	O
window	O
version	O
of	O
CNN	B-MethodName
for	O
relation	B-TaskName
extraction	I-TaskName
.	O

(	O
2015	O
)	O
further	O
extends	O
it	O
with	O
piece	B-MethodName
-	I-MethodName
wise	I-MethodName
maxpooling	I-MethodName
.	O

(	O
2014	O
)	O
is	O
one	O
of	O
the	O
earliest	O
works	O
that	O
applies	O
a	O
simple	O
CNN	B-MethodName
to	O
this	O
task	O
,	O
and	O
Zeng	O
et	O
al	O
.	O

In	O
this	O
paper	O
,	O
we	O
focus	O
on	O
the	O
relational	B-TaskName
reasoning	I-TaskName
in	O
the	O
natural	O
language	O
domain	O
.	O

Relational	B-TaskName
Reasoning	I-TaskName
.	O

Relational	B-TaskName
reasoning	I-TaskName
has	O
been	O
explored	O
in	O
various	O
fields	O
.	O

In	O
sharp	O
contrast	O
,	O
this	O
paper	O
focuses	O
on	O
extracting	B-TaskName
relations	I-TaskName
from	I-TaskName
real	I-TaskName
-	I-TaskName
world	I-TaskName
relation	I-TaskName
datasets	O
.	O

Although	O
they	O
also	O
consider	O
applying	O
GNNs	B-MethodName
to	O
natural	O
language	O
processing	O
tasks	O
,	O
they	O
still	O
perform	O
message	O
-	O
passing	O
on	O
predefined	O
graphs	O
.	O

(	O
2018	O
)	O
apply	O
GNNs	B-MethodName
to	O
multi	O
-	O
hop	O
question	O
answering	O
by	O
encoding	O
co	O
-	O
occurence	O
and	O
coreference	O
relationships	O
.	O

(	O
2018	O
)	O
apply	O
GNNs	B-MethodName
to	O
relation	O
extraction	O
by	O
encoding	O
dependency	O
trees	O
,	O
and	O
De	O
Cao	O
et	O
al	O
.	O

(	O
2017	O
)	O
apply	O
GNNs	B-MethodName
to	O
knowledge	O
base	O
completion	O
tasks	O
.	O

For	O
example	O
,	O
Marcheggiani	O
and	O
Titov	O
(	O
2017	O
)	O
propose	O
to	O
apply	O
GNNs	B-MethodName
to	O
semantic	O
role	O
labeling	O
and	O
Schlichtkrull	O
et	O
al	O
.	O

There	O
are	O
relatively	O
fewer	O
papers	O
discussing	O
how	O
to	O
adapt	O
GNNs	B-MethodName
to	O
natural	O
language	O
tasks	O
.	O

Garcia	O
and	O
Bruna	O
(	O
2018	O
)	O
shows	O
how	O
to	O
use	O
GNNs	B-MethodName
to	O
learn	O
classifiers	O
on	O
image	O
datasets	O
in	O
a	O
few	O
-	O
shot	O
manner	O
.	O

(	O
2017	O
)	O
propose	O
to	O
apply	O
GNNs	B-MethodName
to	O
molecular	O
property	O
prediction	O
tasks	O
.	O

(	O
2016	O
)	O
replace	O
the	O
Almeida	O
-	O
Pineda	O
algorithm	O
with	O
the	O
more	O
generic	O
backpropagation	O
and	O
demonstrate	O
its	O
effectiveness	O
empirically	O
.	O

GNNs	B-MethodName
were	O
first	O
proposed	O
in	O
(	O
Scarselli	O
et	O
al	O
.	O
,	O
2009	O
)	O
and	O
are	O
trained	O
via	O
the	O
Almeida	O
-	O
Pineda	O
algorithm	O
(	O
Almeida	O
,	O
1987	O
)	O
.	O

Graph	B-MethodName
Neural	I-MethodName
Networks	I-MethodName
(	O
GNNs	B-MethodName
)	O
.	O

(	O
2	O
)	O
We	O
verify	O
our	O
GP	B-MethodName
-	I-MethodName
GNNs	I-MethodName
on	O
the	O
task	O
of	O
relation	B-TaskName
extraction	I-TaskName
from	I-TaskName
text	I-TaskName
,	O
which	O
demonstrates	O
its	O
ability	O
on	O
multi	O
-	O
hop	O
relational	O
reasoning	O
as	O
compared	O
to	O
those	O
models	O
which	O
extract	O
relationships	O
separately	O
.	O

Our	O
main	O
contributions	O
are	O
in	O
two	O
-	O
fold	O
:	O
(	O
1	O
)	O
We	O
extend	O
a	O
novel	O
graph	B-MethodName
neural	I-MethodName
network	I-MethodName
model	I-MethodName
with	I-MethodName
generated	I-MethodName
parameters	I-MethodName
,	O
to	O
enable	O
relational	O
message	O
-	O
passing	O
with	O
rich	O
text	O
information	O
,	O
which	O
could	O
be	O
applied	O
to	O
process	B-TaskName
relational	I-TaskName
reasoning	I-TaskName
on	O
unstructured	O
inputs	O
such	O
as	O
natural	O
language	O
.	O

We	O
carry	O
out	O
experiments	O
on	O
Wikipedia	B-DatasetName
corpus	I-DatasetName
aligned	I-DatasetName
with	I-DatasetName
Wikidata	I-DatasetName
knowledge	I-DatasetName
base	I-DatasetName
(	O
Vrandečić	O
and	O
Krötzsch	O
,	O
2014	O
)	O
and	O
build	O
a	O
human	O
annotated	O
test	O
set	O
as	O
well	O
as	O
two	O
distantly	O
labeled	O
test	O
sets	O
with	O
different	O
levels	O
of	O
denseness	O
.	O
Experiment	O
results	O
show	O
that	O
our	O
model	O
outperforms	O
other	O
models	O
on	O
relation	B-TaskName
extraction	I-TaskName
task	I-TaskName
by	O
considering	O
multi	B-MethodName
-	I-MethodName
hop	I-MethodName
relational	I-MethodName
reasoning	I-MethodName
.	O

relation	B-TaskName
extraction	I-TaskName
from	O
text	O
.	O

Given	O
a	O
sentence	O
with	O
several	O
entities	O
marked	O
,	O
we	O
model	O
the	O
interaction	O
between	O
these	O
entities	O
by	O
generating	O
the	O
weights	O
of	O
graph	B-MethodName
neural	I-MethodName
networks	I-MethodName
.	O

Language	O
Spoken	O
Language	O
Cast	O
member	O
Figure	O
1	O
:	O
An	O
example	O
of	O
relation	B-TaskName
extraction	I-TaskName
from	O
plain	O
text	O
.	O

In	O
the	O
experiments	O
,	O
we	O
apply	O
GP	B-MethodName
-	I-MethodName
GNNs	I-MethodName
to	O
a	O
classic	O
natural	O
language	O
relational	O
reasoning	O
task	O
:	O
Léon	O
:	O
The	O
Professional	O
is	O
a	O
1996	O
English	O
-	O
language	O
French	O
thriller	O
film	O
directed	O
by	O
Luc	O
Besson	O
.	O

As	O
compared	O
to	O
traditional	O
GNNs	B-MethodName
,	O
GP	B-MethodName
-	I-MethodName
GNNs	I-MethodName
could	O
learn	O
edge	O
parameters	O
from	O
natural	O
languages	O
,	O
extending	O
it	O
from	O
performing	O
inference	O
on	O
only	O
non	O
-	O
relational	O
graphs	O
or	O
graphs	O
with	O
a	O
limited	O
number	O
of	O
edge	O
types	O
to	O
unstructured	O
inputs	O
such	O
as	O
texts	O
.	O

After	O
that	O
,	O
it	O
employs	O
three	O
modules	O
to	O
process	O
relational	B-TaskName
reasoning	I-TaskName
:	O
(	O
1	O
)	O
an	O
encoding	O
module	O
which	O
enables	O
edges	O
to	O
encode	O
rich	O
information	O
from	O
natural	O
languages	O
,	O
(	O
2	O
)	O
a	O
propagation	O
module	O
which	O
propagates	O
relational	O
information	O
among	O
various	O
nodes	O
,	O
and	O
(	O
3	O
)	O
a	O
classification	O
module	O
which	O
makes	O
predictions	O
with	O
node	O
representations	O
.	O

GP	B-MethodName
-	I-MethodName
GNNs	I-MethodName
first	O
constructs	O
a	O
fullyconnected	O
graph	O
with	O
the	O
entities	O
in	O
the	O
sequence	O
of	O
text	O
.	O

To	O
address	O
this	O
issue	O
,	O
in	O
this	O
paper	O
,	O
we	O
propose	O
graph	B-MethodName
neural	I-MethodName
networks	I-MethodName
with	I-MethodName
generated	I-MethodName
parameters	I-MethodName
(	O
GP	B-MethodName
-	I-MethodName
GNNs	I-MethodName
)	O
,	O
to	O
adapt	O
graph	B-MethodName
neural	I-MethodName
networks	I-MethodName
to	O
solve	O
the	O
natural	B-TaskName
language	I-TaskName
relational	I-TaskName
reasoning	I-TaskName
task	O
.	O

However	O
,	O
most	O
existing	O
GNNs	B-MethodName
can	O
only	O
process	O
multi	O
-	O
hop	O
relational	O
reasoning	O
on	O
pre	O
-	O
defined	O
graphs	O
and	O
can	O
not	O
be	O
directly	O
applied	O
in	O
natural	O
language	O
relational	O
reasoning	O
.	O

1	O
,	O
existing	O
relation	B-TaskName
extraction	I-TaskName
models	O
could	O
easily	O
extract	O
the	O
facts	O
that	O
Luc	O
Besson	O
directed	O
a	O
film	O
Léon	O
:	O
The	O
Professional	O
and	O
that	O
the	O
film	O
is	O
in	O
English	O
,	O
but	O
fail	O
to	O
infer	O
the	O
relationship	O
between	O
Luc	O
Besson	O
and	O
English	O
without	O
multi	O
-	O
hop	O
relational	O
reasoning	O
.	O

These	O
works	O
have	O
demonstrated	O
GNNs	B-MethodName
'	O
strong	O
power	O
to	O
process	O
relational	O
reasoning	O
on	O
graphs	O
.	O

In	O
recent	O
years	O
,	O
graph	B-MethodName
neural	I-MethodName
networks	I-MethodName
(	O
GNNs	B-MethodName
)	O
have	O
been	O
applied	O
to	O
various	O
fields	O
of	O
machine	O
learning	O
,	O
including	O
node	O
classification	O
(	O
Kipf	O
and	O
Welling	O
,	O
2016	O
)	O
,	O
relation	O
classification	O
(	O
Schlichtkrull	O
et	O
al	O
.	O
,	O
2017	O
)	O
,	O
molecular	O
property	O
prediction	O
(	O
Gilmer	O
et	O
al	O
.	O
,	O
2017	O
)	O
,	O
few	O
-	O
shot	O
learning	O
(	O
Garcia	O
and	O
Bruna	O
,	O
2018	O
)	O
,	O
and	O
achieved	O
promising	O
results	O
on	O
these	O
tasks	O
.	O

We	O
verify	O
GP	B-MethodName
-	I-MethodName
GNNs	I-MethodName
in	O
relation	B-TaskName
extraction	I-TaskName
from	O
text	O
,	O
both	O
on	O
bag	O
-	O
and	O
instancesettings	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
graph	B-MethodName
neural	I-MethodName
network	I-MethodName
with	I-MethodName
generated	I-MethodName
parameters	I-MethodName
(	O
GP	B-MethodName
-	I-MethodName
GNNs	I-MethodName
)	O
.	O

Graph	B-MethodName
Neural	I-MethodName
Networks	I-MethodName
with	I-MethodName
Generated	I-MethodName
Parameters	I-MethodName
for	O
Relation	B-TaskName
Extraction	I-TaskName
.	O

However	O
,	O
in	O
our	O
model	O
,	O
since	O
Ohio	O
and	O
Johnson	O
County	O
have	O
no	O
relationship	O
,	O
this	O
wrong	O
relation	O
is	O
not	O
predicted	O
.	O

As	O
we	O
have	O
discussed	O
before	O
,	O
this	O
is	O
due	O
to	O
its	O
mechanism	O
to	O
model	O
cooccurrence	O
of	O
multiple	O
relations	O
.	O

ritory	O
issues	O
.	O

Although	O
"	O
No	O
Relation	O
"	O
is	O
also	O
be	O
seen	O
as	O
a	O
type	O
of	O
relation	O
,	O
we	O
only	O
show	O
other	O
relation	O
types	O
in	O
the	O
graphs	O
.	O

The	O
models	O
take	O
sentences	O
and	O
entity	O
markers	O
as	O
input	O
and	O
produce	O
a	O
graph	O
containing	O
entities	O
(	O
colored	O
and	O
bold	O
)	O
and	O
relations	O
between	O
them	O
.	O

Hao	O
Zhu	O
is	O
supported	O
by	O
Tsinghua	O
Initiative	O
Research	O
Program	O
.	O

This	O
work	O
10	O
http://thunlp.org	O
is	O
jointly	O
supported	O
by	O
the	O
NSFC	O
project	O
under	O
the	O
grant	O
No	O
.	O
61661146007	O
and	O
the	O
NExT++	O
project	O
,	O
the	O
National	O
Research	O
Foundation	O
,	O
Prime	O
Ministers	O
Office	O
,	O
Singapore	O
under	O
its	O
IRC@Singapore	O
Funding	O
Initiative	O
.	O

The	O
authors	O
thank	O
the	O
members	O
of	O
Tsinghua	O
NLP	O
lab	O
10	O
for	O
their	O
thoughtful	O
suggestions	O
.	O

Acknowledgement	O
.	O

For	O
example	O
,	O
in	O
the	O
third	O
case	O
,	O
share	O
border	O
with	O
and	O
located	O
in	O
are	O
both	O
relations	O
about	O
ter-	O
.	O

Tab	O
.	O

Qualitative	O
Results	O
:	O
Case	O
Study	O
.	O

We	O
leave	O
these	O
explorations	O
for	O
future	O
work	O
.	O

In	O
real	O
applications	O
,	O
different	O
variants	O
could	O
be	O
selected	O
for	O
different	O
kind	O
of	O
sentences	O
or	O
we	O
can	O
also	O
ensemble	O
the	O
prediction	O
from	O
different	O
models	O
.	O

This	O
observation	O
reveals	O
that	O
the	O
reasoning	O
mechanism	O
could	O
help	O
us	O
identify	O
relations	O
especially	O
on	O
sentences	O
where	O
there	O
are	O
more	O
entities	O
.	O

However	O
,	O
the	O
improvement	O
of	O
the	O
third	O
layer	O
is	O
much	O
smaller	O
on	O
the	O
overall	O
distantly	O
supervised	O
test	O
set	O
than	O
the	O
one	O
on	O
the	O
dense	O
subset	O
.	O

We	O
could	O
also	O
see	O
from	O
Fig	O
.	O

Here	O
,	O
we	O
fol-	O
P@5	O
%	O
P@10	O
%	O
P@15	O
%	O
P@20	O
%	O
P@5	O
%	O
P@10	O
%	O
P@15	O
%	O
P@20	O
%	O
.	O

Zeng	O
et	O
al	O
.	O

Evaluation	O
Details	O
.	O

Table	O
1	O
shows	O
our	O
best	O
hyper	O
-	O
parameter	O
settings	O
,	O
which	O
are	O
used	O
in	O
all	O
of	O
our	O
experiments	O
.	O

We	O
select	O
the	O
best	O
parameters	O
for	O
the	O
validation	O
set	O
.	O

Hyper	O
-	O
parameters	O
.	O

These	O
models	O
are	O
capable	O
of	O
performing	O
2	O
-	O
hop	O
reasoning	O
and	O
3	O
-	O
hop	O
reasoning	O
,	O
respectively	O
.	O

(	O
,	O
2015	O
)	O
)	O
.	O

(	O
2014Zeng	O
et	O
al	O
.	O

This	O
model	O
divides	O
the	O
whole	O
sentence	O
into	O
three	O
pieces	O
and	O
applies	O
max	O
-	O
pooling	O
after	O
convolution	O
layer	O
piece	O
-	O
wisely	O
.	O

(	O
2015	O
)	O
.	O

(	O
2014	O
)	O
utilize	O
convolutional	O
neural	O
networks	O
to	O
classify	O
relations	O
.	O

Zeng	O
et	O
al	O
.	O

This	O
baseline	O
is	O
implemented	O
by	O
ourselves	O
based	O
on	O
authors	O
'	O
public	O
repo	O
8	O
.	O

This	O
model	O
utilizes	O
attention	O
mechanism	O
to	O
encode	O
the	O
context	O
relations	O
for	O
predicting	O
target	O
relations	O
.	O

We	O
select	O
the	O
following	O
models	O
for	O
comparison	O
,	O
the	O
first	O
four	O
of	O
which	O
are	O
our	O
baseline	O
models	O
.	O

Models	O
for	O
Comparison	O
.	O

There	O
are	O
1,350	O
sentences	O
and	O
more	O
than	O
17,915	O
triples	O
and	O
7,906	O
relational	O
facts	O
in	O
this	O
test	O
set	O
.	O

This	O
test	O
set	O
could	O
be	O
used	O
to	O
test	O
our	O
methods	O
'	O
performance	O
on	O
sentences	O
with	O
the	O
complex	O
interaction	O
between	O
entities	O
.	O

Our	O
criteria	O
are	O
:	O
(	O
1	O
)	O
the	O
number	O
of	O
entities	O
should	O
be	O
strictly	O
larger	O
than	O
2	O
;	O
and	O
(	O
2	O
)	O
there	O
must	O
be	O
at	O
least	O
one	O
circle	O
(	O
with	O
at	O
least	O
three	O
entities	O
)	O
in	O
the	O
ground	O
-	O
truth	O
label	O
of	O
the	O
sentence	O
7	O
.	O

There	O
are	O
350	O
sentences	O
and	O
1,230	O
triples	O
in	O
this	O
test	O
set	O
.	O

Only	O
the	O
instances	O
accepted	O
by	O
all	O
5	O
annotators	O
are	O
incorporated	O
into	O
the	O
human	O
annotated	O
test	O
set	O
.	O

They	O
are	O
asked	O
to	O
decide	O
whether	O
or	O
not	O
the	O
distant	O
supervision	O
is	O
right	O
for	O
every	O
pair	O
of	O
entities	O
.	O

5	O
We	O
use	O
the	O
same	O
training	O
set	O
for	O
all	O
of	O
the	O
experiments	O
.	O

Therefore	O
,	O
we	O
need	O
to	O
modify	O
their	O
dataset	O
:	O
(	O
1	O
)	O
We	O
added	O
reversed	O
edges	O
if	O
they	O
are	O
missing	O
from	O
a	O
given	O
triple	O
,	O
e.g.	O
if	O
triple	O
(	O
Earth	O
,	O
part	O
of	O
,	O
Solar	O
System	O
)	O
exists	O
in	O
the	O
sentence	O
,	O
we	O
add	O
a	O
reversed	O
label	O
,	O
(	O
Solar	O
System	O
,	O
has	O
a	O
member	O
,	O
Earth	O
)	O
,	O
to	O
it	O
;	O
(	O
2	O
)	O
For	O
all	O
of	O
the	O
entity	O
pairs	O
with	O
no	O
relations	O
,	O
we	O
added	O
"	O
NA	O
"	O
labels	O
to	O
them	O
.	O

Datasets	O
.	O

Experiment	O
Settings	O
.	O

Experiments	O
.	O

To	O
make	O
it	O
more	O
efficient	O
,	O
we	O
avoid	O
using	O
loop	O
-	O
based	O
,	O
scalar	O
-	O
oriented	O
code	O
by	O
matrix	O
and	O
vector	O
operations	O
.	O

We	O
use	O
PyTorch	O
(	O
Paszke	O
et	O
al	O
.	O
,	O
2017	O
)	O
to	O
implement	O
our	O
models	O
.	O

In	O
practice	O
,	O
we	O
stack	O
the	O
embeddings	O
for	O
every	O
target	O
entity	O
pairs	O
together	O
to	O
infer	O
the	O
underlying	O
relationship	O
between	O
each	O
pair	O
of	O
entities	O
.	O

We	O
use	O
cross	O
entropy	O
here	O
as	O
the	O
classification	O
loss	O
L	O
=	O
s∈S	O
i	O
=	O
j	O
log	O
P(rv	O
i	O
,	O
v	O
j	O
|i	O
,	O
j	O
,	O
s),(8	O
)	O
where	O
r	O
v	O
i	O
,	O
v	O
j	O
denotes	O
the	O
relation	O
label	O
for	O
entity	O
pair	O
(	O
v	O
i	O
,	O
v	O
j	O
)	O
and	O
S	O
denotes	O
the	O
whole	O
corpus	O
.	O

This	O
could	O
be	O
used	O
for	O
classification	O
:	O
P(rv	O
i	O
,	O
v	O
j	O
|h	O
,	O
t	O
,	O
s	O
)	O
=	O
softmax(MLP(rv	O
i	O
,	O
v	O
j	O
)	O
)	O
,	O
(	O
7	O
)	O
where	O
r	O
v	O
i	O
,	O
v	O
j	O
∈	O
R	O
,	O
and	O
MLP	O
denotes	O
a	O
multi	O
-	O
layer	O
perceptron	O
module	O
.	O

;	O
[	O
h	O
(	O
K	O
)	O
v	O
i	O
h	O
(	O
K	O
)	O
v	O
j	O
]	O
]	O
,	O
(	O
6	O
)	O
where	O
represents	O
element	O
-	O
wise	O
multiplication	O
.	O

The	O
output	O
module	O
takes	O
the	O
embeddings	O
of	O
the	O
target	O
entity	O
pair	O
(	O
v	O
i	O
,	O
v	O
j	O
)	O
as	O
input	O
,	O
which	O
are	O
first	O
converted	O
by	O
:	O
rv	O
i	O
,	O
v	O
j	O
=	O
[	O
[	O
h	O
(	O
1	O
)	O
v	O
i	O
h	O
(	O
1	O
)	O
v	O
j	O
]	O
;	O
[	O
h	O
(	O
2	O
)	O
v	O
i	O
h	O
(	O
2	O
)	O
v	O
j	O
]	O
;	O
.	O

Classification	O
Module	O
.	O

5.4	O
)	O
.	O

In	O
our	O
context	O
,	O
however	O
,	O
since	O
the	O
graph	O
is	O
densely	O
connected	O
,	O
the	O
depth	O
is	O
interpreted	O
simply	O
as	O
giving	O
the	O
model	O
more	O
expressive	O
power	O
.	O

Annotators	O
a	O
subject	O
and	O
a	O
object	O
could	O
also	O
carry	O
the	O
prior	O
knowledge	O
about	O
subject	O
entity	O
and	O
object	O
entity	O
.	O

We	O
set	O
special	O
values	O
for	O
the	O
head	O
and	O
tail	O
entity	O
's	O
initial	O
embeddings	O
as	O
a	O
kind	O
of	O
"	O
flag	O
"	O
messages	O
which	O
we	O
expect	O
to	O
be	O
passed	O
through	O
propagation	O
.	O

The	O
Initial	O
Embeddings	O
of	O
Nodes	O
Suppose	O
we	O
are	O
focusing	O
on	O
extracting	O
the	O
relationship	O
between	O
entity	O
v	O
i	O
and	O
entity	O
v	O
j	O
,	O
the	O
initial	O
embeddings	O
of	O
them	O
are	O
annotated	O
as	O
h	O
(	O
0	O
)	O
v	O
i	O
=	O
a	O
subject	O
,	O
and	O
h	O
(	O
0	O
)	O
v	O
j	O
=	O
a	O
object	O
,	O
while	O
the	O
initial	O
embeddings	O
of	O
other	O
entities	O
are	O
set	O
to	O
all	O
zeros	O
.	O

Next	O
,	O
we	O
use	O
Eq	O
.	O
(	O
2	O
)	O
to	O
propagate	O
information	O
among	O
nodes	O
where	O
the	O
initial	O
embeddings	O
of	O
nodes	O
and	O
number	O
of	O
layers	O
are	O
further	O
specified	O
as	O
follows	O
.	O

Propagation	O
Module	O
.	O

We	O
use	O
notation	O
p	O
i	O
,	O
j	O
t	O
to	O
represent	O
the	O
position	O
embedding	O
for	O
x	O
t	O
corresponding	O
to	O
entity	O
pair	O
(	O
v	O
i	O
,	O
v	O
j	O
)	O
.	O

Each	O
position	O
marker	O
is	O
also	O
mapped	O
to	O
a	O
d	O
p	O
-dimensional	O
vector	O
by	O
a	O
position	O
embedding	O
matrix	O
P	O
∈	O
R	O
3×dp	O
.	O

In	O
this	O
work	O
,	O
we	O
consider	O
a	O
simple	O
entity	O
marking	O
scheme	O
2	O
:	O
we	O
mark	O
each	O
token	O
in	O
the	O
sentence	O
as	O
either	O
belonging	O
to	O
the	O
first	O
entity	O
v	O
i	O
,	O
the	O
second	O
entity	O
v	O
j	O
or	O
to	O
neither	O
of	O
those	O
.	O

Position	O
Embedding	O
.	O

Throughout	O
this	O
paper	O
,	O
we	O
stick	O
to	O
50	O
-	O
dimensional	O
GloVe	O
embeddings	O
pre	O
-	O
trained	O
on	O
a	O
6	O
-	O
billion	O
-	O
word	O
corpus	O
(	O
Pennington	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

,	O
x	O
l−1	O
}	O
to	O
a	O
kdimensional	O
embedding	O
vector	O
x	O
t	O
using	O
a	O
word	O
embedding	O
matrix	O
W	O
e	O
∈	O
R	O
|V	O
|×dw	O
,	O
where	O
|V	O
|	O
is	O
the	O
size	O
of	O
the	O
vocabulary	O
.	O

We	O
first	O
map	O
each	O
token	O
x	O
t	O
of	O
sentence	O
{	O
x	O
0	O
,	O
x	O
1	O
,	O
.	O

Word	O
Representations	O
.	O

Encoding	O
Module	O
.	O

Module	O
Classification	O
Module	O
h	O
(	O
n	O
)	O
1	O
h	O
(	O
n	O
)	O
2	O
h	O
(	O
n	O
)	O
3	O
A	O
(	O
n	O
)	O
1,2	O
A	O
(	O
n	O
)	O
2,3	O
A	O
(	O
n	O
)	O
3,1	O
x	O
1,2	O
3	O
x	O
1,2	O
4	O
x	O
1,2	O
2	O
x	O
1,2	O
1	O
x	O
1,2	O
0	O
Figure	O
2	O
:	O
Overall	O
architecture	O
:	O
an	O
encoding	O
module	O
takes	O
a	O
sequence	O
of	O
vector	O
representations	O
as	O
inputs	O
,	O
and	O
output	O
a	O
transition	O
matrix	O
as	O
output	O
;	O
a	O
propagation	O
module	O
propagates	O
the	O
hidden	O
states	O
from	O
nodes	O
to	O
its	O
neighbours	O
with	O
the	O
generated	O
transition	O
matrix	O
;	O
a	O
classification	O
module	O
provides	O
task	O
-	O
related	O
predictions	O
according	O
to	O
nodes	O
representations	O
.	O

Propagation	O
.	O

Encoding	O
Module	O
.	O

,	O
x	O
l−1	O
)	O
,	O
a	O
set	O
of	O
relations	O
R	O
and	O
a	O
set	O
of	O
entities	O
in	O
this	O
sentence	O
V	O
s	O
=	O
{	O
v	O
1	O
,	O
v	O
2	O
,	O
.	O

Given	O
a	O
sentence	O
s	O
=	O
(	O
x	O
0	O
,	O
x	O
1	O
,	O
.	O

,	O
h	O
K	O
0:|V|−1	O
,	O
Y	O
;	O
θ	O
c	O
)	O
,	O
(	O
3	O
)	O
where	O
θ	O
c	O
denotes	O
the	O
parameters	O
of	O
the	O
classification	O
module	O
,	O
K	O
is	O
the	O
number	O
of	O
layers	O
in	O
propagation	O
module	O
and	O
Y	O
denotes	O
the	O
ground	O
truth	O
label	O
.	O

Generally	O
,	O
the	O
classification	O
module	O
takes	O
node	O
representations	O
as	O
inputs	O
and	O
outputs	O
predictions	O
.	O

Classification	O
Module	O
.	O

Given	O
representations	O
of	O
layer	O
n	O
,	O
the	O
representations	O
of	O
layer	O
n	O
+	O
1	O
are	O
calculated	O
by	O
h	O
(	O
n+1	O
)	O
i	O
=	O
v	O
j	O
∈N	O
(	O
v	O
i	O
)	O
σ(A	O
(	O
n	O
)	O
i	O
,	O
j	O
h	O
(	O
n	O
)	O
j	O
)	O
,	O
(	O
2	O
)	O
where	O
N	O
(	O
v	O
i	O
)	O
denotes	O
the	O
neighbours	O
of	O
node	O
v	O
i	O
in	O
graph	O
G	O
and	O
σ(•	O
)	O
denotes	O
a	O
non	O
-	O
linear	O
activation	O
function	O
.	O

The	O
initial	O
embeddings	O
of	O
nodes	O
,	O
i.e.	O
the	O
representations	O
of	O
layer	O
0	O
,	O
are	O
task	O
-	O
related	O
,	O
which	O
could	O
be	O
embeddings	O
that	O
encode	O
features	O
of	O
nodes	O
or	O
just	O
one	O
-	O
hot	O
embeddings	O
.	O

The	O
propagation	O
module	O
learns	O
representations	O
for	O
nodes	O
layer	O
by	O
layer	O
.	O

Propagation	O
Module	O
.	O

Encoding	O
Module	O
.	O

2	O
.	O

,	O
x	O
i	O
,	O
j	O
l−1	O
extracted	O
from	O
the	O
text	O
.	O

Given	O
a	O
sequence	O
of	O
text	O
with	O
m	O
entities	O
,	O
it	O
aims	O
to	O
reason	O
on	O
both	O
the	O
text	O
and	O
entities	O
and	O
make	O
a	O
prediction	O
of	O
the	O
labels	O
of	O
the	O
entities	O
or	O
entity	O
pairs	O
.	O

The	O
drawback	O
of	O
existing	O
approaches	O
is	O
that	O
they	O
could	O
not	O
make	O
full	O
use	O
of	O
the	O
multihop	O
inference	O
patterns	O
among	O
multiple	O
entity	O
pairs	O
and	O
their	O
relations	O
within	O
the	O
sentence	O
.	O

The	O
most	O
related	O
work	O
is	O
Sorokin	O
and	O
Gurevych	O
(	O
2017	O
)	O
,	O
where	O
the	O
proposed	O
model	O
incorporates	O
contextual	O
relations	O
with	O
an	O
attention	O
mechanism	O
when	O
predicting	O
the	O
relation	O
of	O
a	O
target	O
entity	O
pair	O
.	O

Christopoulou	O
et	O
al	O
.	O

Zeng	O
et	O
al	O
.	O

Peng	O
et	O
al	O
.	O

Lin	O
et	O
al	O
.	O

For	O
example	O
,	O
Zeng	O
et	O
al	O
.	O

Existing	O
works	O
(	O
Zeng	O
et	O
al	O
.	O
,	O
2014(Zeng	O
et	O
al	O
.	O
,	O
,	O
2015;;Lin	O
et	O
al	O
.	O
,	O
2016	O
)	O
have	O
demonstrated	O
that	O
neural	O
networks	O
are	O
capa	O
-	O
ble	O
of	O
capturing	O
the	O
pair	O
-	O
wise	O
relationship	O
between	O
entities	O
in	O
certain	O
situations	O
.	O

(	O
2018	O
)	O
model	O
the	O
interaction	O
of	O
physical	O
objects	O
.	O

(	O
2017	O
)	O
build	O
up	O
a	O
scene	O
graph	O
according	O
to	O
an	O
image	O
,	O
and	O
Kipf	O
et	O
al	O
.	O

(	O
2017	O
)	O
propose	O
a	O
simple	O
neural	O
network	O
to	O
reason	O
the	O
relationship	O
of	O
objects	O
in	O
a	O
picture	O
,	O
Xu	O
et	O
al	O
.	O

For	O
example	O
,	O
Santoro	O
et	O
al	O
.	O

Johnson	O
(	O
2017	O
)	O
introduces	O
a	O
novel	O
neural	O
architecture	O
to	O
generate	O
a	O
graph	O
based	O
on	O
the	O
textual	O
input	O
and	O
dynamically	O
update	O
the	O
relationship	O
during	O
the	O
learning	O
process	O
.	O

Zhang	O
et	O
al	O
.	O

(	O
2017	O
)	O
apply	O
message	O
-	O
passing	O
on	O
a	O
graph	O
constructed	O
by	O
coreference	O
links	O
to	O
answer	O
relational	O
questions	O
.	O

Dhingra	O
et	O
al	O
.	O

(	O
2017	O
)	O
study	O
the	O
effectiveness	O
of	O
message	O
-	O
passing	O
in	O
quantum	O
chemistry	O
.	O

Gilmer	O
et	O
al	O
.	O

Gilmer	O
et	O
al	O
.	O

Later	O
the	O
authors	O
in	O
Li	O
et	O
al	O
.	O

Related	O
Work	O
.	O

Moreover	O
,	O
we	O
also	O
present	O
three	O
datasets	O
,	O
which	O
could	O
help	O
future	O
researchers	O
compare	O
their	O
models	O
in	O
different	O
settings	O
.	O

We	O
also	O
perform	O
a	O
qualitative	O
analysis	O
which	O
shows	O
that	O
our	O
model	O
could	O
discover	O
more	O
relations	O
by	O
reasoning	O
more	O
robustly	O
as	O
compared	O
to	O
baseline	O
models	O
.	O

Modeling	O
the	O
relationship	O
between	O
"	O
Léon	O
"	O
and	O
"	O
English	O
"	O
as	O
well	O
as	O
"	O
Luc	O
Besson	O
"	O
helps	O
discover	O
the	O
relationship	O
between	O
"	O
Luc	O
Besson	O
"	O
and	O
"	O
English	O
"	O
.	O

English	O
Luc	O
Besson	O
.	O

Léon	O
.	O

Enabling	O
multi	O
-	O
hop	O
relational	O
reasoning	O
in	O
natural	O
languages	O
remains	O
an	O
open	O
problem	O
.	O

By	O
considering	O
the	O
reasoning	O
patterns	O
,	O
one	O
can	O
discover	O
that	O
Luc	O
Besson	O
could	O
speak	O
English	O
following	O
a	O
reasoning	O
logic	O
that	O
Luc	O
Besson	O
directed	O
Léon	O
:	O
The	O
Professional	O
and	O
this	O
film	O
is	O
in	O
English	O
indicates	O
Luc	O
Besson	O
could	O
speak	O
English	O
.	O

Consider	O
the	O
example	O
shown	O
in	O
Fig	O
.	O

Besides	O
graphs	O
,	O
relational	O
reasoning	O
is	O
also	O
of	O
great	O
importance	O
in	O
many	O
natural	O
language	O
processing	O
tasks	O
such	O
as	O
question	O
answering	O
,	O
relation	O
extraction	O
,	O
summarization	O
,	O
etc	O
.	O

Relational	O
reasoning	O
aims	O
to	O
abstractly	O
reason	O
about	O
entities	O
/	O
objects	O
and	O
their	O
relations	O
,	O
which	O
is	O
an	O
important	O
part	O
of	O
human	O
intelligence	O
.	O

Introduction	O
.	O

Codes	O
and	O
data	O
are	O
released	O
at	O
https	O
:	O
//github.com	O
/	O
thunlp	O
/	O
gp	O
-	O
gnn	O
.	O

We	O
also	O
perform	O
a	O
qualitative	O
analysis	O
to	O
demonstrate	O
that	O
our	O
model	O
could	O
discover	O
more	O
accurate	O
relations	O
by	O
multi	O
-	O
hop	O
relational	O
reasoning	O
.	O

Experimental	O
results	O
on	O
a	O
humanannotated	O
dataset	O
and	O
two	O
distantly	O
supervised	O
datasets	O
show	O
that	O
multi	O
-	O
hop	O
reasoning	O
mechanism	O
yields	O
significant	O
improvements	O
.	O

The	O
parameters	O
in	O
the	O
propagation	O
module	O
,	O
i.e.	O
the	O
transition	O
matrices	O
used	O
in	O
message	O
passing	O
procedure	O
,	O
are	O
produced	O
by	O
a	O
generator	O
taking	O
natural	O
language	O
sentences	O
as	O
inputs	O
.	O

Hempelmann	O
,	O
Stan	O
Kegel	O
,	O
Andrew	O
Lamont	O
,	O
Beatrice	O
Santorini	O
,	O
Mladen	O
Turković	O
,	O
and	O
Andreas	O
Zimpfer	O
for	O
helping	O
us	O
build	O
our	O
data	O
set	O
.	O

The	O
authors	O
thank	O
John	O
Black	O
,	O
Matthew	O
Collins	O
,	O
Don	O
Hauptman	O
,	O
Christian	O
F.	O

The	O
work	O
described	O
in	O
this	O
paper	O
is	O
supported	O
by	O
the	O
Volkswagen	O
Foundation	O
as	O
part	O
of	O
the	O
Lichtenberg	O
Professorship	O
Program	O
under	O
grant	O
No	O
.	O
I/82806	O
.	O

Acknowledgments	O
.	O

Finally	O
,	O
whereas	O
in	O
this	O
paper	O
we	O
have	O
treated	O
only	O
the	O
task	O
of	O
sense	B-TaskName
disambiguation	I-TaskName
for	O
the	O
case	O
where	O
a	O
word	O
is	O
known	O
a	O
priori	O
to	O
be	O
a	O
pun	O
,	O
we	O
are	O
interested	O
in	O
exploring	O
the	O
requisite	O
problem	O
of	O
pun	B-TaskName
detection	I-TaskName
,	O
where	O
the	O
object	O
is	O
to	O
determine	O
whether	O
or	O
not	O
a	O
given	O
context	O
contains	O
a	O
pun	O
,	O
and	O
more	O
precisely	O
whether	O
any	O
given	O
word	O
in	O
a	O
context	O
is	O
a	O
pun	O
.	O

(	O
2010	O
)	O
.	O

Second	O
,	O
we	O
would	O
like	O
to	O
investigate	O
alternative	O
tie	O
-	O
breaking	O
strategies	O
,	O
such	O
as	O
the	O
domain	O
similarity	O
measures	O
used	O
by	O
Mihalcea	O
et	O
al	O
.	O

Though	O
our	O
data	O
set	O
is	O
probably	O
too	O
small	O
to	O
use	O
with	O
machine	O
learningbased	O
approaches	O
,	O
we	O
are	O
particularly	O
interested	O
in	O
testing	O
knowledge	O
-	O
based	O
disambiguators	O
which	O
rely	O
on	O
measures	O
of	O
graph	O
connectivity	O
rather	O
than	O
gloss	O
overlaps	O
.	O

First	O
,	O
we	O
would	O
like	O
to	O
try	O
adapting	O
and	O
evaluating	O
some	O
additional	O
WSD	B-TaskName
algorithms	O
for	O
use	O
with	O
puns	O
.	O

There	O
are	O
a	O
number	O
of	O
avenues	O
we	O
intend	O
to	O
explore	O
in	O
future	O
work	O
.	O

We	O
showed	O
that	O
knowledge	O
-	O
based	O
disambiguation	O
algorithms	O
naïvely	O
adapted	O
from	O
traditional	O
WSD	B-TaskName
perform	O
poorly	O
,	O
but	O
that	O
extending	O
them	O
with	O
strategies	O
that	O
rely	O
on	O
pun	O
-	O
specific	O
features	O
brings	O
about	O
dramatic	O
improvements	O
in	O
accuracy	B-MetricName
:	O
their	O
recall	B-MetricName
becomes	O
comparable	O
to	O
that	O
of	O
a	O
supervised	O
baseline	O
,	O
and	O
their	O
precision	B-MetricName
greatly	O
exceeds	O
it	O
.	O

The	O
results	O
show	O
pun	O
disambiguation	O
to	O
be	O
a	O
particularly	O
challenging	O
task	O
for	O
NLP	O
,	O
with	O
baseline	O
results	O
far	O
below	O
what	O
is	O
commonly	O
seen	O
in	O
traditional	O
WSD	B-TaskName
.	O

Second	O
,	O
we	O
have	O
shown	O
how	O
evaluation	O
metrics	O
,	O
baselines	O
,	O
and	O
disambiguation	O
algorithms	O
from	O
traditional	O
WSD	B-TaskName
can	O
be	O
adapted	O
to	O
the	O
task	O
of	O
pun	B-TaskName
disambiguation	I-TaskName
,	O
and	O
we	O
have	O
tested	O
these	O
adaptations	O
in	O
a	O
controlled	O
experiment	O
.	O

The	O
data	O
set	O
is	O
large	O
enough	O
,	O
and	O
the	O
manual	O
annotations	O
reliable	O
enough	O
,	O
for	O
a	O
principled	O
evaluation	O
of	O
automatic	O
pun	B-TaskName
disambiguation	I-TaskName
systems	O
.	O

The	O
major	O
contributions	O
of	O
this	O
work	O
are	O
as	O
follows	O
:	O
First	O
,	O
we	O
have	O
produced	O
a	O
new	O
data	O
set	O
consisting	O
of	O
manually	O
sense	O
-	O
annotated	O
homographic	O
puns	O
.	O

In	O
this	O
paper	O
we	O
have	O
introduced	O
the	O
novel	O
task	O
of	O
pun	B-TaskName
disambiguation	I-TaskName
and	O
have	O
proposed	O
and	O
evaluated	O
several	O
computational	O
approaches	O
for	O
it	O
.	O

Conclusion	O
.	O

15.8	B-MetricValue
)	O
.	O

In	O
this	O
case	O
,	O
however	O
,	O
the	O
multi	O
-	O
POS	O
targets	O
actually	O
had	O
lower	O
average	O
polysemy	B-MetricName
than	O
the	O
single	O
-	O
POS	O
ones	O
(	O
13.2	B-MetricValue
vs.	O

Normally	O
such	O
a	O
disparity	O
could	O
be	O
attributed	O
to	O
a	O
difference	O
in	O
polysemy	B-MetricName
:	O
Lesk	B-MethodName
-	O
like	O
systems	O
are	O
more	O
likely	O
to	O
attempt	O
a	O
sense	O
assignment	O
for	O
highly	O
polysemous	O
targets	O
,	O
since	O
there	O
is	O
a	O
greater	O
likelihood	O
of	O
one	O
of	O
the	O
candidate	O
definitions	O
matching	O
the	O
context	O
,	O
though	O
the	O
probability	O
of	O
the	O
assignment	O
being	O
correct	O
is	O
reduced	O
.	O

While	O
recall	O
was	O
lower	O
on	O
targets	O
with	O
mixed	O
POS	O
than	O
those	O
with	O
pure	O
POS	O
,	O
coverage	O
was	O
significantly	O
higher	O
.	O

Still	O
,	O
as	O
with	O
all	O
the	O
other	O
single	O
parts	O
of	O
speech	O
,	O
performance	O
of	O
SEL+cluster	B-MethodName
exceeded	O
the	O
random	B-MethodName
baseline	I-MethodName
.	O

Accuracy	B-MetricName
was	O
lowest	O
on	O
the	O
verbs	O
,	O
which	O
had	O
the	O
highest	O
candidate	O
polysemy	B-MetricName
(	O
21.6	B-MetricValue
)	O
and	O
are	O
known	O
to	O
be	O
particularly	O
difficult	O
to	O
disambiguate	O
even	O
in	O
traditional	O
WSD	B-TaskName
.	O

The	O
last	O
column	O
of	O
each	O
row	O
shows	O
the	O
recall	B-MetricName
of	O
the	O
random	B-MethodName
baseline	I-MethodName
for	O
comparison	O
.	O

Also	O
shown	O
there	O
is	O
a	O
row	O
which	O
aggregates	O
the	O
999	O
targets	O
with	O
"	O
pure	O
"	O
POS	O
,	O
and	O
another	O
for	O
the	O
remaining	O
608	O
instances	O
(	O
"	O
mult	O
.	O
"	O
)	O
,	O
where	O
one	O
or	O
both	O
of	O
the	O
two	O
meanings	O
contain	O
senses	O
for	O
multiple	O
parts	O
of	O
speech	O
,	O
or	O
where	O
the	O
two	O
meanings	O
have	O
different	O
parts	O
of	O
speech	O
.	O

We	O
filtered	O
the	O
results	O
according	O
to	O
whether	O
both	O
gold	O
-	O
standard	O
meanings	O
of	O
the	O
pun	O
contain	O
senses	O
for	O
nouns	O
only	O
,	O
verbs	O
only	O
,	O
adjec-	O
2	O
.	O

We	O
also	O
examined	O
the	O
results	O
of	O
our	O
generally	O
best	O
-	O
performing	O
system	O
,	O
SEL+cluster	B-MethodName
,	O
to	O
see	O
whether	O
there	O
was	O
any	O
relationship	O
with	O
the	O
targets	O
'	O
part	O
of	O
speech	O
.	O

These	O
two	O
systems	O
would	O
therefore	O
be	O
preferable	O
for	O
applications	O
where	O
precision	B-MetricName
is	O
more	O
important	O
than	O
recall	B-MetricName
.	O

Though	O
the	O
three	O
knowledge	O
-	O
based	O
systems	O
are	O
not	O
statistically	O
distinguishable	O
from	O
each	O
other	O
in	O
terms	O
of	O
recall	O
,	O
they	O
do	O
show	O
a	O
statistically	O
significant	O
improvement	O
over	O
SL	B-MethodName
and	O
SEL	B-MethodName
,	O
and	O
the	O
two	O
implementing	O
pun	O
-	O
specific	O
tie	O
-	O
breaking	O
strategies	O
were	O
markedly	O
more	O
accurate	O
than	O
SLEL	B-MethodName
for	O
those	O
targets	O
where	O
they	O
attempted	O
an	O
assignment	O
.	O

This	O
is	O
excellent	O
news	O
,	O
especially	O
in	O
light	O
of	O
the	O
fact	O
that	O
supervised	O
approaches	O
(	O
even	O
baselines	O
like	O
MFS	B-MethodName
)	O
usually	O
outperform	O
their	O
knowledge	O
-	O
based	O
counterparts	O
.	O

Significance	O
testing	O
shows	O
the	O
recall	B-MetricName
scores	O
for	O
SLEL	B-MethodName
,	O
SEL+POS	B-MethodName
,	O
and	O
SEL+cluster	B-MethodName
to	O
be	O
significantly	O
better	O
than	O
the	O
random	B-MethodName
baseline	I-MethodName
,	O
and	O
statistically	O
indistinguishable	O
from	O
that	O
of	O
MFS	B-MethodName
.	O

This	O
strategy	O
also	O
had	O
the	O
best	O
tradeoff	O
between	O
precision	B-MetricName
and	O
recall	B-MetricName
,	O
with	O
an	O
F	B-MetricName
1	I-MetricName
of	O
16.77	B-MetricValue
%	I-MetricValue
.	O

Our	O
"	O
cluster	O
"	O
strategy	O
effected	O
a	O
relative	O
increase	O
in	O
coverage	B-MetricName
of	O
over	O
60	B-MetricValue
%	I-MetricValue
,	O
and	O
gave	O
us	O
the	O
best	O
recall	B-MetricName
(	O
14.10	B-MetricValue
%	I-MetricValue
)	O
.	O

Using	O
the	O
"	O
POS	O
"	O
strategy	O
increased	O
coverage	B-MetricName
by	O
41	B-MetricValue
%	I-MetricValue
,	O
relatively	O
speaking	O
,	O
and	O
gave	O
us	O
our	O
highest	O
observed	O
precision	B-MetricName
of	O
21.21	B-MetricValue
%	I-MetricValue
.	O

We	O
therefore	O
tested	O
our	O
two	O
pun	O
-	O
specific	O
backoff	O
strategies	O
to	O
break	O
this	O
system	O
's	O
ties	O
.	O

Its	O
recall	O
is	O
statistically	O
indistinguishable	O
7	O
from	O
the	O
random	O
baseline	O
,	O
though	O
spot	O
-	O
checks	O
of	O
its	O
unassigned	O
instances	O
show	O
that	O
the	O
problem	O
is	O
very	O
frequently	O
not	O
the	O
lexical	O
gap	O
but	O
rather	O
multiple	O
senses	O
tied	O
for	O
the	O
greatest	O
overlap	O
with	O
the	O
context	O
.	O

Simplified	B-MethodName
extended	I-MethodName
Lesk	I-MethodName
,	O
on	O
the	O
other	O
hand	O
,	O
saw	O
significant	O
increases	O
in	O
coverage	B-MetricName
,	O
precision	B-MetricName
,	O
and	O
recall	B-MetricName
(	O
to	O
42.45	B-MetricValue
%	I-MetricValue
,	O
19.96	B-MetricValue
%	I-MetricValue
,	O
and	O
8.47	B-MetricValue
%	I-MetricValue
,	O
respectively	O
)	O
.	O

Given	O
the	O
near	O
-	O
total	O
coverage	O
,	O
use	O
of	O
a	O
tiebreaking	O
strategy	O
here	O
would	O
have	O
no	O
appreciable	O
effect	O
on	O
the	O
accuracy	O
.	O

Simplified	B-MethodName
lexically	I-MethodName
expanded	I-MethodName
Lesk	I-MethodName
almost	O
completely	O
closed	O
the	O
lexical	O
gap	O
,	O
with	O
nearly	O
complete	O
coverage	B-MetricName
(	O
98.69	B-MetricValue
%	I-MetricValue
)	O
,	O
though	O
this	O
came	O
at	O
the	O
expense	O
of	O
a	O
large	O
drop	O
in	O
precision	B-MetricName
(	O
to	O
13.43	B-MetricValue
%	I-MetricValue
)	O
.	O

This	O
is	O
,	O
in	O
fact	O
,	O
the	O
strategy	O
employed	O
by	O
the	O
ex	O
-	O
tended	O
and	O
lexically	O
expanded	O
variants	O
of	O
simplified	B-MethodName
Lesk	I-MethodName
,	O
and	O
we	O
observed	O
that	O
both	O
were	O
successful	O
to	O
some	O
degree	O
.	O

The	O
use	O
of	O
a	O
tie	O
-	O
breaking	O
strategy	O
would	O
not	O
help	O
much	O
here	O
,	O
though	O
some	O
way	O
of	O
bridging	O
the	O
lexical	O
gap	O
would	O
.	O

Manual	O
examination	O
of	O
the	O
unassigned	O
instances	O
confirmed	O
that	O
failure	O
was	O
usually	O
due	O
to	O
the	O
lack	O
of	O
any	O
lexical	O
overlap	O
whatsoever	O
between	O
the	O
context	O
and	O
definitions	O
.	O

The	O
simplest	O
knowledge	O
-	O
based	O
algorithm	O
we	O
tested	O
,	O
simplified	B-MethodName
Lesk	I-MethodName
,	O
was	O
over	O
twice	O
as	O
accurate	O
as	O
the	O
random	B-MethodName
baseline	I-MethodName
in	O
terms	O
of	O
precision	B-MetricName
(	O
19.74	B-MetricValue
%	I-MetricValue
)	O
,	O
but	O
predictably	O
had	O
very	O
low	O
coverage	B-MetricName
(	O
35.52	B-MetricValue
%	I-MetricValue
)	O
,	O
leading	O
in	O
turn	O
to	O
very	O
low	O
recall	B-MetricName
(	O
7.01	B-MetricValue
%	I-MetricValue
)	O
.	O

Our	O
baselines	O
'	O
low	O
figures	O
are	O
the	O
result	O
of	O
them	O
having	O
to	O
consider	O
senses	O
from	O
every	O
possible	O
lemmatization	O
and	O
part	O
of	O
speech	O
of	O
the	O
target	O
,	O
and	O
underscore	O
the	O
difficulty	O
of	O
our	O
task	O
.	O

These	O
figures	O
are	O
considerably	O
lower	O
than	O
what	O
is	O
typically	O
seen	O
with	O
traditional	O
WSD	O
corpora	O
,	O
where	O
random	B-MethodName
baselines	I-MethodName
achieve	O
accuracies	B-MetricName
of	O
30	B-MetricValue
to	O
60	B-MetricValue
%	I-MetricValue
,	O
and	O
MFS	B-MethodName
baselines	I-MethodName
65	B-MetricValue
to	O
80	B-MetricValue
%	I-MetricValue
(	O
Palmer	O
et	O
al	O
.	O
,	O
2001;Snyder	O
and	O
Palmer	O
,	O
2004;Navigli	O
et	O
al	O
.	O
,	O
2007	O
)	O
.	O

Accuracy	B-MetricName
for	O
the	O
random	B-MethodName
baseline	I-MethodName
annotator	O
was	O
about	O
9	B-MetricValue
%	I-MetricValue
;	O
for	O
the	O
MFS	B-MethodName
baseline	I-MethodName
it	O
was	O
just	O
over	O
13	B-MetricValue
%	I-MetricValue
.	O

All	O
metrics	O
are	O
reported	O
as	O
percentages	O
,	O
and	O
the	O
highest	O
score	O
for	O
each	O
metric	O
(	O
excluding	O
baseline	O
coverage	O
,	O
which	O
is	O
always	O
100	O
%	O
)	O
is	O
highlighted	O
in	O
boldface	O
.	O

Table	O
1	O
shows	O
the	O
coverage	B-MetricName
,	O
precision	B-MetricName
,	O
recall	B-MetricName
,	O
and	O
F	B-MetricName
1	I-MetricName
for	O
simplified	B-MethodName
Lesk	I-MethodName
(	O
SL	B-MethodName
)	O
,	O
simplified	B-MethodName
extended	I-MethodName
Lesk	I-MethodName
(	O
SEL	B-MethodName
)	O
,	O
simplified	B-MethodName
lexically	I-MethodName
expanded	I-MethodName
Lesk	I-MethodName
(	O
SLEL	B-MethodName
)	O
,	O
and	O
the	O
random	B-MethodName
and	O
most	B-MethodName
frequent	I-MethodName
sense	I-MethodName
baselines	O
;	O
for	O
SEL	B-MethodName
we	O
also	O
report	O
results	O
for	O
each	O
of	O
our	O
punspecific	O
tie	O
-	O
breaking	O
strategies	O
.	O

Using	O
the	O
freely	O
available	O
DKPro	O
WSD	O
framework	O
(	O
Miller	O
et	O
al	O
.	O
,	O
2013	O
)	O
,	O
we	O
implemented	O
our	O
pun	B-TaskName
disambiguation	I-TaskName
algorithms	O
,	O
ran	O
them	O
on	O
our	O
full	O
data	O
set	O
,	O
and	O
compared	O
their	O
annotations	O
against	O
those	O
of	O
our	O
manually	O
produced	O
gold	O
standard	O
.	O

Results	O
.	O

In	O
traditional	O
WSD	B-TaskName
,	O
MFS	B-MethodName
baselines	O
are	O
notoriously	O
difficult	O
to	O
beat	O
,	O
even	O
for	O
supervised	O
disambiguation	O
systems	O
,	O
and	O
since	O
they	O
rely	O
on	O
expensive	O
sense	O
-	O
tagged	O
data	O
they	O
are	O
not	O
normally	O
considered	O
a	O
benchmark	O
for	O
the	O
performance	O
of	O
knowledgebased	O
disambiguators	O
.	O

As	O
with	O
our	O
test	O
algorithms	O
,	O
we	O
adapt	O
this	O
technique	O
to	O
pun	O
disambiguation	O
by	O
having	O
it	O
select	O
the	O
two	O
most	O
frequent	O
senses	O
(	O
according	O
to	O
WordNet	O
's	O
built	O
-	O
in	O
sense	O
frequency	O
counts	O
)	O
.	O

As	O
its	O
name	O
suggests	O
,	O
it	O
involves	O
always	O
selecting	O
from	O
the	O
candidates	O
that	O
sense	O
which	O
has	O
the	O
highest	O
frequency	O
in	O
the	O
corpus	O
.	O

The	O
second	O
naïve	O
baseline	O
for	O
WSD	B-TaskName
,	O
known	O
as	O
most	B-MethodName
frequent	I-MethodName
sense	I-MethodName
(	O
MFS	B-MethodName
)	O
,	O
is	O
a	O
supervised	O
baseline	O
,	O
meaning	O
that	O
it	O
depends	O
on	O
a	O
manually	O
senseannotated	O
background	O
corpus	O
.	O

There	O
are	O
δ	O
(	O
t	O
)	O
2	O
possible	O
ways	O
of	O
selecting	O
two	O
unique	O
senses	O
,	O
so	O
the	O
random	O
score	O
for	O
any	O
given	O
instance	O
is	O
score	O
(	O
t	O
)	O
=	O
g	O
1	O
(	O
t	O
)	O
•	O
g	O
2	O
(	O
t	O
)	O
÷	O
δ	O
(	O
t	O
)	O
2	O
.	O

In	O
our	O
pun	B-TaskName
disambiguation	I-TaskName
task	O
,	O
however	O
,	O
a	O
random	O
disambiguator	O
must	O
select	O
two	O
senses	O
-	O
one	O
for	O
each	O
of	O
the	O
sense	O
sets	O
g	O
1	O
(	O
t	O
)	O
and	O
g	O
2	O
(	O
t)-and	O
these	O
senses	O
must	O
be	O
distinct	O
.	O

In	O
traditional	O
WSD	B-TaskName
,	O
the	O
score	O
for	O
a	O
random	O
disambiguator	O
which	O
selects	O
a	O
single	O
sense	O
for	O
a	O
given	O
target	O
t	O
is	O
the	O
number	O
of	O
gold	O
-	O
standard	O
senses	O
divided	O
by	O
the	O
number	O
of	O
candidate	O
senses	O
:	O
score(t	O
)	O
=	O
g(t	O
)	O
÷	O
δ	O
(	O
t	O
)	O
.	O

The	O
first	O
of	O
these	O
naïve	O
baselines	O
is	O
to	O
randomly	O
select	O
from	O
among	O
the	O
candidate	O
senses	O
.	O

However	O
,	O
traditional	O
WSD	B-TaskName
systems	O
are	O
often	O
compared	O
with	O
two	O
naïve	O
baselines	O
(	O
Gale	O
et	O
al	O
.	O
,	O
1992	O
)	O
which	O
can	O
be	O
adapted	O
for	O
our	O
purposes	O
.	O

To	O
our	O
knowledge	O
,	O
ours	O
is	O
the	O
very	O
first	O
study	O
of	O
automatic	O
pun	B-TaskName
disambiguation	I-TaskName
on	O
any	O
scale	O
,	O
so	O
at	O
this	O
point	O
there	O
are	O
no	O
previous	O
systems	O
against	O
which	O
to	O
compare	O
our	O
results	O
.	O

System	O
performance	O
in	O
WSD	B-TaskName
is	O
normally	O
interpreted	O
with	O
reference	O
to	O
one	O
or	O
more	O
baselines	O
.	O

Baselines	O
.	O

(	O
As	O
with	O
traditional	O
WSD	B-TaskName
scoring	O
,	O
various	O
approaches	O
could	O
be	O
used	O
to	O
assign	O
credit	O
for	O
partially	O
correct	O
assignments	O
,	O
though	O
we	O
leave	O
exploration	O
of	O
these	O
to	O
future	O
work	O
.	O
)	O
.	O

Instead	O
,	O
since	O
our	O
systems	O
assign	O
exactly	O
one	O
sense	O
to	O
each	O
of	O
the	O
pun	O
's	O
two	O
sense	O
sets	O
,	O
we	O
count	O
this	O
as	O
a	O
match	O
(	O
scoring	O
1	O
)	O
only	O
if	O
each	O
chosen	O
sense	O
can	O
be	O
found	O
in	O
one	O
of	O
the	O
gold	O
-	O
standard	O
sense	O
sets	O
,	O
and	O
no	O
two	O
gold	O
-	O
standard	O
sense	O
sets	O
contain	O
the	O
same	O
chosen	O
sense	O
.	O

The	O
traditional	O
approach	O
to	O
scoring	O
individual	O
targets	O
is	O
not	O
usable	O
as	O
-	O
is	O
for	O
pun	B-TaskName
disambiguation	I-TaskName
,	O
because	O
each	O
pun	O
carries	O
two	O
disjoint	O
but	O
equally	O
valid	O
sets	O
of	O
sense	O
annotations	O
.	O

Overall	O
performance	O
is	O
reported	O
in	O
terms	O
of	O
coverage	B-MetricName
(	O
the	O
number	O
of	O
targets	O
for	O
which	O
a	O
sense	O
assignment	O
was	O
attempted	O
)	O
,	O
precision	B-MetricName
(	O
the	O
sum	O
of	O
scores	O
divided	O
by	O
the	O
number	O
of	O
attempted	O
targets	O
)	O
,	O
recall	B-MetricName
(	O
the	O
sum	O
of	O
scores	O
divided	O
by	O
the	O
total	O
number	O
of	O
targets	O
in	O
the	O
data	O
set	O
)	O
,	O
and	O
F	B-MetricName
1	I-MetricName
(	O
the	O
harmonic	O
mean	O
of	O
precision	O
and	O
recall	O
)	O
(	O
Palmer	O
et	O
al	O
.	O
,	O
2006	O
)	O
.	O

Where	O
the	O
system	O
selects	O
a	O
single	O
sense	O
for	O
an	O
instance	O
for	O
which	O
there	O
is	O
more	O
than	O
one	O
correct	O
gold	O
standard	O
sense	O
,	O
the	O
multiple	O
tags	O
are	O
interpreted	O
disjunctively	O
-	O
that	O
is	O
,	O
the	O
system	O
receives	O
a	O
score	O
of	O
1	O
if	O
it	O
chose	O
any	O
one	O
of	O
the	O
gold	O
-	O
standard	O
senses	O
,	O
and	O
0	O
otherwise	O
.	O

For	O
the	O
case	O
that	O
the	O
system	O
and	O
gold	O
-	O
standard	O
assignments	O
consist	O
of	O
a	O
single	O
sense	O
each	O
,	O
the	O
exactmatch	O
criterion	O
is	O
used	O
:	O
the	O
system	O
receives	O
a	O
score	O
of	O
1	O
if	O
it	O
chose	O
the	O
sense	O
specified	O
by	O
the	O
gold	O
standard	O
,	O
and	O
0	O
otherwise	O
.	O

In	O
traditional	O
word	B-TaskName
sense	I-TaskName
disambiguation	I-TaskName
,	O
in	O
vitro	O
evaluations	O
are	O
conducted	O
by	O
comparing	O
the	O
senses	O
assigned	O
by	O
the	O
disambiguation	O
system	O
to	O
the	O
goldstandard	O
senses	O
assigned	O
by	O
the	O
human	O
annotators	O
.	O

Scoring	O
.	O

Evaluation	O
.	O

6	O
Our	O
"	O
cluster	O
"	O
fallback	O
works	O
the	O
same	O
as	O
the	O
"	O
POS	O
"	O
one	O
,	O
with	O
the	O
addition	O
that	O
any	O
remaining	O
ties	O
among	O
senses	O
with	O
the	O
second	O
-	O
highest	O
overlap	O
are	O
resolved	O
by	O
preferentially	O
selecting	O
those	O
which	O
are	O
not	O
in	O
the	O
same	O
induced	O
cluster	O
as	O
,	O
and	O
which	O
in	O
Word	O
-	O
Net	O
's	O
semantic	O
network	O
are	O
at	O
least	O
three	O
edges	O
distant	O
from	O
,	O
the	O
sense	O
with	O
the	O
highest	O
overlap	O
.	O

(	O
2014	O
)	O
,	O
we	O
induced	O
a	O
clustering	O
of	O
WordNet	O
senses	O
by	O
aligning	O
WordNet	O
to	O
the	O
more	O
coarse	O
-	O
grained	O
OmegaWiki	O
LSR	O
.	O

Thus	O
,	O
following	O
Matuschek	O
et	O
al	O
.	O

For	O
our	O
second	O
tie	O
-	O
breaking	O
strategy	O
,	O
we	O
posit	O
that	O
since	O
humour	O
derives	O
from	O
the	O
resolution	O
of	O
semantic	O
incongruity	O
(	O
Raskin	O
,	O
1985;Attardo	O
,	O
1994	O
)	O
,	O
puns	O
are	O
more	O
likely	O
to	O
exploit	O
coarse	O
-	O
grained	O
homonymy	O
than	O
than	O
fine	O
-	O
grained	O
systematic	O
polysemy	O
.	O

Our	O
"	O
POS	O
"	O
tie	O
-	O
breaker	O
therefore	O
preferentially	O
selects	O
the	O
best	O
sense	O
,	O
or	O
pair	O
of	O
senses	O
,	O
whose	O
POS	O
matches	O
the	O
one	O
applied	O
to	O
the	O
target	O
by	O
the	O
Stanford	O
POS	O
tagger	O
(	O
Toutanova	O
et	O
al	O
.	O
,	O
2003	O
)	O
.	O

The	O
first	O
is	O
motivated	O
by	O
the	O
informal	O
observation	O
that	O
,	O
though	O
the	O
two	O
meanings	O
of	O
a	O
pun	O
may	O
have	O
different	O
parts	O
of	O
speech	O
,	O
at	O
least	O
one	O
of	O
the	O
parts	O
of	O
speech	O
is	O
grammatical	O
in	O
the	O
context	O
of	O
the	O
sentence	O
,	O
and	O
so	O
would	O
probably	O
be	O
the	O
one	O
assigned	O
by	O
a	O
stochastic	O
or	O
rule	O
-	O
based	O
POS	O
tagger	O
.	O

We	O
therefore	O
devised	O
two	O
pun	O
-	O
specific	O
tie	O
-	O
breaking	O
strategies	O
.	O

The	O
above	O
algorithms	O
fail	O
to	O
make	O
a	O
sense	O
assignment	O
when	O
more	O
than	O
two	O
senses	O
are	O
tied	O
for	O
the	O
highest	O
lexical	O
overlap	O
,	O
or	O
when	O
there	O
is	O
a	O
single	O
highest	O
-	O
scoring	O
sense	O
but	O
multiple	O
senses	O
are	O
tied	O
for	O
the	O
second	O
-	O
highest	O
overlap	O
.	O

Simplified	B-MethodName
lexically	I-MethodName
expanded	I-MethodName
Lesk	I-MethodName
(	O
Miller	O
et	O
al	O
.	O
,	O
2012	O
)	O
is	O
also	O
based	O
on	O
simplified	B-MethodName
Lesk	I-MethodName
,	O
with	O
the	O
extension	O
that	O
every	O
word	O
in	O
the	O
context	O
and	O
sense	O
definitions	O
is	O
expanded	O
with	O
up	O
to	O
100	O
entries	O
from	O
a	O
large	O
distributional	O
thesaurus	O
.	O

Simplified	O
extended	O
Lesk	O
(	O
Ponzetto	O
and	O
Navigli	O
,	O
2010	O
)	O
is	O
similar	O
to	O
simplified	O
Lesk	O
,	O
except	O
that	O
the	O
definition	O
for	O
each	O
sense	O
is	O
concatenated	O
with	O
those	O
of	O
neighbouring	O
senses	O
in	O
Word	O
-	O
Net	O
's	O
semantic	O
network	O
.	O

As	O
we	O
previously	O
demonstrated	O
that	O
puns	O
often	O
transcend	O
part	O
of	O
speech	O
,	O
our	O
pool	O
of	O
candidate	O
senses	O
is	O
constructed	O
as	O
follows	O
:	O
we	O
apply	O
a	O
morphological	O
analyzer	O
to	O
recover	O
all	O
possible	O
lemmas	O
of	O
the	O
target	O
word	O
without	O
respect	O
to	O
part	O
of	O
speech	O
,	O
and	O
for	O
each	O
lemma	O
we	O
add	O
all	O
its	O
senses	O
to	O
the	O
pool	O
.	O

Accordingly	O
we	O
applied	O
this	O
modification	O
to	O
the	O
following	O
knowledge	O
-	O
based	O
WSD	B-TaskName
algorithms	O
:	O
Simplified	B-MethodName
Lesk	I-MethodName
(	O
Kilgarriff	O
and	O
Rosenzweig	O
,	O
2000	O
)	O
disambiguates	O
a	O
target	O
word	O
by	O
examining	O
the	O
definitions	O
5	O
for	O
each	O
of	O
its	O
candidate	O
senses	O
and	O
selecting	O
the	O
single	O
sense	O
-	O
or	O
in	O
our	O
case	O
,	O
the	O
two	O
senses	O
-	O
which	O
have	O
the	O
greatest	O
number	O
of	O
words	O
in	O
common	O
with	O
the	O
context	O
.	O

The	O
most	O
straightforward	O
modification	O
of	O
these	O
techniques	O
to	O
pun	B-TaskName
disambiguation	I-TaskName
,	O
then	O
,	O
is	O
to	O
have	O
the	O
systems	O
select	O
the	O
two	O
top	O
-	O
scoring	O
senses	O
,	O
one	O
for	O
each	O
meaning	O
of	O
the	O
pun	O
.	O

Many	O
approaches	O
to	O
WSD	B-TaskName
,	O
including	O
Lesk	B-MethodName
-	O
like	O
algorithms	O
,	O
involve	O
computing	O
some	O
score	O
for	O
all	O
possible	O
senses	O
of	O
a	O
target	O
word	O
,	O
and	O
then	O
selecting	O
the	O
single	O
highest	O
-	O
scoring	O
one	O
as	O
the	O
"	O
correct	O
"	O
sense	O
.	O

By	O
reframing	O
the	O
task	O
so	O
as	O
to	O
permit	O
the	O
assignment	O
of	O
multiple	O
senses	O
(	O
or	O
groups	O
of	O
senses	O
)	O
,	O
we	O
can	O
allow	O
disambiguation	O
systems	O
to	O
sense	O
-	O
annotate	O
intentionally	O
ambiguous	O
constructions	O
such	O
as	O
puns	O
.	O

We	O
hold	O
that	O
for	O
this	O
third	O
scenario	O
a	O
disambiguator	O
's	O
inability	O
to	O
discriminate	O
senses	O
should	O
not	O
be	O
seen	O
as	O
a	O
failure	O
condition	O
,	O
but	O
rather	O
as	O
a	O
limitation	O
of	O
the	O
WSD	B-TaskName
task	O
as	O
traditionally	O
defined	O
.	O

A	O
third	O
condition	O
under	O
which	O
senses	O
can	O
not	O
be	O
discriminated	O
is	O
when	O
the	O
target	O
word	O
is	O
used	O
in	O
an	O
underspecified	O
or	O
intentionally	O
ambiguous	O
manner	O
.	O

(	O
2014	O
)	O
)	O
.	O

In	O
other	O
cases	O
,	O
the	O
indecision	O
arises	O
because	O
the	O
definitions	O
provided	O
by	O
the	O
sense	O
inventory	O
are	O
too	O
fine	O
-	O
grained	O
;	O
this	O
problem	O
has	O
been	O
addressed	O
,	O
with	O
varying	O
degrees	O
of	O
success	O
,	O
through	O
sense	O
clustering	O
or	O
coarsening	O
techniques	O
(	O
a	O
short	O
but	O
reasonably	O
comprehensive	O
survey	O
of	O
which	O
appears	O
in	O
Matuschek	O
et	O
al	O
.	O

(	O
2012	O
)	O
.	O

In	O
some	O
cases	O
this	O
is	O
because	O
the	O
overlap	O
is	O
negligible	O
or	O
nonexistent	O
;	O
this	O
is	O
known	O
as	O
the	O
lexical	O
gap	O
problem	O
,	O
and	O
various	O
solutions	O
to	O
it	O
are	O
discussed	O
in	O
(	O
inter	O
alia	O
)	O
Miller	O
et	O
al	O
.	O

It	O
has	O
long	O
been	O
observed	O
that	O
gloss	O
overlap	O
-	O
based	O
WSD	B-TaskName
systems	O
,	O
such	O
as	O
those	O
based	O
on	O
the	O
Lesk	B-MethodName
algorithm	I-MethodName
,	O
fail	O
to	O
distinguish	O
between	O
candidate	O
senses	O
when	O
their	O
definitions	O
have	O
a	O
similar	O
overlap	O
with	O
the	O
target	O
word	O
's	O
context	O
.	O

Pun	B-TaskName
disambiguation	I-TaskName
.	O

These	O
observations	O
confirm	O
the	O
concerns	O
we	O
raised	O
in	O
§	O
2.2	O
that	O
pun	O
disambiguators	O
,	O
unlike	O
traditional	O
WSD	B-TaskName
systems	O
,	O
can	O
not	O
always	O
rely	O
on	O
the	O
output	O
of	O
a	O
lemmatizer	O
or	O
part	O
-	O
of	O
-	O
speech	O
tagger	O
to	O
narrow	O
down	O
the	O
list	O
of	O
sense	O
candidates	O
.	O

Similarly	O
,	O
sense	O
annotations	O
for	O
each	O
individual	O
meaning	O
correspond	O
to	O
anywhere	O
from	O
one	O
to	O
four	O
different	O
lemmas	O
,	O
with	O
a	O
mean	O
of	O
1.25	O
.	O

However	O
,	O
for	O
297	O
(	O
22.9	O
%	O
)	O
of	O
our	O
puns	O
,	O
the	O
two	O
meanings	O
had	O
different	O
parts	O
of	O
speech	O
.	O

Only	O
35	O
individual	O
meanings	O
(	O
1.3	O
%	O
)	O
carry	O
sense	O
annotations	O
corresponding	O
to	O
multiple	O
parts	O
of	O
speech	O
.	O

Of	O
the	O
2596	O
individual	O
meanings	O
,	O
1303	O
(	O
50.2	O
%	O
)	O
were	O
annotated	O
with	O
noun	O
senses	O
only	O
,	O
877	O
(	O
33.8	O
%	O
)	O
with	O
verb	O
senses	O
only	O
,	O
340	O
(	O
13.1	O
%	O
)	O
with	O
adjective	O
senses	O
only	O
,	O
and	O
41	O
(	O
1.6	O
%	O
)	O
with	O
adverb	O
senses	O
only	O
.	O

As	O
expected	O
,	O
then	O
,	O
WordNet	O
's	O
sense	O
granularity	O
proved	O
to	O
be	O
somewhat	O
finer	O
than	O
necessary	O
to	O
characterize	O
the	O
meanings	O
in	O
the	O
data	O
set	O
,	O
though	O
only	O
marginally	O
so	O
.	O

The	O
contexts	O
in	O
this	O
data	O
set	O
range	O
in	O
length	O
from	O
3	O
to	O
44	O
words	O
,	O
with	O
an	O
average	O
length	O
of	O
11.9	O
.	O
The	O
2596	O
meanings	O
carry	O
sense	O
key	O
annotations	O
corresponding	O
to	O
anywhere	O
from	O
one	O
to	O
seven	O
WordNet	O
synsets	O
,	O
with	O
an	O
average	O
of	O
1.08	O
.	O

This	O
left	O
us	O
with	O
1607	O
instances	O
,	O
4	O
of	O
which	O
we	O
retained	O
only	O
the	O
1298	O
that	O
had	O
successful	O
(	O
i.e.	O
,	O
not	O
marked	O
as	O
unassignable	O
)	O
annotations	O
for	O
the	O
present	O
study	O
.	O

Where	O
the	O
annotators	O
'	O
sense	O
sets	O
were	O
disjoint	O
or	O
contradictory	O
(	O
including	O
the	O
cases	O
where	O
the	O
annotators	O
disagreed	O
on	O
the	O
pun	O
word	O
)	O
,	O
we	O
had	O
a	O
human	O
adjudicator	O
attempt	O
to	O
resolve	O
the	O
disagreement	O
in	O
favour	O
of	O
one	O
annotator	O
or	O
the	O
other	O
.	O

Where	O
possible	O
,	O
we	O
resolved	O
sense	O
annotation	O
disagreements	O
automatically	O
by	O
taking	O
the	O
intersection	O
of	O
corresponding	O
sense	O
sets	O
.	O

With	O
this	O
method	O
we	O
observe	O
a	O
Krippendorff	B-MetricName
's	I-MetricName
α	I-MetricName
of	O
0.777	B-MetricValue
;	O
this	O
is	O
only	O
slightly	O
below	O
the	O
0.8	O
threshold	O
recommended	O
by	O
Krippendorff	O
,	O
and	O
far	O
higher	O
than	O
what	O
has	O
been	O
reported	O
in	O
other	O
sense	O
annotation	O
studies	O
(	O
Passonneau	O
et	O
al	O
.	O
,	O
2006;Jurgens	O
and	O
Klapaftis	O
,	O
2013	O
)	O
.	O

We	O
therefore	O
find	O
the	O
mapping	O
between	O
elements	O
of	O
the	O
two	O
pairs	O
that	O
gives	O
the	O
lowest	O
total	O
distance	O
,	O
and	O
halve	O
it	O
:	O
d	O
M	O
(	O
{	O
A	O
1	O
,	O
A	O
2	O
}	O
,	O
{	O
B	O
1	O
,	O
B	O
2	O
}	O
)	O
=	O
1	O
2	O
min(d	O
M	O
(	O
A	O
1	O
,	O
B	O
1	O
)	O
+	O
d	O
M	O
(	O
A	O
2	O
,	O
B	O
2	O
)	O
,	O
d	O
M	O
(	O
A	O
1	O
,	O
B	O
2	O
)	O
+	O
d	O
M	O
(	O
A	O
2	O
,	O
B	O
1	O
)	O
)	O
.	O

Whereas	O
standard	O
MASI	O
,	O
d	O
M	O
(	O
A	O
,	O
B	O
)	O
,	O
compares	O
two	O
annotation	O
sets	O
A	O
and	O
B	O
,	O
our	O
annotations	O
take	O
the	O
form	O
of	O
unordered	O
pairs	O
of	O
sets	O
{	O
A	O
1	O
,	O
A	O
2	O
}	O
and	O
{	O
B	O
1	O
,	O
B	O
2	O
}	O
.	O

Our	O
distance	O
metric	O
for	O
α	B-MetricName
is	O
a	O
straightforward	O
adaptation	O
of	O
the	O
MASI	B-MetricName
set	I-MetricName
comparison	I-MetricName
metric	O
(	O
Passonneau	O
,	O
2006	O
)	O
.	O

This	O
is	O
a	O
chance	O
-	O
correcting	O
metric	O
of	O
inter	O
-	O
annotator	O
agreement	O
ranging	O
in	O
(	O
−1	O
,	O
1	O
]	O
,	O
where	O
1	O
indicates	O
perfect	O
agreement	O
,	O
−1	O
perfect	O
disagreement	O
,	O
and	O
0	O
the	O
expected	O
score	O
for	O
random	O
labelling	O
.	O

For	O
the	O
agreed	O
cases	O
,	O
we	O
used	O
DKPro	B-MethodName
Agreement	I-MethodName
(	O
Meyer	O
et	O
al	O
.	O
,	O
2014	O
)	O
to	O
compute	O
Krippendorff	B-MetricName
's	I-MetricName
α	I-MetricName
(	O
Krippendorff	O
,	O
1980	O
)	O
for	O
the	O
sense	O
annotations	O
.	O

Our	O
judges	O
agreed	O
on	O
which	O
word	O
was	O
the	O
pun	O
in	O
1634	O
out	O
of	O
1652	O
cases	O
,	O
a	O
raw	O
agreement	B-MetricName
of	O
98.91	B-MetricValue
%	I-MetricValue
.	O

Analysis	O
.	O

Further	O
details	O
of	O
our	O
annotation	O
tool	O
and	O
its	O
use	O
can	O
be	O
found	O
in	O
Miller	O
and	O
Turković	O
(	O
2015	O
)	O
.	O

Annotators	O
also	O
had	O
the	O
option	O
of	O
marking	O
a	O
meaning	O
as	O
unassignable	O
if	O
WordNet	B-DatasetName
had	O
no	O
corresponding	O
sense	O
key	O
.	O

For	O
cases	O
where	O
the	O
distinction	O
between	O
WordNet	B-DatasetName
's	O
fine	O
-	O
grained	O
senses	O
was	O
irrelevant	O
,	O
the	O
annotators	O
had	O
the	O
option	O
of	O
labelling	O
the	O
meaning	O
with	O
more	O
than	O
one	O
sense	O
key	O
.	O

Using	O
an	O
online	O
annotation	O
tool	O
specially	O
constructed	O
for	O
this	O
study	O
,	O
the	O
annotators	O
applied	O
two	O
sets	O
of	O
sense	O
keys	O
to	O
each	O
instance	O
,	O
one	O
for	O
each	O
of	O
the	O
two	O
meanings	O
of	O
the	O
pun	O
.	O

The	O
filtering	O
reduced	O
the	O
number	O
of	O
instances	O
to	O
1652	O
,	O
whose	O
puns	O
two	O
human	O
judges	O
annotated	O
with	O
sense	O
keys	O
from	O
WordNet	B-DatasetName
3.1	I-DatasetName
(	O
Fellbaum	O
,	O
1998	O
)	O
.	O

This	O
somewhat	O
softer	O
definition	O
of	O
homography	O
allows	O
us	O
to	O
admit	O
a	O
good	O
many	O
morphologically	O
interesting	O
cases	O
which	O
were	O
nonetheless	O
readily	O
recognized	O
by	O
our	O
human	O
annotators	O
.	O

Weak	O
homography	O
:	O
The	O
lexical	O
units	O
corresponding	O
to	O
the	O
two	O
distinct	O
meanings	O
must	O
be	O
spelled	O
exactly	O
the	O
same	O
way	O
,	O
except	O
that	O
particles	O
and	O
inflections	O
may	O
be	O
disregarded	O
.	O

To	O
simplify	O
our	O
manual	O
annotation	O
procedure	O
and	O
our	O
evaluation	O
metrics	O
we	O
excluded	O
these	O
rare	O
outliers	O
.	O

Though	O
many	O
sources	O
state	O
that	O
puns	O
have	O
only	O
two	O
senses	O
(	O
Redfern	O
,	O
1984;Attardo	O
,	O
1994	O
)	O
,	O
our	O
annotators	O
identified	O
a	O
handful	O
of	O
corpus	O
examples	O
where	O
the	O
pun	O
could	O
plausibly	O
be	O
analyzed	O
as	O
carrying	O
three	O
distinct	O
meanings	O
.	O

Two	O
meanings	O
per	O
pun	O
:	O
The	O
pun	O
must	O
have	O
exactly	O
two	O
distinct	O
meanings	O
.	O

Accepting	O
lexical	O
units	O
containing	O
more	O
than	O
one	O
content	O
word	O
would	O
have	O
required	O
our	O
annotators	O
to	O
laboriously	O
partition	O
the	O
pun	O
into	O
(	O
possibly	O
overlapping	O
)	O
sense	O
-	O
bearing	O
units	O
and	O
to	O
assign	O
sense	O
sets	O
to	O
each	O
of	O
them	O
,	O
inflating	O
the	O
complexity	O
of	O
the	O
annotation	O
task	O
to	O
unacceptable	O
levels	O
.	O

This	O
criterion	O
is	O
important	O
because	O
,	O
in	O
our	O
observations	O
,	O
it	O
is	O
often	O
only	O
one	O
word	O
which	O
carries	O
ambiguity	O
in	O
puns	O
on	O
compounds	O
and	O
multi	O
-	O
word	O
expressions	O
.	O

(	O
This	O
criterion	O
simplifies	O
the	O
task	O
detecting	O
the	O
presence	O
and	O
location	O
of	O
puns	O
in	O
a	O
text	O
,	O
a	O
classification	O
task	O
which	O
we	O
intend	O
to	O
investigate	O
in	O
future	O
work	O
.	O
)	O
One	O
content	O
word	O
per	O
pun	O
:	O
The	O
lexical	O
unit	O
that	O
forms	O
the	O
pun	O
must	O
consist	O
of	O
,	O
or	O
contain	O
,	O
only	O
a	O
single	O
content	O
word	O
(	O
i.e.	O
,	O
a	O
noun	O
,	O
verb	O
,	O
adjective	O
,	O
or	O
adverb	O
)	O
,	O
excepting	O
adverbial	O
particles	O
of	O
phrasal	O
verbs	O
.	O

This	O
raw	O
collection	O
of	O
7750	O
one	O
-	O
liners	O
was	O
then	O
filtered	O
by	O
trained	O
human	O
annotators	O
to	O
those	O
instances	O
meeting	O
the	O
following	O
four	O
criteria	O
:	O
One	O
pun	O
per	O
instance	O
:	O
Of	O
all	O
the	O
lexical	O
units	O
in	O
the	O
instance	O
,	O
one	O
and	O
only	O
one	O
may	O
be	O
a	O
pun	O
.	O

We	O
therefore	O
compiled	O
our	O
own	O
corpus	O
by	O
pooling	O
together	O
some	O
of	O
the	O
aforementioned	O
corpora	O
,	O
the	O
user	O
-	O
submitted	O
puns	O
from	O
the	O
Pun	O
of	O
the	O
Day	O
website	O
,	O
3	O
and	O
private	O
collections	O
provided	O
to	O
us	O
by	O
some	O
professional	O
humorists	O
.	O

Though	O
several	O
prior	O
studies	O
have	O
produced	O
corpora	O
of	O
puns	O
,	O
none	O
of	O
them	O
are	O
systematically	O
senseannotated	O
.	O

Construction	O
.	O

Such	O
a	O
corpus	O
is	O
sufficient	O
for	O
evaluating	O
what	O
we	O
term	O
pun	B-TaskName
identification	I-TaskName
or	O
pun	B-TaskName
disambiguation	I-TaskName
-	O
that	O
is	O
,	O
identifying	O
the	O
senses	O
of	O
a	O
term	O
known	O
a	O
priori	O
to	O
be	O
a	O
pun	O
.	O

As	O
in	O
traditional	O
WSD	B-TaskName
,	O
a	O
prerequisite	O
for	O
our	O
research	O
is	O
a	O
corpus	O
of	O
examples	O
,	O
where	O
one	O
or	O
more	O
human	O
annotators	O
have	O
already	O
identified	O
the	O
ambiguous	O
words	O
and	O
marked	O
up	O
their	O
various	O
meanings	O
with	O
reference	O
to	O
a	O
given	O
sense	O
inventory	O
.	O

Data	O
set	O
.	O

This	O
has	O
motivated	O
us	O
to	O
produce	O
our	O
own	O
corpus	O
of	O
puns	O
,	O
the	O
construction	O
and	O
analysis	O
of	O
which	O
is	O
described	O
in	O
the	O
following	O
section	O
.	O

Unfortunately	O
,	O
none	O
of	O
the	O
above	O
-	O
mentioned	O
corpora	O
have	O
been	O
published	O
in	O
full	O
,	O
and	O
moreover	O
many	O
of	O
them	O
contain	O
(	O
sometimes	O
exclusively	O
)	O
the	O
sort	O
of	O
imperfect	O
or	O
otherwise	O
heterographic	O
puns	O
which	O
we	O
mean	O
to	O
exclude	O
from	O
consideration	O
.	O

Such	O
data	O
sets	O
-	O
particularly	O
the	O
larger	O
onesprovided	O
us	O
good	O
evidence	O
that	O
intentionally	O
lexical	O
ambiguous	O
exemplars	O
exist	O
in	O
sufficient	O
numbers	O
to	O
make	O
a	O
rigorous	O
evaluation	O
of	O
our	O
task	O
feasible	O
.	O

Bucaria	O
(	O
2004	O
)	O
conducts	O
a	O
linguistic	O
analysis	O
of	O
a	O
set	O
of	O
135	O
humorous	O
newspaper	O
headlines	O
,	O
about	O
half	O
of	O
which	O
exploit	O
lexical	O
ambiguity	O
.	O

Two	O
studies	O
on	O
cognitive	O
strategies	O
used	O
by	O
second	O
language	O
learners	O
(	O
Kaplan	O
and	O
Lucas	O
,	O
2001;Lucas	O
,	O
2004	O
)	O
used	O
a	O
data	O
set	O
of	O
58	O
jokes	O
compiled	O
from	O
newspaper	O
comics	O
,	O
32	O
of	O
which	O
rely	O
on	O
lexical	O
ambiguity	O
.	O

Zwicky	O
and	O
Zwicky	O
(	O
1986	O
)	O
conduct	O
a	O
phonological	O
analysis	O
on	O
a	O
corpus	O
of	O
several	O
thousand	O
puns	O
,	O
some	O
of	O
which	O
they	O
collected	O
themselves	O
from	O
advertisements	O
and	O
catalogues	O
,	O
and	O
the	O
remainder	O
of	O
which	O
were	O
taken	O
from	O
previously	O
published	O
collections	O
.	O

(	O
2011	O
)	O
compile	O
a	O
corpus	O
of	O
373	O
puns	O
taken	O
from	O
church	O
marquees	O
and	O
literature	O
,	O
and	O
compare	O
it	O
against	O
a	O
general	O
corpus	O
of	O
1515	O
puns	O
drawn	O
from	O
Internet	O
websites	O
and	O
a	O
specialized	O
dictionary	O
.	O

In	O
their	O
study	O
of	O
wordplay	O
in	O
religious	O
advertising	O
,	O
Bell	O
et	O
al	O
.	O

Hong	O
and	O
Ong	O
(	O
2009	O
)	O
also	O
study	O
humour	O
in	O
natural	O
language	O
generation	O
,	O
using	O
a	O
smaller	O
data	O
set	O
of	O
27	O
punning	O
riddles	O
derived	O
from	O
a	O
mix	O
of	O
natural	O
and	O
artificial	O
sources	O
.	O

(	O
2002	O
)	O
use	O
a	O
corpus	O
of	O
374	O
"	O
Tom	O
Swifty	O
"	O
puns	O
taken	O
from	O
the	O
Internet	O
,	O
plus	O
a	O
well	O
-	O
balanced	O
corpus	O
of	O
50	O
humorous	O
and	O
nonhumorous	O
lexical	O
ambiguities	O
generated	O
program	O
-	O
matically	O
(	O
Venour	O
,	O
1999	O
)	O
.	O

In	O
their	O
work	O
on	O
computer	O
-	O
generated	O
humour	O
,	O
Lessard	O
et	O
al	O
.	O

There	O
are	O
a	O
number	O
of	O
English	O
-	O
language	O
corpora	O
of	O
intentional	O
lexical	O
ambiguity	O
which	O
have	O
been	O
used	O
in	O
past	O
work	O
,	O
usually	O
in	O
linguistics	O
or	O
the	O
social	O
sciences	O
.	O

Corpora	O
.	O

The	O
overlap	O
scores	O
for	O
all	O
word	O
pairs	O
were	O
then	O
averaged	O
,	O
and	O
the	O
punchline	O
with	O
the	O
lowest	O
average	O
score	O
selected	O
as	O
the	O
most	O
humorous	O
.	O

Rather	O
,	O
for	O
every	O
word	O
in	O
the	O
setup	O
,	O
the	O
Lesk	B-MethodName
measure	O
was	O
used	O
to	O
select	O
a	O
word	O
in	O
the	O
punchline	O
such	O
that	O
the	O
lexical	O
overlap	O
between	O
each	O
one	O
of	O
their	O
possible	O
definitions	O
was	O
maximized	O
.	O

However	O
,	O
it	O
should	O
be	O
stressed	O
here	O
that	O
the	O
Lesk	B-MethodName
model	O
did	O
not	O
directly	O
account	O
for	O
the	O
possibility	O
that	O
any	O
given	O
word	O
might	O
be	O
ambiguous	O
.	O

The	O
Lesk	B-MethodName
model	O
had	O
an	O
accuracy	B-MetricName
of	O
56	B-MetricValue
%	I-MetricValue
,	O
which	O
is	O
lower	O
than	O
that	O
of	O
a	O
naïve	B-MethodName
polysemy	I-MethodName
model	O
which	O
simply	O
selects	O
the	O
punchline	O
with	O
the	O
highest	O
mean	O
polysemy	O
(	O
66	B-MetricValue
%	I-MetricValue
)	O
and	O
even	O
of	O
a	O
random	B-MethodName
-	I-MethodName
choice	I-MethodName
baseline	I-MethodName
(	O
62	B-MetricValue
%	I-MetricValue
)	O
.	O

They	O
then	O
compare	O
the	O
set	O
-	O
ups	O
against	O
the	O
punchlines	O
using	O
various	O
models	O
of	O
incongruity	O
detection	O
,	O
including	O
many	O
exploiting	O
knowledge	O
-	O
based	O
semantic	O
relatedness	O
such	O
as	O
Lesk	O
.	O

They	O
build	O
a	O
data	O
set	O
consisting	O
of	O
150	O
joke	O
set	O
-	O
ups	O
,	O
each	O
of	O
which	O
is	O
followed	O
by	O
four	O
possible	O
"	O
punchlines	O
"	O
,	O
only	O
one	O
of	O
which	O
is	O
actually	O
humorous	O
(	O
but	O
not	O
necessarily	O
due	O
to	O
a	O
pun	O
)	O
.	O

(	O
2010	O
)	O
.	O

The	O
previous	O
work	O
which	O
is	O
perhaps	O
most	O
relevant	O
to	O
ours	O
is	O
that	O
of	O
Mihalcea	O
et	O
al	O
.	O

Of	O
particular	O
interest	O
is	O
their	O
follow	O
-	O
up	O
analysis	O
(	O
Mihalcea	O
and	O
Strapparava	O
,	O
2006	O
)	O
,	O
where	O
they	O
specifically	O
point	O
to	O
their	O
system	O
's	O
failure	O
to	O
resolve	O
lexical	O
-	O
semantic	O
ambiguity	O
as	O
a	O
stumbling	O
block	O
to	O
better	O
accuracy	O
,	O
and	O
speculate	O
that	O
deeper	O
semantic	O
analysis	O
of	O
the	O
text	O
,	O
such	O
as	O
via	O
word	B-TaskName
sense	I-TaskName
disambiguation	I-TaskName
or	O
domain	B-TaskName
disambiguation	I-TaskName
,	O
could	O
aid	O
in	O
the	O
detection	O
of	O
humorous	O
incongruity	O
and	O
opposition	O
.	O

Mihalcea	O
and	O
Strapparava	O
(	O
2005	O
)	O
treat	O
humour	B-TaskName
recognition	I-TaskName
as	O
a	O
classification	O
task	O
,	O
employing	O
various	O
machine	O
learning	O
techniques	O
on	O
humour	O
-	O
specific	O
stylistic	O
features	O
such	O
as	O
alliteration	O
and	O
antonymy	O
.	O

Their	O
focus	O
on	O
imperfect	O
puns	O
and	O
their	O
use	O
of	O
a	O
fixed	O
syntactic	O
context	O
makes	O
their	O
approach	O
largely	O
inapplicable	O
to	O
perfect	O
puns	O
in	O
running	O
text	O
.	O

Taylor	O
and	O
Mazlack	O
(	O
2004	O
)	O
describe	O
an	O
n	B-MethodName
-	I-MethodName
gram	I-MethodName
-	O
based	O
approach	O
for	O
recognizing	O
when	O
imperfect	O
puns	O
are	O
used	O
for	O
humorous	O
effect	O
in	O
a	O
certain	O
narrow	O
class	O
of	O
English	O
knockknock	O
jokes	O
.	O

However	O
,	O
this	O
work	O
is	O
concerned	O
only	O
with	O
puns	O
which	O
are	O
both	O
imperfect	O
and	O
ungrammatical	O
,	O
relying	O
on	O
syntactic	O
cues	O
rather	O
than	O
the	O
lexical	O
-	O
semantic	O
information	O
we	O
propose	O
to	O
use	O
.	O

Yokogawa	O
(	O
2002	O
)	O
describes	O
a	O
system	O
for	O
detecting	O
the	O
presence	O
of	O
puns	O
in	O
Japanese	O
text	O
.	O

In	O
this	O
subsection	O
we	O
briefly	O
review	O
some	O
prior	O
work	O
which	O
is	O
relevant	O
to	O
ours	O
.	O

There	O
is	O
some	O
previous	O
research	O
on	O
computational	B-TaskName
detection	I-TaskName
and	I-TaskName
comprehension	I-TaskName
of	I-TaskName
humour	I-TaskName
,	O
though	O
by	O
and	O
large	O
it	O
is	O
not	O
concerned	O
specifically	O
with	O
puns	O
;	O
those	O
studies	O
which	O
do	O
analyze	O
puns	O
tend	O
to	O
have	O
a	O
phonological	O
or	O
syntactic	O
rather	O
than	O
semantic	O
bent	O
.	O

Computational	O
humour	O
.	O

Previous	O
work	O
.	O

This	O
allows	O
us	O
to	O
investigate	O
the	O
problem	O
of	O
pun	B-TaskName
identification	I-TaskName
in	O
as	O
controlled	O
a	O
setting	O
as	O
possible	O
.	O

As	O
our	O
research	O
interests	O
are	O
in	O
lexical	O
semantics	O
rather	O
than	O
phonology	O
,	O
we	O
focus	O
on	O
puns	O
which	O
are	O
homographic	O
and	O
monolexemic	O
.	O

The	O
situation	O
becomes	O
even	O
more	O
onerous	O
for	O
heterographic	O
and	O
imperfect	O
puns	O
,	O
which	O
may	O
require	O
the	O
use	O
of	O
pronunciation	O
dictionaries	O
,	O
and	O
application	O
of	O
phonological	O
theories	O
of	O
punning	O
,	O
in	O
order	O
to	O
recover	O
the	O
lemmas	O
(	O
Hempelmann	O
,	O
2003	O
)	O
.	O

For	O
such	O
cases	O
an	O
automated	O
pun	O
identifier	O
would	O
therefore	O
need	O
to	O
account	O
for	O
all	O
possible	O
lemmas	O
for	O
all	O
possible	O
parts	O
of	O
speech	O
of	O
the	O
target	O
word	O
.	O

The	O
moped	O
of	O
the	O
second	O
example	O
is	O
a	O
pun	O
,	O
one	O
of	O
whose	O
meanings	O
is	O
the	O
same	O
inflected	O
form	O
of	O
the	O
verb	O
mope	O
(	O
"	O
to	O
sulk	O
"	O
)	O
and	O
the	O
other	O
of	O
which	O
is	O
the	O
noun	O
moped	O
(	O
"	O
motorized	O
scooter	O
"	O
)	O
.	O

In	O
the	O
first	O
of	O
these	O
sentences	O
,	O
the	O
word	O
moped	O
is	O
unambiguously	O
a	O
verb	O
with	O
the	O
lemma	O
mope	O
,	O
and	O
would	O
be	O
correctly	O
recognized	O
as	O
such	O
by	O
any	O
automatic	O
lemmatizer	O
and	O
part	O
-	O
of	O
-	O
speech	O
tagger	O
.	O

(	O
6	O
)	O
"	O
I	O
want	O
a	O
scooter	O
,	O
"	O
Tom	O
moped	O
.	O

Take	O
the	O
following	O
two	O
examples	O
:	O
(	O
5	O
)	O
Tom	O
moped	O
.	O

No	O
such	O
help	O
is	O
available	O
for	O
puns	O
,	O
at	O
least	O
not	O
in	O
the	O
general	O
case	O
.	O

The	O
pool	O
of	O
candidate	O
senses	O
can	O
therefore	O
be	O
restricted	O
to	O
those	O
whose	O
lexicalizations	O
exactly	O
match	O
the	O
target	O
lemma	O
and	O
part	O
of	O
speech	O
.	O

In	O
traditional	O
word	B-TaskName
sense	I-TaskName
disambiguation	I-TaskName
,	O
the	O
part	O
of	O
speech	O
and	O
lemma	O
of	O
the	O
target	O
word	O
are	O
usually	O
known	O
a	O
priori	O
,	O
or	O
can	O
be	O
determined	O
with	O
high	O
accuracy	O
using	O
off	O
-	O
the	O
-	O
shelf	O
natural	O
language	O
processing	O
tools	O
.	O

In	O
recent	O
years	O
,	O
Lesk	O
variants	O
in	O
which	O
the	O
contexts	O
and	O
definitions	O
are	O
supplemented	O
with	O
entries	O
from	O
a	O
distributional	O
thesaurus	O
(	O
Lin	O
,	O
1998	O
)	O
have	O
achieved	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
for	O
knowledge	O
-	O
based	O
systems	O
on	O
standard	O
data	O
sets	O
(	O
Miller	O
et	O
al	O
.	O
,	O
2012;Basile	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

Though	O
simple	O
,	O
the	O
Lesk	B-MethodName
algorithm	I-MethodName
performs	O
surprisingly	O
well	O
,	O
and	O
has	O
frequently	O
served	O
as	O
the	O
basis	O
of	O
more	O
sophisticated	O
approaches	O
.	O

A	O
seminal	O
knowledge	O
-	O
based	O
example	O
is	O
the	O
Lesk	B-MethodName
algorithm	I-MethodName
(	O
Lesk	O
,	O
1986	O
)	O
which	O
disambiguates	O
a	O
pair	O
of	O
target	O
terms	O
in	O
context	O
by	O
comparing	O
their	O
respective	O
dictionary	O
definitions	O
and	O
selecting	O
the	O
two	O
with	O
the	O
greatest	O
number	O
of	O
words	O
in	O
common	O
.	O

Regardless	O
of	O
the	O
approach	O
,	O
all	O
WSD	B-TaskName
systems	O
work	O
by	O
extracting	O
contextual	O
information	O
for	O
the	O
target	O
word	O
and	O
comparing	O
it	O
against	O
the	O
sense	O
information	O
stored	O
for	O
that	O
word	O
.	O

That	O
is	O
,	O
most	O
of	O
them	O
can	O
not	O
disambiguate	O
words	O
which	O
do	O
not	O
occur	O
in	O
the	O
training	O
data	O
,	O
nor	O
can	O
they	O
select	O
the	O
correct	O
sense	O
of	O
a	O
known	O
word	O
if	O
that	O
sense	O
was	O
never	O
observed	O
in	O
the	O
training	O
data	O
.	O

Moreover	O
,	O
supervised	O
approaches	O
tend	O
to	O
be	O
such	O
that	O
they	O
can	O
disambiguate	O
only	O
those	O
words	O
for	O
which	O
they	O
have	O
seen	O
sufficient	O
training	O
examples	O
to	O
cover	O
all	O
senses	O
.	O

Supervised	O
WSD	B-TaskName
systems	O
generally	O
outperform	O
their	O
knowledge	O
-	O
based	O
counterparts	O
,	O
though	O
this	O
comes	O
at	O
the	O
considerable	O
expense	O
of	O
having	O
human	O
annotators	O
manually	O
disambiguate	O
hundreds	O
or	O
thousands	O
of	O
example	O
sentences	O
.	O

Supervised	O
approaches	O
,	O
on	O
the	O
other	O
hand	O
,	O
use	O
manually	O
sense	O
-	O
annotated	O
corpora	O
as	O
training	O
data	O
for	O
a	O
machine	O
learning	O
system	O
,	O
or	O
as	O
seed	O
data	O
for	O
a	O
bootstrapping	O
process	O
.	O

Knowledge	O
-	O
based	O
approaches	O
restrict	O
themselves	O
to	O
using	O
pre	O
-	O
existing	O
lexicalsemantic	O
resources	O
(	O
LSRs	O
)	O
,	O
or	O
such	O
additional	O
information	O
as	O
can	O
be	O
automatically	O
extracted	O
or	O
mined	O
from	O
raw	O
text	O
corpora	O
.	O

Approaches	O
to	O
WSD	B-TaskName
can	O
be	O
categorized	O
according	O
to	O
the	O
type	O
of	O
knowledge	O
sources	O
used	O
to	O
help	O
discriminate	O
senses	O
.	O

Besides	O
the	O
target	O
term	O
itself	O
,	O
a	O
WSD	B-TaskName
system	O
generally	O
requires	O
two	O
inputs	O
:	O
the	O
context	O
(	O
i.e.	O
,	O
the	O
running	O
text	O
containing	O
the	O
target	O
)	O
,	O
and	O
a	O
sense	O
inventory	O
which	O
specifies	O
all	O
possible	O
senses	O
of	O
the	O
target	O
.	O

Word	B-TaskName
sense	I-TaskName
disambiguation	I-TaskName
(	O
WSD	B-TaskName
)	O
is	O
the	O
task	O
of	O
determining	O
which	O
sense	O
of	O
a	O
polysemous	O
term	O
is	O
the	O
one	O
intended	O
when	O
that	O
term	O
is	O
used	O
in	O
a	O
given	O
communicative	O
act	O
.	O

Word	B-TaskName
sense	I-TaskName
disambiguation	I-TaskName
.	O

We	O
elaborate	O
on	O
the	O
significance	O
of	O
these	O
characteristics	O
in	O
the	O
next	O
section	O
.	O

Other	O
characteristics	O
of	O
puns	O
important	O
for	O
our	O
work	O
include	O
whether	O
they	O
involve	O
compounds	O
,	O
multiword	O
expressions	O
,	O
or	O
proper	O
names	O
,	O
and	O
whether	O
the	O
pun	O
's	O
multiple	O
meanings	O
involve	O
mul	O
-	O
tiple	O
parts	O
of	O
speech	O
.	O

Such	O
puns	O
are	O
commonly	O
known	O
as	O
imperfect	O
puns	O
.	O

Finally	O
,	O
the	O
pun	O
on	O
clothed	O
in	O
(	O
4	O
)	O
is	O
neither	O
homographic	O
nor	O
homophonic	O
,	O
since	O
the	O
word	O
for	O
the	O
secondary	O
meaning	O
,	O
closed	O
,	O
differs	O
in	O
both	O
spelling	O
and	O
pronunciation	O
.	O

The	O
pun	O
on	O
convictions	O
(	O
"	O
strongly	O
held	O
beliefs	O
"	O
and	O
"	O
findings	O
of	O
criminal	O
guilt	O
"	O
)	O
in	O
(	O
3	O
)	O
is	O
both	O
homographic	O
and	O
homophonic	O
.	O

In	O
(	O
2	O
)	O
,	O
the	O
pun	O
on	O
pane	O
(	O
"	O
sheet	O
of	O
glass	O
"	O
)	O
is	O
homophonic	O
but	O
not	O
homographic	O
,	O
since	O
the	O
word	O
for	O
the	O
secondary	O
meaning	O
(	O
"	O
feeling	O
of	O
injury	O
"	O
)	O
is	O
properly	O
spelled	O
pain	O
but	O
pronounced	O
the	O
same	O
.	O

(	O
4	O
)	O
The	O
sign	O
at	O
the	O
nudist	O
camp	O
read	O
,	O
"	O
Clothed	O
until	O
April	O
.	O
"	O
In	O
(	O
1	O
)	O
,	O
the	O
pun	O
on	O
axes	O
is	O
homographic	O
but	O
not	O
homophonic	O
,	O
since	O
the	O
two	O
meanings	O
(	O
"	O
more	O
than	O
one	O
axe	O
"	O
and	O
"	O
more	O
than	O
one	O
axis	O
"	O
)	O
share	O
the	O
same	O
spelling	O
but	O
have	O
different	O
pronunciations	O
.	O

(	O
3	O
)	O
A	O
political	O
prisoner	O
is	O
one	O
who	O
stands	O
behind	O
her	O
convictions	O
.	O

(	O
2	O
)	O
She	O
fell	O
through	O
the	O
window	O
but	O
felt	O
no	O
pane	O
.	O

Puns	O
can	O
be	O
homographic	O
,	O
homophonic	O
,	O
both	O
,	O
or	O
neither	O
,	O
as	O
the	O
following	O
examples	O
illustrate	O
:	O
(	O
1	O
)	O
A	O
lumberjack	O
's	O
world	O
revolves	O
on	O
its	O
axes	O
.	O

A	O
homographic	O
pun	O
exploits	O
distinct	O
meanings	O
of	O
the	O
same	O
written	O
word	O
,	O
and	O
a	O
homophonic	O
pun	O
exploits	O
distinct	O
meanings	O
of	O
the	O
same	O
spoken	O
word	O
.	O

Puns	O
can	O
be	O
classified	O
in	O
various	O
ways	O
(	O
Attardo	O
,	O
1994	O
)	O
,	O
though	O
from	O
the	O
point	O
of	O
view	O
of	O
our	O
particular	O
natural	O
language	O
processing	O
application	O
the	O
most	O
important	O
distinction	O
is	O
between	O
homographic	O
and	O
homophonic	O
puns	O
.	O

Study	O
of	O
literary	O
puns	O
imparts	O
a	O
greater	O
understanding	O
of	O
the	O
cultural	O
or	O
historical	O
context	O
in	O
which	O
the	O
literature	O
was	O
produced	O
,	O
which	O
is	O
often	O
necessary	O
to	O
properly	O
interpret	O
and	O
translate	O
it	O
(	O
Delabastita	O
,	O
1997	O
)	O
.	O

Humorous	O
and	O
non	O
-	O
humorous	O
puns	O
have	O
been	O
the	O
subject	O
of	O
extensive	O
study	O
in	O
the	O
humanities	O
and	O
social	O
sciences	O
,	O
which	O
has	O
led	O
to	O
insights	O
into	O
the	O
nature	O
of	O
language	O
-	O
based	O
humour	O
and	O
wordplay	O
,	O
including	O
their	O
role	O
in	O
commerce	O
,	O
entertainment	O
,	O
and	O
health	O
care	O
;	O
how	O
they	O
are	O
processed	O
in	O
the	O
brain	O
;	O
and	O
how	O
they	O
vary	O
over	O
time	O
and	O
across	O
cultures	O
(	O
Monnot	O
,	O
1982;Culler	O
,	O
1988;Lagerwerf	O
,	O
2002;Bell	O
et	O
al	O
.	O
,	O
2011;Bekinschtein	O
et	O
al	O
.	O
,	O
2011	O
)	O
.	O

Punning	O
is	O
a	O
form	O
of	O
wordplay	O
where	O
a	O
word	O
is	O
used	O
in	O
such	O
a	O
way	O
as	O
to	O
evoke	O
several	O
independent	O
meanings	O
simultaneously	O
.	O

Puns	O
.	O

Background	O
.	O

Finally	O
,	O
we	O
conclude	O
in	O
§	O
7	O
with	O
a	O
review	O
of	O
our	O
research	O
contributions	O
and	O
an	O
outline	O
of	O
our	O
plans	O
for	O
future	O
work	O
.	O

In	O
§	O
§	O
4	O
and	O
5	O
we	O
describe	O
how	O
disambiguation	O
algorithms	O
,	O
evaluation	O
metrics	O
,	O
and	O
baselines	O
from	O
traditional	O
WSD	B-TaskName
can	O
be	O
adapted	O
to	O
the	O
task	O
of	O
pun	B-TaskName
identification	I-TaskName
,	O
and	O
in	O
§	O
6	O
we	O
report	O
and	O
discuss	O
the	O
performance	O
of	O
our	O
adapted	O
systems	O
.	O

In	O
§	O
3	O
we	O
describe	O
the	O
data	O
set	O
produced	O
for	O
our	O
experiments	O
.	O

The	O
remainder	O
of	O
this	O
paper	O
is	O
structured	O
as	O
follows	O
:	O
In	O
the	O
following	O
section	O
we	O
give	O
a	O
brief	O
introduction	O
to	O
puns	O
,	O
WSD	B-TaskName
,	O
and	O
related	O
previous	O
work	O
on	O
computational	O
detection	O
and	O
comprehension	O
of	O
humour	O
.	O

We	O
focus	O
on	O
humorous	O
puns	O
,	O
as	O
these	O
are	O
by	O
far	O
the	O
most	O
commonly	O
encountered	O
and	O
more	O
readily	O
available	O
in	O
(	O
and	O
extractable	O
from	O
)	O
existing	O
text	O
corpora	O
.	O

In	O
the	O
present	O
work	O
,	O
we	O
discuss	O
the	O
adaptation	O
of	O
automatic	O
word	B-TaskName
sense	I-TaskName
disambiguation	I-TaskName
techniques	O
to	O
intentionally	O
ambiguous	O
text	O
and	O
evaluate	O
these	O
adaptations	O
in	O
a	O
controlled	O
setting	O
.	O

2	O
NLP	O
systems	O
could	O
assist	O
translators	O
in	O
flagging	O
intentionally	O
ambiguous	O
words	O
for	O
special	O
attention	O
,	O
and	O
where	O
they	O
are	O
not	O
directly	O
translatable	O
(	O
as	O
is	O
usually	O
the	O
case	O
)	O
,	O
the	O
systems	O
may	O
be	O
able	O
to	O
propose	O
ambiguity	O
-	O
preserving	O
alternatives	O
which	O
best	O
match	O
the	O
original	O
pun	O
's	O
double	O
meaning	O
.	O

These	O
pose	O
particular	O
challenges	O
for	O
translators	O
,	O
who	O
need	O
not	O
only	O
to	O
recognize	O
and	O
comprehend	O
each	O
instance	O
of	O
humour	O
-	O
provoking	O
ambiguity	O
,	O
but	O
also	O
to	O
select	O
and	O
implement	O
an	O
appropriate	O
translation	O
strategy	O
.	O

Some	O
of	O
the	O
most	O
widely	O
disseminated	O
and	O
translated	O
popular	O
discourses	O
-	O
particularly	O
television	O
shows	O
and	O
movies	O
-	O
feature	O
puns	O
and	O
other	O
forms	O
of	O
wordplay	O
as	O
a	O
recurrent	O
and	O
expected	O
feature	O
(	O
Schröter	O
,	O
2005	O
)	O
.	O

Finally	O
,	O
computational	B-TaskName
pun	I-TaskName
detection	I-TaskName
and	O
understanding	O
hold	O
tremendous	O
potential	O
for	O
machine	B-TaskName
-	I-TaskName
assisted	I-TaskName
translation	I-TaskName
.	O

It	O
is	O
not	O
hard	O
to	O
image	O
how	O
computer	O
-	O
assisted	O
detection	O
,	O
classification	O
,	O
and	O
analysis	O
of	O
puns	O
could	O
help	O
scholars	O
in	O
the	O
digital	O
humanities	O
.	O

To	O
give	O
just	O
one	O
example	O
,	O
puns	O
are	O
one	O
of	O
the	O
most	O
intensively	O
studied	O
aspects	O
of	O
Shakespeare	O
's	O
rhetoric	O
,	O
and	O
laborious	O
manual	O
counts	O
have	O
shown	O
their	O
frequency	O
in	O
certain	O
of	O
his	O
plays	O
to	O
range	O
from	O
17	O
to	O
85	O
instances	O
per	O
thousand	O
lines	O
(	O
Keller	O
,	O
2009	O
)	O
.	O

Wordplay	O
is	O
also	O
a	O
perennial	O
topic	O
of	O
scholarship	O
in	O
literary	O
criticism	O
and	O
analysis	O
.	O

Recognizing	O
instances	O
of	O
such	O
lexical	O
ambiguity	O
and	O
understanding	O
their	O
affective	O
connotations	O
would	O
be	O
of	O
benefit	O
to	O
systems	O
performing	O
sentiment	O
analysis	O
on	O
persuasive	O
texts	O
.	O

For	O
instance	O
,	O
puns	O
are	O
particularly	O
common	O
in	O
advertising	O
,	O
where	O
they	O
are	O
used	O
not	O
only	O
to	O
create	O
humour	O
but	O
also	O
to	O
induce	O
in	O
the	O
audience	O
a	O
valenced	O
attitude	O
toward	O
the	O
target	O
(	O
Valitutti	O
et	O
al	O
.	O
,	O
2008	O
)	O
.	O

We	O
consider	O
these	O
to	O
be	O
important	O
research	O
questions	O
with	O
a	O
number	O
of	O
real	O
-	O
world	O
applications	O
.	O

A	O
fundamental	O
problem	O
which	O
has	O
not	O
yet	O
been	O
as	O
widely	O
studied	O
is	O
the	O
automatic	O
detection	O
and	O
identification	O
of	O
intentional	O
lexical	O
ambiguity	O
-	O
that	O
is	O
,	O
given	O
a	O
text	O
,	O
does	O
it	O
contain	O
any	O
lexical	O
items	O
which	O
are	O
used	O
in	O
a	O
deliberately	O
ambiguous	O
manner	O
,	O
and	O
if	O
so	O
,	O
what	O
are	O
the	O
intended	O
meanings	O
?	O

What	O
little	O
research	O
has	O
been	O
done	O
is	O
confined	O
largely	O
to	O
computational	O
mechanisms	O
for	O
pun	B-TaskName
generation	I-TaskName
(	O
in	O
the	O
context	O
of	O
natural	O
language	O
generation	O
for	O
computational	O
humour	O
)	O
and	O
to	O
computational	O
analysis	O
of	O
phonological	O
properties	O
of	O
puns	O
.	O

Perhaps	O
surprisingly	O
,	O
this	O
sort	O
of	O
intentional	O
lexical	O
ambiguity	O
has	O
attracted	O
little	O
attention	O
in	O
the	O
fields	O
of	O
computational	O
linguistics	O
and	O
natural	O
language	O
processing	O
.	O

There	O
are	O
a	O
variety	O
of	O
motivations	O
writers	O
have	O
for	O
employing	O
such	O
constructions	O
,	O
and	O
in	O
turn	O
for	O
why	O
such	O
uses	O
are	O
worthy	O
of	O
scholarly	O
investigation	O
.	O

That	O
is	O
,	O
the	O
writer	O
intends	O
for	O
a	O
certain	O
word	O
or	O
other	O
lexical	O
item	O
to	O
be	O
interpreted	O
as	O
simultaneously	O
carrying	O
two	O
or	O
more	O
separate	O
meanings	O
,	O
or	O
alternatively	O
for	O
it	O
to	O
be	O
unclear	O
which	O
meaning	O
is	O
the	O
intended	O
one	O
.	O

as	O
paronomasia	O
and	O
syllepsis	O
,	O
or	O
more	O
generally	O
as	O
puns	O
,	O
in	O
which	O
homonymic	O
(	O
i.e.	O
,	O
coarse	O
-	O
grained	O
)	O
lexical	O
-	O
semantic	O
ambiguity	O
is	O
a	O
deliberate	O
effect	O
of	O
the	O
communication	O
act	O
.	O

While	O
there	O
has	O
been	O
some	O
research	O
in	O
modelling	O
intentional	O
lexical	O
-	O
semantic	O
underspecification	O
(	O
Jurgens	O
,	O
2014	O
)	O
,	O
it	O
is	O
intended	O
for	O
closely	O
related	O
senses	O
such	O
as	O
those	O
of	O
systematically	O
polysemous	O
terms	O
,	O
not	O
those	O
of	O
coarser	O
-	O
grained	O
homonyms	O
which	O
are	O
the	O
subject	O
of	O
this	O
paper	O
.	O

In	O
the	O
case	O
of	O
systematically	O
polysemous	O
terms	O
(	O
i.e.	O
,	O
words	O
that	O
have	O
several	O
related	O
senses	O
shared	O
in	O
a	O
systematic	O
way	O
by	O
a	O
group	O
of	O
similar	O
words	O
)	O
,	O
it	O
may	O
not	O
be	O
necessary	O
to	O
disambiguate	O
them	O
at	O
all	O
in	O
order	O
to	O
interpret	O
the	O
communication	O
(	O
Buitelaar	O
,	O
2000	O
)	O
.	O

An	O
alternative	O
view	O
is	O
that	O
each	O
word	O
is	O
a	O
single	O
lexical	O
entry	O
whose	O
specific	O
meaning	O
is	O
underspecified	O
until	O
it	O
is	O
activated	O
by	O
the	O
context	O
(	O
Ludlow	O
,	O
1996	O
)	O
.	O

1	O
However	O
,	O
there	O
exists	O
a	O
class	O
of	O
language	O
constructs	O
known	O
1	O
Under	O
this	O
assumption	O
,	O
lexical	O
ambiguity	O
arises	O
due	O
to	O
there	O
being	O
a	O
plurality	O
of	O
words	O
with	O
the	O
same	O
surface	O
form	O
but	O
different	O
meanings	O
,	O
and	O
the	O
task	O
of	O
the	O
interpreter	O
is	O
to	O
select	O
correctly	O
among	O
them	O
.	O

No	O
matter	O
whether	O
it	O
is	O
performed	O
by	O
a	O
human	O
or	O
a	O
machine	O
,	O
WSD	B-TaskName
usually	O
rests	O
on	O
the	O
assumption	O
that	O
there	O
is	O
a	O
single	O
unambiguous	O
communicative	O
intention	O
underlying	O
each	O
word	O
in	O
the	O
document	O
.	O

Word	B-TaskName
sense	I-TaskName
disambiguation	I-TaskName
,	O
or	O
WSD	B-TaskName
,	O
is	O
the	O
task	O
of	O
identifying	O
a	O
word	O
's	O
meaning	O
in	O
context	O
.	O

Introduction	O
.	O

We	O
evaluate	O
several	O
such	O
approaches	O
on	O
a	O
manually	O
sense	O
-	O
annotated	O
collection	O
of	O
English	O
puns	O
and	O
observe	O
performance	O
exceeding	O
that	O
of	O
some	O
knowledge	O
-	O
based	O
and	O
supervised	O
baselines	O
.	O

In	O
this	O
paper	O
we	O
describe	O
how	O
traditional	O
,	O
language	O
-	O
agnostic	O
WSD	B-TaskName
approaches	O
can	O
be	O
adapted	O
to	O
"	O
disambiguate	O
"	O
puns	O
,	O
or	O
rather	O
to	O
identify	O
their	O
double	O
meanings	O
.	O

This	O
deliberate	O
use	O
of	O
lexical	O
ambiguity	O
-	O
i.e.	O
,	O
punningis	O
a	O
particularly	O
common	O
source	O
of	O
humour	O
.	O

However	O
,	O
writers	O
sometimes	O
intend	O
for	O
a	O
word	O
to	O
be	O
interpreted	O
as	O
simultaneously	O
carrying	O
multiple	O
distinct	O
meanings	O
.	O

Automatic	O
disambiguation	O
of	O
English	O
puns	O
.	O

Traditional	O
approaches	O
to	O
word	B-TaskName
sense	I-TaskName
disambiguation	I-TaskName
(	O
WSD	B-TaskName
)	O
rest	O
on	O
the	O
assumption	O
that	O
there	O
exists	O
a	O
single	O
,	O
unambiguous	O
communicative	O
intention	O
underlying	O
every	O
word	O
in	O
a	O
document	O
.	O

We	O
also	O
consider	O
combining	O
both	O
multi	O
-	O
task	O
kernels	O
,	O
yielding	O
k	O
ICM+TXT	B-MethodName
=	O
k	O
ICM	B-MethodName
+	O
k	O
TXT	B-MethodName
.	O

The	O
full	O
multi	O
-	O
task	O
kernel	O
takes	O
form	O
k	O
TXT	B-MethodName
(	O
(	O
t	O
,	O
i	O
)	O
,	O
(	O
t	O
,	O
i	O
)	O
)	O
=	O
k	O
time	O
(	O
t	O
,	O
t	O
)	O
×	O
k	O
text	O
p	O
i	O
j	O
∈E	O
O	O
i	O
x	O
i	O
j	O
,	O
p	O
i	O
j	O
∈E	O
O	O
i	O
x	O
i	O
j	O
.	O

This	O
work	O
was	O
funded	O
by	O
the	O
PHEME	O
FP7	O
project	O
(	O
grant	O
No	O
.	O
611233	O
)	O
and	O
partially	O
supported	O
by	O
the	O
Australian	O
Research	O
Council	O
.	O

for	O
helpful	O
comments	O
.	O

K.	O

We	O
would	O
like	O
to	O
thank	O
Srijith	O
P.	O

Acknowledgments	O
.	O

Our	O
method	O
is	O
generalizable	O
to	O
problems	O
other	O
than	O
modelling	O
rumour	O
popularity	O
,	O
such	O
as	O
predicting	O
success	O
of	O
advertisement	O
campaigns	O
.	O

We	O
showed	O
how	O
text	O
data	O
from	O
social	O
media	O
posts	O
added	O
important	O
information	O
about	O
similarities	O
between	O
different	O
rumours	O
.	O

We	O
demonstrated	O
that	O
joint	O
modelling	O
of	O
collective	O
data	O
over	O
multiple	O
rumours	O
using	O
multi	O
-	O
task	O
learning	O
resulted	O
in	O
more	O
accurate	O
models	O
that	O
are	O
able	O
to	O
recognise	O
and	O
predict	O
commonly	O
occurring	O
temporal	O
patterns	O
.	O

This	O
paper	O
introduced	O
the	O
problem	O
of	O
modelling	B-TaskName
frequency	I-TaskName
profiles	I-TaskName
of	I-TaskName
rumours	I-TaskName
in	O
social	O
media	O
.	O

Conclusions	O
.	O

ICM	B-MethodName
manages	O
to	O
learn	O
correlations	O
more	O
properly	O
in	O
extrapolation	O
setting	O
,	O
where	O
the	O
first	O
hour	O
is	O
fully	O
observed	O
.	O

We	O
hypothesize	O
ICM	B-MethodName
performs	O
poorly	O
because	O
it	O
is	O
hard	O
to	O
learn	O
correct	O
correlations	O
between	O
frequency	O
profiles	O
when	O
training	O
intervals	O
do	O
not	O
form	O
continuous	O
segments	O
of	O
significant	O
sizes	O
.	O

We	O
also	O
found	O
,	O
that	O
for	O
a	O
few	O
rumours	O
ICM	B-MethodName
made	O
a	O
big	O
error	O
by	O
predicting	O
a	O
high	O
frequency	O
at	O
the	O
start	O
of	O
a	O
rumour	O
lifespan	O
when	O
there	O
was	O
no	O
such	O
peak	O
.	O

TXT	B-MethodName
manages	O
to	O
make	O
a	O
significantly	O
smaller	O
error	O
by	O
predicting	O
a	O
large	O
posting	O
frequency	O
there	O
.	O

For	O
example	O
,	O
ICM	B-MethodName
(	O
just	O
as	O
LGCP	B-MethodName
)	O
does	O
not	O
learn	O
there	O
should	O
be	O
a	O
peak	O
at	O
the	O
beginning	O
of	O
a	O
rumour	O
frequency	O
profile	O
depicted	O
in	O
Figure	O
1b	O
.	O

ICM	B-MethodName
turns	O
out	O
to	O
be	O
not	O
very	O
helpful	O
in	O
this	O
setting	O
.	O

As	O
for	O
the	O
multi	O
-	O
task	O
methods	O
,	O
we	O
notice	O
that	O
text	O
is	O
particularly	O
useful	O
,	O
with	O
TXT	B-MethodName
achieving	O
the	O
highest	O
MSE	B-MetricName
score	O
out	O
of	O
all	O
considered	O
models	O
.	O

Considering	O
the	O
output	O
distributions	O
(	O
LL	B-MetricName
)	O
the	O
difference	O
in	O
performance	O
between	O
the	O
Poisson	B-MethodName
Process	I-MethodName
based	O
approaches	O
and	O
GP	B-MethodName
is	O
especially	O
big	O
,	O
demonstrating	O
how	O
well	O
the	O
principled	O
models	O
handle	O
uncertainty	O
in	O
the	O
predictive	O
distributions	O
.	O

Again	O
,	O
HPP	B-MethodName
and	O
GP	B-MethodName
are	O
outperformed	O
by	O
LGCP	B-MethodName
in	O
terms	O
of	O
both	O
MSE	B-MetricName
and	O
LL	B-MetricName
.	O

Unsurprisingly	O
,	O
Interpolate	O
is	O
the	O
strongest	O
baseline	O
,	O
and	O
outperforms	O
the	O
raw	O
LGCP	B-MethodName
method	O
.	O

Next	O
,	O
we	O
move	O
to	O
the	O
interpolation	O
setting	O
.	O

TXT	B-MethodName
makes	O
a	O
lower	O
error	O
than	O
LGCP	B-MethodName
and	O
LGCPICM	B-MethodName
,	O
both	O
of	O
which	O
underestimate	O
the	O
counts	O
in	O
the	O
second	O
hour	O
.	O

In	O
Figure	O
1a	O
we	O
show	O
an	O
example	O
rumour	O
frequency	O
profile	O
for	O
the	O
extrapolation	O
setting	O
.	O

TXT	B-MethodName
turns	O
out	O
to	O
be	O
a	O
good	O
approach	O
to	O
multi	O
-	O
task	O
learning	O
and	O
outperforms	O
ICM	B-MethodName
.	O

ICM	B-MethodName
,	O
TXT	B-MethodName
and	O
ICM+TXT	B-MethodName
multitask	O
learning	O
approaches	O
achieve	O
the	O
best	O
scores	O
and	O
significantly	O
outperform	O
all	O
baselines	O
.	O

Incorporating	O
information	O
about	O
other	O
rumours	O
helps	O
outperform	O
this	O
method	O
.	O

The	O
0	O
baseline	O
is	O
very	O
strong	O
,	O
since	O
many	O
rumours	O
have	O
comparatively	O
little	O
discussion	O
in	O
the	O
second	O
hour	O
of	O
their	O
lifespan	O
relative	O
to	O
the	O
first	O
hour	O
.	O

Changing	O
the	O
model	O
from	O
a	O
GP	B-MethodName
to	O
a	O
better	O
fitting	O
to	O
the	O
modelling	O
temporal	O
count	O
data	O
LGCP	B-MethodName
gives	O
a	O
big	O
improvement	O
,	O
even	O
when	O
a	O
point	O
estimate	O
of	O
the	O
prediction	O
is	O
considered	O
(	O
MSE	B-MetricName
)	O
.	O

This	O
is	O
due	O
to	O
GP	B-MethodName
modelling	O
a	O
distribution	O
with	O
continuous	O
support	O
,	O
which	O
is	O
inappropriate	O
for	O
modelling	O
discrete	O
counts	O
.	O

The	O
left	O
columns	O
of	O
proaches	O
.	O

Experiments	O
.	O

Instead	O
,	O
one	O
could	O
use	O
a	O
GP	B-MethodName
with	O
a	O
non	O
-	O
zero	O
mean	O
function	O
and	O
learn	O
the	O
mean	O
function	O
,	O
a	O
more	O
elegant	O
way	O
of	O
approaching	O
this	O
problem	O
,	O
which	O
we	O
leave	O
for	O
future	O
work	O
.	O

The	O
original	O
counts	O
can	O
be	O
obtained	O
by	O
decrementing	O
1	O
from	O
the	O
predicted	O
counts	O
.	O

We	O
add	O
1	O
to	O
the	O
counts	O
in	O
the	O
intervals	O
to	O
deal	O
with	O
this	O
problem	O
as	O
a	O
preprocessing	O
step	O
.	O

This	O
prevents	O
our	O
method	O
from	O
predicting	O
0	O
counts	O
in	O
these	O
regions	O
.	O

The	O
mean	O
function	O
for	O
the	O
underlying	O
GP	B-MethodName
in	O
LGCP	B-MethodName
methods	O
is	O
assumed	O
to	O
be	O
0	O
,	O
which	O
results	O
in	O
intensity	O
function	O
to	O
be	O
around	O
1	O
in	O
the	O
absence	O
of	O
nearby	O
observations	O
.	O

To	O
ameliorate	O
the	O
problems	O
of	O
data	O
sparsity	O
,	O
we	O
replace	O
words	O
with	O
their	O
Brown	O
cluster	O
ids	O
,	O
using	O
1000	O
clusters	O
acquired	O
on	O
a	O
large	O
scale	O
Twitter	B-DatasetName
corpus	I-DatasetName
(	O
Owoputi	O
et	O
al	O
.	O
,	O
2013	O
)	O
.	O

The	O
excluded	O
time	O
intervals	O
form	O
the	O
test	O
set	O
:	O
either	O
by	O
selecting	O
half	O
at	O
random	O
(	O
interpolation	O
)	O
;	O
or	O
by	O
taking	O
only	O
the	O
second	O
half	O
for	O
testing	O
(	O
extrapolation	O
)	O
.	O

We	O
iterate	O
over	O
rumours	O
using	O
a	O
form	O
of	O
folded	O
cross	O
-	O
validation	O
,	O
where	O
in	O
each	O
iteration	O
we	O
exclude	O
some	O
(	O
but	O
not	O
all	O
)	O
time	O
intervals	O
for	O
a	O
single	O
target	O
rumour	O
.	O

This	O
way	O
,	O
our	O
dataset	O
consists	O
in	O
total	O
of	O
2280	O
intervals	O
.	O

In	O
our	O
experiments	O
,	O
we	O
consider	O
the	O
first	O
two	O
hours	O
of	O
each	O
rumour	O
lifespan	O
,	O
which	O
we	O
split	O
into	O
20	O
evenly	O
spaced	O
intervals	O
.	O

Data	O
preprocessing	O
.	O

The	O
fourth	O
baseline	O
is	O
tailored	O
for	O
the	O
interpolation	O
setting	O
,	O
and	O
uses	O
simple	O
interpolation	O
by	O
averaging	O
over	O
the	O
frequencies	O
of	O
the	O
closest	O
left	O
and	O
right	O
intervals	O
,	O
or	O
the	O
frequency	O
of	O
the	O
closest	O
interval	O
for	O
test	O
intervals	O
on	O
a	O
boundary	O
.	O

The	O
third	O
baseline	O
is	O
to	O
always	O
predict	O
0	O
posts	O
in	O
all	O
intervals	O
.	O

We	O
restrict	O
our	O
focus	O
to	O
RBF	O
kernel	O
and	O
leave	O
inspection	O
of	O
other	O
kernels	O
such	O
as	O
periodic	O
ones	O
for	O
both	O
GP	B-MethodName
and	O
LGCP	B-MethodName
models	O
for	O
future	O
.	O

In	O
our	O
case	O
it	O
is	O
not	O
apparent	O
that	O
rumours	O
exhibit	O
periodic	O
characteristics	O
,	O
as	O
can	O
be	O
seen	O
in	O
Figure	O
1	O
.	O

Authors	O
considered	O
various	O
kernels	O
in	O
their	O
experiments	O
,	O
most	O
notably	O
periodic	O
kernels	O
.	O

The	O
second	O
baseline	O
is	O
Gaussian	B-MethodName
Process	I-MethodName
(	O
GP	B-MethodName
)	O
used	O
for	O
predicting	O
hashtag	O
frequencies	O
in	O
Twitter	O
by	O
Preotiuc	O
-	O
Pietro	O
and	O
Cohn	O
(	O
2013	O
)	O
.	O

We	O
select	O
its	O
intensity	O
λ	O
using	O
maximum	O
likelihood	O
estimate	O
,	O
which	O
equals	O
to	O
the	O
mean	O
frequency	O
of	O
posts	O
in	O
the	O
training	O
intervals	O
.	O

The	O
first	O
is	O
the	O
Homogenous	B-MethodName
Poisson	I-MethodName
Process	I-MethodName
(	O
HPP	B-MethodName
)	O
trained	O
on	O
the	O
training	O
set	O
of	O
the	O
rumour	O
.	O

Baselines	O
We	O
use	O
the	O
following	O
baselines	O
.	O

Since	O
probabilistic	O
models	O
(	O
GP	B-MethodName
,	O
LGCP	B-MethodName
)	O
return	O
distributions	O
over	O
possible	O
outputs	O
,	O
we	O
also	O
evaluate	O
them	O
via	O
the	O
log	B-MetricName
-	I-MetricName
likelihood	I-MetricName
(	O
LL	B-MetricName
)	O
of	O
the	O
true	O
counts	O
under	O
the	O
returned	O
distributions	O
(	O
respectively	O
Gaussian	O
and	O
Poisson	O
distribution	O
)	O
.	O

Evaluation	O
metric	O
We	O
use	O
mean	B-MetricName
squared	I-MetricName
error	I-MetricName
(	O
MSE	B-MetricName
)	O
to	O
measure	O
the	O
difference	O
between	O
true	O
counts	O
and	O
predicted	O
counts	O
in	O
the	O
test	O
intervals	O
.	O

Experimental	O
Setup	O
.	O

Optimization	O
All	O
hyperparameters	O
are	O
optimized	O
by	O
maximizing	O
the	O
marginal	O
likelihood	O
of	O
the	O
data	O
L(E	O
O	O
i	O
|θ	O
)	O
,	O
where	O
θ	O
=	O
(	O
a	O
,	O
l	O
,	O
κ	O
,	O
v	O
,	O
b	O
,	O
c	O
)	O
or	O
a	O
subset	O
thereof	O
,	O
depending	O
on	O
the	O
choice	O
of	O
kernel	O
.	O

We	O
compare	O
text	O
vectors	O
using	O
cosine	O
similarity	O
,	O
k	O
text	O
(	O
x	O
,	O
y	O
)	O
=	O
b	O
+	O
c	O
x	O
T	O
y	O
x	O
y	O
,	O
where	O
the	O
hyperparameters	O
b	O
>	O
0	O
and	O
c	O
>	O
0	O
modulate	O
between	O
text	O
similarity	O
and	O
a	O
global	O
constant	O
similarity	O
.	O

In	O
a	O
second	O
approach	O
,	O
we	O
parametrize	O
the	O
intertask	O
similarity	O
measures	O
by	O
incorporating	O
text	O
of	O
the	O
posts	O
.	O

The	O
kernel	O
takes	O
the	O
form	O
k	O
ICM	B-MethodName
(	O
(	O
t	O
,	O
i	O
)	O
,	O
(	O
t	O
,	O
i	O
)	O
)	O
=	O
k	O
time	O
(	O
t	O
,	O
t	O
)	O
B	O
i	O
,	O
i	O
,	O
where	O
B	O
is	O
a	O
square	O
coregionalization	O
matrix	O
(	O
rank	O
1	O
,	O
B	O
=	O
κI	O
+	O
vv	O
T	O
)	O
,	O
i	O
and	O
i	O
denote	O
the	O
tasks	O
of	O
the	O
two	O
inputs	O
,	O
k	O
time	O
is	O
a	O
kernel	O
for	O
comparing	O
inputs	O
t	O
and	O
t	O
(	O
here	O
RBF	O
)	O
and	O
κ	O
is	O
a	O
vector	O
of	O
values	O
modulating	O
the	O
extent	O
of	O
each	O
task	O
independence	O
.	O

We	O
expect	O
it	O
to	O
find	O
correlations	O
between	O
rumours	O
exhibiting	O
similar	O
temporal	O
patterns	O
.	O

ICM	B-MethodName
parametrizes	O
the	O
kernel	O
by	O
a	O
matrix	O
representing	O
similarities	O
between	O
pairs	O
of	O
tasks	O
.	O

It	O
is	O
a	O
method	O
which	O
has	O
been	O
successfully	O
applied	O
to	O
a	O
range	O
of	O
NLP	O
tasks	O
(	O
Beck	O
et	O
al	O
.	O
,	O
2014;Cohn	O
and	O
Specia	O
,	O
2013	O
)	O
.	O

First	O
,	O
we	O
employ	O
a	O
multiple	O
output	O
GP	B-MethodName
based	O
on	O
the	O
Intrinsic	B-MethodName
Coregionalization	I-MethodName
Model	I-MethodName
(	O
ICM	B-MethodName
)	O
(	O
Álvarez	O
et	O
al	O
.	O
,	O
2012	O
)	O
.	O

We	O
consider	O
two	O
approaches	O
.	O

Multi	O
-	O
task	O
learning	O
and	O
incorporating	O
text	O
In	O
order	O
to	O
exploit	O
similarities	O
across	O
rumours	O
we	O
propose	O
a	O
multi	O
-	O
task	O
approach	O
where	O
each	O
rumour	O
represents	O
a	O
task	O
.	O

The	O
predictive	O
distribution	O
over	O
counts	O
at	O
a	O
particular	O
time	O
interval	O
of	O
length	O
w	O
with	O
a	O
mid	O
-	O
point	O
t	O
*	O
for	O
rumour	O
E	O
i	O
is	O
Poisson	O
distributed	O
with	O
rate	O
wλ	O
i	O
(	O
t	O
*	O
|E	O
O	O
i	O
)	O
.	O

This	O
predictive	O
distribution	O
is	O
then	O
used	O
to	O
obtain	O
the	O
intensity	O
function	O
value	O
at	O
the	O
point	O
t	O
*	O
:	O
λ	O
i	O
(	O
t	O
*	O
|E	O
O	O
i	O
)	O
=	O
exp	O
(	O
f	O
i	O
(	O
t	O
)	O
)	O
p	O
f	O
i	O
(	O
t)|E	O
O	O
i	O
df	O
i	O
.	O

The	O
predictive	O
distribution	O
over	O
time	O
t	O
*	O
is	O
obtained	O
using	O
the	O
approximated	O
posterior	O
.	O

For	O
more	O
details	O
about	O
the	O
model	O
and	O
inference	O
we	O
refer	O
the	O
reader	O
to	O
(	O
Rasmussen	O
and	O
Williams	O
,	O
2005	O
)	O
.	O

There	O
exist	O
various	O
methods	O
to	O
deal	O
with	O
calculating	O
the	O
posterior	O
;	O
here	O
we	O
use	O
the	O
Laplace	O
approximation	O
,	O
where	O
the	O
posterior	O
is	O
approximated	O
by	O
a	O
Gaussian	O
distribution	O
based	O
on	O
the	O
first	O
2	O
moments	O
.	O

It	O
is	O
intractable	O
and	O
approximation	O
techniques	O
are	O
required	O
.	O

The	O
distribution	O
of	O
the	O
posterior	O
p(f	O
i	O
(	O
t)|E	O
O	O
i	O
)	O
at	O
an	O
arbitrary	O
timestamp	O
t	O
is	O
calculated	O
based	O
on	O
the	O
specified	O
prior	O
and	O
the	O
Poisson	O
likelihood	O
.	O

We	O
use	O
a	O
Radial	O
Basis	O
Function	O
(	O
RBF	O
)	O
kernel	O
,	O
k(t	O
,	O
t	O
)	O
=	O
a	O
exp(−(t	O
−	O
t	O
)	O
2	O
/l	O
)	O
,	O
where	O
lengthscale	O
l	O
controls	O
the	O
extent	O
to	O
which	O
nearby	O
points	O
influence	O
one	O
another	O
and	O
a	O
controls	O
the	O
scale	O
of	O
the	O
function	O
.	O

The	O
latent	O
function	O
f	O
is	O
modelled	O
via	O
a	O
Gaussian	B-MethodName
process	I-MethodName
(	O
GP	B-MethodName
)	O
(	O
Rasmussen	O
and	O
Williams	O
,	O
2005	O
):	O
f	O
(	O
t	O
)	O
∼	O
GP(m(t	B-MethodName
)	O
,	O
k(t	O
,	O
t	O
)	O
)	O
,	O
where	O
m	O
is	O
the	O
mean	O
function	O
(	O
equal	O
0	O
)	O
and	O
k	O
is	O
the	O
kernel	O
specifying	O
how	O
outputs	O
covary	O
as	O
a	O
function	O
of	O
the	O
inputs	O
.	O

Here	O
,	O
time	O
is	O
divided	O
into	O
S	O
intervals	O
indexed	O
by	O
s	O
,	O
ṫs	O
is	O
the	O
centre	O
of	O
the	O
s	O
th	O
interval	O
,	O
l	O
s	O
is	O
the	O
length	O
of	O
the	O
s	O
th	O
interval	O
and	O
y	O
s	O
is	O
number	O
of	O
tweets	O
posted	O
during	O
this	O
interval	O
.	O

Following	O
this	O
,	O
we	O
approximate	O
the	O
likelihood	O
as	O
p(E	O
O	O
i	O
|f	O
i	O
)	O
=	O
S	O
s=1	O
Poisson(y	O
s	O
|	O
l	O
s	O
exp	O
f	O
i	O
(	O
ṫs	O
)	O
)	O
.	O

The	O
likelihood	O
(	O
1	O
)	O
is	O
commonly	O
approximated	O
by	O
considering	O
subregions	O
of	O
T	O
and	O
assuming	O
constant	O
intensities	O
in	O
sub	O
-	O
regions	O
of	O
T	O
(	O
Møller	O
and	O
Syversveen	O
,	O
1998;Vanhatalo	O
et	O
al	O
.	O
,	O
2013	O
)	O
to	O
overcome	O
computational	O
difficulties	O
arising	O
due	O
to	O
integration	O
.	O

Then	O
,	O
the	O
likelihood	O
of	O
posts	O
E	O
O	O
i	O
in	O
time	O
interval	O
T	O
given	O
a	O
latent	O
function	O
f	O
i	O
can	O
be	O
obtained	O
as	O
p(E	O
O	O
i	O
|f	O
i	O
)	O
=	O
exp	O
	O
	O
−	O
T	O
−Tte	O
exp	O
(	O
f	O
i	O
(	O
u	O
)	O
)	O
du	O
+	O
m	O
O	O
i	O
j=1	O
f	O
i	O
(	O
t	O
i	O
j	O
)	O
	O
	O
(	O
1	O
)	O
The	O
likelihood	O
of	O
posts	O
in	O
the	O
rumour	O
data	O
is	O
obtained	O
by	O
taking	O
the	O
product	O
of	O
the	O
likelihoods	O
over	O
individual	O
rumours	O
.	O

LGCP	B-MethodName
models	O
the	O
likelihood	O
that	O
a	O
single	O
tweet	O
occurs	O
at	O
time	O
t	O
in	O
the	O
interval	O
[	O
s	O
,	O
t	O
]	O
for	O
a	O
rumour	O
E	O
i	O
given	O
the	O
latent	O
function	O
f	O
i	O
(	O
t	O
)	O
as	O
p(y	O
=	O
1|f	O
i	O
)	O
=	O
exp(f	O
i	O
(	O
t	O
)	O
)	O
exp(−	O
t	O
s	O
exp(f	O
i	O
(	O
u))du	O
)	O
.	O

We	O
associate	O
a	O
distinct	O
intensity	O
function	O
with	O
each	O
rumour	O
as	O
they	O
have	O
varying	O
temporal	O
profiles	O
.	O

We	O
model	O
the	O
occurrence	O
of	O
posts	O
in	O
a	O
rumour	O
E	O
i	O
to	O
follow	O
log	B-MethodName
-	I-MethodName
Gaussian	I-MethodName
Cox	I-MethodName
process	I-MethodName
(	O
LGCP	B-MethodName
)	O
with	O
intensity	O
λ	O
i	O
(	O
t	O
)	O
,	O
where	O
λ	O
i	O
(	O
t	O
)	O
=	O
exp(f	O
i	O
(	O
t	O
)	O
)	O
.	O

The	O
intensity	O
function	O
can	O
be	O
automatically	O
learned	O
from	O
the	O
data	O
set	O
and	O
its	O
complexity	O
depends	O
on	O
the	O
data	O
points	O
.	O

This	O
provides	O
a	O
non	O
-	O
parametric	O
approach	O
to	O
model	O
the	O
intensity	O
function	O
.	O

In	O
fact	O
,	O
the	O
intensity	O
function	O
λ(t	O
)	O
is	O
modelled	O
using	O
a	O
latent	O
function	O
f	O
(	O
t	O
)	O
sampled	O
from	O
a	O
Gaussian	B-MethodName
process	I-MethodName
(	O
Rasmussen	O
and	O
Williams	O
,	O
2005	O
)	O
,	O
such	O
that	O
λ(t	O
)	O
=	O
exp	O
(	O
f	O
(	O
t	O
)	O
)	O
(	O
exponent	O
ensures	O
positivity	O
)	O
.	O

In	O
LGCP	B-MethodName
the	O
intensity	O
function	O
is	O
assumed	O
to	O
be	O
a	O
stochastic	O
process	O
which	O
varies	O
over	O
time	O
.	O

We	O
consider	O
a	O
log	B-MethodName
-	I-MethodName
Gaussian	I-MethodName
Cox	I-MethodName
process	I-MethodName
(	O
LGCP	B-MethodName
)	O
(	O
Møller	O
and	O
Syversveen	O
,	O
1998	O
)	O
,	O
a	O
generalization	O
of	O
inhomogeneous	O
Poisson	O
process	O
.	O

Model	O
.	O

For	O
example	O
,	O
one	O
could	O
use	O
it	O
to	O
predict	O
whether	O
an	O
advertisement	O
campaign	O
would	O
be	O
successful	O
or	O
how	O
a	O
political	O
campaign	O
would	O
proceed	O
.	O

Although	O
our	O
focus	O
here	O
is	O
on	O
rumours	O
,	O
our	O
model	O
is	O
more	O
widely	O
applicable	O
.	O

This	O
corresponds	O
to	O
a	O
scenario	O
where	O
the	O
user	O
seeks	O
to	O
predict	O
the	O
future	O
profile	O
of	O
the	O
rumour	O
,	O
e.g.	O
,	O
to	O
identify	O
rumours	O
that	O
will	O
attract	O
further	O
attention	O
or	O
wither	O
away	O
.	O

The	O
second	O
formulation	O
is	O
that	O
of	O
extrapolation	O
,	O
where	O
all	O
observed	O
posts	O
occur	O
before	O
the	O
test	O
intervals	O
.	O

This	O
corresponds	O
to	O
a	O
situation	O
,	O
e.g.	O
,	O
when	O
a	O
journalist	O
analyses	O
a	O
rumour	O
during	O
short	O
spot	O
checks	O
,	O
but	O
wants	O
to	O
know	O
the	O
prevalence	O
of	O
the	O
rumour	O
at	O
other	O
times	O
,	O
thus	O
limiting	O
the	O
need	O
for	O
constant	O
attention	O
.	O

The	O
first	O
is	O
interpolation	O
,	O
where	O
the	O
test	O
intervals	O
are	O
not	O
ordered	O
in	O
any	O
particular	O
way	O
.	O

Two	O
instantiations	O
of	O
this	O
problem	O
formulation	O
are	O
considered	O
.	O

We	O
also	O
consider	O
a	O
domain	O
adaptation	O
setting	O
,	O
where	O
additionally	O
posts	O
from	O
other	O
rumours	O
are	O
observed	O
R	O
O	O
i	O
=	O
R\E	O
i	O
.	O

Let	O
the	O
number	O
of	O
elements	O
in	O
E	O
O	O
i	O
be	O
m	O
O	O
i	O
.	O

The	O
problem	O
is	O
considered	O
in	O
supervised	O
settings	O
,	O
where	O
posts	O
on	O
this	O
rumour	O
outside	O
of	O
these	O
intervals	O
form	O
the	O
training	O
set	O
E	O
O	O
i	O
=	O
{	O
p	O
i	O
j	O
:	O
t	O
i	O
j	O
∈	O
K	O
i	O
k=1	O
[	O
s	O
i	O
k	O
,	O
e	O
i	O
k	O
]	O
}	O
.	O

To	O
evaluate	O
the	O
predicted	O
density	O
for	O
a	O
given	O
rumour	O
E	O
i	O
we	O
leave	O
out	O
posts	O
from	O
a	O
set	O
of	O
intervals	O
T	O
te	O
=	O
{	O
[	O
s	O
i	O
k	O
,	O
e	O
i	O
k	O
]	O
}	O
K	O
i	O
k=1	O
(	O
where	O
s	O
i	O
k	O
and	O
e	O
i	O
k	O
are	O
respectively	O
start	O
and	O
end	O
points	O
of	O
interval	O
k	O
for	O
rumour	O
i	O
)	O
and	O
estimate	O
performance	O
at	O
predicting	O
counts	O
in	O
them	O
by	O
the	O
trained	O
model	O
.	O

Posts	O
occur	O
at	O
different	O
timestamps	O
,	O
yielding	O
varying	O
density	O
of	O
posts	O
over	O
time	O
,	O
which	O
we	O
are	O
interested	O
in	O
estimating	O
.	O

Posts	O
are	O
tuples	O
p	O
i	O
j	O
=	O
(	O
x	O
i	O
j	O
,	O
t	O
i	O
j	O
)	O
,	O
where	O
x	O
i	O
j	O
is	O
text	O
(	O
in	O
our	O
case	O
a	O
bag	O
of	O
words	O
text	O
representation	O
)	O
and	O
t	O
i	O
j	O
is	O
a	O
timestamp	O
describing	O
post	O
p	O
i	O
j	O
,	O
measured	O
in	O
time	O
elapsed	O
since	O
the	O
first	O
post	O
on	O
rumour	O
E	O
i	O
.	O

[	O
0	O
,	O
l	O
]	O
of	O
length	O
l=2	O
hours	O
,	O
a	O
set	O
of	O
n	O
rumours	O
R	O
=	O
{	O
E	O
i	O
}	O
n	O
i=1	O
,	O
where	O
rumour	O
E	O
i	O
consists	O
of	O
a	O
set	O
of	O
m	O
i	O
posts	O
E	O
i	O
=	O
{	O
p	O
i	O
j	O
}	O
m	O
i	O
j=1	O
.	O

Problem	O
Definition	O
Let	O
us	O
consider	O
a	O
time	O
interval	O
.	O

This	O
results	O
in	O
114	O
rumours	O
consisting	O
of	O
a	O
total	O
of	O
4098	O
tweets	O
.	O

Since	O
some	O
rumours	O
have	O
few	O
posts	O
,	O
we	O
consider	O
only	O
those	O
with	O
at	O
least	O
15	O
posts	O
in	O
the	O
first	O
hour	O
as	O
rumours	O
of	O
particular	O
interest	O
.	O

All	O
source	O
tweets	O
are	O
categorized	O
as	O
rumour	O
vs	O
non	O
-	O
rumour	O
,	O
other	O
tweets	O
from	O
the	O
same	O
thread	O
are	O
assigned	O
automatically	O
as	O
belonging	O
to	O
the	O
same	O
event	O
as	O
the	O
source	O
tweet	O
.	O

It	O
contains	O
both	O
source	O
tweets	O
and	O
the	O
conversational	O
threads	O
around	O
these	O
(	O
where	O
available	O
)	O
.	O

Data	O
We	O
use	O
the	O
Ferguson	B-DatasetName
rumour	I-DatasetName
data	O
set	O
(	O
Zubiaga	O
et	O
al	O
.	O
,	O
2015	O
)	O
,	O
consisting	O
of	O
tweets	O
collected	O
in	O
August	O
and	O
September	O
2014	O
during	O
the	O
Ferguson	O
unrest	O
.	O

In	O
this	O
section	O
we	O
describe	O
the	O
data	O
and	O
we	O
formalize	O
the	O
problem	O
of	O
modelling	O
rumour	O
popularity	O
.	O

Data	O
&	O
Problem	O
.	O

However	O
,	O
these	O
models	O
were	O
mainly	O
used	O
for	O
network	O
modelling	O
rather	O
than	O
revealing	O
complex	O
temporal	O
patterns	O
,	O
which	O
may	O
emerge	O
only	O
implicitly	O
,	O
and	O
are	O
more	O
limited	O
in	O
the	O
kinds	O
of	O
temporal	O
patterns	O
that	O
may	O
be	O
represented	O
.	O

One	O
example	O
is	O
application	O
of	O
Hawkes	B-MethodName
processes	I-MethodName
(	O
Yang	O
and	O
Zha	O
,	O
2013	O
)	O
,	O
a	O
probabilistic	O
framework	O
for	O
modelling	O
self	O
-	O
excitatory	O
phenomena	O
.	O

Related	O
also	O
is	O
the	O
extensive	O
work	O
done	O
in	O
spatio	O
-	O
temporal	O
modelling	O
of	O
meme	O
spread	O
.	O

In	O
contrast	O
here	O
we	O
deal	O
with	O
temporal	O
text	O
data	O
,	O
and	O
model	O
several	O
correlated	O
outputs	O
rather	O
than	O
their	O
single	O
output	O
.	O

(	O
2012	O
)	O
developed	O
a	O
spatio	O
-	O
temporal	O
model	O
of	O
conflict	O
events	O
in	O
Afghanistan	O
.	O

The	O
log	B-MethodName
-	I-MethodName
Gaussian	I-MethodName
Cox	I-MethodName
process	I-MethodName
has	O
been	O
applied	O
for	O
disease	O
and	O
conflict	O
mapping	O
,	O
e.g.	O
Zammit	O
-	O
Mangion	O
et	O
al	O
.	O

We	O
use	O
the	O
proposed	O
GP	B-MethodName
-	O
based	O
method	O
as	O
a	O
baseline	O
to	O
demonstrate	O
the	O
benefit	O
of	O
using	O
our	O
approaches	O
.	O

In	O
contrast	O
we	O
take	O
a	O
more	O
principled	O
approach	O
,	O
using	O
a	O
point	O
process	O
.	O

It	O
made	O
several	O
simplifications	O
,	O
including	O
discretising	O
time	O
and	O
treating	O
the	O
problem	O
of	O
modelling	O
counts	O
as	O
regression	O
,	O
which	O
are	O
both	O
inappropriate	O
.	O

The	O
work	O
most	O
closely	O
related	O
modelled	O
hash	O
tag	O
frequency	O
time	O
-	O
series	O
in	O
Twitter	O
using	O
GP	B-MethodName
(	O
Preotiuc	O
-	O
Pietro	O
and	O
Cohn	O
,	O
2013	O
)	O
.	O

The	O
problem	O
of	O
modelling	O
the	O
temporal	O
nature	O
of	O
social	O
media	O
explicitly	O
has	O
received	O
little	O
attention	O
.	O

However	O
,	O
none	O
of	O
these	O
studies	O
tried	O
to	O
model	O
rumour	O
dynamics	O
.	O

(	O
2014	O
)	O
showed	O
how	O
Facebook	O
constitutes	O
a	O
rich	O
source	O
of	O
rumours	O
and	O
conversation	O
threads	O
on	O
the	O
topic	O
.	O

Friggeri	O
et	O
al	O
.	O

(	O
2013	O
)	O
analyzed	O
rumours	O
in	O
tweets	O
about	O
the	O
2011	O
London	O
riots	O
and	O
showed	O
that	O
they	O
follow	O
similar	O
lifecycles	O
.	O

There	O
have	O
been	O
several	O
descriptive	O
studies	O
of	O
rumours	O
in	O
social	O
media	O
,	O
e.g.	O
Procter	O
et	O
al	O
.	O

Related	O
Work	O
.	O

Demonstrates	O
how	O
incorporating	O
text	O
into	O
multi	O
-	O
task	O
learning	O
improves	O
results	O
.	O

Incorporates	O
multi	O
-	O
task	O
learning	O
to	O
generalize	O
across	O
disparate	O
rumours	O
;	O
and	O
3	O
.	O

Introduces	O
the	O
problem	O
of	O
modelling	B-TaskName
rumour	I-TaskName
frequency	I-TaskName
profiles	I-TaskName
,	O
and	O
presents	O
a	O
method	O
based	O
on	O
a	O
log	B-MethodName
-	I-MethodName
Gaussian	I-MethodName
Cox	I-MethodName
process	I-MethodName
;	O
2	O
.	O

This	O
paper	O
makes	O
the	O
following	O
contributions	O
:	O
1	O
.	O

prediction	O
of	O
rumour	O
popularity	O
.	O

We	O
demonstrate	O
how	O
text	O
from	O
observed	O
posts	O
can	O
be	O
used	O
to	O
weight	O
influence	O
across	O
rumours	O
.	O

In	O
this	O
way	O
statistics	O
over	O
a	O
larger	O
training	O
set	O
are	O
shared	O
,	O
enabling	O
more	O
reliable	O
predictions	O
for	O
distant	O
time	O
periods	O
,	O
in	O
which	O
no	O
posts	O
from	O
the	O
target	O
rumour	O
have	O
been	O
observed	O
.	O

To	O
overcome	O
this	O
difficulty	O
we	O
propose	O
a	O
multi	O
-	O
task	O
learning	O
approach	O
,	O
where	O
patterns	O
are	O
correlated	O
across	O
multiple	O
rumours	O
.	O

Modelling	O
the	O
frequency	O
profile	O
of	O
a	O
rumour	O
based	O
on	O
posts	O
is	O
extremely	O
challenging	O
,	O
since	O
many	O
rumours	O
consist	O
of	O
only	O
a	O
small	O
number	O
of	O
posts	O
and	O
exhibit	O
complex	O
patterns	O
.	O

GP	B-MethodName
is	O
a	O
nonparametric	O
model	O
which	O
allows	O
for	O
powerful	O
modelling	O
of	O
the	O
underlying	O
intensity	O
function	O
.	O

We	O
model	O
frequency	O
profiles	O
using	O
a	O
log	O
-	B-MethodName
Gaussian	I-MethodName
Cox	I-MethodName
process	I-MethodName
(	O
Møller	O
and	O
Syversveen	O
,	O
1998	O
)	O
,	O
a	O
point	O
process	O
where	O
the	O
log	O
-	O
intensity	O
of	O
the	O
Poisson	O
distribution	O
is	O
modelled	O
via	O
a	O
Gaussian	B-MethodName
Process	I-MethodName
(	O
GP	B-MethodName
)	O
.	O

The	O
posterior	O
distribution	O
can	O
then	O
be	O
used	O
for	O
several	O
inference	O
problems	O
,	O
e.g.	O
to	O
query	O
the	O
expected	O
count	O
of	O
posts	O
,	O
or	O
to	O
find	O
the	O
probability	O
of	O
a	O
count	O
of	O
posts	O
occurring	O
during	O
an	O
arbitrary	O
time	O
interval	O
.	O

This	O
framework	O
models	O
count	O
data	O
in	O
a	O
continuous	O
time	O
through	O
the	O
underlying	O
intensity	O
of	O
a	O
Poisson	O
distribution	O
.	O

Since	O
posts	O
occur	O
at	O
continuous	O
timestamps	O
,	O
and	O
their	O
density	O
is	O
typically	O
a	O
smooth	O
function	O
of	O
time	O
,	O
we	O
base	O
our	O
model	O
on	O
point	O
processes	O
,	O
which	O
have	O
been	O
shown	O
to	O
model	O
well	O
such	O
data	O
in	O
epidemiology	O
and	O
conflict	O
mapping	O
(	O
Brix	O
and	O
Diggle	O
,	O
2001;Zammit	O
-	O
Mangion	O
et	O
al	O
.	O
,	O
2012	O
)	O
.	O

This	O
paper	O
considers	O
the	O
problem	O
of	O
modelling	O
temporal	O
frequency	O
profiles	O
of	O
rumours	O
by	O
taking	O
into	O
account	O
both	O
the	O
temporal	O
and	O
textual	O
information	O
.	O

A	O
second	O
factor	O
is	O
text	O
from	O
the	O
posts	O
themselves	O
,	O
where	O
phrases	O
such	O
as	O
not	O
true	O
,	O
unconfirmed	O
,	O
or	O
debunk	O
help	O
users	O
judge	O
veracity	O
and	O
thus	O
limit	O
rumour	O
spread	O
(	O
Zhao	O
et	O
al	O
.	O
,	O
2015	O
)	O
.	O

One	O
is	O
the	O
dynamics	O
of	O
post	O
occurrences	O
,	O
e.g.	O
if	O
the	O
frequency	O
profile	O
decays	O
quickly	O
,	O
chances	O
are	O
it	O
would	O
not	O
attract	O
further	O
attention	O
.	O

Two	O
characteristics	O
can	O
help	O
determine	O
if	O
a	O
rumour	O
will	O
continue	O
to	O
be	O
discussed	O
.	O

Figure	O
1	O
shows	O
two	O
example	O
rumours	O
from	O
our	O
dataset	O
(	O
see	O
Section	O
3	O
):	O
online	O
discussion	O
of	O
rumour	O
#	O
10	O
quickly	O
drops	O
away	O
,	O
whereas	O
rumour	O
#	O
37	O
takes	O
a	O
lot	O
longer	O
to	O
die	O
out	O
.	O

The	O
challenge	O
comes	O
from	O
the	O
observation	O
that	O
different	O
rumours	O
exhibit	O
different	O
trajectories	O
.	O

Another	O
application	O
of	O
modelling	O
rumour	O
dynamics	O
could	O
be	O
to	O
predict	O
the	O
prevalence	O
of	O
a	O
rumour	O
throughout	O
its	O
lifespan	O
,	O
based	O
on	O
occasional	O
spot	O
checks	O
by	O
journalists	O
.	O

An	O
effective	O
early	O
warning	O
system	O
of	O
this	O
kind	O
is	O
of	O
interest	O
to	O
government	O
bodies	O
and	O
news	O
outlets	O
,	O
who	O
struggle	O
with	O
monitoring	O
and	O
verifying	O
social	O
media	O
posts	O
during	O
emergencies	O
and	O
social	O
unrests	O
.	O

One	O
such	O
example	O
is	O
the	O
false	O
rumour	O
of	O
rioters	O
breaking	O
into	O
McDonald	O
's	O
during	O
the	O
2011	O
England	O
riots	O
.	O

The	O
ability	O
to	O
model	O
rumour	O
dynamics	O
helps	O
with	O
identifying	O
those	O
,	O
which	O
,	O
if	O
not	O
debunked	O
early	O
,	O
will	O
likely	O
spread	O
very	O
fast	O
.	O

Introduction	O
.	O

Our	O
experiments	O
demonstrate	O
that	O
our	O
model	O
outperforms	O
several	O
strong	O
baseline	O
methods	O
for	O
rumour	O
frequency	O
prediction	O
evaluated	O
on	O
tweets	O
from	O
the	O
2014	O
Ferguson	O
riots	O
.	O

To	O
generalize	O
over	O
different	O
rumours	O
,	O
we	O
present	O
a	O
multi	O
-	O
task	O
learning	O
method	O
parametrized	O
by	O
the	O
text	O
in	O
posts	O
which	O
allows	O
data	O
statistics	O
to	O
be	O
shared	O
between	O
groups	O
of	O
similar	O
rumours	O
.	O

This	O
paper	O
develops	O
a	O
model	O
of	O
rumour	O
prevalence	O
using	O
a	O
point	O
process	O
,	O
namely	O
a	O
log	O
-	O
Gaussian	O
Cox	O
process	O
,	O
to	O
infer	O
an	O
underlying	O
continuous	O
temporal	O
probabilistic	O
model	O
of	O
post	O
frequencies	O
.	O

Rumours	O
on	O
social	O
media	O
exhibit	O
complex	O
temporal	O
patterns	O
.	O

Point	B-TaskName
Process	I-TaskName
Modelling	I-TaskName
of	O
Rumour	O
Dynamics	O
in	O
Social	O
Media	O
.	O

TC	O
and	O
TB	O
were	O
supported	O
by	O
the	O
Australian	O
Research	O
Council	O
.	O

Overall	O
,	O
we	O
conclude	O
that	O
the	O
DIFFVEC	B-MethodName
approach	O
has	O
impressive	O
utility	O
over	O
a	O
broad	O
range	O
of	O
lexical	O
relations	O
,	O
especially	O
under	O
supervised	B-TaskName
classification	I-TaskName
.	O

Negative	B-MethodName
sampling	I-MethodName
also	O
improves	O
classification	B-TaskName
when	O
the	O
training	O
and	O
test	O
vocabulary	O
are	O
split	O
to	O
minimise	O
lexical	O
memorisation	O
.	O

Classification	B-TaskName
performs	O
less	O
well	O
over	O
open	O
data	O
,	O
although	O
with	O
the	O
introduction	O
of	O
automatically	O
-	O
generated	O
negative	O
samples	O
,	O
the	O
results	O
improve	O
substantially	O
.	O

In	O
contrast	O
,	O
classification	B-TaskName
over	O
the	O
DIFFVECs	B-MethodName
works	O
extremely	O
well	O
in	O
a	O
closed	O
-	O
world	O
setting	O
,	O
showing	O
that	O
dimensions	O
of	O
DIFFVECs	B-MethodName
encode	O
lexical	O
relations	O
.	O

Using	O
clustering	B-TaskName
we	O
showed	O
that	O
many	O
types	O
of	O
morphosyntactic	O
and	O
morphosemantic	O
differences	O
are	O
captured	O
by	O
DIFFVECs	B-MethodName
,	O
but	O
that	O
lexical	O
semantic	O
relations	O
are	O
captured	O
less	O
well	O
,	O
a	O
finding	O
which	O
is	O
consistent	O
with	O
previous	O
work	O
(	O
Köper	O
et	O
al	O
.	O
,	O
2015	O
)	O
.	O

At	O
the	O
maximum	O
level	O
of	O
random	O
word	O
pairs	O
in	O
the	O
test	O
data	O
,	O
the	O
F	B-MetricName
-	I-MetricName
score	I-MetricName
for	O
the	O
negative	O
sampling	O
classifier	O
is	O
higher	O
than	O
for	O
the	O
standard	O
classifier	O
.	O

This	O
benefit	O
comes	O
at	O
the	O
expense	O
of	O
recall	B-MetricName
,	O
which	O
is	O
much	O
lower	O
when	O
negative	O
sampling	O
is	O
used	O
(	O
note	O
that	O
recall	O
stays	O
relatively	O
constant	O
as	O
random	O
word	O
pairs	O
are	O
added	O
,	O
as	O
the	O
vast	O
majority	O
of	O
them	O
do	O
not	O
correspond	O
to	O
any	O
relation	O
)	O
.	O

In	O
comparison	O
,	O
the	O
precision	B-MetricName
when	O
negative	O
sampling	O
is	O
used	O
shows	O
only	O
a	O
small	O
drop	O
-	O
off	O
,	O
indicating	O
that	O
negative	O
sampling	O
is	O
effective	O
at	O
maintaining	O
precision	B-MetricName
in	O
an	O
OPEN	O
-	O
WORLD	O
setting	O
even	O
when	O
the	O
training	O
and	O
test	O
vocabulary	O
are	O
disjoint	O
.	O

Observe	O
that	O
the	O
precision	B-MetricName
for	O
the	O
standard	O
clas	O
-	O
sifier	O
decreases	O
rapidly	O
as	O
more	O
random	O
word	O
pairs	O
are	O
added	O
to	O
the	O
test	O
data	O
.	O

(	O
2015b	O
)	O
recently	O
showed	O
that	O
supervised	O
methods	O
using	O
DIFF	B-MethodName
-	I-MethodName
VECs	I-MethodName
achieve	O
artificially	O
high	O
results	O
as	O
a	O
result	O
of	O
"	O
lexical	O
memorisation	O
"	O
over	O
frequent	O
words	O
asso-	O
ciated	O
with	O
the	O
hypernym	O
relation	O
.	O

The	O
most	O
striking	O
difference	O
in	O
performance	O
was	O
for	O
LEXSEM	O
Mero	O
,	O
where	O
the	O
standard	O
classifier	O
generated	O
many	O
false	O
positive	O
noun	O
pairs	O
(	O
e.g.	O
(	O
series	O
,	O
radio	O
)	O
)	O
,	O
but	O
the	O
false	B-MetricName
positive	I-MetricName
rate	I-MetricName
was	O
considerably	O
reduced	O
with	O
negative	O
sampling	O
.	O

Overall	O
this	O
leads	O
to	O
higher	O
F	B-MetricName
-	I-MetricName
scores	I-MetricName
,	O
as	O
shown	O
in	O
Figure	O
3	O
,	O
other	O
than	O
for	O
hypernyms	O
(	O
LEXSEM	O
Hyper	O
)	O
and	O
prefixes	O
(	O
PREFIX	O
)	O
.	O

This	O
follows	O
from	O
the	O
adversarial	O
training	O
scenario	O
:	O
using	O
negative	O
distractors	O
results	O
in	O
a	O
more	O
conservative	O
classifier	O
,	O
that	O
correctly	O
classifies	O
the	O
vast	O
majority	O
of	O
the	O
random	O
word	O
pairs	O
as	O
not	O
corresponding	O
to	O
a	O
given	O
relation	O
,	O
resulting	O
in	O
higher	O
precision	B-MetricName
at	O
the	O
expense	O
of	O
a	O
small	B-MetricName
drop	O
in	O
recall	O
.	O

The	O
results	O
are	O
much	O
lower	O
than	O
for	O
the	O
closed	O
-	O
word	O
setting	O
(	O
Table	O
4	O
)	O
,	O
most	O
notably	O
in	O
terms	O
of	O
precision	B-MetricName
(	O
P	O
)	O
.	O

We	O
train	O
9	O
binary	B-MethodName
RBF	I-MethodName
-	I-MethodName
kernel	I-MethodName
SVM	I-MethodName
classifiers	I-MethodName
on	O
the	O
training	O
partition	O
,	O
and	O
evaluate	O
on	O
our	O
randomly	O
augmented	O
test	O
set	O
.	O

The	O
test	O
data	O
is	O
augmented	O
with	O
an	O
equal	O
quantity	O
of	O
random	O
pairs	O
,	O
generated	O
as	O
follows	O
:	O
(	O
1	O
)	O
sample	O
a	O
seed	O
lexicon	O
by	O
drawing	O
words	O
proportional	O
to	O
their	O
frequency	O
in	O
Wikipedia	O
;	O
11	O
Table	O
5	O
:	O
Precision	O
(	O
P	O
)	O
and	O
recall	O
(	O
R	O
)	O
for	O
OPEN	O
-	O
WORLD	O
classification	O
,	O
using	O
the	O
binary	O
classifier	O
without	O
(	O
"	O
Orig	O
"	O
)	O
and	O
with	O
(	O
"	O
+	O
neg	O
"	O
)	O
negative	O
samples	O
.	O

This	O
setting	O
aims	O
to	O
illustrate	O
whether	O
a	O
DIFF	B-MethodName
-	I-MethodName
VEC	I-MethodName
-	O
based	O
classifier	O
is	O
capable	O
of	O
differentiating	O
related	O
word	O
pairs	O
from	O
noise	O
,	O
and	O
can	O
be	O
applied	O
to	O
open	O
data	O
to	O
learn	O
new	O
related	O
word	O
pairs	O
.	O

We	O
observe	O
no	O
real	O
difference	O
between	O
w2v	B-MethodName
wiki	I-MethodName
and	O
SVD	B-MethodName
wiki	I-MethodName
,	O
supporting	O
the	O
hypothesis	O
of	O
Levy	O
et	O
al	O
.	O

Somewhat	O
surprisingly	O
,	O
given	O
the	O
small	O
dimensionality	O
of	O
the	O
input	O
(	O
vectors	O
of	O
size	O
300	O
for	O
all	O
three	O
methods	O
)	O
,	O
we	O
found	O
that	O
the	O
linear	B-MethodName
SVM	I-MethodName
slightly	O
outperformed	O
a	O
non	B-MethodName
-	I-MethodName
linear	I-MethodName
SVM	I-MethodName
using	O
an	O
RBF	B-MethodName
kernel	I-MethodName
.	O

The	O
PREFIX	O
relation	O
achieved	O
markedly	O
lower	O
recall	O
,	O
resulting	O
in	O
a	O
lower	O
F	B-MetricName
-	I-MetricName
score	I-MetricName
,	O
due	O
to	O
large	O
differences	O
in	O
the	O
predominant	O
usages	O
associated	O
with	O
the	O
respective	O
words	O
(	O
e.g.	O
,	O
(	O
union	O
,	O
reunion	O
)	O
,	O
where	O
the	O
vector	O
for	O
union	O
is	O
heavily	O
biased	O
by	O
contexts	O
associated	O
with	O
trade	O
unions	O
,	O
but	O
reunion	O
is	O
heavily	O
biased	O
by	O
contexts	O
relating	O
to	O
social	O
get	O
-	O
togethers	O
;	O
and	O
(	O
entry	O
,	O
reentry	O
)	O
,	O
where	O
entry	O
is	O
associated	O
with	O
competitions	O
and	O
entrance	O
to	O
schools	O
,	O
while	O
reentry	O
is	O
associated	O
with	O
space	O
travel	O
)	O
.	O

That	O
is	O
,	O
with	O
a	O
simple	O
linear	O
transformation	O
of	O
the	O
embedding	O
dimensions	O
,	O
we	O
are	O
able	O
to	O
achieve	O
near	O
-	O
perfect	O
results	O
.	O

Most	O
of	O
the	O
relationseven	O
the	O
most	O
difficult	O
ones	O
from	O
our	O
clustering	B-TaskName
experiment	O
-are	O
classified	O
with	O
very	O
high	O
Fscore	B-MetricName
.	O

The	O
SVM	B-MethodName
achieves	O
a	O
higher	O
F	B-MetricName
-	I-MetricName
score	I-MetricName
than	O
the	O
baseline	O
on	O
almost	O
every	O
relation	O
,	O
particularly	O
on	O
LEXSEM	O
Hyper	O
,	O
and	O
the	O
lower	O
-	O
frequency	O
NOUN	O
SP	O
,	O
NOUN	O
Coll	O
,	O
and	O
PREFIX	O
.	O

We	O
use	O
an	O
SVM	B-MethodName
with	O
a	O
linear	O
kernel	O
,	O
and	O
report	O
results	O
from	O
10	B-HyperparameterValue
-	O
fold	B-HyperparameterName
cross	O
-	O
validation	O
in	O
Table	O
4	O
.	O

As	O
a	O
baseline	O
,	O
we	O
cluster	O
the	O
data	O
as	O
described	O
in	O
§	O
4	O
,	O
running	O
the	O
clusterer	O
several	O
times	O
over	O
the	O
9	O
-	O
relation	O
data	O
to	O
select	O
the	O
optimal	O
V	B-MetricName
-	I-MetricName
Measure	I-MetricName
value	O
based	O
on	O
the	O
development	O
data	O
,	O
resulting	O
in	O
50	O
clusters	O
.	O

For	O
the	O
CLOSED	O
-	O
WORLD	O
setting	O
,	O
we	O
train	O
and	O
test	O
a	O
multiclass	O
classifier	O
on	O
datasets	O
comprising	O
DIFFVEC	B-MethodName
,	O
r	O
pairs	O
,	O
where	O
r	O
is	O
one	O
of	O
our	O
nine	O
relation	O
types	O
,	O
and	O
DIFFVEC	B-MethodName
is	O
based	O
on	O
one	O
of	O
w2v	B-MethodName
,	O
w2v	B-MethodName
wiki	I-MethodName
and	O
SVD	B-MethodName
.	O

A	O
natural	O
question	O
is	O
whether	O
we	O
can	O
accurately	O
characterise	B-TaskName
lexical	I-TaskName
relations	I-TaskName
through	O
supervised	O
learning	O
over	O
the	O
DIFFVECs	B-MethodName
.	O
For	O
these	O
experiments	O
we	O
use	O
the	O
w2v	B-MethodName
,	O
w2v	B-MethodName
wiki	I-MethodName
,	O
and	O
SVD	B-MethodName
wiki	I-MethodName
embeddings	O
exclusively	O
(	O
based	O
on	O
their	O
superior	O
performance	O
in	O
the	O
clustering	O
experiment	O
)	O
,	O
and	O
a	O
subset	O
of	O
the	O
relations	O
which	O
is	O
both	O
representative	O
of	O
the	O
breadth	O
of	O
the	O
full	O
relation	O
set	O
,	O
and	O
for	O
which	O
we	O
have	O
sufficient	O
data	O
for	O
supervised	O
training	O
and	O
evaluation	O
,	O
namely	O
:	O
NOUN	O
Coll	O
,	O
LEXSEM	O
Event	O
,	O
LEXSEM	O
Hyper	O
,	O
LEXSEM	O
Mero	O
,	O
NOUN	O
SP	O
,	O
PREFIX	O
,	O
VERB	O
3	O
,	O
VERB	O
3Past	O
,	O
and	O
VERB	O
Past	O
(	O
see	O
Table	O
2	O
)	O
.	O

Classification	B-TaskName
.	O

Given	O
the	O
encouraging	O
results	O
from	O
our	O
clustering	B-TaskName
experiment	O
,	O
we	O
next	O
evaluate	O
DIFFVECs	B-MethodName
in	O
a	O
supervised	B-TaskName
relation	I-TaskName
classification	I-TaskName
setting	O
.	O

This	O
is	O
interesting	O
from	O
a	O
DIFFVEC	B-MethodName
point	O
of	O
view	O
,	O
since	O
it	O
shows	O
that	O
the	O
lexical	O
semantics	O
of	O
one	O
word	O
in	O
the	O
pair	O
can	O
overwhelm	O
the	O
semantic	O
content	O
of	O
the	O
DIFFVEC	B-MethodName
(	O
something	O
that	O
we	O
return	O
to	O
investigate	O
in	O
§	O
5.4	O
)	O
.	O

This	O
polysemy	O
results	O
in	O
the	O
distance	O
represented	O
in	O
the	O
DIFFVEC	B-MethodName
for	O
such	O
pairs	O
being	O
above	O
average	O
for	O
VERB	O
3	O
,	O
and	O
consequently	O
clustered	O
with	O
other	O
cross	O
-	O
POS	O
relations	O
.	O

Considering	O
w2v	B-MethodName
embeddings	O
,	O
for	O
VERB	O
3	O
there	O
was	O
a	O
single	O
cluster	O
consisting	O
of	O
around	O
90	O
%	O
of	O
VERB	O
3	O
word	O
pairs	O
.	O

Looking	O
across	O
the	O
different	O
lexical	O
relation	O
types	O
,	O
the	O
morphosyntactic	O
paradigm	O
relations	O
(	O
NOUN	O
SP	O
and	O
the	O
three	O
VERB	O
relations	O
)	O
are	O
by	O
far	O
the	O
easiest	O
to	O
capture	O
.	O

The	O
results	O
are	O
in	O
Table	O
3	O
,	O
with	O
the	O
lowest	O
entropy	B-MetricName
(	O
purest	O
clustering	O
)	O
for	O
each	O
relation	O
indicated	O
in	O
bold	O
.	O

Since	O
the	O
samples	O
are	O
distributed	O
nonuniformly	O
,	O
we	O
normalise	O
entropy	B-MetricName
results	O
for	O
each	O
method	O
by	O
log(n	O
)	O
where	O
n	B-HyperparameterName
is	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
samples	I-HyperparameterName
in	O
a	O
particular	O
relation	O
.	O

For	O
each	O
embedding	O
method	O
,	O
we	O
present	O
the	O
entropy	B-MetricName
for	O
the	O
cluster	O
size	O
where	O
V	B-MetricName
-	I-MetricName
measure	I-MetricName
was	O
maximised	O
over	O
the	O
development	O
data	O
.	O

We	O
additionally	O
calculated	O
the	O
entropy	B-MetricName
for	O
each	O
lexical	O
relation	O
,	O
based	O
on	O
the	O
distribution	O
of	O
instances	O
belonging	O
to	O
a	O
given	O
relation	O
across	O
the	O
different	O
clusters	O
(	O
and	O
simple	O
MLE	O
)	O
.	O

However	O
,	O
both	O
methods	O
still	O
perform	O
well	O
above	O
SENNA	B-MethodName
and	O
HLBL	B-MethodName
,	O
and	O
w2v	B-MethodName
has	O
a	O
clear	O
empirical	O
advantage	O
over	O
GloVe	B-MethodName
.	O
We	O
note	O
that	O
SVD	B-MethodName
wiki	I-MethodName
performs	O
almost	O
as	O
well	O
as	O
w2v	B-MethodName
wiki	I-MethodName
,	O
consistent	O
with	O
the	O
results	O
of	O
Levy	O
et	O
al	O
.	O

HLBL	B-MethodName
and	O
SENNA	B-MethodName
performed	O
very	O
The	O
lower	O
V	B-MetricName
-	I-MetricName
measure	I-MetricName
for	O
w2v	B-MethodName
wiki	I-MethodName
and	O
GloVe	B-MethodName
wiki	I-MethodName
(	O
as	O
compared	O
to	O
w2v	B-MethodName
and	O
GloVe	B-MethodName
,	O
respectively	O
)	O
indicates	O
that	O
the	O
volume	O
of	O
training	O
data	O
plays	O
a	O
role	O
in	O
the	O
clustering	O
results	O
.	O

GloVe	B-MethodName
and	O
SVD	B-MethodName
mirror	O
this	O
result	O
,	O
but	O
are	O
consistently	O
below	O
w2v	B-MethodName
at	O
a	O
V	B-MetricName
-	I-MetricName
Measure	I-MetricName
of	O
around	O
0.31	B-MetricValue
.	O

8	O
Observe	O
that	O
w2v	B-MethodName
achieves	O
the	O
best	O
results	O
,	O
with	O
a	O
V	B-MetricName
-	I-MetricName
Measure	I-MetricName
value	O
of	O
around	O
0.36	B-MetricValue
,	O
9	O
which	O
is	O
relatively	O
constant	O
over	O
varying	O
numbers	O
of	O
clusters	O
.	O

We	O
show	O
results	O
for	O
different	O
numbers	B-HyperparameterName
of	I-HyperparameterName
clusters	I-HyperparameterName
,	O
from	O
N	B-HyperparameterName
=	O
10	B-HyperparameterValue
in	O
steps	O
of	O
10	O
,	O
up	O
to	O
N	B-HyperparameterName
=	O
80	B-HyperparameterValue
(	O
beyond	O
which	O
the	O
clustering	O
quality	O
diminishes	O
)	O
.	O

Figure	O
2	O
presents	O
V	B-MetricName
-	I-MetricName
Measure	I-MetricName
values	O
over	O
the	O
test	O
data	O
for	O
each	O
of	O
the	O
four	O
word	O
embedding	O
models	O
.	O

Spectral	B-MethodName
clustering	I-MethodName
has	O
two	O
hyperparameters	O
:	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
clusters	I-HyperparameterName
,	O
and	O
the	O
pairwise	B-HyperparameterName
similarity	I-HyperparameterName
measure	I-HyperparameterName
for	O
comparing	O
DIFF	B-MethodName
-	I-MethodName
VECs	I-MethodName
.	O
We	O
tune	O
the	O
hyperparameters	O
over	O
development	O
data	O
,	O
in	O
the	O
form	O
of	O
15	O
%	O
of	O
the	O
data	O
obtained	O
by	O
random	O
sampling	O
,	O
selecting	O
the	O
configuration	O
that	O
maximises	O
the	O
V	B-MetricName
-	I-MetricName
Measure	I-MetricName
(	O
Rosenberg	O
and	O
Hirschberg	O
,	O
2007	O
)	O
.	O

We	O
cluster	O
the	O
DIFFVECs	B-MethodName
between	O
all	O
word	O
pairs	O
in	O
our	O
dataset	O
using	O
spectral	B-MethodName
clustering	I-MethodName
(	O
Von	O
Luxburg	O
,	O
2007	O
)	O
.	O

In	O
order	O
to	O
test	O
these	O
assumptions	O
,	O
we	O
cluster	O
our	O
15	O
-	O
relation	O
closed	O
-	O
world	O
dataset	O
in	O
the	O
first	O
instance	O
,	O
and	O
evaluate	O
against	O
the	O
lexical	O
resources	O
in	O
§	O
3.2	O
.	O
As	O
further	O
motivation	O
,	O
we	O
projected	O
the	O
DIFF	B-MethodName
-	I-MethodName
VEC	I-MethodName
space	O
for	O
a	O
small	O
number	O
of	O
samples	O
of	O
each	O
class	O
using	O
t	O
-	O
SNE	O
(	O
Van	O
der	O
Maaten	O
and	O
Hinton	O
,	O
2008	O
)	O
,	O
and	O
found	O
that	O
many	O
of	O
the	O
morphosyntactic	O
relations	O
(	O
VERB	O
3	O
,	O
VERB	O
Past	O
,	O
VERB	O
3Past	O
,	O
NOUN	O
SP	O
)	O
form	O
tight	O
clusters	O
(	O
Figure	O
1	O
)	O
.	O

Assuming	O
DIFFVECs	B-MethodName
are	O
capable	O
of	O
capturing	O
all	O
lexical	O
relations	O
equally	O
,	O
we	O
would	O
expect	O
clustering	O
to	O
be	O
able	O
to	O
identify	O
sets	O
of	O
word	O
pairs	O
with	O
high	O
relational	O
similarity	O
,	O
or	O
equivalently	O
clusters	O
of	O
similar	O
offset	O
vectors	O
.	O

Clustering	B-TaskName
.	O

(	O
2006a	O
)	O
,	O
Princeton	B-DatasetName
Word	I-DatasetName
-	I-DatasetName
Net	I-DatasetName
(	O
Fellbaum	O
,	O
1998	O
)	O
,	O
Wiktionary	B-DatasetName
,	O
5	O
and	O
a	O
web	B-DatasetName
lexicon	I-DatasetName
of	I-DatasetName
collective	I-DatasetName
nouns	I-DatasetName
,	O
6	O
as	O
listed	O
in	O
Table	O
2	O
.	O

The	O
final	O
dataset	O
consists	O
of	O
12,458	O
triples	O
relation	O
,	O
word	O
1	O
,	O
word	O
2	O
,	O
comprising	O
15	O
relation	O
types	O
,	O
extracted	O
from	O
SemEval'12	B-DatasetName
(	O
Jurgens	O
et	O
al	O
.	O
,	O
2012	O
)	O
,	O
BLESS	B-DatasetName
(	O
Baroni	O
and	O
Lenci	O
,	O
2011	O
)	O
,	O
the	O
MSR	B-DatasetName
analogy	I-DatasetName
dataset	O
(	O
Mikolov	O
et	O
al	O
.	O
,	O
2013c	O
)	O
,	O
the	O
light	B-DatasetName
verb	I-DatasetName
dataset	O
of	O
Tan	O
et	O
al	O
.	O

In	O
order	O
to	O
evaluate	O
the	O
applicability	O
of	O
the	O
DIFF	B-MethodName
-	I-MethodName
VEC	I-MethodName
approach	O
to	O
relations	O
of	O
different	O
types	O
,	O
we	O
assembled	O
a	O
set	O
of	O
lexical	O
relations	O
in	O
three	O
broad	O
categories	O
:	O
lexical	O
semantic	O
relations	O
,	O
morphosyntactic	O
paradigm	O
relations	O
,	O
and	O
morphosemantic	O
relations	O
.	O

For	O
the	O
other	O
models	O
we	O
used	O
the	O
following	O
parameter	O
values	O
:	O
for	O
w2v	B-MethodName
,	O
context	B-HyperparameterName
window	I-HyperparameterName
=	O
8	B-HyperparameterValue
,	O
negative	B-HyperparameterName
samples	I-HyperparameterName
=	O
25	B-HyperparameterValue
,	O
hs	B-HyperparameterName
=	O
0	B-HyperparameterValue
,	O
sample	B-HyperparameterName
=	O
1e-4	B-HyperparameterValue
,	O
and	O
iterations	B-HyperparameterName
=	O
15	B-HyperparameterValue
;	O
and	O
for	O
GloVe	B-MethodName
,	O
context	B-HyperparameterName
window	I-HyperparameterName
=	O
15	B-HyperparameterValue
,	O
x	B-HyperparameterName
max	I-HyperparameterName
=	O
10	B-HyperparameterValue
,	O
and	O
iterations	B-HyperparameterName
=	O
15	B-HyperparameterValue
.	O

(	O
2015a	O
)	O
in	O
setting	O
the	O
context	B-HyperparameterName
window	I-HyperparameterName
size	O
to	O
2	B-HyperparameterValue
,	O
negative	B-HyperparameterName
sampling	I-HyperparameterName
parameter	I-HyperparameterName
to	O
1	B-HyperparameterValue
,	O
eigenvalue	B-HyperparameterName
weighting	I-HyperparameterName
to	O
0.5	B-HyperparameterValue
,	O
and	O
context	B-HyperparameterName
distribution	I-HyperparameterName
smoothing	I-HyperparameterName
to	O
0.75	B-HyperparameterValue
;	O
other	O
parameters	O
were	O
assigned	O
their	O
default	O
values	O
.	O

For	O
the	O
SVD	B-MethodName
model	I-MethodName
,	O
we	O
followed	O
the	O
recommendations	O
of	O
Levy	O
et	O
al	O
.	O

For	O
w2v	B-MethodName
wiki	I-MethodName
,	O
GloVe	B-MethodName
wiki	I-MethodName
and	O
SVD	B-MethodName
wiki	I-MethodName
we	O
used	O
English	B-DatasetName
Wikipedia	I-DatasetName
.	O

(	O
2010	O
)	O
,	O
trained	O
on	O
the	O
Reuters	B-DatasetName
English	I-DatasetName
newswire	I-DatasetName
corpus	O
.	O

For	O
HLBL	B-MethodName
and	O
SENNA	B-MethodName
,	O
we	O
use	O
the	O
pre	O
-	O
trained	O
embeddings	O
from	O
Turian	O
et	O
al	O
.	O

The	O
final	O
model	O
,	O
SENNA	B-MethodName
(	O
Collobert	O
and	O
Weston	O
,	O
2008	O
)	O
,	O
was	O
initially	O
proposed	O
for	O
multi	O
-	O
task	O
training	O
of	O
several	O
language	O
processing	O
tasks	O
,	O
from	O
language	O
modelling	O
through	O
to	O
semantic	O
role	O
labelling	O
.	O

HLBL	B-MethodName
(	O
Mnih	O
and	O
Hinton	O
,	O
2009	O
)	O
is	O
a	O
log	O
-	O
bilinear	O
formulation	O
of	O
an	O
n	O
-	O
gram	O
language	O
model	O
,	O
which	O
predicts	O
the	O
ith	O
word	O
based	O
on	O
context	O
words	O
(	O
i	O
−	O
n	O
,	O
.	O

The	O
SVD	B-MethodName
model	I-MethodName
(	O
Levy	O
et	O
al	O
.	O
,	O
2015a	O
)	O
uses	O
positive	O
pointwise	O
mutual	O
information	O
(	O
PMI	O
)	O
matrix	O
defined	O
as	O
:	O
PPMI(w	O
,	O
c	O
)	O
=	O
max(log	O
P	O
(	O
w	O
,	O
c	O
)	O
P	O
(	O
w	O
)	O
P	O
(	O
c	O
)	O
,	O
0	O
)	O
,	O
where	O
P	O
(	O
w	O
,	O
c	O
)	O
is	O
the	O
joint	O
probability	O
of	O
word	O
w	O
and	O
context	O
c	O
,	O
and	O
P	O
(	O
w	O
)	O
and	O
P	O
(	O
c	O
)	O
are	O
their	O
marginal	O
probabilities	O
.	O

The	O
model	O
was	O
trained	O
on	O
English	B-DatasetName
Wikipedia	I-DatasetName
and	O
the	O
English	B-DatasetName
Gigaword	I-DatasetName
corpus	O
version	O
5	O
.	O

The	O
GloVe	B-MethodName
model	O
(	O
Pennington	O
et	O
al	O
.	O
,	O
2014	O
)	O
is	O
based	O
on	O
a	O
similar	O
bilinear	O
formulation	O
,	O
framed	O
as	O
a	O
low	O
-	O
rank	O
decomposition	O
of	O
the	O
matrix	O
of	O
corpus	O
co	O
-	O
occurrence	O
frequencies	O
:	O
J	O
=	O
1	O
2	O
V	O
i	O
,	O
j=1	O
f	O
(	O
P	O
ij	O
)	O
(	O
w	O
i	O
wj	O
−	O
log	O
P	O
ij	O
)	O
2	O
,	O
where	O
w	O
i	O
is	O
a	O
vector	O
for	O
the	O
left	O
context	O
,	O
w	O
j	O
is	O
a	O
vector	O
for	O
the	O
right	O
context	O
,	O
P	O
ij	O
is	O
the	O
relative	O
frequency	O
of	O
word	O
j	O
in	O
the	O
context	O
of	O
word	O
i	O
,	O
and	O
f	O
is	O
a	O
heuristic	O
weighting	O
function	O
to	O
balance	O
the	O
influence	O
of	O
high	O
versus	O
low	O
term	O
frequencies	O
.	O

2	O
Google	B-DatasetName
News	I-DatasetName
data	O
was	O
used	O
to	O
train	O
the	O
model	O
.	O

1	O
w2v	B-MethodName
CBOW	I-MethodName
(	O
Continuous	O
Bag	O
-	O
Of	O
-	O
Words	O
;	O
Mikolov	O
et	O
al	O
.	O

We	O
additionally	O
normalise	O
the	O
w2v	B-MethodName
wiki	I-MethodName
and	O
SVD	B-MethodName
wiki	I-MethodName
vectors	O
to	O
unit	O
length	O
;	O
GloVe	B-MethodName
wiki	I-MethodName
is	O
natively	O
normalised	O
by	O
column	O
.	O

For	O
consistency	O
of	O
comparison	O
,	O
we	O
train	O
SVD	B-MethodName
as	O
well	O
as	O
a	O
version	O
of	O
w2v	B-MethodName
and	O
GloVe	B-MethodName
(	O
which	O
we	O
call	O
w2v	B-MethodName
wiki	I-MethodName
and	O
GloVe	B-MethodName
wiki	I-MethodName
,	O
respectively	O
)	O
on	O
the	O
English	B-TaskName
Wikipedia	I-TaskName
corpus	O
(	O
comparable	O
in	O
size	O
to	O
the	O
training	O
data	O
of	O
SENNA	B-MethodName
and	O
HLBL	B-MethodName
)	O
,	O
and	O
apply	O
the	O
preprocessing	O
of	O
Levy	O
et	O
al	O
.	O

We	O
also	O
include	O
SVD	B-MethodName
(	O
Levy	O
et	O
al	O
.	O
,	O
2015a	O
)	O
,	O
a	O
count	O
-	O
based	O
model	O
which	O
factorises	O
a	O
positive	O
PMI	O
(	O
PPMI	O
)	O
matrix	O
.	O

We	O
consider	O
four	O
highly	O
successful	O
word	O
embedding	O
models	O
in	O
our	O
experiments	O
:	O
w2v	B-MethodName
(	O
Mikolov	O
et	O
al	O
.	O
,	O
2013a;Mikolov	O
et	O
al	O
.	O
,	O
2013b	O
)	O
,	O
GloVe	B-MethodName
(	O
Pennington	O
et	O
al	O
.	O
,	O
2014	O
)	O
,	O
SENNA	B-MethodName
(	O
Collobert	O
and	O
Weston	O
,	O
2008	O
)	O
,	O
and	O
HLBL	B-MethodName
(	O
Mnih	O
and	O
Hinton	O
,	O
2009	O
)	O
,	O
as	O
detailed	O
below	O
.	O

(	O
2015a	O
)	O
,	O
to	O
test	O
the	O
generalisability	O
of	O
DIFFVECs	B-MethodName
to	O
count	O
-	O
based	O
word	O
embeddings	O
.	O

As	O
the	O
focus	O
of	O
this	O
paper	O
is	O
not	O
the	O
word	O
embedding	O
pre	O
-	O
training	O
approaches	O
so	O
much	O
as	O
the	O
utility	O
of	O
the	O
DIFFVECs	B-MethodName
for	O
lexical	B-TaskName
relation	I-TaskName
learning	I-TaskName
,	O
we	O
take	O
a	O
selection	O
of	O
four	O
pre	O
-	O
trained	O
word	O
embeddings	O
with	O
strong	O
currency	O
in	O
the	O
literature	O
,	O
as	O
detailed	O
in	O
§	O
3.1	O
.	O
We	O
also	O
include	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
count	O
-	O
based	O
approach	O
of	O
Levy	O
et	O
al	O
.	O

Such	O
dimensions	O
could	O
be	O
identified	O
and	O
exploited	O
as	O
part	O
of	O
a	O
clustering	B-TaskName
or	O
classification	B-TaskName
method	O
,	O
in	O
the	O
context	O
of	O
identifying	O
relations	O
between	O
word	O
pairs	O
or	O
classes	O
of	O
DIFFVECs	B-MethodName
.	O
In	O
order	O
to	O
test	O
the	O
generalisability	O
of	O
the	O
DIFF	B-MethodName
-	I-MethodName
VEC	I-MethodName
method	O
,	O
we	O
require	O
:	O
(	O
1	O
)	O
word	O
embeddings	O
,	O
and	O
(	O
2	O
)	O
a	O
set	O
of	O
lexical	O
relations	O
to	O
evaluate	O
against	O
.	O

A	O
second	O
assumption	O
is	O
that	O
there	O
exist	O
dimensions	O
,	O
or	O
directions	O
,	O
in	O
the	O
embedding	O
vector	O
spaces	O
responsible	O
for	O
a	O
particular	O
lexical	O
relation	O
.	O

While	O
a	O
range	O
of	O
methods	O
have	O
been	O
proposed	O
for	O
composing	O
word	O
vectors	O
(	O
Baroni	O
et	O
al	O
.	O
,	O
2012;Weeds	O
et	O
al	O
.	O
,	O
2014;Roller	O
et	O
al	O
.	O
,	O
2014	O
)	O
,	O
in	O
this	O
research	O
we	O
focus	O
exclusively	O
on	O
DIFFVEC	B-MethodName
(	O
i.e.	O
w	O
2	O
−	O
w	O
1	O
)	O
.	O

Our	O
starting	O
point	O
for	O
lexical	B-TaskName
relation	I-TaskName
learning	I-TaskName
is	O
the	O
assumption	O
that	O
important	O
information	O
about	O
various	O
types	O
of	O
relations	O
is	O
implicitly	O
embedded	O
in	O
the	O
offset	O
vectors	O
.	O

We	O
define	O
the	O
task	O
of	O
lexical	B-TaskName
relation	I-TaskName
learning	I-TaskName
to	O
take	O
a	O
set	O
of	O
(	O
ordered	O
)	O
word	O
pairs	O
{	O
(	O
w	O
i	O
,	O
w	O
j	O
)	O
}	O
and	O
a	O
set	O
of	O
binary	O
lexical	O
relations	O
R	O
=	O
{	O
r	O
k	O
}	O
,	O
and	O
map	O
each	O
word	O
pair	O
(	O
w	O
i	O
,	O
w	O
j	O
)	O
as	O
follows	O
:	O
(	O
a	O
)	O
(	O
w	O
i	O
,	O
w	O
j	O
)	O
→	O
r	O
k	O
∈	O
R	O
,	O
i.e.	O
the	O
"	O
closed	O
-	O
world	O
"	O
setting	O
,	O
where	O
we	O
assume	O
that	O
all	O
word	O
pairs	O
can	O
be	O
uniquely	O
classified	O
according	O
to	O
a	O
relation	O
in	O
R	O
;	O
or	O
(	O
b	O
)	O
(	O
w	O
i	O
,	O
w	O
j	O
)	O
→	O
r	O
k	O
∈	O
R	O
∪	O
{	O
φ	O
}	O
where	O
φ	O
signifies	O
the	O
fact	O
that	O
none	O
of	O
the	O
relations	O
in	O
R	O
apply	O
to	O
the	O
word	O
pair	O
in	O
question	O
,	O
i.e.	O
the	O
"	O
open	O
-	O
world	O
"	O
setting	O
.	O

However	O
,	O
their	O
evaluation	O
is	O
performed	O
in	O
the	O
context	O
of	O
relational	O
similarity	O
,	O
and	O
they	O
do	O
not	O
perform	O
clustering	B-TaskName
or	O
classification	B-TaskName
on	O
the	O
DIFFVECs	B-MetricName
.	O
General	O
Approach	O
and	O
Resources	O
.	O

(	O
2013	O
)	O
divide	O
antonym	O
pairs	O
into	O
semantic	O
classes	O
such	O
as	O
quality	O
,	O
time	O
,	O
gender	O
,	O
and	O
distance	O
,	O
finding	O
that	O
for	O
about	O
two	O
-	O
thirds	O
of	O
antonym	O
classes	O
,	O
DIFFVECs	B-MethodName
are	O
significantly	O
more	O
correlated	O
than	O
random	O
.	O

Neural	O
networks	O
have	O
also	O
been	O
developed	O
for	O
joint	O
learning	O
of	O
lexical	O
and	O
relational	O
similarity	O
,	O
making	O
use	O
of	O
the	O
WordNet	O
relation	O
hierarchy	O
(	O
Bordes	O
et	O
al	O
.	O
,	O
2013;Socher	O
et	O
al	O
.	O
,	O
2013;Xu	O
et	O
al	O
.	O
,	O
2014;Yu	O
and	O
Dredze	O
,	O
2014;Faruqui	O
et	O
al	O
.	O
,	O
2015;Fried	O
and	O
Duh	O
,	O
2015	O
)	O
.	O

(	O
2014	O
)	O
similarly	O
use	O
embeddings	O
to	O
predict	O
hypernym	O
relations	O
,	O
in	O
this	O
case	O
clustering	O
words	O
by	O
topic	O
to	O
show	O
that	O
hypernym	O
DIFFVECs	B-MethodName
can	O
be	O
broken	O
down	O
into	O
more	O
fine	O
-	O
grained	O
relations	O
.	O

This	O
has	O
given	O
rise	O
to	O
a	O
series	O
of	O
papers	O
exploring	O
the	O
DIFFVEC	B-MethodName
idea	O
in	O
different	O
contexts	O
.	O

Recently	O
,	O
attention	O
has	O
turned	O
to	O
using	O
vector	O
space	O
models	O
of	O
words	O
for	O
relation	B-TaskName
classification	I-TaskName
and	O
relational	B-TaskName
similarity	I-TaskName
prediction	I-TaskName
.	O

Relation	B-TaskName
learning	I-TaskName
is	O
an	O
important	O
and	O
long	O
-	O
standing	O
task	O
in	O
NLP	O
and	O
has	O
been	O
the	O
focus	O
of	O
a	O
number	O
of	O
shared	O
tasks	O
(	O
Girju	O
et	O
al	O
.	O
,	O
2007;Hendrickx	O
et	O
al	O
.	O
,	O
2010;Jurgens	O
et	O
al	O
.	O
,	O
2012	O
)	O
.	O

On	O
the	O
other	O
hand	O
,	O
relational	B-TaskName
similarity	I-TaskName
prediction	I-TaskName
involves	O
assessing	O
the	O
degree	O
to	O
which	O
a	O
word	O
pair	O
(	O
A	O
,	O
B	O
)	O
stands	O
in	O
the	O
same	O
relation	O
as	O
another	O
pair	O
(	O
C	O
,	O
D	O
)	O
,	O
or	O
to	O
complete	O
an	O
analogy	O
A	O
:	O
B	O
:	O
:	O
C	O
:	O
-?-	O
.	O

In	O
the	O
Open	B-TaskName
Information	I-TaskName
Extraction	I-TaskName
paradigm	O
(	O
Banko	O
et	O
al	O
.	O
,	O
2007;Weikum	O
and	O
Theobald	O
,	O
2010	O
)	O
,	O
also	O
known	O
as	O
unsupervised	B-TaskName
relation	I-TaskName
extraction	I-TaskName
,	O
the	O
relations	O
themselves	O
are	O
also	O
learned	O
from	O
the	O
text	O
(	O
e.g.	O
in	O
the	O
form	O
of	O
text	O
labels	O
)	O
.	O

Given	O
a	O
word	O
pair	O
,	O
the	O
relation	B-TaskName
classification	I-TaskName
task	O
involves	O
assigning	O
a	O
word	O
pair	O
to	O
the	O
correct	O
relation	O
from	O
a	O
pre	O
-	O
defined	O
set	O
.	O

In	O
relation	B-TaskName
extraction	I-TaskName
,	O
related	O
word	O
pairs	O
in	O
a	O
corpus	O
and	O
the	O
relevant	O
relation	O
are	O
identified	O
.	O

Relation	B-TaskName
learning	I-TaskName
in	O
NLP	O
includes	O
relation	B-TaskName
extraction	I-TaskName
,	O
relation	B-TaskName
classification	I-TaskName
,	O
and	O
relational	B-TaskName
similarity	I-TaskName
prediction	I-TaskName
.	O

We	O
also	O
find	O
that	O
hyper	B-MethodName
-	I-MethodName
parameter	I-MethodName
optimised	I-MethodName
count	I-MethodName
-	I-MethodName
based	I-MethodName
methods	I-MethodName
are	O
competitive	O
with	O
predictbased	O
methods	O
under	O
both	O
clustering	O
and	O
supervised	O
relation	O
classification	O
,	O
in	O
line	O
with	O
the	O
findings	O
of	O
Levy	O
et	O
al	O
.	O

Second	O
,	O
we	O
perform	O
classification	B-TaskName
over	O
the	O
DIFF	B-MethodName
-	I-MethodName
VECs	I-MethodName
and	O
obtain	O
remarkably	O
high	O
accuracy	B-MetricName
in	O
a	O
closed	O
-	O
world	O
setting	O
(	O
over	O
a	O
predefined	O
set	O
of	O
word	O
pairs	O
,	O
each	O
of	O
which	O
corresponds	O
to	O
a	O
lexical	O
relation	O
in	O
the	O
training	O
data	O
)	O
.	O

First	O
,	O
we	O
cluster	O
the	O
DIFFVECs	B-MethodName
to	O
test	O
whether	O
the	O
clusters	O
map	O
onto	O
true	O
lexical	O
relations	O
.	O

We	O
then	O
apply	O
DIFFVECs	B-MethodName
to	O
two	O
new	O
tasks	O
:	O
unsupervised	B-TaskName
and	O
supervised	B-TaskName
relation	I-TaskName
extraction	I-TaskName
.	O

Moreover	O
,	O
the	O
task	O
does	O
not	O
explore	O
the	O
full	O
implications	O
of	O
DIFFVECs	B-MethodName
as	O
meaningful	O
vector	O
space	O
objects	O
in	O
their	O
own	O
right	O
,	O
because	O
it	O
only	O
looks	O
for	O
a	O
one	O
-	O
best	O
answer	O
to	O
the	O
particular	O
lexical	O
analogies	O
in	O
the	O
test	O
set	O
.	O

The	O
success	O
of	O
the	O
simple	O
offset	O
method	O
on	O
analogy	O
completion	O
suggests	O
that	O
the	O
difference	O
vectors	O
(	O
"	O
DIFFVEC	B-MethodName
"	O
hereafter	O
)	O
must	O
themselves	O
be	O
meaningful	O
:	O
their	O
direction	O
and/or	O
magnitude	O
encodes	O
a	O
lexical	O
relation	O
.	O

Recently	O
,	O
attention	O
has	O
been	O
focused	O
on	O
identifying	B-TaskName
lexical	I-TaskName
relations	I-TaskName
using	O
word	O
embeddings	O
,	O
which	O
are	O
dense	O
,	O
low	O
-	O
dimensional	O
vectors	O
obtained	O
either	O
from	O
a	O
"	O
predict	O
-	O
based	O
"	O
neural	O
network	O
trained	O
to	O
predict	O
word	O
contexts	O
,	O
or	O
a	O
"	O
countbased	O
"	O
traditional	O
distributional	O
similarity	O
method	O
combined	O
with	O
dimensionality	O
reduction	O
.	O

Learning	O
to	O
identify	B-TaskName
lexical	I-TaskName
relations	I-TaskName
is	O
a	O
fundamental	O
task	O
in	O
natural	O
language	O
processing	O
(	O
"	O
NLP	O
"	O
)	O
,	O
and	O
can	O
contribute	O
to	O
many	O
NLP	O
applications	O
including	O
paraphrasing	O
and	O
generation	O
,	O
machine	O
translation	O
,	O
and	O
ontology	O
building	O
(	O
Banko	O
et	O
al	O
.	O
,	O
2007;Hendrickx	O
et	O
al	O
.	O
,	O
2010	O
)	O
.	O

Take	O
and	O
Took	O
,	O
Gaggle	O
and	O
Goose	O
,	O
Book	O
and	O
Read	O
:	O
Evaluating	O
the	O
Utility	O
of	O
Vector	O
Differences	O
for	O
Lexical	B-TaskName
Relation	I-TaskName
Learning	I-TaskName
.	O

LR	O
was	O
supported	O
by	O
EPSRC	O
grant	O
EP	O
/	O
I037512/1	O
and	O
ERC	O
Starting	O
Grant	O
DisCoTex	O
(	O
306920	O
)	O
.	O

Acknowledgments	O
.	O

This	O
paper	O
is	O
the	O
first	O
to	O
test	O
the	O
generalisability	O
of	O
the	O
vector	O
difference	O
approach	O
across	O
a	O
broad	O
range	O
of	O
lexical	O
relations	O
(	O
in	O
raw	O
number	O
and	O
also	O
variety	O
)	O
.	O

Conclusions	O
.	O

The	O
results	O
are	O
shown	O
in	O
Figure	O
4	O
.	O

We	O
then	O
train	O
classifiers	O
with	O
and	O
without	O
negative	O
sampling	O
(	O
§	O
5.3	O
)	O
,	O
incrementally	O
adding	O
the	O
random	O
word	O
pairs	O
from	O
§	O
5.2	O
to	O
the	O
test	O
data	O
(	O
from	O
no	O
random	O
word	O
pairs	O
to	O
five	O
times	O
the	O
original	O
size	O
of	O
the	O
test	O
data	O
)	O
to	O
investigate	O
the	O
interaction	O
of	O
negative	O
sampling	O
with	O
greater	O
diversity	O
in	O
the	O
test	O
set	O
when	O
there	O
is	O
a	O
split	O
vocabulary	O
.	O

(	O
2015b	O
)	O
in	O
splitting	O
our	O
vocabulary	O
into	O
training	O
and	O
test	O
partitions	O
,	O
to	O
ensure	O
there	O
is	O
no	O
overlap	O
between	O
training	O
and	O
test	O
vocabulary	O
.	O

To	O
address	O
this	O
effect	O
,	O
we	O
follow	O
Levy	O
et	O
al	O
.	O

Weeds	O
.	O

Lexical	O
Memorisation	O
.	O

For	O
example	O
,	O
(	O
animal	O
,	O
cat	O
)	O
,	O
(	O
animal	O
,	O
dog	O
)	O
,	O
and	O
(	O
animal	O
,	O
pig	O
)	O
all	O
share	O
the	O
superclass	O
animal	O
,	O
and	O
the	O
model	O
thus	O
learns	O
to	O
classify	O
as	O
positive	O
any	O
word	O
pair	O
with	O
animal	O
as	O
the	O
first	O
word	O
.	O

(	O
2014	O
)	O
and	O
Levy	O
et	O
al	O
.	O

et	O
al	O
.	O

The	O
classifier	O
was	O
able	O
to	O
capture	O
(	O
herd	O
,	O
horses	O
)	O
but	O
not	O
(	O
run	O
,	O
salmon	O
)	O
,	O
(	O
party	O
,	O
jays	O
)	O
or	O
(	O
singular	O
,	O
boar	O
)	O
as	O
instances	O
of	O
NOUN	O
Coll	O
,	O
possibly	O
because	O
of	O
polysemy	O
.	O

For	O
example	O
,	O
the	O
standard	O
classifier	O
for	O
NOUN	O
Coll	O
learned	O
to	O
match	O
word	O
pairs	O
including	O
an	O
animal	O
name	O
(	O
e.g.	O
,	O
(	O
plague	O
,	O
rats	O
)	O
)	O
,	O
while	O
training	O
with	O
negative	O
samples	O
resulted	O
in	O
much	O
more	O
conservative	O
predictions	O
and	O
consequently	O
much	O
lower	O
recall	O
.	O

Observe	O
that	O
the	O
precision	O
is	O
much	O
higher	O
and	O
recall	O
somewhat	O
lower	O
compared	O
to	O
the	O
classifier	O
trained	O
with	O
only	O
positive	O
samples	O
.	O

12	O
The	O
results	O
are	O
shown	O
in	O
the	O
right	O
half	O
of	O
Table	O
5	O
(	O
as	O
"	O
+	O
neg	O
"	O
)	O
.	O

After	O
training	O
our	O
classifier	O
,	O
we	O
evaluate	O
its	O
predictions	O
in	O
the	O
same	O
way	O
as	O
in	O
§	O
5.2	O
,	O
using	O
the	O
same	O
test	O
set	O
combining	O
related	O
and	O
random	O
word	O
pairs	O
.	O

Both	O
types	O
of	O
distractors	O
are	O
added	O
to	O
the	O
training	O
set	O
,	O
such	O
that	O
there	O
are	O
equal	O
numbers	O
of	O
valid	O
relations	O
,	O
opposite	O
pairs	O
and	O
shuffled	O
pairs	O
.	O

This	O
is	O
targeted	O
at	O
relations	O
that	O
take	O
specific	O
word	O
classes	O
in	O
particular	O
positions	O
,	O
e.g.	O
,	O
(	O
VB	O
,	O
VBD	O
)	O
word	O
pairs	O
,	O
so	O
that	O
the	O
model	O
learns	O
to	O
encode	O
the	O
relation	O
rather	O
than	O
simply	O
learning	O
the	O
properties	O
of	O
the	O
word	O
classes	O
.	O

shuffled	O
pairs	O
:	O
generated	O
by	O
replacing	O
w	O
2	O
with	O
a	O
random	O
word	O
w	O
2	O
from	O
the	O
same	O
relation	O
,	O
Shuff	O
w1	O
,	O
w2	O
=	O
word	O
2	O
−	O
word	O
1	O
.	O

This	O
ensures	O
the	O
classifier	O
adequately	O
captures	O
the	O
asymmetry	O
in	O
the	O
relations	O
.	O

distractors	O
:	O
opposite	O
pairs	O
:	O
generated	O
by	O
switching	O
the	O
order	O
of	O
word	O
pairs	O
,	O
Oppos	O
w1	O
,	O
w2	O
=	O
word	O
1	O
−	O
word	O
2	O
.	O

To	O
this	O
end	O
,	O
we	O
automatically	O
generated	O
two	O
types	O
of	O
negative	O
dings	O
.	O

The	O
basic	O
intuition	O
behind	O
this	O
approach	O
is	O
to	O
construct	O
samples	O
which	O
will	O
force	O
the	O
model	O
to	O
learn	O
decision	O
boundaries	O
that	O
more	O
tightly	O
capture	O
the	O
true	O
scope	O
of	O
a	O
given	O
relation	O
.	O

To	O
address	O
the	O
problem	O
of	O
incorrectly	O
classifying	O
random	O
word	O
pairs	O
as	O
valid	O
relations	O
,	O
we	O
retrain	O
the	O
classifier	O
on	O
a	O
dataset	O
comprising	O
both	O
valid	O
and	O
automatically	O
-	O
generated	O
negative	O
distractor	O
samples	O
.	O

OPEN	O
-	O
WORLD	O
Training	O
with	O
Negative	O
Sampling	O
.	O

That	O
is	O
,	O
the	O
model	O
captures	O
syntax	O
,	O
but	O
lacks	O
the	O
ability	O
to	O
capture	O
lexical	O
paradigms	O
,	O
and	O
tends	O
to	O
overgenerate	O
.	O

For	O
instance	O
,	O
the	O
random	O
pairs	O
(	O
have	O
,	O
works	O
)	O
,	O
(	O
turn	O
,	O
took	O
)	O
,	O
and	O
(	O
works	O
,	O
started	O
)	O
were	O
incorrectly	O
classified	O
as	O
VERB	O
3	O
,	O
VERB	O
Past	O
and	O
VERB	O
3Past	O
,	O
respectively	O
.	O

The	O
results	O
of	O
our	O
experiments	O
are	O
presented	O
in	O
the	O
left	O
half	O
of	O
Table	O
5	O
,	O
in	O
which	O
we	O
report	O
on	O
results	O
over	O
the	O
combination	O
of	O
the	O
original	O
test	O
data	O
from	O
§	O
5.1	O
and	O
the	O
random	O
word	O
pairs	O
,	O
noting	O
that	O
recall	O
(	O
R	O
)	O
for	O
OPEN	O
-	O
WORLD	O
takes	O
the	O
form	O
of	O
relative	O
recall	O
(	O
Pantel	O
et	O
al	O
.	O
,	O
2004	O
)	O
over	O
the	O
positively	O
-	O
classified	O
word	O
pairs	O
.	O

Fully	O
annotating	O
our	O
random	O
word	O
pairs	O
is	O
prohibitively	O
expensive	O
,	O
so	O
instead	O
,	O
we	O
manually	O
annotated	O
only	O
the	O
word	O
pairs	O
which	O
were	O
positively	O
classified	O
by	O
one	O
of	O
our	O
models	O
.	O

This	O
procedure	O
generates	O
word	O
pairs	O
that	O
are	O
representative	O
of	O
the	O
frequency	O
profile	O
of	O
our	O
corpus	O
.	O

(	O
2	O
)	O
take	O
the	O
Cartesian	O
product	O
over	O
pairs	O
of	O
words	O
from	O
the	O
seed	O
lexicon	O
;	O
(	O
3	O
)	O
sample	O
word	O
pairs	O
uniformly	O
from	O
this	O
set	O
.	O

10	O
For	O
these	O
experiments	O
,	O
we	O
train	O
a	O
binary	O
classifier	O
for	O
each	O
relation	O
type	O
,	O
using	O
2	O
3	O
of	O
our	O
relation	O
data	O
for	O
training	O
and	O
1	O
3	O
for	O
testing	O
.	O

We	O
now	O
turn	O
to	O
a	O
more	O
challenging	O
evaluation	O
setting	O
:	O
a	O
test	O
set	O
including	O
word	O
pairs	O
drawn	O
at	O
random	O
.	O

OPEN	O
-	O
WORLD	O
Classification	O
.	O

The	O
impact	O
of	O
the	O
training	O
data	O
volume	O
for	O
pre	O
-	O
training	O
of	O
the	O
embeddings	O
is	O
also	O
less	O
pronounced	O
than	O
in	O
the	O
case	O
of	O
our	O
clustering	O
experiment	O
.	O

(	O
2015a	O
)	O
that	O
under	O
appropriate	O
parameter	O
settings	O
,	O
count	O
-	O
based	O
methods	O
achieve	O
high	O
results	O
.	O

We	O
label	O
each	O
cluster	O
with	O
the	O
majority	O
class	O
based	O
on	O
the	O
training	O
instances	O
,	O
and	O
evaluate	O
the	O
resultant	O
labelling	O
for	O
the	O
test	O
instances	O
.	O

CLOSED	O
-	O
WORLD	O
Classification	O
.	O

(	O
2015b	O
)	O
for	O
hypernyms	O
,	O
by	O
experimenting	O
with	O
disjoint	O
training	O
and	O
test	O
vocabulary	O
.	O

(	O
2014	O
)	O
and	O
Levy	O
et	O
al	O
.	O

For	O
both	O
settings	O
,	O
we	O
further	O
investigate	O
whether	O
there	O
is	O
a	O
lexical	O
memorisation	O
effect	O
for	O
a	O
broad	O
range	O
of	O
relation	O
types	O
of	O
the	O
sort	O
identified	O
by	O
Weeds	O
et	O
al	O
.	O

We	O
consider	O
two	O
applications	O
:	O
(	O
1	O
)	O
a	O
CLOSED	O
-	O
WORLD	O
setting	O
similar	O
to	O
the	O
unsupervised	O
evaluation	O
,	O
in	O
which	O
the	O
classifier	O
only	O
encounters	O
word	O
pairs	O
which	O
correspond	O
to	O
one	O
of	O
the	O
nine	O
relations	O
;	O
and	O
(	O
2	O
)	O
a	O
more	O
challenging	O
OPEN	O
-	O
WORLD	O
setting	O
where	O
random	O
word	O
pairs	O
-which	O
may	O
or	O
may	O
not	O
correspond	O
to	O
one	O
of	O
our	O
relations	O
-are	O
included	O
in	O
the	O
evaluation	O
.	O

LEXSEM	O
Mero	O
was	O
also	O
split	O
into	O
multiple	O
clusters	O
along	O
topical	O
lines	O
,	O
with	O
separate	O
clusters	O
for	O
weapons	O
,	O
dwellings	O
,	O
vehicles	O
,	O
etc	O
.	O

A	O
related	O
phenomenon	O
was	O
observed	O
for	O
NOUN	O
Coll	O
,	O
where	O
the	O
instances	O
were	O
assigned	O
to	O
a	O
large	O
mixed	O
cluster	O
containing	O
word	O
pairs	O
where	O
the	O
second	O
word	O
referred	O
to	O
an	O
animal	O
,	O
reflecting	O
the	O
fact	O
that	O
most	O
of	O
the	O
collective	O
nouns	O
in	O
our	O
dataset	O
relate	O
to	O
animals	O
,	O
e.g.	O
(	O
stand	O
,	O
horse	O
)	O
,	O
(	O
ambush	O
,	O
tigers	O
)	O
,	O
(	O
antibiotics	O
,	O
bacteria	O
)	O
.	O

Here	O
,	O
the	O
noun	O
saw	O
is	O
ambiguous	O
with	O
a	O
high	O
-	O
frequency	O
past	O
-	O
tense	O
verb	O
;	O
hurt	O
and	O
wipe	O
also	O
have	O
ambigous	O
POS	O
.	O

For	O
VERB	O
Past	O
,	O
a	O
single	O
relatively	O
pure	O
cluster	O
was	O
generated	O
,	O
with	O
minor	O
contamination	O
due	O
to	O
pairs	O
such	O
as	O
(	O
hurt	O
,	O
saw	O
)	O
,	O
(	O
utensil	O
,	O
saw	O
)	O
,	O
and	O
(	O
wipe	O
,	O
saw	O
)	O
.	O

Example	O
VERB	O
3	O
pairs	O
incorrectly	O
clustered	O
are	O
:	O
(	O
study	O
,	O
studies	O
)	O
,	O
(	O
run	O
,	O
runs	O
)	O
,	O
and	O
(	O
like	O
,	O
likes	O
)	O
.	O

Most	O
errors	O
resulted	O
from	O
POS	O
ambiguity	O
,	O
leading	O
to	O
confusion	O
with	O
VERB	O
-	O
NOUN	O
in	O
particular	O
.	O

The	O
lexical	O
semantic	O
relations	O
,	O
on	O
the	O
other	O
hand	O
,	O
are	O
the	O
hardest	O
to	O
capture	O
for	O
all	O
embeddings	O
.	O

(	O
2015a	O
)	O
.	O

Under	O
the	O
additional	O
assumption	O
that	O
a	O
given	O
word	O
pair	O
corresponds	O
to	O
a	O
unique	O
lexical	O
relation	O
(	O
in	O
line	O
with	O
our	O
definition	O
of	O
the	O
lexical	O
relation	O
learning	O
task	O
in	O
§	O
3	O
)	O
,	O
a	O
hard	O
clustering	O
approach	O
is	O
appropriate	O
.	O

7	O
.	O

We	O
manually	O
filtered	O
the	O
data	O
to	O
remove	O
duplicates	O
(	O
e.g.	O
,	O
as	O
part	O
of	O
merging	O
the	O
two	O
sources	O
of	O
LEXSEM	O
Hyper	O
intances	O
)	O
,	O
and	O
normalise	O
directionality	O
.	O

(	O
2013c	O
)	O
,	O
but	O
we	O
include	O
a	O
much	O
wider	O
range	O
of	O
lexical	O
semantic	O
relations	O
,	O
especially	O
those	O
standardly	O
evaluated	O
in	O
the	O
relation	O
classification	O
literature	O
.	O

There	O
is	O
some	O
overlap	O
between	O
our	O
relations	O
and	O
those	O
included	O
in	O
the	O
analogy	O
task	O
of	O
Mikolov	O
et	O
al	O
.	O

We	O
additionally	O
constrained	O
the	O
dataset	O
to	O
the	O
words	O
occurring	O
in	O
all	O
embedding	O
sets	O
.	O

4	O
Consequently	O
we	O
excluded	O
symmetric	O
lexical	O
relations	O
such	O
as	O
synonymy	O
.	O

We	O
constrained	O
the	O
relations	O
to	O
be	O
binary	O
and	O
to	O
have	O
fixed	O
directionality	O
.	O

Lexical	O
Relations	O
.	O

During	O
the	O
training	O
phase	O
,	O
for	O
each	O
model	O
we	O
set	O
a	O
word	O
frequency	O
threshold	O
of	O
5	O
.	O

(	O
2015a	O
)	O
,	O
3	O
i.e.	O
,	O
lower	O
-	O
cased	O
all	O
words	O
and	O
removed	O
non	O
-	O
textual	O
elements	O
.	O

We	O
followed	O
the	O
same	O
preprocessing	O
procedure	O
described	O
in	O
Levy	O
et	O
al	O
.	O

In	O
both	O
cases	O
,	O
the	O
embeddings	O
were	O
scaled	O
by	O
the	O
global	O
standard	O
deviation	O
over	O
the	O
word	O
-	O
embedding	O
matrix	O
,	O
W	O
scaled	O
=	O
0.1	O
×	O
W	O
σ(W	O
)	O
.	O

,	O
w	O
i−1	O
,	O
w	O
k	O
)	O
,	O
where	O
the	O
last	O
c	O
−	O
1	O
words	O
are	O
used	O
as	O
context	O
,	O
and	O
f	O
(	O
x	O
)	O
is	O
a	O
non	O
-	O
linear	O
function	O
of	O
the	O
input	O
,	O
defined	O
as	O
a	O
multi	O
-	O
layer	O
perceptron	O
.	O

,	O
w	O
i−1	O
,	O
w	O
i	O
)	O
+	O
f	O
(	O
w	O
i−c	O
,	O
.	O

Here	O
we	O
focus	O
on	O
the	O
statistical	O
language	O
modelling	O
component	O
,	O
which	O
has	O
a	O
pairwise	O
ranking	O
objective	O
to	O
maximise	O
the	O
relative	O
score	O
of	O
each	O
word	O
in	O
its	O
local	O
context	O
:	O
J	O
=	O
1	O
T	O
T	O
i=1	O
V	O
k=1	O
max	O
0	O
,	O
1	O
−	O
f	O
(	O
w	O
i−c	O
,	O
.	O

where	O
wi	O
=	O
n−1	O
j=1	O
C	O
j	O
w	O
i−j	O
is	O
the	O
context	O
embedding	O
,	O
C	O
j	O
is	O
a	O
scaling	O
matrix	O
,	O
and	O
b	O
*	O
is	O
a	O
bias	O
term	O
.	O

This	O
leads	O
to	O
the	O
following	O
training	O
objective	O
:	O
J	O
=	O
1	O
T	O
T	O
i=1	O
exp	O
(	O
w	O
i	O
w	O
i	O
+	O
b	O
i	O
)	O
V	O
k=1	O
exp	O
(	O
w	O
i	O
w	O
k	O
+	O
b	O
k	O
)	O
,	O
duty	O
,	O
denoting	O
either	O
the	O
embedding	O
for	O
the	O
ith	O
token	O
,	O
wi	O
,	O
or	O
kth	O
word	O
type	O
,	O
w	O
k	O
.	O

,	O
i	O
−	O
2	O
,	O
i	O
−	O
1	O
)	O
.	O

The	O
matrix	O
is	O
factorised	O
by	O
singular	O
value	O
decomposition	O
.	O

We	O
use	O
the	O
focus	O
word	O
vectors	O
,	O
W	O
=	O
{	O
w	O
k	O
}	O
V	O
k=1	O
,	O
normalised	O
such	O
that	O
each	O
w	O
k	O
=	O
1	O
.	O

(	O
2013a	O
)	O
)	O
predicts	O
a	O
word	O
from	O
its	O
context	O
using	O
a	O
model	O
with	O
the	O
objective	O
:	O
J	O
=	O
1	O
T	O
T	O
i=1	O
log	O
exp	O
w	O
i	O
j∈[−c,+c],j	O
=	O
0	O
wi+j	O
V	O
k=1	O
exp	O
w	O
k	O
j∈[−c,+c],j	O
=	O
0	O
wi+j	O
where	O
w	O
i	O
and	O
wi	O
are	O
the	O
vector	O
representations	O
for	O
the	O
ith	O
word	O
(	O
as	O
a	O
focus	O
or	O
context	O
word	O
,	O
respectively	O
)	O
,	O
V	O
is	O
the	O
vocabulary	O
size	O
,	O
T	O
is	O
the	O
number	O
of	O
tokens	O
in	O
the	O
corpus	O
,	O
and	O
c	O
is	O
the	O
context	O
window	O
size	O
.	O

(	O
2015a	O
)	O
.	O

Word	O
Embeddings	O
.	O

Dimensions	O
.	O

Name	O
.	O

(	O
2013c	O
)	O
)	O
,	O
but	O
also	O
including	O
morphosyntactic	O
and	O
morphosemantic	O
relations	O
(	O
see	O
§	O
3.2	O
)	O
.	O

To	O
this	O
end	O
,	O
we	O
construct	O
a	O
dataset	O
from	O
a	O
variety	O
of	O
sources	O
,	O
focusing	O
on	O
lexical	O
semantic	O
relations	O
(	O
which	O
are	O
less	O
well	O
represented	O
in	O
the	O
analogy	O
dataset	O
of	O
Mikolov	O
et	O
al	O
.	O

For	O
the	O
lexical	O
relations	O
,	O
we	O
want	O
a	O
range	O
of	O
relations	O
that	O
is	O
representative	O
of	O
the	O
types	O
of	O
relational	O
learning	O
tasks	O
targeted	O
in	O
the	O
literature	O
,	O
and	O
where	O
there	O
is	O
availability	O
of	O
annotated	O
data	O
.	O

(	O
2015	O
)	O
is	O
somewhat	O
more	O
constrained	O
than	O
the	O
set	O
we	O
use	O
,	O
there	O
is	O
a	O
good	O
deal	O
of	O
overlap	O
.	O

Although	O
the	O
set	O
of	O
relations	O
tested	O
by	O
Köper	O
et	O
al	O
.	O

They	O
test	O
a	O
variety	O
of	O
relations	O
including	O
word	O
similarity	O
,	O
antonyms	O
,	O
synonyms	O
,	O
hypernyms	O
,	O
and	O
meronyms	O
,	O
in	O
a	O
novel	O
analogy	O
task	O
.	O

(	O
2015	O
)	O
undertake	O
a	O
systematic	O
study	O
of	O
morphosyntactic	O
and	O
semantic	O
relations	O
on	O
word	O
embeddings	O
produced	O
with	O
word2vec	O
(	O
"	O
w2v	O
"	O
hereafter	O
;	O
see	O
§	O
3.1	O
)	O
for	O
English	O
and	O
German	O
.	O

Köper	O
et	O
al	O
.	O

Roller	O
and	O
Erk	O
(	O
2016	O
)	O
analyse	O
the	O
performance	O
of	O
vector	O
concatenation	O
and	O
difference	O
on	O
the	O
task	O
of	O
predicting	O
lexical	O
entailment	O
and	O
show	O
that	O
vector	O
concatenation	O
overwhelmingly	O
learns	O
to	O
detect	O
Hearst	O
patterns	O
(	O
e.g.	O
,	O
including	O
,	O
such	O
as	O
)	O
.	O

(	O
2015	O
)	O
train	O
a	O
classifier	O
on	O
word	O
pairs	O
,	O
using	O
word	O
embeddings	O
to	O
predict	O
coordinates	O
,	O
hypernyms	O
,	O
and	O
meronyms	O
.	O

Necs	O
¸ulescu	O
et	O
al	O
.	O

Makrai	O
et	O
al	O
.	O

However	O
,	O
there	O
has	O
been	O
no	O
systematic	O
investigation	O
of	O
the	O
range	O
of	O
relations	O
for	O
which	O
the	O
vector	O
difference	O
method	O
is	O
most	O
effective	O
,	O
although	O
there	O
have	O
been	O
some	O
smallerscale	O
investigations	O
in	O
this	O
direction	O
.	O

Another	O
strand	O
of	O
work	O
responding	O
to	O
the	O
vector	O
difference	O
approach	O
has	O
analysed	O
the	O
structure	O
of	O
predict	O
-	O
based	O
embedding	O
models	O
in	O
order	O
to	O
help	O
explain	O
their	O
success	O
on	O
the	O
analogy	O
and	O
other	O
tasks	O
(	O
Levy	O
and	O
Goldberg	O
,	O
2014a;Levy	O
and	O
Goldberg	O
,	O
2014b;Arora	O
et	O
al	O
.	O
,	O
2015	O
)	O
.	O

Fu	O
et	O
al	O
.	O

Kim	O
and	O
de	O
Marneffe	O
(	O
2013	O
)	O
use	O
word	O
embeddings	O
to	O
derive	O
representations	O
of	O
adjective	O
scales	O
,	O
e.g.	O
hot	O
-	O
warm	O
-	O
coolcold	O
.	O

(	O
2013	O
)	O
,	O
who	O
combine	O
a	O
neural	O
language	O
model	O
with	O
a	O
pattern	O
-	O
based	O
classifier	O
.	O

The	O
original	O
analogy	O
dataset	O
has	O
been	O
used	O
to	O
evaluate	O
predict	O
-	O
based	O
language	O
models	O
by	O
Mnih	O
and	O
Kavukcuoglu	O
(	O
2013	O
)	O
and	O
also	O
Zhila	O
et	O
al	O
.	O

An	O
exciting	O
development	O
,	O
and	O
the	O
inspiration	O
for	O
this	O
paper	O
,	O
has	O
been	O
the	O
demonstration	O
that	O
vector	O
difference	O
over	O
word	O
embeddings	O
(	O
Mikolov	O
et	O
al	O
.	O
,	O
2013c	O
)	O
can	O
be	O
used	O
to	O
model	O
word	O
analogy	O
tasks	O
.	O

Distributional	O
word	O
vectors	O
have	O
been	O
used	O
for	O
detection	O
of	O
relations	O
such	O
as	O
hypernymy	O
(	O
Geffet	O
and	O
Dagan	O
,	O
2005;Kotlerman	O
et	O
al	O
.	O
,	O
2010;Lenci	O
and	O
Benotto	O
,	O
2012;Weeds	O
et	O
al	O
.	O
,	O
2014;Rimell	O
,	O
2014;Santus	O
et	O
al	O
.	O
,	O
2014	O
)	O
and	O
qualia	O
structure	O
(	O
Yamada	O
et	O
al	O
.	O
,	O
2009	O
)	O
.	O

A	O
lexical	O
relation	O
is	O
a	O
binary	O
relation	O
r	O
holding	O
between	O
a	O
word	O
pair	O
(	O
w	O
i	O
,	O
w	O
j	O
)	O
;	O
for	O
example	O
,	O
the	O
pair	O
(	O
cart	O
,	O
wheel	O
)	O
stands	O
in	O
the	O
WHOLE	O
-	O
PART	O
relation	O
.	O

Background	O
and	O
Related	O
Work	O
.	O

(	O
2015a	O
)	O
.	O

We	O
find	O
that	O
this	O
improves	O
the	O
model	O
performance	O
substantially	O
.	O

We	O
then	O
investigate	O
methods	O
for	O
better	O
attuning	O
the	O
learned	O
class	O
representation	O
to	O
the	O
lexical	O
relations	O
,	O
focusing	O
on	O
methods	O
for	O
automatically	O
synthesising	O
negative	O
instances	O
.	O

When	O
we	O
move	O
to	O
an	O
open	O
-	O
world	O
setting	O
including	O
random	O
word	O
pairs	O
-many	O
of	O
which	O
do	O
not	O
correspond	O
to	O
any	O
lexical	O
relation	O
in	O
the	O
training	O
data	O
-the	O
results	O
are	O
poor	O
.	O

We	O
find	O
that	O
the	O
clustering	O
works	O
remarkably	O
well	O
,	O
although	O
syntactic	O
relations	O
are	O
captured	O
better	O
than	O
semantic	O
ones	O
.	O

In	O
this	O
paper	O
,	O
we	O
introduce	O
a	O
new	O
,	O
larger	O
dataset	O
covering	O
many	O
well	O
-	O
known	O
lexical	O
relation	O
types	O
from	O
the	O
linguistics	O
and	O
cognitive	O
science	O
literature	O
.	O

Previous	O
analogy	O
completion	O
tasks	O
used	O
with	O
word	O
embeddings	O
have	O
limited	O
coverage	O
of	O
lexical	O
relation	O
types	O
.	O

For	O
example	O
,	O
the	O
paris	O
−	O
france	O
vector	O
appears	O
to	O
encode	O
CAPITAL	O
-	O
OF	O
,	O
presumably	O
by	O
cancelling	O
out	O
the	O
features	O
of	O
paris	O
that	O
are	O
France	O
-	O
specific	O
,	O
and	O
retaining	O
the	O
features	O
that	O
distinguish	O
a	O
capital	O
city	O
(	O
Levy	O
and	O
Goldberg	O
,	O
2014a	O
)	O
.	O

The	O
key	O
operation	O
in	O
these	O
models	O
is	O
vector	O
difference	O
,	O
or	O
vector	O
offset	O
.	O

Remarkably	O
,	O
since	O
the	O
model	O
is	O
not	O
trained	O
for	O
this	O
task	O
,	O
the	O
relational	O
structure	O
of	O
the	O
vector	O
space	O
appears	O
to	O
be	O
an	O
emergent	O
property	O
.	O

The	O
results	O
extend	O
to	O
several	O
semantic	O
relations	O
such	O
as	O
CAPITAL	O
-	O
OF	O
(	O
paris−france+poland	O
≈	O
warsaw	O
)	O
and	O
morphosyntactic	O
relations	O
such	O
as	O
PLURALISATION	O
(	O
cars	O
−	O
car	O
+	O
apple	O
≈	O
apples	O
)	O
.	O

A	O
well	O
-	O
known	O
example	O
involves	O
predicting	O
the	O
vector	O
queen	O
from	O
the	O
vector	O
combination	O
king	O
−	O
man	O
+	O
woman	O
,	O
where	O
linear	O
operations	O
on	O
word	O
vectors	O
appear	O
to	O
capture	O
the	O
lexical	O
relation	O
governing	O
the	O
analogy	O
,	O
in	O
this	O
case	O
OPPOSITE	O
-	O
GENDER	O
.	O

(	O
2013a	O
)	O
and	O
other	O
similar	O
language	O
models	O
have	O
been	O
shown	O
to	O
perform	O
well	O
on	O
an	O
analogy	O
completion	O
task	O
(	O
Mikolov	O
et	O
al	O
.	O
,	O
2013b;Mikolov	O
et	O
al	O
.	O
,	O
2013c;Levy	O
and	O
Goldberg	O
,	O
2014a	O
)	O
,	O
in	O
the	O
space	O
of	O
relational	O
sim	O
-	O
ilarity	O
prediction	O
(	O
Turney	O
,	O
2006	O
)	O
,	O
where	O
the	O
task	O
is	O
to	O
predict	O
the	O
missing	O
word	O
in	O
analogies	O
such	O
as	O
A	O
:	O
B	O
:	O
:	O
C	O
:	O
-?-	O
.	O

The	O
skipgram	O
model	O
of	O
Mikolov	O
et	O
al	O
.	O

Introduction	O
.	O

We	O
find	O
that	O
word	O
embeddings	O
capture	O
a	O
surprising	O
amount	O
of	O
information	O
,	O
and	O
that	O
,	O
under	O
suitable	O
supervised	O
training	O
,	O
vector	O
subtraction	O
generalises	O
well	O
to	O
a	O
broad	O
range	O
of	O
relations	O
,	O
including	O
over	O
unseen	O
lexical	O
items	O
.	O

In	O
this	O
paper	O
,	O
we	O
carry	O
out	O
such	O
an	O
evaluation	O
in	O
two	O
learning	O
settings	O
:	O
(	O
1	O
)	O
spectral	O
clustering	O
to	O
induce	O
word	O
relations	O
,	O
and	O
(	O
2	O
)	O
supervised	O
learning	O
to	O
classify	O
vector	O
differences	O
into	O
relation	O
types	O
.	O

Prior	O
work	O
has	O
evaluated	O
this	O
intriguing	O
result	O
using	O
a	O
word	O
analogy	O
prediction	O
formulation	O
and	O
hand	O
-	O
selected	O
relations	O
,	O
but	O
the	O
generality	O
of	O
the	O
finding	O
over	O
a	O
broader	O
range	O
of	O
lexical	O
relation	O
types	O
and	O
different	O
learning	O
settings	O
has	O
not	O
been	O
evaluated	O
.	O

Recent	O
work	O
has	O
shown	O
that	O
simple	O
vector	O
subtraction	O
over	O
word	O
embeddings	O
is	O
surprisingly	O
effective	O
at	O
capturing	O
different	O
lexical	O
relations	O
,	O
despite	O
lacking	O
explicit	O
supervision	O
.	O

Acknowledgments	O
.	O

In	O
the	O
future	O
,	O
we	O
will	O
use	O
the	O
proposed	O
automatically	O
data	O
labeling	O
method	O
to	O
more	O
event	O
types	O
and	O
explore	O
more	O
models	O
to	O
extract	O
events	O
by	O
using	O
automatically	O
labeled	O
data	O
.	O

The	O
experimental	O
results	O
show	O
the	O
quality	O
of	O
our	O
large	O
scale	O
automatically	O
labeled	O
data	O
is	O
competitive	O
with	O
elaborately	O
human	O
-	O
annotated	O
data	O
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
an	O
approach	O
to	O
automatically	O
label	O
training	O
data	O
for	O
EE	B-TaskName
.	O

Conclusion	O
and	O
Future	O
Work	O
.	O

In	O
other	O
words	O
,	O
the	O
approach	O
can	O
not	O
extract	O
whole	O
event	O
with	O
different	O
types	O
automatically	O
.	O

However	O
,	O
the	O
method	O
can	O
only	O
extract	O
arguments	O
of	O
one	O
plane	O
crash	O
type	O
and	O
need	O
flight	O
number	O
strings	O
as	O
input	O
.	O

(	O
2014	O
)	O
extended	O
the	O
distant	O
supervision	O
approach	O
to	O
fill	O
slots	O
in	O
plane	O
crash	O
.	O

Reschke	O
et	O
al	O
.	O

4	B-MetricValue
%	I-MetricValue
(	O
Nguyen	O
et	O
al	O
.	O
,	O
2016	O
)	O
respectively	O
.	O

The	O
best	O
reported	O
supervised	O
RE	B-TaskName
and	O
EE	B-TaskName
system	O
got	O
a	O
F1	B-MetricName
-	O
score	O
of	O
88.0	B-MetricValue
%	I-MetricValue
(	O
Wang	O
et	O
al	O
.	O
,	O
2016	O
)	O
and	O
55	B-MetricValue
.	O

For	O
the	O
reasons	O
that	O
an	O
event	O
is	O
more	O
complicated	O
than	O
a	O
relation	O
and	O
the	O
task	O
of	O
EE	B-TaskName
is	O
more	O
difficult	O
than	O
RE	B-TaskName
.	O

But	O
DS	O
for	O
RE	B-TaskName
can	O
not	O
directly	O
use	O
for	O
EE	B-TaskName
.	O

Distant	O
supervision	O
have	O
been	O
used	O
in	O
relation	B-TaskName
extraction	I-TaskName
for	O
automatically	O
labeling	O
training	O
data	O
(	O
Mintz	O
et	O
al	O
.	O
,	O
2009;Hinton	O
et	O
al	O
.	O
,	O
2012;Krause	O
et	O
al	O
.	O
,	O
2012;Krishnamurthy	O
and	O
Mitchell	O
,	O
2012;Berant	O
et	O
al	O
.	O
,	O
2013;Surdeanu	O
et	O
al	O
.	O
,	O
2012;Zeng	O
et	O
al	O
.	O
,	O
2015	O
)	O
.	O

But	O
extracted	O
events	O
may	O
not	O
be	O
easy	O
to	O
be	O
mapped	O
to	O
events	O
for	O
a	O
particular	O
knowledge	O
base	O
.	O

Unsupervised	O
methods	O
can	O
extract	O
large	O
numbers	O
of	O
events	O
without	O
using	O
labeled	O
data	O
(	O
Chambers	O
and	O
Jurafsky	O
,	O
2011;Cheung	O
et	O
al	O
.	O
,	O
2013;Huang	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

However	O
,	O
these	O
supervised	O
methods	O
depend	O
on	O
the	O
quality	O
of	O
the	O
training	O
data	O
and	O
labeled	O
training	O
data	O
is	O
expensive	O
to	O
produce	O
.	O

(	O
Ahn	O
,	O
2006;Ji	O
and	O
Grishman	O
,	O
2008;Hong	O
et	O
al	O
.	O
,	O
2011;McClosky	O
et	O
al	O
.	O
,	O
2011;Li	O
et	O
al	O
.	O
,	O
2013Li	O
et	O
al	O
.	O
,	O
,	O
2014;;Chen	O
et	O
al	O
.	O
,	O
2015;Nguyen	O
and	O
Grishman	O
,	O
2015;Nguyen	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

The	O
human	O
evaluation	O
results	O
are	O
presented	O
in	O
Table	O
7	O
.	O

Instead	O
,	O
we	O
calculate	O
the	O
precision	B-MetricName
of	O
the	O
top	O
n	O
extracted	O
event	O
instances	O
.	O

Because	O
the	O
number	O
of	O
these	O
event	O
instances	O
in	O
the	O
test	O
data	O
is	O
unknown	O
,	O
we	O
can	O
not	O
calculate	O
the	O
recall	B-MetricName
in	O
this	O
case	O
.	O

In	O
the	O
manual	O
evaluation	O
,	O
we	O
manually	O
check	O
the	O
newly	O
discovered	O
event	O
instances	O
that	O
are	O
not	O
in	O
Freebase	B-DatasetName
.	O

We	O
also	O
perform	O
a	O
manual	O
evaluation	O
to	O
eliminate	O
these	O
problems	O
.	O

Because	O
the	O
incomplete	O
nature	O
of	O
Freebase	B-DatasetName
,	O
heldout	O
evaluation	O
suffers	O
from	O
false	O
negatives	O
problem	O
.	O

Human	O
Evaluation	O
.	O

We	O
can	O
see	O
that	O
multi	O
-	O
instance	O
learning	O
is	O
effective	O
to	O
alleviate	O
the	O
noise	O
problem	O
in	O
our	O
distant	B-TaskName
supervised	I-TaskName
event	I-TaskName
extraction	I-TaskName
.	O

Figure	O
7	O
and	O
Figure	O
8	O
show	O
the	O
precision	B-MetricName
-	I-MetricName
recall	I-MetricName
(	O
P	B-MetricName
-	I-MetricName
R	I-MetricName
)	O
curves	O
for	O
each	O
method	O
in	O
the	O
two	O
stages	O
of	O
event	O
extraction	O
respectively	O
.	O

We	O
use	O
the	O
following	O
criteria	O
to	O
judge	O
the	O
correctness	O
of	O
each	O
predicted	O
event	O
automatically	O
:	O
(	O
1	O
)	O
An	O
event	O
is	O
correct	O
if	O
its	O
key	O
arguments	O
and	O
event	O
type	O
match	O
those	O
of	O
an	O
event	O
instance	O
in	O
Freebase	B-DatasetName
;	O
(	O
2	O
)	O
An	O
argument	O
is	O
correctly	O
classified	O
if	O
its	O
event	O
type	O
and	O
argument	O
role	O
match	O
those	O
of	O
any	O
of	O
the	O
argument	O
instance	O
in	O
the	O
corresponding	O
Freebase	B-DatasetName
event	O
.	O

In	O
the	O
held	O
-	O
out	O
evaluation	O
,	O
we	O
hold	O
out	O
part	O
of	O
the	O
Freebase	B-DatasetName
event	O
data	O
during	O
training	O
,	O
and	O
compare	O
newly	O
discovered	O
event	O
instances	O
against	O
this	O
heldout	O
data	O
.	O

Held	O
-	O
out	O
Evaluation	O
.	O

Following	O
previous	O
work	O
(	O
Mintz	O
et	O
al	O
.	O
,	O
2009	O
)	O
in	O
distant	O
supervised	O
RE	B-TaskName
,	O
we	O
evaluate	O
our	O
method	O
in	O
two	O
ways	O
:	O
held	O
-	O
out	O
and	O
manual	O
evaluation	O
.	O

Table	O
6	O
:	O
Effects	O
of	O
TCF	O
,	O
TETF	O
,	O
TR	O
and	O
FrameNet	B-DatasetName
.	O

Such	O
improvements	O
are	O
higher	O
than	O
improvements	O
gained	O
by	O
other	O
methods	O
(	O
TCF	B-MethodName
,	O
IEF	B-MethodName
,	O
TR	B-MethodName
)	O
,	O
which	O
demonstrates	O
the	O
effectiveness	O
of	O
the	O
usage	O
of	O
FrameNet	B-DatasetName
.	O

When	O
we	O
use	O
FrameNet	B-DatasetName
to	O
generate	O
triggers	O
,	O
compared	O
with	O
ACE+TR	B-MethodName
,	O
we	O
get	O
a	O
1.0	O
improvement	O
on	O
trigger	O
classification	O
and	O
a	O
1.7	O
improvement	O
on	O
argument	O
classification	O
.	O

Results	O
are	O
shown	O
in	O
Table	O
6	O
,	O
Compared	O
with	O
ACE+TCF	B-MethodName
and	O
ACE+TETF	B-MethodName
,	O
ACE+TR	B-MethodName
gains	O
a	O
higher	O
performance	O
in	O
both	O
stages	O
.	O

Then	O
we	O
evaluate	O
the	O
performance	O
of	O
these	O
methods	O
by	O
using	O
above	O
automatic	O
evaluations	O
.	O

Trigger	O
examples	O
generated	O
by	O
TR+Framenet	B-MethodName
are	O
shown	O
in	O
Table	O
2	O
.	O

FrameNet	B-DatasetName
was	O
used	O
to	O
filter	O
noisy	O
verbal	O
triggers	O
and	O
expand	O
nominal	O
triggers	O
.	O

We	O
set	O
these	O
hyper	O
parameters	O
as	O
0.8	B-HyperparameterValue
,	O
0.9	B-HyperparameterValue
and	O
0.8	B-HyperparameterValue
,	O
respectively	O
,	O
which	O
are	O
determined	O
by	O
grid	O
search	O
from	O
(	O
0.5	O
,	O
0.6	O
,	O
0.7	O
,	O
0.8	O
,	O
0.9	O
,	O
1.0	O
)	O
.	O

TCF	B-HyperparameterName
,	O
TETF	B-HyperparameterName
and	O
TR	B-HyperparameterName
respectively	O
use	O
the	O
trigger	B-HyperparameterName
candidate	I-HyperparameterName
frequency	I-HyperparameterName
,	O
trigger	B-HyperparameterName
event	I-HyperparameterName
type	I-HyperparameterName
frequency	I-HyperparameterName
and	O
trigger	B-HyperparameterName
rate	I-HyperparameterName
to	O
sort	O
trigger	O
candidates	O
of	O
each	O
type	O
of	O
events	O
.	O

We	O
specifically	O
select	O
two	O
methods	O
as	O
baselines	O
:	O
TCF	B-MethodName
and	O
TETF	B-MethodName
.	O

In	O
this	O
section	O
,	O
we	O
prove	O
the	O
effectiveness	O
of	O
TR	B-HyperparameterName
and	O
FrameNet	B-DatasetName
to	O
find	O
triggers	O
.	O

Impact	O
of	O
Trigger	B-HyperparameterName
Rate	I-HyperparameterName
and	O
FrameNet	B-DatasetName
.	O

However	O
,	O
when	O
we	O
set	O
k	B-HyperparameterName
=	O
1	B-HyperparameterValue
,	O
although	O
more	O
labeled	O
data	O
are	O
generated	O
,	O
the	O
precision	O
could	O
not	O
be	O
guaranteed	O
.	O

For	O
example	O
,	O
if	O
k	B-HyperparameterName
=	O
2	B-HyperparameterValue
,	O
we	O
will	O
get	O
25	O
,	O
797	O
sentences	O
labeled	O
as	O
people.marriage	O
events	O
and	O
we	O
will	O
get	O
534	O
labeled	O
sentences	O
,	O
if	O
k	B-HyperparameterName
=	O
3	B-HyperparameterValue
.	O

As	O
a	O
result	O
,	O
less	O
training	O
data	O
is	O
generated	O
.	O

The	O
reason	O
is	O
that	O
the	O
heuristics	O
for	O
data	O
labeling	O
are	O
stricter	O
as	O
k	B-HyperparameterName
grows	O
.	O

Then	O
,	O
the	O
F1	B-MetricName
value	O
reduces	O
as	O
k	B-HyperparameterName
grows	O
.	O

Figure	O
6	O
shows	O
the	O
results	O
,	O
when	O
we	O
set	O
k	B-HyperparameterName
=	O
2	B-HyperparameterValue
,	O
the	O
method	O
achieves	O
a	O
best	O
Figure	O
6	O
:	O
Effects	O
of	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
key	I-HyperparameterName
arguments	I-HyperparameterName
performance	O
in	O
both	O
stages	O
.	O

To	O
explore	O
the	O
impact	O
of	O
different	O
numbers	O
of	O
key	O
arguments	O
,	O
we	O
sort	O
all	O
arguments	O
of	O
each	O
type	O
of	O
events	O
according	O
to	O
KR	O
value	O
and	O
select	O
top	O
k	B-HyperparameterName
arguments	O
as	O
the	O
key	O
arguments	O
.	O

Examples	O
are	O
shown	O
in	O
Table	O
2	O
.	O

ACE+KR	B-MethodName
achieve	O
the	O
best	O
performance	O
in	O
both	O
stages	O
.	O

Results	O
are	O
shown	O
in	O
Table	O
5	O
.	O

After	O
that	O
we	O
evaluate	O
these	O
methods	O
by	O
using	O
above	O
automatic	O
evaluations	O
based	O
on	O
ACE	B-DatasetName
data	O
.	O

Then	O
we	O
choose	O
the	O
same	O
number	B-HyperparameterName
of	I-HyperparameterName
key	I-HyperparameterName
arguments	I-HyperparameterName
in	O
all	O
methods	O
and	O
use	O
these	O
key	O
arguments	O
to	O
label	O
data	O
.	O

We	O
specifically	O
select	O
two	O
methods	O
as	O
baselines	O
for	O
comparison	O
with	O
our	O
KR	O
method	O
:	O
ER	O
and	O
RS	O
,	O
which	O
use	O
the	O
event	O
relevance	O
and	O
role	O
salience	O
to	O
sort	O
arguments	O
of	O
each	O
type	O
of	O
events	O
respectively	O
.	O

In	O
this	O
section	O
,	O
we	O
prove	O
the	O
effectiveness	O
of	O
KR	O
to	O
find	O
key	O
arguments	O
and	O
explore	O
the	O
impact	O
of	O
different	O
numbers	O
of	O
key	O
arguments	O
to	O
automatically	O
generate	O
data	O
.	O

Impact	O
of	O
Key	O
Rate	O
.	O

Discussion	O
.	O

This	O
demonstrates	O
that	O
our	O
large	O
scale	O
automatically	O
labeled	O
data	O
is	O
competitive	O
with	O
elaborately	O
humanannotated	O
data	O
.	O

Also	O
,	O
we	O
provide	O
a	O
DMCNN	B-MethodName
-	I-MethodName
MIL	I-MethodName
model	O
for	O
this	O
data	O
as	O
a	O
baseline	O
for	O
further	O
research	O
.	O

We	O
can	O
see	O
that	O
DMCNNs	B-MethodName
-	I-MethodName
MIL	I-MethodName
achieves	O
the	O
best	O
performance	O
.	O

Performance	O
of	O
DMCNN	B-MethodName
-	I-MethodName
MIL	I-MethodName
.	O

(	O
2	O
)	O
Chen	O
's	O
DMCNN	B-MethodName
,	O
which	O
is	O
the	O
best	O
reported	O
CNN	O
-	O
based	O
system	O
(	O
Chen	O
et	O
al	O
.	O
,	O
2015	O
)	O
.	O

Finally	O
,	O
we	O
shows	O
the	O
performance	O
of	O
DMCNNs	B-MethodName
-	I-MethodName
MIL	I-MethodName
on	O
our	O
automatically	O
labeled	O
data	O
.	O

In	O
order	O
to	O
alleviate	O
the	O
wrong	O
label	O
problem	O
,	O
we	O
use	O
Multi	B-MethodName
-	I-MethodName
instance	I-MethodName
Learning	I-MethodName
(	O
MIL	B-MethodName
)	O
for	O
two	O
DMCNNs	B-MethodName
.	O
Because	O
the	O
second	O
stage	O
is	O
more	O
complicated	O
and	O
limited	O
in	O
space	O
,	O
we	O
take	O
the	O
MIL	O
used	O
in	O
arguments	O
classification	O
as	O
an	O
example	O
and	O
describes	O
as	O
follows	O
:	O
We	O
define	O
all	O
of	O
the	O
parameters	O
for	O
the	O
stage	O
of	O
argument	O
classification	O
to	O
be	O
trained	O
in	O
DM	B-MethodName
-	I-MethodName
CNNs	I-MethodName
as	O
θ	O
.	O

We	O
employ	O
two	O
similar	O
Dynamic	B-MethodName
Multi	I-MethodName
-	I-MethodName
pooling	I-MethodName
Convolutional	I-MethodName
Neural	I-MethodName
Networks	I-MethodName
with	O
Multi	B-MethodName
-	I-MethodName
instance	I-MethodName
Learning	I-MethodName
(	O
DMCNNs	B-MethodName
-	I-MethodName
MIL	I-MethodName
)	O
for	O
above	O
two	O
stages	O
.	O

The	O
Dynamic	B-MethodName
Multi	I-MethodName
-	I-MethodName
pooling	I-MethodName
Convolutional	I-MethodName
Neural	I-MethodName
Networks	I-MethodName
(	O
DMCNNs	B-MethodName
)	O
is	O
the	O
best	O
reported	O
CNN	O
-	O
based	O
model	O
for	O
event	O
extraction	O
(	O
Chen	O
et	O
al	O
.	O
,	O
2015	O
)	O
by	O
using	O
human	O
-	O
annotated	O
training	O
data	O
.	O

We	O
call	O
this	O
Expanded	B-DatasetName
Data	I-DatasetName
(	O
ED	B-DatasetName
)	O
as	O
ED	B-DatasetName
Only	I-DatasetName
.	O

Moreover	O
,	O
compared	O
with	O
Chen	O
's	O
DM	B-MethodName
-	I-MethodName
CNN	I-MethodName
trained	O
with	O
ACE	B-DatasetName
,	O
DMCNN	B-MethodName
trained	O
with	O
ED	B-DatasetName
Only	I-DatasetName
achieves	O
a	O
competitive	O
performance	O
.	O

This	O
demonstrates	O
that	O
our	O
automatically	O
generated	O
labeled	O
data	O
could	O
expand	O
human	O
annotated	O
training	O
data	O
effectively	O
.	O

Compared	O
with	O
all	O
models	O
,	O
DMCNN	B-MethodName
trained	O
with	O
ACE+ED	B-DatasetName
achieves	O
the	O
highest	O
performance	O
.	O

The	O
results	O
are	O
shown	O
in	O
Table	O
4	O
.	O

(	O
3	O
)	O
Nguyen	O
's	O
JRNN	O
,	O
which	O
is	O
the	O
state	O
-	O
ofthe	O
-	O
arts	O
system	O
(	O
Nguyen	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

(	O
1	O
)	O
Li	O
's	O
structure	O
,	O
which	O
is	O
the	O
best	O
reported	O
structured	O
-	O
based	O
system	O
(	O
Li	O
et	O
al	O
.	O
,	O
2013	O
)	O
.	O

We	O
select	O
three	O
baselines	O
trained	O
with	O
ACE	B-DatasetName
data	O
.	O

And	O
we	O
use	O
the	O
same	O
evaluation	O
metric	O
P	B-MetricName
,	O
R	B-MetricName
,	O
F	B-MetricName
as	O
ACE	B-DatasetName
task	O
defined	O
.	O

Then	O
we	O
use	O
such	O
data	O
to	O
train	O
the	O
same	O
event	O
extraction	O
model	O
(	O
DMCNN	B-MethodName
)	O
and	O
evaluate	O
them	O
on	O
the	O
ACE	B-DatasetName
testing	O
data	O
set	O
.	O

(	O
2	O
)	O
We	O
directly	O
add	O
our	O
automatically	O
labeled	O
data	O
of	O
mapped	O
event	O
types	O
to	O
ACE	B-DatasetName
training	O
data	O
and	O
we	O
call	O
this	O
training	O
data	O
as	O
ACE+ED	B-DatasetName
.	O

(	O
1	O
)	O
we	O
delete	O
the	O
human	O
annotated	O
ACE	B-DatasetName
data	O
for	O
these	O
mapped	O
event	O
types	O
in	O
ACE	B-DatasetName
dataset	O
and	O
add	O
our	O
automatically	O
labeled	O
data	O
to	O
remainder	O
ACE	B-DatasetName
training	O
data	O
.	O

We	O
mapped	O
these	O
types	O
of	O
events	O
manually	O
and	O
we	O
add	O
them	O
into	O
ACE	B-DatasetName
training	O
corpus	O
in	O
two	O
ways	O
.	O

For	O
example	O
,	O
our	O
people.marriage	O
events	O
can	O
be	O
mapped	O
to	O
life.marry	O
events	O
in	O
ACE2005	B-DatasetName
dataset	O
.	O

In	O
our	O
automatically	O
labeled	O
data	O
,	O
there	O
are	O
some	O
event	O
types	O
that	O
can	O
correspond	O
to	O
those	O
in	O
ACE	B-DatasetName
dataset	O
.	O

To	O
prove	O
the	O
effectiveness	O
of	O
the	O
proposed	O
approach	O
automatically	O
,	O
we	O
add	O
automatically	O
generated	O
labeled	O
data	O
into	O
ACE	B-DatasetName
dataset	O
to	O
expand	O
the	O
training	O
sets	O
and	O
see	O
whether	O
the	O
performance	O
of	O
the	O
event	O
extractor	O
trained	O
on	O
such	O
expanded	O
training	O
sets	O
is	O
improved	O
.	O

Automatic	O
Evaluations	O
of	O
Labeled	O
Data	O
.	O

Our	O
automatically	O
generated	O
data	O
can	O
achieve	O
a	O
precision	B-MetricName
of	O
88.9	B-MetricValue
and	O
85.4	B-MetricValue
for	O
trigger	O
labeling	O
and	O
argument	O
labeling	O
re-	O
Table	O
4	O
:	O
Overall	O
performance	O
on	O
ACE	B-DatasetName
blind	O
test	O
data	O
spectively	O
,	O
which	O
demonstrates	O
that	O
our	O
automatically	O
labeled	O
data	O
is	O
of	O
high	O
quality	O
.	O

Average	B-MetricName
Precision	I-MetricName
Trigger	O
Labeling	O
88.9	B-MetricValue
Argument	O
Labeling	O
85.4	B-MetricValue
Table	O
3	O
:	O
Manual	O
Evaluation	O
Results	O
We	O
repeat	O
above	O
evaluation	O
process	O
on	O
the	O
final	O
72	O
,	O
611	O
labeled	O
data	O
three	O
times	O
and	O
the	O
average	O
precision	O
is	O
shown	O
in	O
Table	O
3	O
.	O

Two	O
hyper	O
parameters	O
,	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
key	I-HyperparameterName
arguments	I-HyperparameterName
and	O
the	O
value	O
of	O
TR	B-HyperparameterName
in	O
our	O
automatically	O
data	O
labeling	O
,	O
are	O
set	O
as	O
2	O
and	O
0.8	O
,	O
by	O
grid	O
search	O
respectively	O
.	O

Method	O
of	O
Event	B-TaskName
Extraction	I-TaskName
.	O

Finally	O
,	O
we	O
propose	O
a	O
Soft	O
Distant	O
Supervision	O
and	O
use	O
it	O
to	O
automatically	O
generate	O
training	O
data	O
,	O
which	O
assumes	O
that	O
any	O
sentence	O
containing	O
all	O
key	O
arguments	O
in	O
Freebase	B-DatasetName
and	O
a	O
corresponding	O
trigger	O
word	O
is	O
likely	O
to	O
express	O
that	O
event	O
in	O
some	O
way	O
,	O
and	O
arguments	O
occurring	O
in	O
that	O
sentence	O
are	O
likely	O
to	O
play	O
the	O
corresponding	O
roles	O
in	O
that	O
event	O
.	O

Finally	O
,	O
we	O
choose	O
verbs	O
with	O
high	O
T	B-HyperparameterName
R	I-HyperparameterName
values	O
as	O
the	O
trigger	O
words	O
for	O
each	O
event	O
type	O
.	O

Finally	O
we	O
employ	O
Trigger	B-HyperparameterName
Rate	I-HyperparameterName
(	O
TR	B-HyperparameterName
)	O
,	O
which	O
is	O
the	O
product	O
of	O
TCF	B-HyperparameterName
and	O
TETF	B-HyperparameterName
to	O
estimate	O
the	O
probability	O
of	O
a	O
verb	O
to	O
be	O
a	O
trigger	O
,	O
which	O
is	O
formulated	O
as	O
follows	O
:	O
T	O
R	O
ij	O
=	O
T	O
CF	O
ij	O
*	O
T	O
ET	O
F	O
i	O
(	O
4	O
)	O
T	O
CF	O
ij	O
=	O
Count(V	O
i	O
,	O
ET	O
S	O
j	O
)	O
Count(ET	O
S	O
j	O
)	O
(	O
5	O
)	O
T	O
ET	O
F	O
i	O
=	O
log	O
Sum(ET	O
)	O
1	O
+	O
Count(ET	O
I	O
i	O
)	O
(	O
6	O
)	O
where	O
T	O
R	O
ij	O
is	O
the	O
trigger	O
rate	O
of	O
i	O
-	O
th	O
verb	O
to	O
jth	O
event	O
type	O
,	O
Count(V	O
i	O
,	O
ET	O
S	O
j	O
)	O
is	O
the	O
number	O
of	O
sentences	O
,	O
which	O
express	O
j	O
-	O
th	O
type	O
of	O
event	O
and	O
contain	O
i	O
-	O
th	O
verb	O
,	O
Count(ET	O
S	O
j	O
)	O
is	O
the	O
number	O
of	O
sentences	O
expressing	O
j	O
-	O
th	O
event	O
type	O
,	O
Count(ET	O
I	O
i	O
)	O
is	O
the	O
number	O
of	O
event	O
types	O
,	O
which	O
have	O
the	O
labeled	O
sentences	O
containing	O
i	O
-	O
th	O
verb	O
.	O

Thus	O
we	O
propose	O
Trigger	B-HyperparameterName
Candidate	I-HyperparameterName
Frequency	I-HyperparameterName
(	O
TCF	B-HyperparameterName
)	O
and	O
Trigger	B-HyperparameterName
Event	I-HyperparameterName
Type	I-HyperparameterName
Frequency	I-HyperparameterName
(	O
TETF	B-HyperparameterName
)	O
to	O
evaluate	O
above	O
two	O
aspects	O
.	O

Finally	O
,	O
KR	O
is	O
computed	O
as	O
follows	O
:	O
KR	O
ij	O
=	O
RS	O
ij	O
*	O
ER	O
i	O
(	O
3	O
)	O
We	O
compute	O
KR	O
for	O
all	O
arguments	O
of	O
each	O
event	O
type	O
,	O
and	O
sort	O
them	O
according	O
to	O
KR	O
.	O

We	O
propose	O
to	O
compute	O
ER	O
as	O
follows	O
:	O
ER	O
i	O
=	O
log	O
Sum(ET	O
)	O
1	O
+	O
Count(ET	O
Ci	O
)	O
(	O
2	O
)	O
where	O
ER	O
i	O
is	O
the	O
event	O
relevance	O
of	O
i	O
-	O
th	O
argument	O
,	O
Sum	O
(	O
ET	O
)	O
is	O
the	O
number	O
of	O
all	O
event	O
types	O
in	O
knowledge	O
base	O
and	O
Count(ET	O
C	O
i	O
)	O
is	O
the	O
number	O
of	O
event	O
types	O
containing	O
i	O
-	O
th	O
argument	O
.	O

Event	O
Relevance	O
(	O
ER	O
)	O
reflects	O
the	O
ability	O
in	O
which	O
an	O
argument	O
can	O
be	O
used	O
to	O
discriminate	O
d	O
-	O
ifferent	O
event	O
types	O
.	O

We	O
define	O
RS	O
as	O
follows	O
:	O
RS	O
ij	O
=	O
Count(A	O
i	O
,	O
ET	O
j	O
)	O
Count(ET	O
j	O
)	O
(	O
1	O
)	O
where	O
RS	O
ij	O
is	O
the	O
role	O
saliency	O
of	O
i	O
-	O
th	O
argument	O
to	O
j	O
-	O
th	O
event	O
type	O
,	O
Count(A	O
i	O
,	O
ET	O
j	O
)	O
is	O
the	O
number	O
of	O
Arguemnt	O
i	O
occurring	O
in	O
all	O
instances	O
of	O
eventT	O
ype	O
j	O
in	O
Freebase	O
and	O
Count(ET	O
j	O
)	O
is	O
the	O
number	O
of	O
instances	O
of	O
eventT	O
ype	O
j	O
in	O
Freebase	O
.	O

Role	O
Saliency	O
(	O
RS	O
)	O
reflects	O
the	O
saliency	O
of	O
an	O
argument	O
to	O
represent	O
a	O
specific	O
event	O
instance	O
of	O
a	O
given	O
event	O
type	O
.	O

We	O
propose	O
to	O
use	O
Key	O
Rate	O
(	O
KR	O
)	O
to	O
estimate	O
the	O
importance	O
of	O
an	O
argument	O
to	O
a	O
type	O
of	O
event	O
,	O
which	O
is	O
decided	O
by	O
two	O
factors	O
:	O
Role	O
Saliency	O
and	O
Event	O
Relevance	O
.	O

Each	O
frame	O
has	O
a	O
set	O
of	O
lemmas	O
with	O
part	O
of	O
speech	O
tags	O
that	O
can	O
evoke	O
the	O
frame	O
,	O
which	O
are	O
called	O
LUs	O
.	O
For	O
example	O
,	O
appoint.v	O
is	O
a	O
LU	O
of	O
Appointing	O
frame	O
in	O
FrameNet	B-DatasetName
,	O
which	O
can	O
be	O
mapped	O
to	O
people.appointment	O
events	O
in	O
Freebase	B-DatasetName
.	O

FrameNet	B-DatasetName
3	I-DatasetName
is	O
a	O
linguistic	O
resource	O
storing	O
information	O
about	O
lexical	O
and	O
predicate	O
argument	O
semantics	O
(	O
Baker	O
et	O
al	O
.	O
,	O
1998	O
)	O
.	O

According	O
to	O
the	O
statistics	O
of	O
the	O
Freebase	B-DatasetName
released	O
on	O
23	O
th	O
April	O
,	O
2015	O
,	O
there	O
are	O
around	O
1885	O
CVTs	O
and	O
around	O
14	O
million	O
CVTs	O
instances	O
.	O

To	O
solve	O
the	O
data	O
labeling	O
problem	O
,	O
we	O
propose	O
to	O
automatically	O
label	O
training	O
data	O
for	O
event	B-TaskName
extraction	I-TaskName
via	O
world	O
knowledge	O
and	O
linguistic	O
knowledge	O
,	O
which	O
can	O
detect	O
key	O
arguments	O
and	O
trigger	O
words	O
for	O
each	O
event	O
type	O
and	O
employ	O
them	O
to	O
label	O
events	O
in	O
texts	O
automatically	O
.	O

Stage	O
.	O

Manual	O
Evaluations	O
of	O
Labeled	O
Data	O
.	O

Each	O
sample	O
is	O
independently	O
annotated	O
by	O
three	O
annotators	O
6	O
(	O
including	O
one	O
of	O
the	O
authors	O
and	O
two	O
of	O
our	O
colleagues	O
who	O
are	O
familiar	O
with	O
event	O
extraction	O
task	O
)	O
and	O
the	O
final	O
decision	O
is	O
made	O
by	O
voting	O
.	O

It	O
is	O
very	O
easy	O
to	O
annotate	O
a	O
sample	O
for	O
annotators	O
,	O
thus	O
the	O
annotated	O
results	O
are	O
expected	O
to	O
be	O
of	O
high	O
quality	O
.	O

Otherwise	O
"	O
N	O
"	O
is	O
labeled	O
.	O

"	O
Y	O
"	O
:	O
the	O
word	O
highlighted	O
in	O
the	O
given	O
sentence	O
indeed	O
triggers	O
an	O
event	O
of	O
the	O
corresponding	O
type	O
or	O
the	O
word	O
indeed	O
plays	O
the	O
corresponding	O
role	O
in	O
that	O
event	O
.	O

Annotators	O
are	O
asked	O
to	O
assign	O
one	O
of	O
two	O
labels	O
to	O
each	O
sample	O
.	O

Figure	O
5	O
gives	O
some	O
samples	O
.	O

Each	O
selected	O
sample	O
is	O
a	O
sentence	O
with	O
a	O
highlighted	O
trigger	O
,	O
labeled	O
arguments	O
and	O
corresponding	O
event	O
type	O
and	O
argument	O
roles	O
.	O

We	O
randomly	O
select	O
500	O
samples	O
from	O
our	O
automatically	O
labeled	O
data	O
.	O

We	O
firstly	O
manually	O
evaluate	O
the	O
precision	B-MetricName
of	O
our	O
automatically	O
generated	O
labeled	O
data	O
.	O

Compared	O
with	O
nearly	O
6	O
,	O
000	O
human	O
annotated	O
labeled	O
sentence	O
in	O
ACE	B-DatasetName
,	O
our	O
method	O
can	O
automatically	O
generate	O
large	O
scale	O
labeled	O
training	O
data	O
.	O

Finally	O
,	O
72	O
,	O
611	O
labeled	O
sentences	O
are	O
generated	O
automatically	O
.	O

Thus	O
,	O
we	O
leverage	O
these	O
rough	O
labeled	O
data	O
and	O
FrameNet	B-DatasetName
to	O
find	O
triggers	O
and	O
use	O
SDS	O
to	O
generate	O
labeled	O
data	O
.	O

However	O
,	O
these	O
sentences	O
miss	O
labeling	O
triggers	O
.	O

When	O
we	O
merely	O
use	O
two	O
key	O
arguments	O
to	O
label	O
data	O
,	O
we	O
will	O
obtain	O
421	O
,	O
602	O
labeled	O
sentences	O
.	O

Table	O
2	O
shows	O
the	O
statistics	O
of	O
the	O
five	O
largest	O
automatically	O
labeled	O
events	O
among	O
selected	O
21	O
Freebase	B-DatasetName
events	O
.	O

By	O
using	O
the	O
proposed	O
methods	O
,	O
a	O
large	O
set	O
of	O
labeled	O
data	O
could	O
be	O
generated	O
automatically	O
.	O

Our	O
Automatically	O
Labeled	O
Data	O
.	O

Then	O
,	O
we	O
conduct	O
automatic	O
evaluations	O
for	O
our	O
labeled	O
data	O
based	O
on	O
ACE	B-DatasetName
corpus	O
and	O
analyze	O
effects	O
of	O
different	O
approaches	O
to	O
automatically	O
label	O
training	O
data	O
.	O

In	O
this	O
section	O
,	O
we	O
first	O
manually	O
evaluate	O
our	O
automatically	O
labeled	O
data	O
.	O

Experiments	O
.	O

Given	O
all	O
(	O
T	O
)	O
training	O
bags	O
(	O
M	O
i	O
,	O
y	O
i	O
)	O
,	O
we	O
can	O
define	O
the	O
objective	O
function	O
using	O
cross	O
-	O
entropy	O
at	O
the	O
bag	O
level	O
as	O
follows	O
:	O
J	O
(	O
θ	O
)	O
=	O
T	O
i=1	O
log	O
p(y	O
i	O
|m	O
j	O
i	O
,	O
θ	O
)	O
(	O
9	O
)	O
where	O
j	O
is	O
constrained	O
as	O
follows	O
:	O
j	O
*	O
=	O
arg	O
max	O
j	O
p(r|m	O
j	O
i	O
,	O
θ	O
)	O
1	O
≤	O
j	O
≤	O
q	O
i	O
(	O
10	O
)	O
To	O
compute	O
the	O
network	O
parameter	O
θ	O
,	O
we	O
maximize	O
the	O
log	O
likelihood	O
J	O
(	O
θ	O
)	O
through	O
stochastic	O
gradient	O
descent	O
over	O
mini	O
-	O
batches	O
with	O
the	O
Adadelta	O
(	O
Zeiler	O
,	O
2012	O
)	O
update	O
rule	O
.	O

Thus	O
,	O
we	O
define	O
the	O
objective	O
function	O
on	O
the	O
bags	O
.	O

And	O
the	O
objective	O
of	O
multi	O
-	O
instance	O
learning	O
is	O
to	O
discriminate	O
bags	O
rather	O
than	O
instances	O
.	O

To	O
obtain	O
the	O
conditional	O
probability	O
p(r|m	O
j	O
i	O
,	O
θ	O
)	O
,	O
we	O
apply	O
a	O
softmax	O
operation	O
over	O
all	O
argument	O
role	O
types	O
:	O
p(r|m	O
j	O
i	O
,	O
θ	O
)	O
=	O
e	O
or	O
n	O
k=1	O
e	O
o	O
k	O
(	O
8)	O
where	O
,	O
n	O
is	O
the	O
number	O
of	O
roles	O
.	O

Given	O
an	O
input	O
instance	O
m	O
j	O
i	O
,	O
the	O
network	O
with	O
the	O
parameter	O
θ	O
outputs	O
a	O
vector	O
O	O
,	O
where	O
the	O
r	O
-	O
th	O
component	O
O	O
r	O
corresponds	O
to	O
the	O
score	O
associated	O
with	O
argument	O
role	O
r.	O

In	O
stage	O
of	O
argument	O
classification	O
,	O
we	O
take	O
sentences	O
containing	O
the	O
same	O
argument	O
candidate	O
and	O
triggers	O
with	O
a	O
same	O
event	O
type	O
as	O
a	O
bag	O
and	O
all	O
instances	O
in	O
a	O
bag	O
are	O
considered	O
independently	O
.	O

Suppose	O
that	O
there	O
are	O
T	O
bags	O
{	O
M	O
1	O
,	O
M	O
2	O
,	O
...	O
,	O
M	O
T	O
}	O
and	O
that	O
the	O
i	O
-	O
th	O
bag	O
contains	O
q	O
i	O
instances	O
(	O
sentences	O
)	O
M	O
i	O
=	O
m	O
1	O
i	O
,	O
m	O
2	O
i	O
,	O
...	O
,	O
m	O
q	O
i	O
i	O
,	O
the	O
objective	O
of	O
multi	O
-	O
instance	O
learning	O
is	O
to	O
predict	O
the	O
labels	O
of	O
the	O
unseen	O
bags	O
.	O

However	O
,	O
our	O
automatically	O
labeled	O
data	O
face	O
a	O
noise	O
problem	O
,	O
which	O
is	O
a	O
intrinsic	O
problem	O
of	O
using	O
DS	O
to	O
construct	O
training	O
data	O
(	O
Hoffmann	O
et	O
al	O
.	O
,	O
2011;Surdeanu	O
et	O
al	O
.	O
,	O
2012	O
)	O
.	O

We	O
call	O
this	O
stage	O
as	O
argument	O
classification	O
.	O

If	O
the	O
key	O
arguments	O
participate	O
a	O
Freebase	B-DatasetName
event	O
,	O
the	O
second	O
stage	O
is	O
conducted	O
,	O
which	O
aims	O
to	O
assign	O
arguments	O
to	O
the	O
event	O
and	O
identify	O
their	O
corresponding	O
roles	O
.	O

The	O
first	O
stage	O
is	O
called	O
Event	O
Classification	O
,	O
which	O
aims	O
to	O
predict	O
whether	O
the	O
key	O
argument	O
candidates	O
participate	O
in	O
a	O
Freebase	B-DatasetName
event	O
.	O

In	O
this	O
paper	O
,	O
event	B-TaskName
extraction	I-TaskName
is	O
formulated	O
as	O
a	O
two	O
-	O
stage	O
,	O
multi	O
-	O
class	O
classification	O
task	O
.	O

Automatically	O
labeled	O
data	O
generation	O
.	O

And	O
we	O
use	O
nouns	O
with	O
high	O
confidence	O
in	O
the	O
mapped	O
frame	O
to	O
expand	O
trigger	O
lexicon	O
.	O

Finally	O
,	O
we	O
select	O
the	O
frame	O
contains	O
max	O
similarity	O
of	O
e	O
i	O
and	O
e	O
j	O
,	O
k	O
as	O
the	O
mapped	O
frame	O
,	O
which	O
can	O
be	O
formulated	O
as	O
follows	O
:	O
f	O
rame(i	O
)	O
=	O
arg	O
max	O
j	O
(	O
similarity(e	O
i	O
,	O
e	O
j	O
,	O
k	O
)	O
)	O
(	O
7	O
)	O
Then	O
,	O
we	O
filter	O
the	O
verb	O
,	O
which	O
is	O
in	O
initial	O
verbal	O
trigger	O
word	O
lexicon	O
and	O
not	O
in	O
the	O
mapping	O
frame	O
.	O

Specifically	O
,	O
we	O
use	O
the	O
average	O
word	O
embedding	O
of	O
all	O
words	O
in	O
i	O
-	O
th	O
Freebase	B-DatasetName
event	O
type	O
name	O
e	O
i	O
and	O
word	O
embedding	O
of	O
k	O
-	O
th	O
lexical	O
units	O
of	O
j	O
-	O
th	O
frame	O
e	O
j	O
,	O
k	O
to	O
compute	O
the	O
semantic	O
similarity	O
.	O

As	O
the	O
success	O
of	O
word	O
embedding	O
in	O
capturing	O
semantics	O
of	O
words	O
(	O
Turian	O
et	O
al	O
.	O
,	O
2010	O
)	O
,	O
we	O
employ	O
word	O
embedding	O
to	O
map	O
the	O
events	O
in	O
Freebase	B-DatasetName
to	O
frames	O
in	O
FrameNet	B-DatasetName
.	O

Thus	O
,	O
we	O
propose	O
to	O
use	O
linguistic	O
resource	O
FrameNet	B-DatasetName
to	O
filter	O
noisy	O
verbal	O
triggers	O
and	O
expand	O
nominal	O
triggers	O
.	O

Because	O
the	O
number	O
of	O
nouns	O
in	O
one	O
sentence	O
is	O
usually	O
larger	O
than	O
that	O
of	O
verbs	O
,	O
it	O
is	O
hard	O
to	O
use	O
TR	O
to	O
find	O
nominal	O
triggers	O
.	O

The	O
nominal	O
triggers	O
like	O
marriage	O
are	O
missing	O
.	O

However	O
,	O
this	O
initial	O
trigger	O
lexicon	O
is	O
noisy	O
and	O
merely	O
contains	O
verbal	O
triggers	O
.	O

We	O
can	O
obtain	O
an	O
initial	O
verbal	O
trigger	O
lexicon	O
by	O
above	O
trigger	O
word	O
detection	O
.	O

Trigger	O
Word	O
Filtering	O
and	O
Expansion	O
.	O

Intuitively	O
,	O
if	O
a	O
verb	O
occurs	O
more	O
times	O
than	O
other	O
verbs	O
in	O
the	O
labeled	O
sentences	O
of	O
one	O
event	O
type	O
,	O
the	O
verb	O
tends	O
to	O
trigger	O
this	O
type	O
of	O
event	O
;	O
and	O
if	O
a	O
verb	O
occurs	O
in	O
sentences	O
of	O
every	O
event	O
types	O
,	O
like	O
is	O
,	O
the	O
verb	O
will	O
have	O
a	O
low	O
probability	O
to	O
trigger	O
events	O
.	O

As	O
shown	O
in	O
Figure	O
1	O
,	O
threw	O
is	O
a	O
trigger	O
of	O
Attack	O
event	O
.	O

For	O
example	O
,	O
in	O
ACE	B-DatasetName
2005	I-DatasetName
English	O
data	O
,	O
there	O
are	O
60	O
%	O
of	O
events	O
triggered	O
by	O
verbs	O
.	O

In	O
a	O
sentence	O
,	O
a	O
verb	O
tend	O
to	O
express	O
an	O
occurrence	O
of	O
an	O
event	O
.	O

Then	O
we	O
use	O
these	O
labeled	O
sentences	O
to	O
detect	O
triggers	O
.	O

Finally	O
,	O
we	O
select	O
sentences	O
that	O
contains	O
all	O
key	O
arguments	O
of	O
an	O
event	O
instance	O
in	O
Freebase	B-DatasetName
as	O
sentences	O
expressing	O
corresponding	O
events	O
.	O

At	O
first	O
,	O
we	O
use	O
Standford	O
CoreNLP	O
tool	O
5	O
to	O
converts	O
the	O
raw	O
Wikipedia	O
texts	O
into	O
a	O
sequence	O
of	O
sentences	O
,	O
attaches	O
NLP	O
annotations	O
(	O
POS	O
tag	O
,	O
NER	O
tag	O
)	O
.	O

After	O
detecting	O
key	O
arguments	O
for	O
every	O
event	O
types	O
,	O
we	O
use	O
these	O
key	O
arguments	O
to	O
label	O
sentences	O
that	O
may	O
express	O
events	O
in	O
Wikipedia	O
.	O

Trigger	O
Word	O
Detection	O
.	O

Then	O
we	O
choose	O
top	O
K	O
arguments	O
as	O
key	O
arguments	O
.	O

If	O
an	O
argument	O
occurs	O
in	O
every	O
event	O
type	O
,	O
the	O
argument	O
will	O
have	O
a	O
low	O
event	O
relevance	O
.	O

If	O
we	O
tend	O
to	O
use	O
an	O
argument	O
to	O
distinguish	O
one	O
event	O
instance	O
form	O
other	O
instances	O
of	O
a	O
given	O
event	O
type	O
,	O
this	O
argument	O
will	O
play	O
a	O
salient	O
role	O
in	O
the	O
given	O
event	O
type	O
.	O

We	O
call	O
these	O
arguments	O
as	O
key	O
arguments	O
.	O

For	O
example	O
,	O
compared	O
with	O
arguments	O
like	O
time	O
,	O
location	O
and	O
so	O
on	O
,	O
spouses	O
are	O
key	O
arguments	O
in	O
a	O
marriage	O
event	O
.	O

Some	O
arguments	O
play	O
indispensable	O
roles	O
in	O
an	O
event	O
,	O
and	O
serve	O
as	O
vital	O
clues	O
when	O
distinguishing	O
different	O
events	O
.	O

Intuitively	O
,	O
arguments	O
of	O
a	O
type	O
of	O
event	O
play	O
different	O
roles	O
.	O

This	O
section	O
illustrates	O
how	O
to	O
detect	O
key	O
arguments	O
for	O
each	O
event	O
type	O
via	O
Freebase	B-DatasetName
.	O

Key	O
Argument	O
Detection	O
.	O

Method	O
of	O
Generating	O
Training	O
Data	O
.	O

tection	O
,	O
which	O
prioritizes	O
arguments	O
of	O
each	O
event	O
type	O
and	O
selects	O
key	O
arguments	O
for	O
each	O
type	O
of	O
event	O
;	O
(	O
ii	O
)	O
Trigger	O
word	O
detection	O
,	O
which	O
uses	O
key	O
arguments	O
to	O
label	O
sentences	O
that	O
may	O
express	O
events	O
preliminarily	O
,	O
and	O
then	O
detect	O
triggers	O
;	O
(	O
iii	O
)	O
Trigger	O
word	O
filtering	O
and	O
expansion	O
,	O
which	O
uses	O
FrameNet	B-DatasetName
to	O
filter	O
noisy	O
triggers	O
and	O
expand	O
triggers	O
;	O
(	O
iv	O
)	O
Automatically	O
labeled	O
data	O
generation	O
,	O
which	O
uses	O
a	O
SDS	O
to	O
label	O
events	O
in	O
sentences	O
.	O

We	O
use	O
Wikipedia	O
because	O
it	O
is	O
relatively	O
up	O
-	O
to	O
-	O
date	O
,	O
and	O
much	O
of	O
the	O
information	O
in	O
Freebase	B-DatasetName
is	O
derived	O
from	O
Wikipedia	O
.	O

All	O
6.3	O
million	O
articles	O
in	O
it	O
are	O
used	O
in	O
our	O
experiments	O
.	O

Wikipedia	O
4	O
that	O
we	O
used	O
was	O
released	O
on	O
January	O
,	O
2016	O
.	O

Thus	O
we	O
use	O
FrameNet	B-DatasetName
to	O
detect	O
triggers	O
in	O
our	O
automatically	O
data	O
labeling	O
process	O
.	O

And	O
a	O
LUs	O
of	O
the	O
frame	O
plays	O
a	O
similar	O
role	O
as	O
the	O
trigger	O
of	O
an	O
event	O
.	O

Each	O
frame	O
of	O
FrameNet	B-DatasetName
can	O
be	O
taken	O
as	O
a	O
semantic	O
frame	O
of	O
a	O
type	O
of	O
events	O
(	O
Liu	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

FrameNet	B-DatasetName
contains	O
more	O
than	O
1	O
,	O
000	O
frames	O
and	O
10	O
,	O
000	O
Lexical	O
Units	O
(	O
LUs	O
)	O
.	O

After	O
filtering	O
out	O
useless	O
and	O
meaningless	O
CVTs	O
,	O
such	O
as	O
CVTs	O
about	O
user	O
profiles	O
and	O
website	O
information	O
,	O
we	O
select	O
21	O
types	O
of	O
CVTs	O
with	O
around	O
3.8	O
million	O
instances	O
for	O
experiments	O
,	O
which	O
mainly	O
involves	O
events	O
about	O
education	O
,	O
military	O
,	O
sports	O
and	O
so	O
on	O
.	O

In	O
this	O
paper	O
,	O
we	O
regard	O
these	O
CVTs	O
as	O
events	O
,	O
type	O
of	O
CVTs	O
as	O
event	O
type	O
,	O
CVT	O
instances	O
as	O
event	O
instances	O
,	O
values	O
in	O
CVTs	O
as	O
arguments	O
in	O
events	O
and	O
roles	O
of	O
CVTs	O
as	O
the	O
roles	O
of	O
arguments	O
play	O
in	O
the	O
event	O
,	O
respectively	O
.	O

Spouse	O
,	O
from	O
,	O
to	O
and	O
location	O
of	O
ceremony	O
are	O
roles	O
of	O
the	O
people.marriage	O
CVTs	O
.	O
Barack	O
Obama	O
,	O
Michelle	O
Obama	O
,	O
10/3/1992	O
and	O
Trinity	O
United	O
Church	O
of	O
Christ	O
are	O
the	O
values	O
of	O
the	O
instances	O
.	O

As	O
shown	O
in	O
Figure	O
3	O
,	O
people.marriage	O
is	O
one	O
type	O
of	O
CVTs	O
.	O
There	O
are	O
many	O
instances	O
of	O
people.marriage	O
and	O
the	O
marriage	O
of	O
Barack	O
Obama	O
and	O
Michelle	O
Obama	O
is	O
numbered	O
as	O
m.02nqglv	O
.	O

To	O
understand	O
our	O
method	O
easily	O
,	O
we	O
first	O
introduce	O
them	O
as	O
follows	O
:	O
Freebase	B-DatasetName
is	O
a	O
semantic	O
knowledge	O
base	O
(	O
Bollacker	O
et	O
al	O
.	O
,	O
2008	O
)	O
,	O
which	O
makes	O
use	O
of	O
mediators	O
(	O
also	O
called	O
compound	O
value	O
types	O
,	O
CVTs	O
)	O
to	O
merge	O
multiple	O
values	O
into	O
a	O
single	O
value	O
.	O

The	O
articles	O
in	O
Wikipedia	O
are	O
used	O
as	O
unstructured	O
texts	O
to	O
be	O
labeled	O
.	O

In	O
this	O
paper	O
,	O
we	O
respectively	O
use	O
Freebase	B-DatasetName
as	O
our	O
world	O
knowledge	O
containing	O
event	O
instance	O
and	O
FrameNet	B-DatasetName
as	O
the	O
linguistic	O
knowledge	O
containing	O
trigger	O
information	O
.	O

Background	O
.	O

Also	O
,	O
our	O
automatically	O
labeled	O
data	O
can	O
augment	O
traditional	O
humanannotated	O
data	O
,	O
which	O
could	O
significantly	O
improve	O
the	O
extraction	O
performance	O
.	O

•	O
The	O
experimental	O
results	O
show	O
that	O
the	O
quality	O
of	O
our	O
large	O
scale	O
automatically	O
labeled	O
data	O
is	O
competitive	O
with	O
elaborately	O
humanannotated	O
data	O
.	O

Moreover	O
,	O
we	O
employ	O
FrameNet	B-DatasetName
to	O
filter	O
noisy	O
triggers	O
and	O
expand	O
more	O
triggers	O
.	O

•	O
We	O
propose	O
an	O
approach	O
to	O
figure	O
out	O
key	O
arguments	O
of	O
an	O
event	O
by	O
using	O
Freebase	B-DatasetName
,	O
and	O
use	O
them	O
to	O
automatically	O
detect	O
events	O
and	O
corresponding	O
trigger	O
words	O
.	O

All	O
the	O
labeled	O
data	O
in	O
this	O
paper	O
have	O
been	O
released	O
and	O
can	O
be	O
downloaded	O
freely	O
2	O
.	O

In	O
summary	O
,	O
the	O
contributions	O
of	O
this	O
paper	O
are	O
as	O
follows	O
:	O
•	O
To	O
our	O
knowledge	O
,	O
it	O
is	O
the	O
first	O
work	O
to	O
automatically	O
label	O
data	O
for	O
large	O
scale	O
EE	B-TaskName
via	O
world	O
knowledge	O
and	O
linguistic	O
knowledge	O
.	O

In	O
addition	O
,	O
we	O
employ	O
a	O
CNNbased	O
EE	B-TaskName
approach	O
with	O
multi	O
-	O
instance	O
learning	O
for	O
the	O
automatically	O
labeled	O
data	O
as	O
a	O
baseline	O
for	O
further	O
research	O
on	O
this	O
data	O
.	O

Finally	O
,	O
we	O
evaluate	O
the	O
quality	O
of	O
the	O
automatically	O
labeled	O
training	O
data	O
by	O
both	O
manual	O
and	O
automatic	O
evaluations	O
.	O

At	O
first	O
,	O
we	O
put	O
forward	O
an	O
approach	O
to	O
prioritize	O
arguments	O
and	O
select	O
key	O
or	O
representative	O
arguments	O
(	O
see	O
details	O
in	O
Section	O
3.1	O
)	O
for	O
each	O
event	O
type	O
by	O
using	O
Freebase	B-DatasetName
;	O
Secondly	O
,	O
we	O
merely	O
use	O
key	O
arguments	O
to	O
label	O
events	O
and	O
figure	O
out	O
trigger	O
words	O
;	O
Thirdly	O
,	O
an	O
external	O
linguistic	O
knowledge	O
resource	O
,	O
FrameNet	B-DatasetName
,	O
is	O
employed	O
to	O
filter	O
noisy	O
trigger	O
words	O
and	O
expand	O
more	O
triggers	O
;	O
After	O
that	O
,	O
we	O
propose	O
a	O
Soft	O
Distant	O
Supervision	O
(	O
SDS	O
)	O
for	O
EE	B-TaskName
to	O
automatically	O
label	O
training	O
data	O
,	O
which	O
assumes	O
that	O
any	O
sentence	O
containing	O
all	O
key	O
arguments	O
in	O
Freebase	B-DatasetName
and	O
a	O
corresponding	O
trigger	O
word	O
is	O
likely	O
to	O
express	O
that	O
event	O
in	O
some	O
way	O
,	O
and	O
arguments	O
occurring	O
in	O
that	O
sentence	O
are	O
likely	O
to	O
play	O
the	O
corresponding	O
roles	O
in	O
that	O
event	O
.	O

As	O
shown	O
in	O
To	O
solve	O
above	O
problems	O
,	O
we	O
propose	O
an	O
approach	O
to	O
automatically	O
generate	O
labeled	O
data	O
for	O
large	O
scale	O
EE	B-TaskName
by	O
jointly	O
using	O
world	O
knowledge	O
(	O
Freebase	B-DatasetName
)	O
and	O
linguistic	O
knowledge	O
(	O
FrameNet	B-DatasetName
)	O
.	O

Simply	O
employing	O
all	O
arguments	O
in	O
the	O
knowledge	O
base	O
to	O
label	O
back	O
in	O
sentences	O
will	O
generate	O
few	O
sentences	O
as	O
training	O
samples	O
.	O

However	O
,	O
arguments	O
for	O
a	O
specific	O
event	O
instance	O
are	O
usually	O
mentioned	O
in	O
multiple	O
sentences	O
.	O

Following	O
DS	O
in	O
RE	B-TaskName
,	O
we	O
could	O
naturally	O
assume	O
that	O
a	O
sentence	O
contains	O
all	O
arguments	O
of	O
an	O
event	O
in	O
the	O
knowledge	O
base	O
tend	O
to	O
express	O
that	O
event	O
,	O
and	O
the	O
verbs	O
occur	O
in	O
these	O
sentences	O
tend	O
to	O
evoke	O
this	O
type	O
of	O
events	O
.	O

To	O
resolve	O
the	O
trigger	O
missing	O
problem	O
mentioned	O
above	O
,	O
we	O
need	O
to	O
discover	O
trigger	O
words	O
before	O
employing	O
distant	O
supervision	O
to	O
automatically	O
label	O
event	O
arguments	O
.	O

Unfortunately	O
,	O
triggers	O
are	O
not	O
given	O
out	O
in	O
existing	O
knowledge	O
bases	O
.	O

Following	O
ACE	B-DatasetName
,	O
we	O
can	O
use	O
trigger	O
words	O
to	O
represent	O
event	O
instance	O
,	O
like	O
married	O
for	O
people.marriage	O
event	O
instance	O
.	O

In	O
ACE	B-DatasetName
event	O
extraction	O
program	O
,	O
an	O
event	O
instance	O
is	O
represented	O
as	O
a	O
trigger	O
word	O
,	O
which	O
is	O
the	O
main	O
word	O
that	O
most	O
clearly	O
represents	O
an	O
event	O
occurrence	O
in	O
sentences	O
,	O
like	O
threw	O
in	O
Figure	O
1	O
.	O

Thus	O
we	O
can	O
not	O
directly	O
use	O
an	O
event	O
instance	O
and	O
an	O
argument	O
,	O
like	O
m.02nqglv	O
and	O
Barack	O
Obama	O
,	O
to	O
label	O
back	O
in	O
sentences	O
.	O

For	O
example	O
,	O
in	O
Freebase	B-DatasetName
,	O
the	O
aforementioned	O
marriage	O
event	O
instance	O
is	O
represented	O
as	O
m.02nqglv	O
(	O
see	O
details	O
in	O
Section	O
2	O
)	O
.	O

However	O
,	O
an	O
event	O
instance	O
is	O
a	O
virtual	O
node	O
in	O
existing	O
knowledge	O
bases	O
and	O
mentioned	O
implicitly	O
in	O
texts	O
.	O

It	O
seems	O
that	O
we	O
could	O
use	O
an	O
event	O
instance	O
and	O
an	O
argument	O
to	O
automatically	O
generate	O
training	O
data	O
for	O
argument	O
identification	O
just	O
like	O
DS	O
for	O
RE	B-TaskName
.	O

For	O
example	O
,	O
Barack	O
Obama	O
plays	O
a	O
Spouse	O
role	O
in	O
this	O
marriage	O
event	O
instance	O
.	O

DS	O
for	O
RE	B-TaskName
uses	O
two	O
entities	O
to	O
automatically	O
label	O
training	O
data	O
;	O
In	O
comparison	O
,	O
the	O
left	O
part	O
in	O
Figure	O
3	O
shows	O
a	O
marriage	O
event	O
of	O
Barack	O
Obama	O
and	O
M	O
ichelle	O
Obama	O
,	O
where	O
the	O
dash	O
circle	O
represents	O
the	O
marriage	O
event	O
instance	O
of	O
Barack	O
Obama	O
and	O
M	O
ichelle	O
Obama	O
,	O
rectangles	O
represent	O
arguments	O
of	O
the	O
event	O
instance	O
,	O
and	O
each	O
edge	O
connecting	O
an	O
argument	O
and	O
the	O
event	O
instance	O
expresses	O
the	O
role	O
of	O
the	O
argument	O
.	O

In	O
Figure	O
3	O
,	O
the	O
right	O
part	O
shows	O
an	O
example	O
of	O
spouse	O
of	O
relation	O
between	O
Barack	O
Obama	O
and	O
M	O
ichelle	O
Obama	O
,	O
where	O
two	O
rectangles	O
represent	O
two	O
entities	O
and	O
the	O
edge	O
connecting	O
them	O
represents	O
their	O
relation	O
.	O

EE	B-TaskName
aims	O
to	O
detect	O
an	O
event	O
instance	O
of	O
a	O
specific	O
type	O
and	O
extract	O
their	O
arguments	O
and	O
roles	O
,	O
formulated	O
as	O
(	O
event	O
instance	O
,	O
event	O
type	O
;	O
role	O
1	O
,	O
argument	O
1	O
;	O
role	O
2	O
,	O
argument	O
2	O
;	O
...	O
;	O
role	O
n	O
,	O
argument	O
n	O
)	O
,	O
which	O
can	O
be	O
regarded	O
as	O
a	O
kind	O
of	O
multiple	O
or	O
complicated	O
relational	O
data	O
.	O

However	O
,	O
when	O
we	O
use	O
DS	O
for	O
RE	B-TaskName
to	O
EE	B-TaskName
,	O
we	O
meet	O
following	O
challenges	O
:	O
Triggers	O
are	O
not	O
given	O
out	O
in	O
existing	O
knowledge	O
bases	O
.	O

And	O
DS	O
for	O
RE	B-TaskName
assumes	O
that	O
if	O
two	O
entities	O
have	O
a	O
relationship	O
in	O
a	O
known	O
knowledge	O
base	O
,	O
then	O
all	O
sentences	O
that	O
mention	O
these	O
two	O
entities	O
will	O
express	O
that	O
relationship	O
in	O
some	O
way	O
(	O
Mintz	O
et	O
al	O
.	O
,	O
2009	O
)	O
.	O

Recent	O
improvements	O
of	O
Distant	O
Supervision	O
(	O
DS	O
)	O
have	O
been	O
proven	O
to	O
be	O
effective	O
to	O
label	O
training	O
data	O
for	O
Relation	B-TaskName
Extraction	I-TaskName
(	O
RE	B-TaskName
)	O
,	O
which	O
aims	O
to	O
predict	O
semantic	O
re-	O
lations	O
between	O
pairs	O
of	O
entities	O
,	O
formulated	O
as	O
(	O
entity	O
1	O
,	O
relation	O
,	O
entity	O
2	O
)	O
.	O

Figure	O
1	O
shows	O
an	O
example	O
of	O
labeled	O
sentence	O
.	O

This	O
paper	O
aims	O
to	O
automatically	O
generate	O
training	O
data	O
for	O
EE	B-TaskName
,	O
which	O
involves	O
labeling	O
triggers	O
,	O
event	O
types	O
,	O
arguments	O
and	O
their	O
roles	O
.	O

Person	O
Time	O
Therefore	O
,	O
for	O
extracting	O
large	O
scale	O
events	O
,	O
especially	O
in	O
open	O
domain	O
scenarios	O
,	O
how	O
to	O
automatically	O
and	O
efficiently	O
generate	O
sufficient	O
training	O
data	O
is	O
an	O
important	O
problem	O
.	O

Marry	O
Person	O
.	O

Moreover	O
,	O
those	O
predefined	O
33	O
event	O
types	O
are	O
in	O
low	O
coverage	O
for	O
Natural	O
Language	O
Processing	O
(	O
NLP	O
)	O
applications	O
on	O
large	O
-	O
scale	O
data	O
.	O

As	O
Figure	O
2	O
shown	O
,	O
nearly	O
60	O
%	O
of	O
event	O
types	O
in	O
ACE	B-DatasetName
2005	I-DatasetName
have	O
less	O
than	O
100	O
labeled	O
samples	O
and	O
there	O
are	O
even	O
three	O
event	O
types	O
which	O
have	O
less	O
than	O
ten	O
labeled	O
samples	O
.	O

In	O
ACE	B-DatasetName
2005	I-DatasetName
,	O
all	O
33	O
event	O
types	O
are	O
manually	O
predefined	O
and	O
the	O
corresponding	O
event	O
information	O
(	O
including	O
triggers	O
,	O
event	O
types	O
,	O
arguments	O
and	O
their	O
roles	O
)	O
are	O
manually	O
annotated	O
only	O
in	O
599	O
English	O
documents	O
since	O
the	O
annotation	O
process	O
is	O
extremely	O
expensive	O
.	O

Although	O
this	O
paradigm	O
was	O
widely	O
studied	O
,	O
existing	O
approaches	O
still	O
suffer	O
from	O
high	O
costs	O
for	O
manually	O
labeling	O
training	O
data	O
and	O
low	O
coverage	O
of	O
predefined	O
event	O
types	O
.	O

2016	O
;	O
Chen	O
et	O
al	O
.	O
,	O
2015;Li	O
et	O
al	O
.	O
,	O
2014;Hong	O
et	O
al	O
.	O
,	O
2011;Ji	O
and	O
Grishman	O
,	O
2008	O
)	O
usually	O
adopted	O
supervised	O
learning	O
paradigm	O
which	O
relies	O
on	O
elaborate	O
human	O
-	O
annotated	O
data	O
,	O
such	O
as	O
ACE	B-DatasetName
2005	I-DatasetName
1	O
,	O
to	O
train	O
extractors	O
.	O

To	O
this	O
end	O
,	O
so	O
far	O
most	O
methods	O
(	O
Nguyen	O
et	O
al	O
.	O
,	O
Mi	O
chel	O
l	O
e	O
Obama	O
and	O
Barack	O
Obama	O
were	O
on	O
October	O
3	O
,	O
1992	O
.	O

For	O
example	O
,	O
in	O
the	O
sentence	O
shown	O
in	O
Figure	O
1	O
,	O
an	O
EE	B-TaskName
system	O
is	O
expected	O
to	O
identify	O
an	O
Attack	O
event	O
triggered	O
by	O
threw	O
and	O
extract	O
the	O
corresponding	O
five	O
augments	O
with	O
different	O
roles	O
:	O
Yesterday	O
(	O
Role	O
=	O
Time	O
)	O
,	O
demonstrators	O
(	O
Role	O
=	O
Attacker	O
)	O
,	O
stones	O
(	O
Role	O
=	O
Instrument	O
)	O
,	O
soldiers	O
(	O
Role	O
=	O
Target	O
)	O
,	O
and	O
Israeli	O
(	O
Role	O
=	O
Place	O
)	O
.	O

Event	B-TaskName
Extraction	I-TaskName
(	O
EE	B-TaskName
)	O
,	O
a	O
challenging	O
task	O
in	O
Information	O
Extraction	O
,	O
aims	O
at	O
detecting	O
and	O
typing	O
events	O
(	O
Event	O
Detection	O
)	O
,	O
and	O
extracting	O
arguments	O
with	O
different	O
roles	O
(	O
Argument	O
Identification	O
)	O
from	O
natural	O
-	O
language	O
texts	O
.	O

Introduction	O
.	O

And	O
our	O
automatically	O
labeled	O
data	O
can	O
incorporate	O
with	O
human	O
-	O
labeled	O
data	O
,	O
then	O
improve	O
the	O
performance	O
of	O
models	O
learned	O
from	O
these	O
data	O
.	O

The	O
experimental	O
results	O
show	O
that	O
the	O
quality	O
of	O
our	O
large	O
scale	O
automatically	O
labeled	O
data	O
is	O
competitive	O
with	O
elaborately	O
human	O
-	O
labeled	O
data	O
.	O

However	O
,	O
hand	O
-	O
labeled	O
training	O
data	O
is	O
expensive	O
to	O
produce	O
,	O
in	O
low	O
coverage	O
of	O
event	O
types	O
,	O
and	O
limited	O
in	O
size	O
,	O
which	O
makes	O
supervised	O
methods	O
hard	O
to	O
extract	O
large	O
scale	O
of	O
events	O
for	O
knowledge	O
base	O
population	O
.	O

Modern	O
models	O
of	O
event	B-TaskName
extraction	I-TaskName
for	O
tasks	O
like	O
ACE	B-DatasetName
are	O
based	O
on	O
supervised	O
learning	O
of	O
events	O
from	O
small	O
hand	O
-	O
labeled	O
data	O
.	O

Automatically	O
Labeled	O
Data	O
Generation	O
for	O
Large	O
Scale	O
Event	B-TaskName
Extraction	I-TaskName
.	O

It	O
demonstrates	O
the	O
effectiveness	O
of	O
our	O
TR	O
methods	O
.	O

Then	O
we	O
generate	O
initial	O
trigger	O
lexicon	O
by	O
using	O
all	O
trigger	O
candidates	O
with	O
high	O
TCF	O
value	O
,	O
TETF	O
value	O
or	O
TR	O
value	O
.	O

Then	O
we	O
automatically	O
evaluate	O
the	O
performance	O
by	O
using	O
automatic	O
evaluations	O
proposed	O
above	O
.	O

Following	O
(	O
Nguyen	O
et	O
al	O
.	O
,	O
2016;Chen	O
et	O
al	O
.	O
,	O
2015;Li	O
et	O
al	O
.	O
,	O
2013	O
)	O
,	O
we	O
used	O
the	O
same	O
test	O
set	O
with	O
40	O
newswire	O
articles	O
and	O
the	O
same	O
development	O
set	O
with	O
30	O
documents	O
and	O
the	O
rest	O
529	O
documents	O
are	O
used	O
for	O
ACE	O
training	O
set	O
.	O

The	O
authors	O
would	O
like	O
to	O
thank	O
Chuck	O
Rosenberg	O
,	O
Tom	O
Duerig	O
,	O
Neil	O
Alldrin	O
,	O
Zhen	O
Li	O
,	O
Filipe	O
Gonc	O
¸alves	O
,	O
Mia	O
Chen	O
,	O
Zhifeng	O
Chen	O
,	O
Samy	O
Bengio	O
,	O
Yu	O
Zhang	O
,	O
Kevin	O
Swersky	O
,	O
Felix	O
Hill	O
and	O
the	O
ACL	O
anonymous	O
reviewers	O
for	O
their	O
valuable	O
advice	O
and	O
feedback	O
.	O

Acknowledgments	O
.	O

We	O
expect	O
that	O
integrating	O
Picturebook	B-MethodName
with	O
these	O
embeddings	O
to	O
lead	O
to	O
further	O
performance	O
improvements	O
as	O
well	O
.	O

Recently	O
,	O
contextualized	O
word	O
representations	O
have	O
shown	O
promising	O
improvements	O
when	O
combined	O
with	O
existing	O
embeddings	O
(	O
Melamud	O
et	O
al	O
.	O
,	O
2016;Peters	O
et	O
al	O
.	O
,	O
2017;McCann	O
et	O
al	O
.	O
,	O
2017;Peters	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

In	O
future	O
work	O
,	O
we	O
would	O
like	O
to	O
explore	O
other	O
aspects	O
of	O
search	O
engines	O
for	O
language	O
grounding	O
as	O
well	O
as	O
the	O
effect	O
these	O
embeddings	O
may	O
have	O
on	O
learning	O
generic	O
sentence	O
representations	O
(	O
Kiros	O
et	O
al	O
.	O
,	O
2015b;Hill	O
et	O
al	O
.	O
,	O
2016;Conneau	O
et	O
al	O
.	O
,	O
2017a;Logeswaran	O
and	O
Lee	O
,	O
2018	O
)	O
.	O

Through	O
the	O
use	O
of	O
multimodal	O
gating	O
,	O
our	O
models	O
lead	O
to	O
interpretable	O
weightings	O
of	O
abstract	O
vs	O
concrete	O
words	O
.	O

In	O
this	O
work	O
we	O
demonstrated	O
that	O
Picturebook	B-MethodName
complements	O
traditional	O
embeddings	O
on	O
a	O
wide	O
variety	O
of	O
tasks	O
.	O

Picturebook	B-MethodName
embeddings	O
offer	O
an	O
alternative	O
approach	O
to	O
constructing	O
word	O
representations	O
grounded	O
in	O
image	O
search	O
engines	O
.	O

Traditionally	O
,	O
word	O
representations	O
have	O
been	O
built	O
on	O
co	O
-	O
occurrences	O
of	O
neighbouring	O
words	O
;	O
and	O
such	O
representations	O
only	O
make	O
use	O
of	O
the	O
statistics	O
of	O
the	O
text	O
distribution	O
.	O

Conclusion	O
.	O

We	O
also	O
observe	O
tags	O
which	O
are	O
exclusively	O
Glove	B-MethodName
oriented	O
,	O
namely	O
adverbs	O
(	O
RB	O
)	O
,	O
prepositions	O
(	O
IN	O
)	O
and	O
determiners	O
(	O
DT	O
)	O
.	O

The	O
highest	O
scoring	O
Picturebook	B-MethodName
words	O
are	O
almost	O
all	O
singular	O
and	O
plural	O
nouns	O
(	O
NN	O
/	O
NNS	O
)	O
.	O

These	O
results	O
are	O
shown	O
in	O
Figure	O
1	O
.	O

MIXER	O
(	O
Ranzato	O
et	O
al	O
.	O
,	O
2016	O
)	O
21.8	O
Beam	O
Search	O
Optimization	O
(	O
Wiseman	O
and	O
Rush	O
,	O
2016	O
)	O
25.5	O
Actor	O
-	O
Critic	O
+	O
Log	O
Likelihood	O
(	O
Bahdanau	O
et	O
al	O
.	O
,	O
2017	O
)	O
28.5	O
Neural	O
Phrase	O
-	O
based	O
Machine	O
Translation	O
(	O
Huang	O
et	O
al	O
.	O
,	O
2018a	O
)	O
29.9	O
Finally	O
we	O
analyze	O
the	O
parts	O
-	O
of	O
-	O
speech	O
(	O
POS	O
)	O
of	O
the	O
highest	O
activated	O
words	O
.	O

Appendix	O
A	O
contains	O
examples	O
of	O
words	O
that	O
most	O
strongly	O
activate	O
Glove	B-MethodName
and	O
Picturebook	B-MethodName
gates	O
.	O

These	O
results	O
provide	O
evidence	O
that	O
our	O
gating	O
mechanism	O
actively	O
prefers	O
Glove	B-MethodName
embeddings	O
for	O
abstract	O
words	O
and	O
Picturebook	B-MethodName
embeddings	O
for	O
concrete	O
words	O
.	O

Moreover	O
,	O
this	O
result	O
holds	O
true	O
across	O
all	O
datasets	O
,	O
even	O
those	O
that	O
are	O
not	O
inherently	O
visual	O
.	O

We	O
observe	O
that	O
gates	O
have	O
high	O
correlations	O
with	O
concreteness	B-MetricName
ratings	I-MetricName
and	O
strong	O
negative	O
correlations	O
with	O
image	B-MetricName
dispersion	I-MetricName
scores	O
.	O

Table	O
10	O
illustrates	O
the	O
result	O
of	O
this	O
analysis	O
.	O

We	O
then	O
compute	O
the	O
Spearman	B-MetricName
correlation	I-MetricName
of	O
mean	O
gate	O
activations	O
with	O
a	O
)	O
concreteness	B-MetricName
ratings	I-MetricName
and	O
b	O
)	O
image	B-MetricName
dispersion	I-MetricName
scores	O
.	O

4	O
For	O
concreteness	O
ratings	O
,	O
we	O
take	O
the	O
intersection	O
of	O
words	O
that	O
have	O
ratings	O
with	O
the	O
dataset	O
vocabulary	O
.	O

For	O
each	O
word	O
,	O
we	O
compute	O
the	O
mean	O
gate	O
activation	O
value	O
for	O
Picturebook	B-MethodName
embeddings	O
.	O

On	O
the	O
other	O
hand	O
,	O
low	O
dispersion	O
ratings	O
were	O
more	O
associated	O
with	O
concrete	O
words	O
.	O

(	O
2014	O
)	O
that	O
abstract	O
words	O
tend	O
to	O
have	O
higher	O
dispersion	O
ratings	O
,	O
due	O
to	O
having	O
much	O
higher	O
variety	O
in	O
the	O
types	O
of	O
images	O
returned	O
from	O
a	O
query	O
.	O

It	O
was	O
shown	O
in	O
Kiela	O
et	O
al	O
.	O

Image	O
dispersion	O
is	O
the	O
average	O
distance	O
between	O
all	O
pairs	O
of	O
images	O
returned	O
from	O
a	O
search	O
query	O
.	O

(	O
2013	O
)	O
which	O
provides	O
ratings	O
for	O
40,000	O
English	O
lemmas	O
.	O

For	O
concreteness	O
ratings	O
,	O
we	O
use	O
the	O
dataset	O
of	O
Brysbaert	O
et	O
al	O
.	O

In	O
our	O
first	O
experiment	O
,	O
we	O
aim	O
to	O
determine	O
how	O
well	O
gate	O
activations	O
correlate	O
to	O
a	O
)	O
human	O
judgments	O
of	O
concreteness	O
and	O
b	O
)	O
image	O
dispersion	O
(	O
Kiela	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

In	O
this	O
section	O
we	O
perform	O
an	O
extensive	O
analysis	O
of	O
the	O
gating	O
mechanism	O
for	O
models	O
trained	O
across	O
datasets	O
used	O
in	O
our	O
experiments	O
.	O

Gate	O
Analysis	O
.	O

This	O
indicates	O
that	O
while	O
our	O
embeddings	O
are	O
useful	O
for	O
smaller	O
MT	B-TaskName
experiments	O
,	O
further	O
research	O
is	O
needed	O
on	O
how	O
to	O
best	O
incorporate	O
grounded	O
representations	O
in	O
larger	O
translation	O
tasks	O
.	O

However	O
,	O
we	O
were	O
not	O
able	O
to	O
improve	O
upon	O
BLEU	B-MetricName
scores	O
from	O
equivalent	O
models	O
that	O
do	O
not	O
use	O
Picturebook	B-MethodName
.	O

For	O
these	O
tasks	O
,	O
we	O
found	O
that	O
models	O
that	O
incorporate	O
Picturebook	B-MethodName
led	O
to	O
faster	O
convergence	O
.	O

We	O
explored	O
the	O
use	O
of	O
Picturebook	B-MethodName
for	O
larger	O
machine	B-TaskName
translation	I-TaskName
tasks	O
,	O
including	O
the	O
popular	O
WMT14	B-DatasetName
benchmarks	O
.	O

Limitations	O
.	O

We	O
also	O
note	O
that	O
our	O
models	O
may	O
not	O
be	O
directly	O
comparable	O
to	O
previously	O
published	O
seq2seq	O
models	O
from	O
(	O
Wiseman	O
and	O
Rush	O
,	O
2016;Bahdanau	O
et	O
al	O
.	O
,	O
2017	O
)	O
since	O
we	O
used	O
a	O
deeper	O
encoder	O
and	O
decoder	O
.	O

We	O
note	O
that	O
the	O
NPMT	O
is	O
not	O
a	O
seq2seq	O
model	O
and	O
can	O
be	O
augmented	O
with	O
our	O
Picturebook	B-MethodName
embeddings	O
.	O

We	O
(	O
Huang	O
et	O
al	O
.	O
,	O
2018a	O
)	O
.	O

We	O
also	O
report	O
results	O
for	O
the	O
IWSLT	B-DatasetName
2014	I-DatasetName
German	I-DatasetName
-	I-DatasetName
English	I-DatasetName
task	O
(	O
Cettolo	O
et	O
al	O
.	O
,	O
2014	O
)	O
in	O
Table	O
9	O
.	O

Compared	O
to	O
our	O
baseline	O
,	O
we	O
report	O
a	O
gain	O
of	O
0.3	B-MetricValue
and	O
1.1	B-MetricValue
BLEU	B-MetricName
for	O
German	O
!	O
English	O
and	O
English	O
!	O
German	O
respectively	O
.	O

On	O
the	O
English	O
!	O
French	O
task	O
,	O
the	O
Picturebook	B-MethodName
models	O
do	O
on	O
average	O
1.2	B-MetricValue
BLEU	B-MetricName
better	O
or	O
1.0	B-MetricValue
METEOR	B-MetricName
over	O
our	O
baseline	O
.	O

We	O
suspect	O
this	O
is	O
due	O
to	O
the	O
fact	O
that	O
we	O
did	O
not	O
use	O
BPE	O
.	O

On	O
the	O
German	O
task	O
,	O
compared	O
to	O
the	O
previously	O
best	O
published	O
results	O
(	O
Caglayan	O
et	O
al	O
.	O
,	O
2017	O
)	O
we	O
do	O
better	O
in	O
BLEU	B-MetricName
but	O
slightly	O
worse	O
in	O
METEOR	B-MetricName
.	O

On	O
the	O
English	O
!	O
German	O
tasks	O
,	O
we	O
find	O
our	O
Picturebook	B-MethodName
model	O
to	O
perform	O
on	O
average	O
0.8	B-MetricValue
BLEU	B-MetricName
or	O
0.7	B-MetricValue
METEOR	B-MetricName
over	O
our	O
baseline	O
.	O

We	O
did	O
not	O
experiment	O
with	O
regularizing	O
the	O
norm	O
of	O
the	O
embeddings	O
.	O

We	O
find	O
the	O
gating	O
mechanism	O
not	O
to	O
help	O
much	O
with	O
the	O
MT	B-TaskName
task	O
since	O
the	O
trainable	O
embeddings	O
are	O
free	O
to	O
change	O
their	O
norm	O
magnitudes	O
.	O

Since	O
seq2seq	O
MT	B-TaskName
models	O
are	O
typically	O
trained	O
without	O
Glove	B-MethodName
embeddings	O
,	O
we	O
also	O
did	O
not	O
use	O
Glove	B-MethodName
embeddings	O
for	O
this	O
task	O
,	O
but	O
rather	O
we	O
combine	O
randomly	O
initialized	O
learnable	O
embeddings	O
with	O
the	O
fixed	O
Picturebook	B-MethodName
embeddings	O
.	O

This	O
is	O
also	O
highlighted	O
where	O
our	O
French	O
models	O
perform	O
better	O
than	O
our	O
German	O
models	O
relatively	O
,	O
due	O
to	O
the	O
compounding	O
nature	O
of	O
German	O
words	O
.	O

We	O
believe	O
this	O
is	O
due	O
to	O
the	O
fact	O
we	O
did	O
not	O
use	O
Byte	O
Pair	O
Encoding	O
(	O
BPE	O
)	O
(	O
Sennrich	O
et	O
al	O
.	O
,	O
2016	O
)	O
,	O
and	O
ME	B-MetricName
-	I-MetricName
TEOR	I-MetricName
captures	O
word	O
stemming	O
(	O
Denkowski	O
and	O
Lavie	O
,	O
2014	O
)	O
.	O

We	O
find	O
our	O
models	O
to	O
perform	O
better	O
in	O
BLEU	B-MetricName
than	O
METEOR	B-MetricName
relatively	O
com	O
-	O
pared	O
to	O
(	O
Caglayan	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

Table	O
7	O
summarizes	O
our	O
English	O
!	O
German	O
results	O
and	O
Table	O
8	O
summarizes	O
our	O
English	O
!	O
French	O
results	O
.	O

We	O
use	O
the	O
standard	O
seq2seq	O
(	O
Sutskever	O
et	O
al	O
.	O
,	O
2015	O
)	O
with	O
content	O
-	O
based	O
attention	O
(	O
Bahdanau	O
et	O
al	O
.	O
,	O
2015	O
)	O
model	O
and	O
we	O
describe	O
our	O
hyperparmeters	O
in	O
Appendix	O
B.	O

(	O
2017	O
)	O
,	O
the	O
winner	O
of	O
the	O
WMT	O
17	O
Multimodal	O
Machine	O
Translation	O
competition	O
(	O
Elliott	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

We	O
compare	O
our	O
Picturebook	B-MethodName
models	O
with	O
other	O
text	O
-	O
only	O
nonensembled	O
models	O
on	O
the	O
Flickr	B-DatasetName
Test2016	I-DatasetName
,	O
Flickr	B-DatasetName
Test2017	I-DatasetName
and	O
MSCOCO	B-DatasetName
test	O
sets	O
from	O
Caglayan	O
et	O
al	O
.	O

We	O
experiment	O
with	O
the	O
Multi30k	B-DatasetName
(	O
Elliott	O
et	O
al	O
.	O
,	O
2016(Elliott	O
et	O
al	O
.	O
,	O
,	O
2017	O
)	O
)	O
dataset	O
for	O
MT	B-TaskName
.	O

Machine	B-TaskName
Translation	I-TaskName
.	O

(	O
2018	O
)	O
,	O
which	O
are	O
more	O
sophisticated	O
methods	O
that	O
incorporate	O
generative	O
modelling	O
,	O
reinforcement	O
learning	O
and	O
attention	O
.	O

(	O
2018b	O
)	O
;	O
Lee	O
et	O
al	O
.	O

(	O
2018	O
)	O
;	O
Huang	O
et	O
al	O
.	O

Our	O
reported	O
results	O
have	O
been	O
recently	O
outperformed	O
by	O
Gu	O
et	O
al	O
.	O

However	O
,	O
using	O
contextual	O
gating	O
results	O
in	O
improvements	O
over	O
the	O
baseline	O
on	O
all	O
metrics	O
except	O
R@1	B-MetricName
for	O
image	O
annotation	O
.	O

Glove+Picturebook	B-MethodName
improves	O
over	O
the	O
Glove	B-MethodName
baseline	O
for	O
image	O
search	O
but	O
falls	O
short	O
on	O
image	O
annotation	O
.	O

(	O
2017	O
)	O
with	O
the	O
exception	O
of	O
Recall@10	B-MetricName
for	O
image	O
annotation	O
,	O
where	O
it	O
performs	O
slightly	O
worse	O
.	O

Our	O
Glove	B-MethodName
baseline	O
was	O
able	O
to	O
match	O
or	O
outperform	O
the	O
reported	O
results	O
in	O
Faghri	O
et	O
al	O
.	O

BoW	O
(	O
Zhang	O
et	O
al	O
.	O
,	O
2015	O
)	O
88.8	O
96.6	O
92.2	O
58.0	O
68.9	O
54.6	O
90.4	O
ngrams	O
(	O
Zhang	O
et	O
al	O
.	O
,	O
2015	O
)	O
92.0	O
98.6	O
95.6	O
56.3	O
68.5	O
54.3	O
92.0	O
ngrams	O
TFIDF	O
(	O
Zhang	O
et	O
al	O
.	O
,	O
2015	O
)	O
92.4	O
98.7	O
95.4	O
54.8	O
68.5	O
52.4	O
91.5	O
fastText	O
(	O
Joulin	O
et	O
al	O
.	O
,	O
2017	O
)	O
91	O
Image	O
Annotation	O
Image	O
Search	O
Model	O
R@1	B-MetricName
R@5	B-MetricName
R@10	B-MetricName
Med	B-MetricName
r	I-MetricName
R@1	B-MetricName
R@5	B-MetricName
R@10	B-MetricName
Med	B-MetricName
r	I-MetricName
VSE++	B-MethodName
(	O
Faghri	O
et	O
Table	O
6	O
:	O
COCO	O
test	O
-	O
set	O
results	O
for	O
image	O
-	O
sentence	O
retrieval	O
experiments	O
.	O

Med	B-MetricName
r	I-MetricName
is	O
the	O
median	B-MetricName
rank	I-MetricName
(	O
low	O
is	O
good	O
)	O
.	O

R@K	B-MetricName
is	O
Recall@K	B-MetricName
(	O
high	O
is	O
good	O
)	O
.	O

Our	O
models	O
use	O
VSE++	B-MethodName
.	O

P.	O

Amz	O
.	O

F.	O

Amz	O
.	O

A.	O

Yah	O
.	O

Yelp	O
F.	O

Model	O
AG	O
DBP	O
Yelp	O
P.	O

Table	O
6	O
displays	O
our	O
results	O
on	O
this	O
task	O
.	O

Full	O
details	O
of	O
the	O
hyperparameters	O
are	O
in	O
Appendix	O
B.	O

As	O
in	O
previous	O
work	O
,	O
we	O
report	O
the	O
mean	B-MetricName
Recall@K	I-MetricName
(	O
R@K	B-MetricName
)	O
and	O
the	O
median	B-MetricName
rank	I-MetricName
over	O
1000	O
images	O
and	O
5000	O
sentences	O
.	O

We	O
re	O
-	O
implement	O
their	O
model	O
with	O
2	O
modifications	O
:	O
1	O
)	O
we	O
replace	O
the	O
unidirectional	O
LSTM	O
encoder	O
with	O
a	O
BiLSTM	O
-	O
Max	O
sentence	O
encoder	O
and	O
2	O
)	O
we	O
use	O
Inception	B-MethodName
-	I-MethodName
V3	I-MethodName
(	O
Szegedy	O
et	O
al	O
.	O
,	O
2016	O
)	O
as	O
our	O
CNN	O
instead	O
of	O
ResNet	O
152	O
(	O
He	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

(	O
2015a	O
)	O
by	O
using	O
hard	O
negatives	O
instead	O
of	O
summing	O
over	O
contrastive	O
examples	O
.	O

VSE++	B-MethodName
improves	O
over	O
the	O
original	O
CNN	O
-	O
LSTM	O
embedding	O
method	O
of	O
Kiros	O
et	O
al	O
.	O

Here	O
,	O
we	O
utilize	O
VSE++	B-MethodName
(	O
Faghri	O
et	O
al	O
.	O
,	O
2017	O
)	O
as	O
our	O
base	O
model	O
and	O
evaluate	O
on	O
the	O
COCO	B-DatasetName
dataset	O
(	O
Lin	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

We	O
next	O
consider	O
experiments	O
that	O
map	O
images	O
and	O
sentences	O
into	O
a	O
common	O
vector	O
space	O
for	O
retrieval	O
.	O

Image	B-TaskName
-	I-TaskName
Sentence	I-TaskName
Ranking	I-TaskName
.	O

We	O
note	O
that	O
the	O
best	O
performing	O
methods	O
on	O
these	O
tasks	O
are	O
based	O
on	O
convolutional	O
neural	O
networks	O
(	O
Conneau	O
et	O
al	O
.	O
,	O
2017b	O
)	O
.	O

This	O
result	O
shows	O
that	O
our	O
embeddings	O
are	O
able	O
to	O
work	O
as	O
a	O
general	O
text	O
embedding	O
,	O
though	O
they	O
typically	O
lag	O
behind	O
Glove	B-MethodName
.	O

Our	O
results	O
show	O
that	O
Picturebook	B-MethodName
embeddings	O
,	O
while	O
minimally	O
aiding	O
in	O
performance	O
,	O
can	O
perform	O
reasonably	O
well	O
on	O
their	O
own	O
-outperforming	O
the	O
n	O
-	O
gram	O
baselines	O
of	O
(	O
Zhang	O
et	O
al	O
.	O
,	O
2015	O
)	O
on	O
5	O
out	O
of	O
7	O
tasks	O
and	O
the	O
unigram	O
fastText	O
baseline	O
on	O
all	O
7	O
tasks	O
.	O

Perhaps	O
unsurprisingly	O
,	O
adding	O
Picturebook	B-MethodName
to	O
Glove	B-MethodName
matches	O
or	O
only	O
slightly	O
improves	O
on	O
5	O
out	O
of	O
7	O
tasks	O
and	O
obtains	O
a	O
lower	O
result	O
on	O
AG	O
News	O
and	O
Yahoo	O
.	O

Our	O
experimental	O
results	O
are	O
provided	O
in	O
Table	O
5	O
.	O

Hyperparameter	O
details	O
are	O
reported	O
in	O
Appendix	O
B.	O

(	O
2015	O
)	O
and	O
compare	O
bag	O
-	O
ofwords	O
models	O
against	O
n	O
-	O
gram	O
baselines	O
provided	O
by	O
the	O
authors	O
as	O
well	O
as	O
fastText	O
(	O
Joulin	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

We	O
experiment	O
with	O
7	O
datasets	O
provided	O
by	O
Zhang	O
et	O
al	O
.	O

Our	O
next	O
set	O
of	O
experiments	O
aims	O
to	O
determine	O
how	O
well	O
Picturebook	B-MethodName
embeddings	O
do	O
on	O
tasks	O
that	O
are	O
primarily	O
non	O
-	O
visual	O
,	O
such	O
as	O
topic	B-TaskName
and	I-TaskName
sentiment	I-TaskName
classification	I-TaskName
.	O

Sentiment	B-TaskName
and	I-TaskName
Topic	I-TaskName
Classification	I-TaskName
.	O

3	O
.	O

(	O
2017a	O
)	O
,	O
from	O
which	O
we	O
improve	O
on	O
their	O
accuracy	B-MetricName
from	O
85.0	B-MetricValue
to	O
86.8	B-MetricValue
on	O
the	O
development	O
set	O
.	O

Finally	O
we	O
note	O
the	O
strength	O
of	O
our	O
own	O
Glove	B-MethodName
baseline	O
over	O
the	O
reported	O
results	O
of	O
Conneau	O
et	O
al	O
.	O

Adding	O
contextual	O
gating	O
was	O
necessary	O
to	O
improve	O
over	O
the	O
Glove	B-MethodName
baseline	O
on	O
SNLI	B-DatasetName
.	O

While	O
non	O
-	O
contextual	O
gating	O
is	O
sufficient	O
to	O
improve	O
bag	O
-	O
of	O
-	O
words	O
methods	O
,	O
with	O
BiLSTM	O
-	O
Max	O
it	O
slightly	O
hurts	O
performance	O
over	O
the	O
Glove	B-MethodName
baseline	O
.	O

It	O
is	O
worth	O
noting	O
the	O
effect	O
that	O
different	O
encoders	O
have	O
when	O
using	O
our	O
embeddings	O
.	O

(	O
2018	O
)	O
.	O

For	O
BiLSTM	O
-	O
Max	O
,	O
our	O
contextual	O
gating	O
sets	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
SNLI	B-DatasetName
sentence	O
encoding	O
methods	O
(	O
methods	O
without	O
interaction	O
layers	O
)	O
,	O
outperforming	O
the	O
recently	O
proposed	O
methods	O
of	O
I	O
m	O
and	O
Cho	O
(	O
2017	O
)	O
;	O
Shen	O
et	O
al	O
.	O

For	O
BoW	O
models	O
,	O
adding	O
Picturebook	B-MethodName
embeddings	O
to	O
Glove	B-MethodName
results	O
in	O
significant	O
gains	O
across	O
all	O
three	O
tasks	O
.	O

Table	O
4	O
displays	O
our	O
results	O
.	O

The	O
full	O
details	O
of	O
hyperparameters	O
are	O
discussed	O
in	O
Appendix	O
B.	O

Due	O
to	O
the	O
small	O
size	O
of	O
the	O
dataset	O
,	O
we	O
only	O
experiment	O
with	O
BoW	O
on	O
SICK	B-DatasetName
.	O

We	O
explore	O
the	O
use	O
of	O
two	O
types	O
of	O
sentential	O
encoders	O
:	O
Bag	O
-	O
of	O
-	O
Words	O
(	O
BoW	O
)	O
and	O
BiLSTM	O
-	O
Max	O
(	O
Conneau	O
et	O
al	O
.	O
,	O
2017a	O
For	O
SICK	O
,	O
we	O
follow	O
previous	O
work	O
and	O
report	O
average	O
results	O
across	O
5	O
runs	O
(	O
Tai	O
et	O
al	O
.	O
,	O
2015	O
)	O
.	O

The	O
first	O
two	O
are	O
natural	O
language	O
inference	O
tasks	O
and	O
the	O
third	O
is	O
a	O
sentence	O
semantic	O
relatedness	O
task	O
.	O

We	O
next	O
consider	O
experiments	O
on	O
3	O
pairwise	O
prediction	O
datasets	O
:	O
SNLI	B-DatasetName
(	O
Bowman	O
et	O
al	O
.	O
,	O
2015	O
)	O
,	O
MultiNLI	B-DatasetName
(	O
Williams	O
et	O
al	O
.	O
,	O
2017	O
)	O
and	O
SICK	B-DatasetName
(	O
Marelli	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

Sentential	O
Inference	O
and	O
Relatedness	O
.	O

All	O
subsequent	O
experiments	O
use	O
10	O
images	O
with	O
semantic	O
Picturebook	B-MethodName
.	O

(	O
2016	O
)	O
showed	O
that	O
after	O
10	O
-	O
20	O
images	O
,	O
performance	O
tends	O
to	O
saturate	O
.	O

Kiela	O
et	O
al	O
.	O

Finally	O
we	O
note	O
that	O
adding	O
more	O
images	O
nearly	O
consistently	O
improves	O
similarity	O
scores	O
across	O
categories	O
.	O

This	O
indicates	O
the	O
importance	O
of	O
the	O
type	O
of	O
similarity	O
used	O
for	O
training	O
the	O
model	O
.	O

We	O
observe	O
a	O
performance	O
difference	O
between	O
our	O
visual	O
and	O
semantic	O
embeddings	O
:	O
on	O
all	O
categories	O
except	O
verbs	O
,	O
the	O
semantic	O
embeddings	O
outperform	O
visual	O
ones	O
,	O
even	O
on	O
the	O
most	O
concrete	O
categories	O
.	O

We	O
also	O
compare	O
to	O
a	O
convolutional	O
network	O
trained	O
with	O
visual	O
similarity	O
.	O

For	O
the	O
hardest	O
subset	O
of	O
words	O
,	O
Picturebook	B-MethodName
performs	O
slightly	O
better	O
than	O
Glove	B-MethodName
while	O
Glove	B-MethodName
performs	O
better	O
across	O
all	O
pairs	O
.	O

Next	O
we	O
observe	O
that	O
the	O
performance	O
of	O
Picturebook	B-MethodName
gets	O
progressively	O
better	O
across	O
each	O
concreteness	O
quartile	O
rating	O
,	O
with	O
a	O
20	O
point	O
improvement	O
over	O
Glove	B-MethodName
for	O
the	O
most	O
concrete	O
category	O
.	O

This	O
result	O
confirms	O
that	O
Glove	B-MethodName
and	O
Picturebook	B-MethodName
capture	O
very	O
different	O
properties	O
of	O
words	O
.	O

For	O
adjectives	O
and	O
the	O
most	O
abstract	O
category	O
,	O
Glove	B-MethodName
performs	O
significantly	O
better	O
,	O
while	O
for	O
the	O
most	O
concrete	O
category	O
Picturebook	B-MethodName
is	O
significantly	O
better	O
.	O

First	O
,	O
we	O
observe	O
that	O
combining	O
Glove	B-MethodName
and	O
Picturebook	B-MethodName
leads	O
to	O
improved	O
similarity	O
across	O
most	O
categories	O
.	O

Table	O
3	O
displays	O
our	O
results	O
,	O
from	O
which	O
several	O
observations	O
can	O
be	O
made	O
.	O

By	O
default	O
,	O
we	O
use	O
10	O
images	O
for	O
each	O
embedding	O
using	O
the	O
semantic	O
convolutional	O
network	O
.	O

We	O
also	O
report	O
results	O
combining	O
Glove	B-MethodName
and	O
Picturebook	B-MethodName
by	O
summing	O
their	O
two	O
independent	O
similarity	O
scores	O
.	O

Note	O
that	O
this	O
reduces	O
to	O
negative	O
cosine	O
distance	O
when	O
using	O
only	O
1	O
image	O
per	O
word	O
.	O

2	O
That	O
is	O
,	O
the	O
score	O
is	O
minus	O
the	O
smallest	O
cosine	O
distance	O
between	O
all	O
pairs	O
of	O
images	O
of	O
the	O
two	O
words	O
.	O

For	O
computing	O
a	O
score	O
between	O
2	O
word	O
pairs	O
with	O
Picturebook	O
,	O
we	O
set	O
s(w	O
(	O
1	O
)	O
,	O
w	O
(	O
2	O
)	O
)	O
=	O
min	O
i	O
,	O
j	O
d(e	O
i	O
,	O
e	O
(	O
2	O
)	O
j	O
)	O
.	O

are	O
computed	O
via	O
cosine	O
similarity	O
.	O

Some	O
rows	O
are	O
copied	O
across	O
sections	O
for	O
ease	O
of	O
reading	O
.	O

Bracketed	O
numbers	O
signify	O
the	O
number	O
of	O
images	O
used	O
.	O

Best	O
results	O
per	O
section	O
are	O
underlined	O
.	O

Best	O
results	O
overall	O
are	O
bolded	O
.	O

For	O
Glove	O
,	O
scores	O
Table	O
3	O
:	O
SimLex-999	B-DatasetName
results	O
(	O
Spearman	O
's	O
⇢	O
)	O
.	O

This	O
is	O
an	O
interesting	O
category	O
since	O
image	O
-	O
based	O
word	O
embeddings	O
are	O
perhaps	O
less	O
likely	O
to	O
confuse	O
similarity	O
with	O
relatedness	O
than	O
distributional	O
-	O
based	O
methods	O
.	O

The	O
hardest	O
pairs	O
are	O
those	O
for	O
which	O
similarity	O
is	O
difficult	O
to	O
distinguish	O
from	O
relatedness	O
.	O

For	O
the	O
concreteness	O
quartiles	O
,	O
the	O
first	O
quartile	O
corresponds	O
to	O
the	O
most	O
abstract	O
words	O
,	O
while	O
the	O
last	O
corresponds	O
to	O
the	O
most	O
concrete	O
words	O
.	O

We	O
use	O
the	O
SimLex-999	B-DatasetName
dataset	O
(	O
Hill	O
et	O
al	O
.	O
,	O
2015	O
)	O
and	O
report	O
results	O
across	O
9	O
categories	O
:	O
all	O
(	O
the	O
whole	O
evaluation	O
)	O
,	O
adjectives	O
,	O
nouns	O
,	O
verbs	O
,	O
concreteness	O
quartiles	O
and	O
the	O
hardest	O
333	O
pairs	O
.	O

Our	O
first	O
quantitative	O
experiment	O
aims	O
to	O
determine	O
how	O
well	O
Picturebook	B-MethodName
embeddings	O
capture	O
word	O
similarity	O
.	O

We	O
also	O
report	O
nearest	O
neighbour	O
examples	O
across	O
languages	O
in	O
Appendix	O
A.1	O
.	O
Word	B-TaskName
similarity	I-TaskName
.	O

For	O
example	O
,	O
the	O
word	O
'	O
is	O
'	O
returns	O
words	O
related	O
to	O
terrorists	O
and	O
ISIS	O
and	O
'	O
it	O
'	O
returns	O
words	O
related	O
to	O
scary	O
and	O
clowns	O
due	O
to	O
the	O
2017	O
film	O
of	O
the	O
same	O
name	O
.	O

Finally	O
,	O
it	O
's	O
worth	O
highlighting	O
that	O
the	O
most	O
frequent	O
association	O
of	O
a	O
word	O
may	O
not	O
be	O
what	O
is	O
represented	O
in	O
image	O
search	O
results	O
.	O

Words	O
like	O
'	O
sun	O
'	O
also	O
return	O
the	O
corresponding	O
word	O
in	O
different	O
languages	O
,	O
such	O
as	O
'	O
Sol	O
'	O
in	O
Spanish	O
and	O
'	O
Soleil	O
'	O
in	O
French	O
.	O

Searching	O
for	O
cities	O
returns	O
cities	O
which	O
have	O
visually	O
similar	O
characteristics	O
.	O

Some	O
words	O
capture	O
multimodality	O
,	O
such	O
as	O
'	O
deep	O
'	O
referring	O
both	O
to	O
deep	O
sea	O
as	O
well	O
as	O
to	O
AI	O
.	O

Often	O
this	O
captures	O
visual	O
similarity	O
as	O
well	O
.	O

These	O
results	O
can	O
be	O
interpreted	O
as	O
follows	O
:	O
the	O
words	O
that	O
appear	O
as	O
neighbours	O
are	O
those	O
which	O
have	O
semantically	O
similar	O
images	O
to	O
that	O
of	O
the	O
query	O
.	O

In	O
order	O
to	O
get	O
a	O
sense	O
of	O
the	O
representations	O
our	O
model	O
learns	O
,	O
we	O
first	O
compute	O
nearest	O
neighbour	O
results	O
of	O
several	O
words	O
,	O
shown	O
in	O
Table	O
2	O
.	O

Nearest	O
neighbours	O
.	O

In	O
most	O
experiments	O
,	O
we	O
end	O
up	O
with	O
baselines	O
that	O
are	O
stronger	O
than	O
what	O
has	O
previously	O
been	O
reported	O
.	O

Since	O
the	O
use	O
of	O
Picturebook	B-MethodName
embeddings	O
adds	O
extra	O
parameters	O
to	O
our	O
models	O
,	O
we	O
include	O
a	O
baseline	O
for	O
each	O
experiment	O
(	O
either	O
based	O
on	O
Glove	O
or	O
learned	O
embeddings	O
)	O
that	O
we	O
extensively	O
tune	O
.	O

Hyperparameter	O
details	O
of	O
each	O
experiment	O
are	O
included	O
in	O
the	O
appendix	O
.	O

To	O
evaluate	O
the	O
effectiveness	O
of	O
our	O
embeddings	O
,	O
we	O
perform	O
both	O
quantitative	O
and	O
qualitative	O
evaluation	O
across	O
a	O
wide	O
range	O
of	O
natural	O
language	O
processing	O
tasks	O
.	O

Experiments	O
.	O

A	O
similar	O
technique	O
to	O
tie	O
the	O
softmax	O
matrix	O
as	O
the	O
transpose	O
of	O
the	O
embedding	O
matrix	O
can	O
be	O
found	O
in	O
language	O
modelling	O
(	O
Press	O
and	O
Wolf	O
,	O
2017;Inan	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

In	O
practice	O
,	O
we	O
find	O
adding	O
additional	O
parameters	O
helps	O
with	O
learning	O
:	O
p(y	O
i	O
|h	O
)	O
=	O
exp(hh	O
,	O
e	O
i	O
+	O
e	O
0	O
i	O
i	O
+	O
b	O
i	O
)	O
P	O
j	O
exp(hh	O
,	O
e	O
j	O
+	O
e	O
0	O
j	O
i	O
+	O
b	O
j	O
)	O
(	O
7	O
)	O
where	O
e	O
0	O
i	O
is	O
a	O
trainable	O
weight	O
vector	O
per	O
word	O
and	O
b	O
i	O
is	O
a	O
trainable	O
bias	O
per	O
word	O
.	O

This	O
can	O
be	O
easily	O
implemented	O
by	O
setting	O
the	O
output	O
softmax	O
matrix	O
as	O
the	O
transpose	O
of	O
the	O
Picturebook	B-MethodName
embedding	O
matrix	O
E	O
p	O
.	O

Let	O
h	O
be	O
our	O
internal	O
representation	O
of	O
our	O
model	O
(	O
i.e.	O
,	O
seq2seq	O
decoder	O
state	O
)	O
,	O
and	O
e	O
i	O
be	O
the	O
i	O
-	O
th	O
word	O
embedding	O
from	O
our	O
Picturebook	B-MethodName
embedding	O
matrix	O
E	O
p	O
:	O
p(y	O
i	O
|h	O
)	O
=	O
exp(hh	O
,	O
e	O
i	O
i	O
)	O
P	O
j	O
exp(hh	O
,	O
e	O
j	O
i)(6	O
)	O
Given	O
a	O
representation	O
h	O
,	O
Equation	O
6	O
simply	O
finds	O
the	O
most	O
similar	O
word	O
in	O
the	O
embedding	O
space	O
.	O

We	O
introduce	O
a	O
differentiable	O
mechanism	O
which	O
allows	O
us	O
to	O
align	O
words	O
across	O
source	O
and	O
target	O
languages	O
in	O
the	O
Picturebook	B-MethodName
embedding	O
domain	O
.	O

We	O
want	O
to	O
perform	O
this	O
inverse	O
image	O
search	O
operation	O
given	O
its	O
Picturebook	B-MethodName
embedding	O
.	O

For	O
example	O
,	O
given	O
the	O
word	O
'	O
bicycle	O
'	O
in	O
English	O
and	O
its	O
Picturebook	B-MethodName
embedding	O
,	O
we	O
want	O
to	O
find	O
the	O
closest	O
French	O
word	O
that	O
would	O
generate	O
this	O
representation	O
(	O
i.e.	O
,	O
'	O
vélo	O
'	O
)	O
.	O

Given	O
a	O
Picturebook	B-MethodName
embedding	O
,	O
we	O
want	O
to	O
find	O
the	O
closest	O
word	O
or	O
phrase	O
aligned	O
to	O
the	O
representation	O
.	O

In	O
generative	O
modelling	O
problems	O
(	O
i.e.	O
,	O
MT	B-TaskName
)	O
,	O
we	O
want	O
to	O
perform	O
the	O
opposite	O
operation	O
.	O

Up	O
until	O
now	O
,	O
we	O
have	O
only	O
discussed	O
scenarios	O
where	O
we	O
have	O
a	O
word	O
and	O
we	O
want	O
to	O
perform	O
this	O
implicit	O
search	O
operation	O
.	O

Picturebook	B-MethodName
embeddings	O
can	O
be	O
seen	O
as	O
a	O
form	O
of	O
implicit	O
image	O
search	O
:	O
given	O
a	O
word	O
(	O
or	O
phrase	O
)	O
,	O
image	O
search	O
the	O
word	O
query	O
and	O
concatenate	O
the	O
embeddings	O
of	O
the	O
images	O
produced	O
by	O
a	O
CNN	O
.	O

Inverse	B-MethodName
Picturebook	I-MethodName
.	O

We	O
experiment	O
with	O
contextual	O
gating	O
for	O
all	O
experiments	O
that	O
use	O
a	O
bidirectional	O
-	O
LSTM	O
encoder	O
.	O

For	O
contextual	O
gates	O
,	O
we	O
use	O
the	O
same	O
approach	O
as	O
above	O
except	O
we	O
replace	O
the	O
controller	O
(	O
e	O
g	O
,	O
e	O
p	O
)	O
with	O
inputs	O
that	O
have	O
been	O
fed	O
through	O
a	O
bidirectional	O
-	O
LSTM	O
,	O
e.g.	O
(	O
BiLSTM(e	O
g	O
)	O
,	O
BiLSTM(e	O
p	O
)	O
)	O
.	O

In	O
some	O
cases	O
it	O
may	O
be	O
beneficial	O
to	O
use	O
contextual	O
gates	O
that	O
are	O
aware	O
of	O
the	O
sentence	O
that	O
words	O
appear	O
in	O
to	O
decide	O
how	O
to	O
weight	O
Glove	O
and	O
Picturebook	O
embeddings	O
.	O

The	O
gating	O
described	O
above	O
is	O
non	O
-	O
contextual	O
,	O
in	O
the	O
sense	O
that	O
each	O
embedding	O
computes	O
a	O
gate	O
value	O
independent	O
of	O
the	O
context	O
the	O
words	O
occur	O
in	O
.	O

Contextual	O
Gating	O
.	O

We	O
leave	O
comparison	O
of	O
alternative	O
fusion	O
strategies	O
for	O
future	O
work	O
.	O

We	O
chose	O
this	O
form	O
of	O
fusion	O
over	O
other	O
approaches	O
,	O
such	O
as	O
CCA	O
variants	O
and	O
metric	O
learning	O
methods	O
,	O
to	O
allow	O
for	O
easier	O
interpretability	O
and	O
analysis	O
.	O

On	O
some	O
experiments	O
we	O
found	O
it	O
beneficial	O
to	O
include	O
a	O
skip	O
connection	O
from	O
the	O
hidden	O
layer	O
of	O
.	O

Similar	O
gating	O
mechanisms	O
can	O
be	O
found	O
in	O
LSTMs	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
and	O
other	O
multimodal	O
models	O
(	O
Arevalo	O
et	O
al	O
.	O
,	O
2017;Wang	O
et	O
al	O
.	O
,	O
2018;Kiela	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

The	O
gating	O
DNN	O
allows	O
the	O
model	O
to	O
learn	O
how	O
visual	O
a	O
word	O
is	O
as	O
a	O
function	O
of	O
its	O
input	O
e	O
p	O
and	O
e	O
g	O
.	O

We	O
fuse	O
our	O
embeddings	O
using	O
a	O
multimodal	O
gating	O
mechanism	O
:	O
g	O
=	O
(	O
e	O
g	O
,	O
e	O
p	O
)	O
(	O
4	O
)	O
e	O
=	O
g	O
(	O
e	O
g	O
)	O
+	O
(	O
1	O
g	O
)	O
(	O
e	O
p	O
)	O
(	O
5	O
)	O
where	O
is	O
a	O
1	O
hidden	O
layer	O
DNN	O
with	O
ReLU	O
activations	O
and	O
sigmoid	O
outputs	O
,	O
and	O
are	O
1	O
hidden	O
layer	O
DNNs	O
with	O
ReLU	O
activations	O
and	O
tanh	O
outputs	O
.	O

Let	O
e	O
g	O
=	O
e	O
g	O
(	O
w	O
)	O
be	O
our	O
other	O
embedding	O
(	O
i.e.	O
,	O
Glove	O
)	O
for	O
a	O
word	O
w	O
and	O
e	O
p	O
=	O
e	O
p	O
(	O
w	O
)	O
be	O
our	O
Picturebook	B-MethodName
embedding	O
.	O

Consequently	O
,	O
we	O
would	O
like	O
to	O
fuse	O
our	O
Picturebook	B-MethodName
embeddings	O
with	O
other	O
sources	O
of	O
information	O
,	O
for	O
example	O
Glove	O
embeddings	O
(	O
Pennington	O
et	O
al	O
.	O
,	O
2014	O
)	O
or	O
randomly	O
initialized	O
embeddings	O
that	O
will	O
be	O
trained	O
.	O

Picturebook	B-MethodName
embeddings	O
on	O
their	O
own	O
are	O
likely	O
to	O
be	O
useful	O
for	O
representing	O
concrete	O
words	O
but	O
it	O
is	O
not	O
clear	O
whether	O
they	O
will	O
be	O
of	O
benefit	O
for	O
abstract	O
words	O
.	O

Multimodal	O
Fusion	O
Gating	O
.	O

As	O
we	O
will	O
show	O
in	O
our	O
experiments	O
,	O
the	O
semantic	O
Picturebook	B-MethodName
embeddings	O
result	O
in	O
representations	O
that	O
are	O
more	O
useful	O
for	O
natural	O
language	O
processing	O
tasks	O
than	O
the	O
visual	O
embeddings	O
.	O

In	O
our	O
experiments	O
we	O
consider	O
two	O
types	O
of	O
Picturebook	B-MethodName
embedding	O
:	O
one	O
trained	O
through	O
optimizing	O
for	O
visual	O
similarity	O
and	O
another	O
for	O
semantic	O
similarity	O
.	O

As	O
an	O
example	O
,	O
an	O
image	O
of	O
a	O
blue	O
car	O
would	O
have	O
high	O
visual	O
similarity	O
to	O
other	O
blue	O
cars	O
but	O
would	O
have	O
higher	O
semantic	O
similarity	O
to	O
cars	O
of	O
the	O
same	O
make	O
,	O
independent	O
of	O
color	O
.	O

We	O
consider	O
two	O
types	O
of	O
image	O
similarity	O
:	O
visual	O
and	O
semantic	O
.	O

The	O
training	O
procedure	O
is	O
heavily	O
influenced	O
by	O
the	O
choice	O
of	O
similarity	O
function	O
r	O
i	O
,	O
j	O
.	O

Visual	O
vs	O
Semantic	O
Similarity	O
.	O

To	O
obtain	O
the	O
full	O
collection	O
of	O
embeddings	O
,	O
we	O
run	O
the	O
full	O
Glove	O
vocabulary	O
(	O
2.2	O
M	O
words	O
)	O
through	O
image	O
search	O
to	O
obtain	O
a	O
corresponding	O
Picturebook	B-MethodName
embedding	O
to	O
each	O
word	O
in	O
the	O
Glove	O
vocabulary	O
.	O

Most	O
of	O
our	O
experiments	O
use	O
k	O
=	O
10	O
images	O
resulting	O
in	O
a	O
word	O
embedding	O
size	O
of	O
640	O
.	O

In	O
our	O
model	O
,	O
each	O
embedding	O
results	O
in	O
a	O
64	O
-	O
dimensional	O
vector	O
with	O
the	O
final	O
Picturebook	O
embedding	O
being	O
64	O
⇤	O
k	O
dimensions	O
.	O

;	O
f	O
(	O
p	O
w	O
k	O
)	O
]	O
(	O
3	O
)	O
namely	O
,	O
the	O
concatenation	O
of	O
the	O
feature	O
vectors	O
in	O
ranked	O
order	O
.	O

The	O
Picturebook	B-MethodName
embedding	O
for	O
a	O
word	O
w	O
is	O
then	O
represented	O
as	O
:	O
e	O
p	O
(	O
w	O
)	O
=	O
[	O
f	O
(	O
p	O
w	O
1	O
)	O
;	O
f	O
(	O
p	O
w	O
2	O
)	O
;	O
.	O

,	O
p	O
w	O
k	O
.	O

We	O
first	O
perform	O
an	O
image	O
search	O
with	O
query	O
w	O
to	O
obtain	O
a	O
ranked	O
list	O
of	O
images	O
p	O
w	O
1	O
,	O
.	O

Suppose	O
we	O
would	O
like	O
to	O
obtain	O
a	O
Picturebook	B-MethodName
embedding	O
for	O
a	O
given	O
word	O
w.	O

After	O
the	O
model	O
is	O
trained	O
,	O
we	O
can	O
use	O
the	O
convolutional	O
network	O
as	O
a	O
feature	O
extractor	O
for	O
images	O
by	O
computing	O
an	O
embedding	O
vector	O
f	O
(	O
p	O
)	O
for	O
an	O
image	O
p.	O

(	O
2014	O
)	O
for	O
additional	O
details	O
of	O
training	O
,	O
including	O
the	O
specifics	O
of	O
the	O
architecture	O
used	O
.	O

We	O
refer	O
the	O
reader	O
to	O
Wang	O
et	O
al	O
.	O

The	O
model	O
is	O
trained	O
end	O
-	O
to	O
-	O
end	O
using	O
a	O
proprietary	O
dataset	O
with	O
100	O
+	O
million	O
images	O
.	O

The	O
objective	O
function	O
that	O
is	O
optimized	O
is	O
given	O
by	O
:	O
min	O
X	O
i	O
⇠	O
i	O
+	O
kW	O
k	O
2	O
2	O
s.t	O
.	O
:	O
l(p	O
i	O
,	O
p	O
+	O
i	O
,	O
p	O
i	O
)	O
	O
⇠	O
i	O
8p	O
i	O
,	O
p	O
+	O
i	O
,	O
p	O
i	O
such	O
that	O
r(p	O
i	O
,	O
p	O
+	O
i	O
)	O
>	O
r(p	O
i	O
,	O
p	O
i	O
)	O
(	O
2	O
)	O
where	O
⇠	O
i	O
are	O
slack	O
variables	O
and	O
W	O
is	O
a	O
vector	O
of	O
the	O
network	O
's	O
model	O
parameters	O
.	O

Suppose	O
we	O
have	O
available	O
pairwise	O
relevance	O
scores	O
r	O
i	O
,	O
j	O
=	O
r(p	O
i	O
,	O
p	O
j	O
)	O
indicating	O
the	O
similarity	O
of	O
images	O
p	O
i	O
and	O
p	O
j	O
.	O

We	O
define	O
the	O
following	O
hinge	O
loss	O
for	O
a	O
given	O
triplet	O
as	O
follows	O
:	O
l(p	O
i	O
,	O
p	O
+	O
i	O
,	O
p	O
i	O
)	O
=	O
max{0	O
,	O
g	O
+	O
D(f	O
(	O
p	O
i	O
)	O
,	O
f(p	O
+	O
i	O
)	O
)	O
D(f	O
(	O
p	O
i	O
)	O
,	O
f(p	O
i	O
)	O
)	O
}	O
(	O
1	O
)	O
where	O
f	O
(	O
p	O
i	O
)	O
represents	O
the	O
embedding	O
of	O
image	O
p	O
i	O
,	O
D(•	O
,	O
•	O
)	O
is	O
the	O
Euclidean	O
distance	O
and	O
g	O
is	O
a	O
margin	O
(	O
gap	O
)	O
hyperparameter	O
.	O

Let	O
p	O
i	O
,	O
p	O
+	O
i	O
,	O
p	O
i	O
denote	O
a	O
triplet	O
of	O
query	O
,	O
positive	O
and	O
negative	O
images	O
,	O
respectively	O
.	O

(	O
2014	O
)	O
.	O

The	O
convolutional	O
network	O
used	O
to	O
obtain	O
Picturebook	B-MethodName
embeddings	O
is	O
based	O
off	O
of	O
Wang	O
et	O
al	O
.	O

Inducing	O
Picturebook	B-MethodName
Embeddings	O
.	O

We	O
can	O
perform	O
all	O
of	O
these	O
operations	O
offline	O
to	O
construct	O
a	O
matrix	O
E	O
p	O
representing	O
the	O
Picturebook	B-MethodName
embeddings	O
over	O
a	O
vocabulary	O
.	O

Our	O
Picturebook	B-MethodName
embeddings	O
reflect	O
the	O
search	O
rankings	O
by	O
concatenating	O
the	O
individual	O
embeddings	O
in	O
the	O
order	O
of	O
the	O
search	O
results	O
.	O

We	O
then	O
pass	O
each	O
image	O
through	O
a	O
CNN	O
trained	O
with	O
a	O
semantic	O
ranking	O
objective	O
to	O
extract	O
its	O
embedding	O
.	O

Given	O
a	O
word	O
(	O
or	O
phrase	O
)	O
,	O
we	O
image	O
search	O
for	O
the	O
top	O
-	O
k	O
images	O
and	O
extract	O
the	O
images	O
.	O

Our	O
Picturebook	B-MethodName
embeddings	O
ground	O
language	O
using	O
the	O
'	O
snapshots	O
'	O
returned	O
by	O
an	O
image	O
search	O
engine	O
.	O

Picturebook	B-MethodName
Embeddings	O
.	O

The	O
use	O
of	O
image	O
search	O
allows	O
us	O
to	O
obtain	O
visual	O
embeddings	O
for	O
a	O
virtually	O
unlimited	O
vocabulary	O
without	O
needing	O
a	O
mapping	O
function	O
.	O

However	O
,	O
their	O
analysis	O
is	O
restricted	O
to	O
word	O
similarity	O
tasks	O
and	O
they	O
require	O
text	O
-	O
to	O
-	O
image	O
regression	O
to	O
obtain	O
visual	O
embeddings	O
for	O
unseen	O
words	O
,	O
due	O
to	O
the	O
use	O
of	O
ImageNet	O
.	O

(	O
2018	O
)	O
who	O
also	O
consider	O
fusing	O
Glove	O
embeddings	O
with	O
visual	O
features	O
.	O

The	O
work	O
that	O
most	O
closely	O
matches	O
ours	O
is	O
that	O
of	O
Wang	O
et	O
al	O
.	O

(	O
2018	O
)	O
describe	O
an	O
asymmetric	O
gate	O
that	O
allows	O
one	O
modality	O
to	O
'	O
attend	O
'	O
to	O
the	O
other	O
.	O

(	O
2017	O
)	O
introduce	O
a	O
gating	O
mechanism	O
inspired	O
by	O
the	O
LSTM	O
while	O
Kiela	O
et	O
al	O
.	O

Arevalo	O
et	O
al	O
.	O

More	O
recently	O
,	O
gating	O
-	O
based	O
approaches	O
have	O
been	O
developed	O
for	O
fusing	O
traditional	O
word	O
embeddings	O
with	O
visual	O
representations	O
.	O

also	O
fused	O
text	O
-	O
based	O
representations	O
with	O
imagebased	O
representations	O
(	O
Bruni	O
et	O
al	O
.	O
,	O
2014;Lazaridou	O
et	O
al	O
.	O
,	O
2015;Chrupala	O
et	O
al	O
.	O
,	O
2015;Mao	O
et	O
al	O
.	O
,	O
2016;Silberer	O
et	O
al	O
.	O
,	O
2017;Kiela	O
et	O
al	O
.	O
,	O
2017;Collell	O
et	O
al	O
.	O
,	O
2017;Zablocki	O
et	O
al	O
.	O
,	O
2018	O
)	O
and	O
representations	O
derived	O
from	O
a	O
knowledge	O
-	O
graph	O
(	O
Thoma	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

Various	O
work	O
has	O
Method	O
tasks	O
(	O
Bergsma	O
and	O
Durme	O
,	O
2011	O
)	O
bilingual	O
lexicons	O
(	O
Bergsma	O
and	O
Goebel	O
,	O
2011	O
)	O
lexical	O
preference	O
(	O
Kiela	O
et	O
al	O
.	O
,	O
2014	O
)	O
word	O
similarity	O
(	O
Kiela	O
et	O
al	O
.	O
,	O
2015a	O
)	O
lexical	O
entailment	O
detection	O
(	O
Kiela	O
et	O
al	O
.	O
,	O
2015b	O
)	O
bilingual	O
lexicons	O
(	O
Shutova	O
et	O
al	O
.	O
,	O
2016	O
)	O
metaphor	O
identification	O
(	O
Bulat	O
et	O
al	O
.	O
,	O
2015	O
)	O
predicting	O
property	O
norms	O
(	O
Kiela	O
,	O
2016	O
)	O
toolbox	O
(	O
Vulic	O
et	O
al	O
.	O
,	O
2016	O
)	O
bilingual	O
lexicons	O
(	O
Kiela	O
et	O
al	O
.	O
,	O
2016	O
)	O
word	O
similarity	O
(	O
Anderson	O
et	O
al	O
.	O
,	O
2017	O
)	O
decoding	O
brain	O
activity	O
(	O
Glavas	O
et	O
al	O
.	O
,	O
2017	O
)	O
semantic	O
text	O
similarity	O
(	O
Bhaskar	O
et	O
al	O
.	O
,	O
2017	O
)	O
abstract	O
vs	O
concrete	O
nouns	O
(	O
Hartmann	O
and	O
Sogaard	O
,	O
2017	O
)	O
bilingual	O
lexicons	O
(	O
Bulat	O
et	O
al	O
.	O
,	O
2017	O
)	O
decoding	O
brain	O
activity	O
Table	O
1	O
:	O
Existing	O
methods	O
that	O
use	O
image	O
search	O
for	O
grounding	O
and	O
their	O
corresponding	O
tasks	O
.	O

Our	O
work	O
also	O
relates	O
to	O
existing	O
multimodal	O
models	O
combining	O
different	O
representations	O
of	O
the	O
data	O
(	O
Hill	O
and	O
Korhonen	O
,	O
2014	O
)	O
.	O

Our	O
approach	O
differs	O
from	O
the	O
above	O
methods	O
in	O
three	O
main	O
ways	O
:	O
a	O
)	O
we	O
obtain	O
searchgrounded	O
representations	O
for	O
over	O
2	O
million	O
words	O
as	O
opposed	O
to	O
a	O
few	O
thousand	O
,	O
b	O
)	O
we	O
apply	O
our	O
representations	O
to	O
a	O
higher	O
diversity	O
of	O
tasks	O
than	O
previously	O
considered	O
,	O
and	O
c	O
)	O
we	O
introduce	O
a	O
multimodal	O
gating	O
mechanism	O
that	O
allows	O
for	O
a	O
more	O
flexible	O
integration	O
of	O
features	O
than	O
mere	O
concatenation	O
.	O

There	O
has	O
also	O
been	O
other	O
work	O
using	O
other	O
image	O
sources	O
such	O
as	O
ImageNet	O
(	O
Kiela	O
and	O
Bottou	O
,	O
2014;Collell	O
and	O
Moens	O
,	O
2016	O
)	O
over	O
the	O
WordNet	O
synset	O
vocabulary	O
,	O
and	O
using	O
Flickr	O
photos	O
and	O
captions	O
(	O
Joulin	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

Table	O
1	O
illustrates	O
existing	O
methods	O
that	O
utilize	O
image	O
search	O
and	O
the	O
tasks	O
considered	O
in	O
their	O
work	O
.	O

The	O
use	O
of	O
image	O
search	O
for	O
obtaining	O
word	O
representations	O
is	O
not	O
new	O
.	O

Related	O
Work	O
.	O

In	O
particular	O
,	O
networks	O
trained	O
with	O
semantic	O
labels	O
result	O
in	O
better	O
embeddings	O
than	O
those	O
trained	O
with	O
visual	O
labels	O
,	O
even	O
when	O
evaluating	O
similarity	O
on	O
concrete	O
words	O
.	O

•	O
We	O
highlight	O
the	O
importance	O
of	O
the	O
convolutional	O
network	O
used	O
to	O
extract	O
embeddings	O
.	O

We	O
also	O
show	O
that	O
Picturebook	B-MethodName
gate	O
activations	O
are	O
negatively	O
correlated	O
with	O
image	O
dispersion	O
(	O
Kiela	O
et	O
al	O
.	O
,	O
2014	O
)	O
,	O
indicating	O
that	O
our	O
model	O
selectively	O
chooses	O
between	O
word	O
embeddings	O
based	O
on	O
their	O
abstraction	O
level	O
.	O

•	O
We	O
perform	O
an	O
extensive	O
analysis	O
of	O
our	O
gating	O
mechanism	O
,	O
showing	O
that	O
the	O
gate	O
activations	O
for	O
Picturebook	B-MethodName
embeddings	O
are	O
highly	O
correlated	O
with	O
human	O
judgments	O
of	O
concreteness	O
.	O

This	O
is	O
useful	O
for	O
generative	O
modelling	O
tasks	O
.	O

Given	O
a	O
Picturebook	B-MethodName
embedding	O
,	O
we	O
find	O
the	O
closest	O
words	O
which	O
would	O
generate	O
the	O
embedding	O
.	O

•	O
We	O
introduce	O
Inverse	O
Picturebook	B-MethodName
to	O
perform	O
the	O
inverse	O
lookup	O
operation	O
.	O

We	O
apply	O
our	O
approach	O
to	O
over	O
a	O
dozen	O
datasets	O
and	O
several	O
different	O
tasks	O
:	O
word	B-TaskName
similarity	I-TaskName
,	O
sentence	B-TaskName
relatedness	I-TaskName
,	O
natural	B-TaskName
language	I-TaskName
inference	I-TaskName
,	O
topic	B-TaskName
/	I-TaskName
sentiment	I-TaskName
classification	I-TaskName
,	O
image	B-TaskName
sentence	I-TaskName
ranking	I-TaskName
and	O
Machine	B-TaskName
Translation	I-TaskName
(	O
MT	B-TaskName
)	O
.	O

•	O
We	O
introduce	O
a	O
multimodal	O
gating	O
mechanism	O
to	O
selectively	O
choose	O
between	O
Glove	B-MethodName
and	O
Picturebook	B-MethodName
embeddings	O
in	O
a	O
task	O
-	O
dependent	O
way	O
.	O

The	O
main	O
contributions	O
of	O
our	O
work	O
are	O
as	O
follows	O
:	O
•	O
We	O
obtain	O
Picturebook	B-MethodName
embeddings	O
for	O
the	O
2.2	O
million	O
words	O
that	O
occur	O
in	O
the	O
Glove	O
vocabulary	O
(	O
Pennington	O
et	O
al	O
.	O
,	O
2014	O
)	O
1	O
,	O
allowing	O
each	O
word	O
to	O
have	O
a	O
Glove	B-MethodName
embedding	O
and	O
a	O
parallel	O
grounded	O
word	O
representation	O
.	O

This	O
collection	O
of	O
word	O
representations	O
that	O
we	O
visually	O
1	O
Common	B-DatasetName
Crawl	I-DatasetName
,	O
840B	O
tokens	O
ground	O
via	O
image	O
search	O
is	O
2	O
-	O
3	O
orders	O
of	O
magnitude	O
larger	O
than	O
prior	O
work	O
.	O

Using	O
Google	O
image	O
search	O
,	O
a	O
Picturebook	B-MethodName
embedding	O
for	O
a	O
word	O
is	O
obtained	O
by	O
concatenating	O
the	O
k	O
-	O
feature	O
vectors	O
of	O
our	O
convolutional	O
network	O
on	O
the	O
top	O
-	O
k	O
retrieved	O
search	O
results	O
.	O

Picturebook	B-MethodName
embeddings	O
are	O
obtained	O
through	O
a	O
convolutional	O
network	O
trained	O
with	O
a	O
semantic	O
ranking	O
objective	O
on	O
a	O
proprietary	O
image	O
dataset	O
with	O
over	O
100	O
+	O
million	O
images	O
(	O
Wang	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

In	O
this	O
paper	O
we	O
introduce	O
Picturebook	B-MethodName
embeddings	O
produced	O
by	O
image	O
search	O
using	O
words	O
as	O
queries	O
.	O

While	O
several	O
authors	O
have	O
considered	O
this	O
approach	O
,	O
it	O
has	O
been	O
largely	O
limited	O
to	O
a	O
few	O
thousand	O
queries	O
and	O
only	O
a	O
small	O
number	O
of	O
tasks	O
.	O

These	O
word	O
embeddings	O
are	O
grounded	O
via	O
the	O
retrieved	O
images	O
.	O

This	O
involves	O
retrieving	O
the	O
top	O
-	O
k	O
images	O
from	O
a	O
search	O
engine	O
,	O
running	O
those	O
through	O
a	O
convolutional	O
network	O
and	O
aggregating	O
the	O
results	O
.	O

A	O
very	O
different	O
way	O
to	O
obtain	O
word	O
embeddings	O
is	O
to	O
aggregate	O
features	O
obtained	O
by	O
using	O
the	O
word	O
as	O
a	O
query	O
for	O
an	O
image	O
search	O
engine	O
.	O

While	O
immensely	O
successful	O
,	O
this	O
lookup	O
operation	O
is	O
typically	O
learned	O
through	O
co	O
-	O
occurrence	O
objectives	O
or	O
a	O
task	O
-	O
dependent	O
reward	O
signal	O
.	O

The	O
dominant	O
approach	O
to	O
learning	O
distributed	O
word	O
representations	O
is	O
through	O
indexing	O
a	O
learned	O
matrix	O
.	O

One	O
place	O
to	O
incorporate	O
grounding	O
is	O
in	O
the	O
lookup	O
table	O
that	O
maps	O
tokens	O
to	O
vectors	O
.	O

embodied	O
cognition	O
,	O
search	O
engines	O
allow	O
us	O
to	O
get	O
a	O
form	O
of	O
quasi	O
-	O
grounding	O
from	O
high	O
-	O
coverage	O
'	O
snapshots	O
'	O
of	O
our	O
physical	O
world	O
provided	O
by	O
the	O
interaction	O
of	O
millions	O
of	O
users	O
.	O

While	O
true	O
natural	O
language	O
understanding	O
may	O
require	O
fully	O
*	O
Both	O
authors	O
contributed	O
equally	O
to	O
this	O
work	O
.	O

Search	O
engines	O
allow	O
us	O
to	O
obtain	O
correspondences	O
between	O
language	O
and	O
images	O
that	O
are	O
far	O
less	O
restricted	O
than	O
existing	O
multimodal	O
datasets	O
which	O
typically	O
have	O
restricted	O
vocabularies	O
.	O

One	O
source	O
of	O
grounding	O
,	O
which	O
has	O
been	O
utilized	O
in	O
existing	O
work	O
,	O
is	O
image	O
search	O
engines	O
.	O

In	O
recent	O
years	O
,	O
a	O
large	O
amount	O
of	O
research	O
has	O
focused	O
on	O
integrating	O
vision	O
and	O
language	O
to	O
obtain	O
visually	O
grounded	O
word	O
and	O
sentence	O
representations	O
.	O

Constructing	B-TaskName
grounded	I-TaskName
representations	I-TaskName
of	I-TaskName
natural	I-TaskName
language	I-TaskName
is	O
a	O
promising	O
step	O
towards	O
achieving	O
human	O
-	O
like	O
language	O
learning	O
.	O

Introduction	O
.	O

We	O
also	O
show	O
that	O
gate	O
activations	O
corresponding	O
to	O
Picturebook	B-MethodName
embeddings	O
are	O
highly	O
correlated	O
to	O
human	O
judgments	O
of	O
concreteness	O
ratings	O
.	O

We	O
experiment	O
and	O
report	O
results	O
across	O
a	O
wide	O
range	O
of	O
tasks	O
:	O
word	B-TaskName
similarity	I-TaskName
,	O
natural	B-TaskName
language	I-TaskName
inference	I-TaskName
,	O
semantic	B-TaskName
relatedness	I-TaskName
,	O
sentiment	B-TaskName
/	I-TaskName
topic	I-TaskName
classification	I-TaskName
,	O
image	B-TaskName
-	I-TaskName
sentence	I-TaskName
ranking	I-TaskName
and	O
machine	B-TaskName
translation	I-TaskName
.	O

We	O
also	O
introduce	O
Inverse	B-MethodName
Picturebook	I-MethodName
,	O
a	O
mechanism	O
to	O
map	O
a	O
Picturebook	B-MethodName
embedding	O
back	O
into	O
words	O
.	O

We	O
introduce	O
a	O
multimodal	O
gating	O
function	O
to	O
fuse	O
our	O
Picturebook	B-MethodName
embeddings	O
with	O
other	O
word	O
representations	O
.	O

For	O
each	O
word	O
in	O
a	O
vocabulary	O
,	O
we	O
extract	O
the	O
top	O
-	O
k	O
images	O
from	O
Google	O
image	O
search	O
and	O
feed	O
the	O
images	O
through	O
a	O
convolutional	O
network	O
to	O
extract	O
a	O
word	O
embedding	O
.	O

We	O
introduce	O
Picturebook	B-MethodName
,	O
a	O
large	O
-	O
scale	O
lookup	O
operation	O
to	O
ground	O
language	O
via	O
'	O
snapshots	O
'	O
of	O
our	O
physical	O
world	O
accessed	O
through	O
image	O
search	O
.	O

Illustrative	B-TaskName
Language	I-TaskName
Understanding	I-TaskName
:	O
Large	O
-	O
Scale	O
Visual	B-TaskName
Grounding	I-TaskName
with	I-TaskName
Image	I-TaskName
Search	I-TaskName
.	O

This	O
work	O
has	O
been	O
supported	O
by	O
the	O
National	O
Council	O
of	O
Scientific	O
and	O
Technological	O
Development	O
from	O
Brazil	O
(	O
CNPq	O
)	O
under	O
the	O
grants	O
203065/2014	O
-	O
0	O
and	O
206971/2014	O
-	O
1	O
.	O

Acknowledgments	O
.	O

Using	O
a	O
new	O
delexicalized	O
version	O
of	O
the	O
WebNLG	B-DatasetName
corpus	O
(	O
made	O
publicly	O
available	O
)	O
,	O
we	O
showed	O
that	O
the	O
neural	O
model	O
substantially	O
improves	O
over	O
two	O
strong	O
baselines	O
in	O
terms	O
of	O
accuracy	B-MetricName
of	O
the	O
referring	O
expressions	O
and	O
fluency	B-MetricName
of	O
the	O
lexicalized	O
texts	O
.	O

NeuralREG	B-MethodName
decides	O
both	O
on	O
referential	O
form	O
and	O
on	O
referential	O
content	O
in	O
an	O
integrated	O
,	O
end	O
-	O
to	O
-	O
end	O
approach	O
,	O
without	O
using	O
explicit	O
features	O
.	O

We	O
introduced	O
a	O
deep	O
learning	O
model	O
for	O
the	O
generation	B-TaskName
of	I-TaskName
referring	I-TaskName
expressions	I-TaskName
in	O
discourse	O
texts	O
.	O

Conclusion	O
.	O

This	O
shows	O
the	O
importance	O
of	O
the	O
attention	O
mechanism	O
in	O
the	O
decoding	O
step	O
of	O
NeuralREG	B-MethodName
in	O
order	O
to	O
generate	O
fine	O
-	O
grained	O
referring	O
expressions	O
in	O
discourse	O
.	O

Finally	O
,	O
our	O
NeuralREG	B-MethodName
variant	O
with	O
the	O
lowest	O
results	O
were	O
our	O
'	O
vanilla	O
'	O
sequence	B-MethodName
-	I-MethodName
to	I-MethodName
-	I-MethodName
sequence	I-MethodName
(	O
Seq2Seq	B-MethodName
)	O
,	O
whose	O
the	O
lexicalized	O
texts	O
were	O
significantly	O
less	O
fluent	O
and	O
clear	O
than	O
the	O
original	O
ones	O
.	O

This	O
result	O
appears	O
to	O
be	O
not	O
consistent	O
with	O
the	O
findings	O
of	O
Libovický	O
and	O
Helcl	O
(	O
2017	O
)	O
,	O
who	O
reported	O
better	O
results	O
on	O
multi	O
-	O
modal	O
machine	O
translation	O
with	O
hierarchical	B-MethodName
-	I-MethodName
attention	I-MethodName
as	O
opposed	O
to	O
the	O
flat	O
variants	O
(	O
Specia	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

Surprisingly	O
,	O
the	O
most	O
complex	O
variant	O
(	O
HierAtt	B-MethodName
)	O
with	O
a	O
hierarchical	B-MethodName
-	I-MethodName
attention	I-MethodName
mechanism	I-MethodName
gave	O
lower	O
results	O
than	O
CAtt	B-MethodName
,	O
producing	O
lexicalized	O
texts	O
which	O
were	O
rated	O
as	O
less	O
fluent	O
than	O
the	O
original	O
ones	O
and	O
not	O
significantly	O
more	O
fluent	O
from	O
the	O
ones	O
generated	O
by	O
the	O
baselines	O
.	O

The	O
texts	O
lexicalized	O
by	O
this	O
variant	O
were	O
also	O
considered	O
statistically	O
more	O
fluent	O
than	O
the	O
ones	O
generated	O
by	O
the	O
two	O
proposed	O
baselines	O
in	O
the	O
human	O
evaluation	O
.	O

Although	O
all	O
the	O
versions	O
performed	O
relatively	O
similar	O
,	O
the	O
concatenativeattention	B-MethodName
(	O
CAtt	B-MethodName
)	O
version	O
generated	O
the	O
closest	O
referring	O
expressions	O
from	O
the	O
gold	O
-	O
standard	O
ones	O
and	O
presented	O
the	O
highest	O
textual	O
accuracy	O
in	O
the	O
automatic	O
evaluation	O
.	O

NeuralREG	B-MethodName
was	O
implemented	O
with	O
3	O
different	O
decoding	O
architectures	O
:	O
Seq2Seq	B-MethodName
,	O
CAtt	B-MethodName
and	O
HierAtt	B-MethodName
.	O

(	O
2016	O
)	O
also	O
did	O
not	O
perform	O
well	O
in	O
the	O
generation	O
of	O
pronouns	O
,	O
revealing	O
a	O
poor	O
capacity	O
to	O
detect	O
highly	O
salient	O
entities	O
in	O
a	O
text	O
.	O

However	O
,	O
the	O
approach	O
of	O
Castro	O
Ferreira	O
et	O
al	O
.	O

OnlyNames	B-MethodName
,	O
as	O
the	O
name	O
already	O
reveals	O
,	O
does	O
not	O
manage	O
to	O
generate	O
any	O
pronouns	O
.	O

However	O
,	O
they	O
performed	O
poorly	O
when	O
it	O
came	O
to	O
pronominalization	O
,	O
which	O
is	O
an	O
important	O
ingredient	O
for	O
fluent	O
,	O
coherent	O
text	O
.	O

These	O
baselines	O
performed	O
relatively	O
well	O
because	O
they	O
frequently	O
generated	O
full	O
names	O
,	O
which	O
occur	O
often	O
for	O
our	O
wikified	O
references	O
.	O

Baselines	O
We	O
introduced	O
two	O
strong	O
baselines	O
which	O
generated	O
roughly	O
half	O
of	O
the	O
referring	O
expressions	O
identical	O
to	O
the	O
gold	O
standard	O
in	O
an	O
automatic	O
evaluation	O
.	O

Besides	O
the	O
REG	B-TaskName
task	O
,	O
these	O
data	O
can	O
be	O
useful	O
for	O
many	O
other	O
tasks	O
related	O
to	O
,	O
for	O
instance	O
,	O
the	O
NLG	O
process	O
(	O
Reiter	O
and	O
Dale	O
,	O
2000;Gatt	O
and	O
Krahmer	O
,	O
2018	O
)	O
and	O
Wikification	O
(	O
Moussallem	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

Data	O
The	O
collection	O
of	O
referring	O
expressions	O
used	O
in	O
our	O
experiments	O
was	O
extracted	O
from	O
a	O
novel	O
,	O
delexicalized	O
and	O
publicly	O
available	O
version	O
of	O
the	O
WebNLG	B-DatasetName
corpus	O
(	O
Gardent	O
et	O
al	O
.	O
,	O
2017a	O
,	O
b	O
)	O
,	O
where	O
the	O
discourse	O
entities	O
were	O
replaced	O
with	O
general	O
tags	O
for	O
decreasing	O
the	O
data	O
sparsity	O
.	O

Later	O
in	O
a	O
complementary	O
human	O
evaluation	O
,	O
the	O
texts	O
with	O
referring	O
expressions	O
generated	O
by	O
a	O
variant	O
of	O
our	O
novel	O
model	O
were	O
considered	O
statistically	O
more	O
fluent	O
than	O
the	O
texts	O
lexicalized	O
by	O
the	O
two	O
baselines	O
.	O

In	O
an	O
automatic	O
evaluation	O
on	O
a	O
collection	O
of	O
78,901	O
referring	O
expressions	O
to	O
1,501	O
Wikipedia	O
entities	O
,	O
the	O
different	O
versions	O
of	O
the	O
model	O
all	O
yielded	O
better	O
results	O
than	O
the	O
two	O
(	O
competitive	O
)	O
baselines	O
.	O

The	O
model	O
was	O
implemented	O
using	O
an	O
encoder	O
-	O
decoder	O
approach	O
where	O
a	O
target	O
referent	O
and	O
its	O
surrounding	O
linguistic	O
contexts	O
were	O
first	O
encoded	O
and	O
combined	O
into	O
a	O
single	O
vector	O
representation	O
which	O
subsequently	O
was	O
decoded	O
into	O
a	O
referring	O
expression	O
to	O
the	O
target	O
,	O
suitable	O
for	O
the	O
specific	O
discourse	O
context	O
.	O

It	O
generates	O
referring	O
expressions	O
for	O
discourse	O
entities	O
by	O
simultaneously	O
selecting	O
form	O
and	O
content	O
without	O
any	O
need	O
of	O
feature	O
extraction	O
techniques	O
.	O

This	O
study	O
introduced	O
NeuralREG	B-MethodName
,	O
an	O
end	O
-	O
to	O
-	O
end	O
approach	O
based	O
on	O
neural	O
networks	O
which	O
tackles	O
the	O
full	O
Referring	B-TaskName
Expression	I-TaskName
Generation	I-TaskName
process	O
.	O

Discussion	O
.	O

Finally	O
,	O
the	O
original	O
texts	O
were	O
rated	O
significantly	O
higher	O
than	O
both	O
baselines	O
in	O
terms	O
of	O
the	O
three	O
metrics	O
,	O
also	O
than	O
NeuralREG+Seq2Seq	B-MethodName
and	O
Neu	B-MethodName
-	I-MethodName
ralREG+HierAtt	I-MethodName
in	O
terms	O
of	O
fluency	B-MetricName
,	O
and	O
than	O
NeuralREG+Seq2Seq	B-MethodName
in	O
terms	O
of	O
clarity	B-MetricName
.	O

The	O
results	O
for	O
the	O
3	O
different	O
decoding	O
methods	O
of	O
NeuralREG	B-MethodName
also	O
did	O
not	O
reveal	O
a	O
significant	O
difference	O
.	O

In	O
comparison	O
with	O
the	O
neural	O
models	O
,	O
NeuralREG+CAtt	B-MethodName
significantly	O
outperformed	O
the	O
baselines	O
in	O
terms	O
of	O
fluency	B-MetricName
,	O
whereas	O
the	O
other	O
comparisons	O
between	O
baselines	O
and	O
neural	O
models	O
were	O
not	O
statistically	O
significant	O
.	O

Different	O
from	O
the	O
automatic	O
evaluation	O
,	O
the	O
results	O
of	O
both	O
baselines	O
were	O
not	O
statistically	O
significant	O
for	O
the	O
three	O
metrics	O
.	O

To	O
test	O
the	O
statistical	O
significance	O
of	O
the	O
pairwise	O
comparisons	O
,	O
we	O
used	O
the	O
Wilcoxon	O
signedrank	O
test	O
corrected	O
for	O
multiple	O
comparisons	O
using	O
the	O
Bonferroni	O
method	O
.	O

Concerning	O
the	O
size	O
of	O
the	O
triple	O
sets	O
,	O
we	O
did	O
not	O
find	O
any	O
clear	O
pattern	O
.	O

Inspection	O
of	O
the	O
Table	O
reveals	O
a	O
clear	O
pattern	O
:	O
all	O
three	O
neural	O
models	O
scored	O
higher	O
than	O
the	O
baselines	O
on	O
all	O
metrics	O
,	O
with	O
especially	O
NeuralREG+CAtt	B-MethodName
approaching	O
the	O
ratings	O
for	O
the	O
original	O
sentences	O
,	O
although	O
-again	O
-differences	O
between	O
the	O
neural	O
models	O
were	O
small	O
.	O

Results	O
Table	O
3	O
summarizes	O
the	O
results	O
.	O

English	O
(	O
44	O
)	O
,	O
while	O
14	O
and	O
2	O
self	O
-	O
reported	O
as	O
fluent	O
or	O
having	O
a	O
basic	O
proficiency	O
,	O
respectively	O
.	O

before	O
his	O
death	O
in	O
california	O
he	O
had	O
been	O
awarded	O
the	O
distinguished	O
service	O
medal	O
by	O
the	O
us	O
navy	O
an	O
award	O
higher	O
than	O
the	O
department	O
of	O
commerce	O
gold	O
medal	O
.	O

Original	O
alan	O
shepard	O
was	O
born	O
in	O
new	O
hampshire	O
on	O
18	O
november	O
1923	O
.	O

before	O
his	O
death	O
in	O
california	O
he	O
had	O
been	O
awarded	O
the	O
distinguished	O
service	O
medal	O
an	O
award	O
higher	O
than	O
the	O
department	O
of	O
commerce	O
gold	O
medal	O
.	O

alan	O
shephard	O
was	O
born	O
in	O
new	O
hampshire	O
on	O
1923	O
-	O
11	O
-	O
18	O
.	O

HierAtt	O
.	O

before	O
his	O
death	O
in	O
california	O
he	O
had	O
been	O
awarded	O
the	O
distinguished	O
service	O
medal	O
by	O
the	O
us	O
navy	O
an	O
award	O
higher	O
than	O
the	O
department	O
of	O
commerce	O
gold	O
medal	O
.	O

alan	O
shepard	O
was	O
born	O
in	O
new	O
hampshire	O
on	O
1923	O
-	O
11	O
-	O
18	O
.	O

CAtt	O
.	O

before	O
his	O
death	O
in	O
california	O
him	O
had	O
been	O
awarded	O
the	O
distinguished	O
service	O
medal	O
by	O
the	O
united	O
states	O
navy	O
an	O
award	O
higher	O
than	O
the	O
department	O
of	O
commerce	O
gold	O
medal	O
.	O

alan	O
shepard	O
was	O
born	O
in	O
new	O
hampshire	O
on	O
1923	O
-	O
11	O
-	O
18	O
.	O

Seq2Seq	O
.	O

before	O
alan	O
shepard	O
death	O
in	O
california	O
him	O
had	O
been	O
awarded	O
distinguished	O
service	O
medal	O
an	O
award	O
higher	O
than	O
department	O
of	O
commerce	O
gold	O
medal	O
.	O

alan	O
shepard	O
was	O
born	O
in	O
new	O
hampshire	O
on	O
1923	O
-	O
11	O
-	O
18	O
.	O

Ferreira	O
.	O

before	O
alan	O
shepard	O
death	O
in	O
california	O
alan	O
shepard	O
had	O
been	O
awarded	O
distinguished	O
service	O
medal	O
(	O
united	O
states	O
navy	O
)	O
an	O
award	O
higher	O
than	O
department	O
of	O
commerce	O
gold	O
medal	O
.	O

OnlyNames	B-MethodName
alan	O
shepard	O
was	O
born	O
in	O
new	O
hampshire	O
on	O
1923	O
-	O
11	O
-	O
18	O
.	O

Model	O
Text	O
.	O

The	O
majority	O
declared	O
themselves	O
native	O
speakers	O
of	O
.	O

Their	O
average	O
age	O
was	O
36	O
years	O
and	O
27	O
of	O
them	O
were	O
females	O
.	O

Participants	O
We	O
recruited	O
60	O
participants	O
,	O
10	O
per	O
list	O
,	O
via	O
Mechanical	O
Turk	O
.	O

The	O
experiment	O
is	O
available	O
on	O
the	O
website	O
of	O
the	O
author	O
3	O
.	O

Once	O
introduced	O
to	O
a	O
trial	O
,	O
the	O
participants	O
were	O
asked	O
to	O
rate	O
the	O
fluency	B-MetricName
(	O
"	O
does	O
the	O
text	O
flow	O
in	O
a	O
natural	O
,	O
easy	O
to	O
read	O
manner	O
?	O
"	O
)	O
,	O
grammaticality	B-MetricName
(	O
"	O
is	O
the	O
text	O
grammatical	O
(	O
no	O
spelling	O
or	O
grammatical	O
errors	O
)	O
?	O
"	O
)	O
and	O
clarity	B-MetricName
(	O
"	O
does	O
the	O
text	O
clearly	O
express	O
the	O
data	O
?	O
"	O
)	O
of	O
each	O
target	O
text	O
on	O
a	O
7	B-MetricName
-	I-MetricName
Likert	I-MetricName
scale	I-MetricName
,	O
focussing	O
on	O
the	O
highlighted	O
referring	O
expressions	O
.	O

The	O
experiment	O
had	O
a	O
latin	O
-	O
square	O
design	O
,	O
distributing	O
the	O
144	O
trials	O
over	O
6	O
different	O
lists	O
such	O
that	O
each	O
participant	O
rated	O
24	O
trials	O
,	O
one	O
for	O
each	O
of	O
the	O
24	O
corpus	O
instances	O
,	O
making	O
sure	O
that	O
participants	O
saw	O
equal	O
numbers	O
of	O
triple	O
set	O
sizes	O
and	O
generated	O
versions	O
.	O

Method	O
.	O

For	O
each	O
size	O
group	O
,	O
we	O
randomly	O
selected	O
4	O
instances	O
(	O
of	O
varying	O
degrees	O
of	O
variation	O
between	O
the	O
generated	O
texts	O
)	O
giving	O
rise	O
to	O
144	O
trials	O
(=	O
6	O
triple	O
set	O
sizes	O
*	O
4	O
instances	O
*	O
6	O
text	O
versions	O
)	O
,	O
each	O
consisting	O
of	O
a	O
set	O
of	O
triples	O
and	O
a	O
target	O
text	O
describing	O
it	O
with	O
the	O
lexicalized	O
referring	O
expressions	O
highlighted	O
in	O
yellow	O
.	O

Instances	O
were	O
chosen	O
following	O
2	O
criteria	O
:	O
the	O
number	O
of	O
triples	O
in	O
the	O
source	O
set	O
(	O
ranging	O
from	O
2	O
to	O
7	O
)	O
and	O
the	O
differences	O
between	O
the	O
target	O
texts	O
.	O

For	O
each	O
of	O
the	O
selected	O
instances	O
,	O
we	O
took	O
into	O
account	O
its	O
source	O
triple	O
set	O
and	O
its	O
6	O
target	O
texts	O
:	O
one	O
original	O
(	O
randomly	O
chosen	O
)	O
and	O
its	O
versions	O
with	O
the	O
referring	O
expressions	O
generated	O
by	O
each	O
of	O
the	O
5	O
models	O
introduced	O
in	O
this	O
study	O
(	O
two	O
baselines	O
,	O
three	O
neural	O
models	O
)	O
.	O

Material	O
We	O
quasi	O
-	O
randomly	O
selected	O
24	O
instances	O
from	O
the	O
delexicalized	O
version	O
of	O
the	O
WebNLG	O
corpus	O
related	O
to	O
the	O
test	O
part	O
of	O
the	O
re	O
-	O
ferring	O
expression	O
collection	O
.	O

Complementary	O
to	O
the	O
automatic	O
evaluation	O
,	O
we	O
performed	O
an	O
evaluation	O
with	O
human	O
judges	O
,	O
comparing	O
the	O
quality	O
judgments	O
of	O
the	O
original	O
texts	O
to	O
the	O
versions	O
generated	O
by	O
our	O
various	O
models	O
.	O

Human	O
Evaluation	O
.	O

The	O
more	O
complex	O
Neural	B-MethodName
-	I-MethodName
REG+HierAtt	I-MethodName
yielded	O
the	O
lowest	O
results	O
,	O
even	O
though	O
the	O
differences	O
with	O
the	O
other	O
two	O
models	O
were	O
small	O
and	O
not	O
even	O
statistically	O
significant	O
in	O
many	O
of	O
the	O
cases	O
.	O

The	O
results	O
for	O
the	O
different	O
decoding	O
methods	O
for	O
NeuralREG	B-MethodName
were	O
similar	O
,	O
with	O
the	O
Neu	B-MethodName
-	I-MethodName
ralREG+CAtt	I-MethodName
performing	O
slightly	O
better	O
in	O
terms	O
of	O
the	O
BLEU	B-TaskName
score	O
,	O
text	O
accuracy	B-TaskName
and	O
String	B-TaskName
Edit	I-TaskName
Distance	I-TaskName
.	O

Especially	O
noteworthy	O
was	O
the	O
score	O
on	O
pronoun	O
accuracy	O
,	O
indicating	O
that	O
the	O
model	O
was	O
well	O
capable	O
of	O
predicting	O
when	O
to	O
generate	O
a	O
pronominal	O
reference	O
in	O
our	O
dataset	O
.	O

When	O
considering	O
the	O
texts	O
lexicalized	O
with	O
the	O
referring	O
expressions	O
produced	O
by	O
NeuralREG	B-MethodName
,	O
at	O
least	O
28	O
%	O
of	O
them	O
are	O
similar	O
to	O
the	O
original	O
texts	O
.	O

This	O
means	O
that	O
NeuralREG	B-MethodName
predicted	O
3	O
out	O
of	O
4	O
references	O
completely	O
correct	O
,	O
whereas	O
the	O
incorrect	O
ones	O
needed	O
an	O
average	O
of	O
2	O
post	O
-	O
edition	O
operations	O
in	O
character	O
level	O
to	O
be	O
equal	O
to	O
the	O
gold	O
-	O
standard	O
.	O

They	O
achieved	O
BLEU	B-MetricName
scores	O
,	O
text	O
and	O
referential	O
accuracies	O
as	O
well	O
as	O
string	B-MetricName
edit	I-MetricName
distances	I-MetricName
in	O
the	O
range	O
of	O
79.01	B-MetricValue
-	O
79.39	B-MetricValue
,	O
28%-30	B-MetricValue
%	I-MetricValue
,	O
73%-74	B-MetricValue
%	I-MetricValue
and	O
2.25	B-MetricValue
-	I-MetricValue
2.36	I-MetricValue
,	O
respectively	O
.	O

Importantly	O
,	O
the	O
three	O
NeuralREG	B-MethodName
variant	O
models	O
statistically	O
outperformed	O
the	O
two	O
baseline	O
systems	O
.	O

dropout	B-HyperparameterName
probability	I-HyperparameterName
0.3	B-HyperparameterValue
and	O
beam	B-HyperparameterName
size	I-HyperparameterName
5	B-HyperparameterValue
,	O
and	O
Neu	B-HyperparameterName
-	I-HyperparameterName
ralREG+HierAtt	I-HyperparameterName
with	O
dropout	B-HyperparameterName
probability	I-HyperparameterName
of	O
0.3	B-HyperparameterValue
and	O
beam	B-HyperparameterName
size	I-HyperparameterName
of	O
1	B-HyperparameterValue
selected	O
based	O
on	O
the	O
highest	O
accuracy	O
on	O
the	O
development	O
set	O
.	O

Rankings	O
were	O
determined	O
by	O
statistical	O
significance	O
.	O

We	O
reported	O
results	O
on	O
the	O
test	O
set	O
for	O
Neu	B-MethodName
-	I-MethodName
ralREG+Seq2Seq	I-MethodName
and	O
NeuralREG+CAtt	B-MethodName
using	O
(	O
2	O
)	O
Accuracy	B-MetricName
(	O
Acc	B-MetricName
.	O
)	O
,	O
Precision	B-MetricName
(	O
Prec	B-MetricName
.	O
)	O
,	O
Recall	B-MetricName
(	O
Rec	B-MetricName
.	O
)	O
and	O
F	B-MetricName
-	I-MetricName
Score	I-MetricName
results	O
in	O
the	O
prediction	O
of	O
pronominal	O
forms	O
;	O
and	O
(	O
3	O
)	O
Accuracy	B-MetricName
(	O
Acc	B-MetricName
.	O
)	O
and	O
BLEU	B-MetricName
score	O
results	O
of	O
the	O
texts	O
with	O
the	O
generated	O
referring	O
expressions	O
.	O

(	O
2016	O
)	O
performed	O
statistically	O
better	O
than	O
On	B-MethodName
-	I-MethodName
lyNames	I-MethodName
on	O
all	O
metrics	O
due	O
to	O
its	O
capability	O
,	O
albeit	O
to	O
a	O
limited	O
extent	O
,	O
to	O
predict	O
pronominal	O
references	O
(	O
which	O
OnlyNames	B-MethodName
obviously	O
can	O
not	O
)	O
.	O

The	O
method	O
based	O
on	O
Castro	O
Ferreira	O
et	O
al	O
.	O

The	O
first	O
thing	O
to	O
note	O
in	O
the	O
results	O
of	O
the	O
first	O
table	O
is	O
that	O
the	O
baselines	O
in	O
the	O
top	O
two	O
rows	O
performed	O
quite	O
strong	O
on	O
this	O
task	O
,	O
generating	O
more	O
than	O
half	O
of	O
the	O
referring	O
expressions	O
exactly	O
as	O
in	O
the	O
goldstandard	O
.	O

Results	O
Table	O
1	O
summarizes	O
the	O
results	O
for	O
all	O
models	O
on	O
all	O
metrics	O
on	O
the	O
test	O
set	O
and	O
Table	O
2	O
depicts	O
a	O
text	O
example	O
lexicalized	O
by	O
each	O
model	O
.	O

The	O
results	O
described	O
in	O
the	O
next	O
section	O
were	O
obtained	O
on	O
the	O
test	O
set	O
by	O
the	O
NeuralREG	B-MethodName
version	O
with	O
the	O
highest	O
accuracy	O
on	O
the	O
development	O
set	O
over	O
the	O
epochs	O
.	O

For	O
each	O
decoding	O
version	O
(	O
Seq2Seq	O
,	O
CAtt	O
and	O
HierAtt	O
)	O
,	O
we	O
searched	O
for	O
the	O
best	O
combination	O
of	O
drop	B-HyperparameterName
-	I-HyperparameterName
out	I-HyperparameterName
probability	I-HyperparameterName
of	O
0.2	B-HyperparameterValue
or	O
0.3	B-HyperparameterValue
in	O
both	O
the	O
encoding	O
and	O
decoding	O
layers	O
,	O
using	O
beam	B-HyperparameterName
search	I-HyperparameterName
with	I-HyperparameterName
a	I-HyperparameterName
size	I-HyperparameterName
of	O
1	B-HyperparameterValue
or	O
5	B-HyperparameterValue
with	O
predictions	O
up	O
to	O
30	O
tokens	O
or	O
until	O
2	O
ending	O
tokens	O
were	O
predicted	O
(	O
EOS	O
)	O
.	O

We	O
ran	O
each	O
model	O
for	O
60	B-HyperparameterValue
epochs	B-HyperparameterName
,	O
applying	O
early	O
stopping	O
for	O
model	O
selection	O
based	O
on	O
accuracy	O
on	O
the	O
development	O
set	O
with	O
patience	O
of	O
20	B-HyperparameterValue
epochs	B-HyperparameterName
.	O

Models	O
were	O
trained	O
using	O
stochastic	O
gradient	O
descent	O
with	O
Adadelta	O
(	O
Zeiler	O
,	O
2012	O
)	O
and	O
mini	O
-	O
batches	B-HyperparameterName
of	I-HyperparameterName
size	I-HyperparameterName
40	B-HyperparameterValue
.	O

All	O
non	O
-	O
recurrent	O
matrices	O
were	O
initialized	O
following	O
the	O
method	O
of	O
Glorot	O
and	O
Bengio	O
(	O
2010	O
)	O
.	O

Source	O
and	O
target	O
word	O
embeddings	O
were	O
300D	B-HyperparameterValue
each	O
and	O
trained	O
jointly	O
with	O
the	O
model	O
,	O
whereas	O
hidden	B-HyperparameterName
units	I-HyperparameterName
were	O
512D	B-HyperparameterValue
for	O
each	O
direction	O
,	O
totaling	O
1024D	B-HyperparameterValue
in	O
the	O
bidirection	O
layers	O
.	O

Settings	O
NeuralREG	B-MethodName
was	O
implemented	O
using	O
Dynet	O
(	O
Neubig	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

To	O
test	O
the	O
statistical	O
significance	O
of	O
the	O
BLEU	B-TaskName
scores	O
of	O
the	O
models	O
,	O
we	O
used	O
a	O
bootstrap	O
resampling	O
together	O
with	O
an	O
approximate	O
randomization	O
method	O
(	O
Clark	O
et	O
al	O
.	O
,	O
2011	O
)	O
2	O
.	O

Post	O
-	O
hoc	O
McNemar	O
's	O
and	O
Wilcoxon	O
signed	O
ranked	O
tests	O
adjusted	O
by	O
the	O
Bonferroni	O
method	O
were	O
used	O
to	O
test	O
the	O
statistical	O
significance	O
of	O
the	O
models	O
in	O
terms	O
of	O
accuracy	B-MetricName
and	O
string	B-MetricName
edit	I-MetricName
distance	I-MetricName
,	O
respectively	O
.	O

Since	O
our	O
model	O
does	O
not	O
handle	O
referring	O
expressions	O
for	O
constants	O
(	O
dates	O
and	O
numbers	O
)	O
,	O
we	O
just	O
copied	O
their	O
source	O
version	O
into	O
the	O
template	O
.	O

Finally	O
,	O
we	O
lexicalized	O
the	O
original	O
templates	O
with	O
the	O
referring	O
expressions	O
produced	O
by	O
the	O
models	O
and	O
compared	O
them	O
with	O
the	O
original	O
texts	O
in	O
the	O
corpus	O
using	O
accuracy	B-MetricName
and	O
BLEU	B-MetricName
score	O
(	O
Papineni	O
et	O
al	O
.	O
,	O
2002	O
)	O
as	O
a	O
measure	O
of	O
fluency	O
.	O

Since	O
pronouns	O
are	O
highlighted	O
as	O
the	O
most	O
likely	O
referential	O
form	O
to	O
be	O
used	O
when	O
a	O
referent	O
is	O
salient	O
in	O
the	O
discourse	O
,	O
as	O
argued	O
in	O
the	O
introduction	O
,	O
we	O
also	O
computed	O
pronoun	O
accuracy	B-MetricName
,	O
precision	B-MetricName
,	O
recall	B-MetricName
and	O
F1	B-MetricName
-	O
score	O
in	O
order	O
to	O
evaluate	O
the	O
performance	O
of	O
the	O
models	O
for	O
capturing	O
discourse	O
salience	O
.	O

We	O
compared	O
the	O
referring	O
expressions	O
produced	O
by	O
the	O
evaluated	O
models	O
with	O
the	O
goldstandards	O
ones	O
using	O
accuracy	B-MetricName
and	O
String	B-MetricName
Edit	I-MetricName
Distance	I-MetricName
(	O
Levenshtein	B-MetricName
,	O
1966	O
)	O
.	O

Data	O
We	O
evaluated	O
our	O
models	O
on	O
the	O
training	O
,	O
development	O
and	O
test	O
referring	O
expression	O
sets	O
described	O
in	O
Section	O
3.3	O
.	O
Metrics	O
.	O

Automatic	O
evaluation	O
.	O

All	O
features	O
were	O
extracted	O
automatically	O
from	O
the	O
texts	O
using	O
the	O
sentence	O
tokenizer	O
and	O
dependency	O
parser	O
of	O
Stanford	O
CoreNLP	O
(	O
Manning	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

Text	O
and	O
sentence	O
information	O
statuses	O
mark	O
whether	O
a	O
reference	O
is	O
a	O
initial	O
or	O
a	O
subsequent	O
mention	O
to	O
an	O
entity	O
in	O
the	O
text	O
and	O
the	O
sentence	O
,	O
respectively	O
.	O

Regarding	O
the	O
features	O
,	O
syntactic	O
position	O
distinguishes	O
whether	O
a	O
reference	O
is	O
the	O
subject	O
,	O
object	O
or	O
subject	O
determiner	O
(	O
genitive	O
)	O
in	O
a	O
sentence	O
.	O

Finally	O
,	O
if	O
a	O
referring	O
expression	O
is	O
not	O
found	O
in	O
the	O
training	O
set	O
for	O
a	O
given	O
entity	O
,	O
the	O
same	O
method	O
as	O
OnlyNames	B-MethodName
is	O
used	O
.	O

In	O
case	O
a	O
referring	O
expression	O
for	O
a	O
wiki	O
target	O
is	O
not	O
found	O
in	O
this	O
way	O
,	O
a	O
backoff	O
method	O
is	O
applied	O
by	O
removing	O
one	O
factor	O
at	O
a	O
time	O
in	O
the	O
following	O
order	O
:	O
sentence	O
information	O
status	O
,	O
text	O
information	O
status	O
and	O
grammatical	O
position	O
.	O

Once	O
the	O
choice	O
of	O
referential	O
form	O
is	O
made	O
,	O
the	O
most	O
frequent	O
variant	O
is	O
chosen	O
in	O
the	O
training	O
corpus	O
given	O
the	O
referent	O
,	O
syntactic	O
position	O
and	O
information	O
status	O
.	O

P	O
(	O
f	O
|	O
X	O
)	O
∝	O
P	O
(	O
f	O
)	O
x∈X	O
P	O
(	O
x	O
|	O
f	O
)	O
f	O
∈F	O
P	O
(	O
f	O
)	O
x∈X	O
P	O
(	O
x	O
|	O
f	O
)	O
(	O
14	O
)	O
The	O
method	O
calculates	O
the	O
likelihood	O
of	O
each	O
referential	O
form	O
f	O
given	O
a	O
set	O
of	O
features	O
X	O
,	O
consisting	O
of	O
grammatical	O
position	O
and	O
information	O
status	O
(	O
new	O
or	O
given	O
in	O
the	O
text	O
and	O
sentence	O
)	O
.	O

The	O
choice	O
is	O
made	O
by	O
a	O
Naive	O
Bayes	O
method	O
as	O
Equation	O
14	O
depicts	O
.	O

Ferreira	O
works	O
by	O
first	O
choosing	O
whether	O
a	O
reference	O
should	O
be	O
a	O
proper	O
name	O
,	O
pronoun	O
,	O
description	O
or	O
demonstrative	O
.	O

This	O
method	O
refers	O
to	O
each	O
entity	O
by	O
their	O
Wikipedia	O
ID	O
,	O
replacing	O
each	O
underscore	O
in	O
the	O
ID	O
for	O
whitespaces	O
(	O
e.g.	O
,	O
Appleton	O
International	O
Airport	O
to	O
"	O
Appleton	O
International	O
Airport	O
"	O
)	O
.	O

OnlyNames	B-MethodName
is	O
motivated	O
by	O
the	O
similarity	O
among	O
the	O
Wikipedia	O
ID	O
of	O
an	O
element	O
and	O
a	O
proper	O
name	O
reference	O
to	O
it	O
.	O

(	O
2016	O
)	O
,	O
dubbed	O
Ferreira	O
.	O

We	O
compared	O
the	O
performance	O
of	O
NeuralREG	B-MethodName
against	O
two	O
baselines	O
:	O
OnlyNames	B-MethodName
and	O
a	O
model	O
based	O
on	O
the	O
choice	O
of	O
referential	O
form	O
method	O
of	O
Castro	O
Ferreira	O
et	O
al	O
.	O

Models	O
for	O
Comparison	O
.	O

In	O
order	O
to	O
find	O
the	O
referring	O
expression	O
y	O
that	O
maximizes	O
the	O
likelihood	O
in	O
Equation	O
11	O
,	O
we	O
apply	O
a	O
beam	O
search	O
with	O
length	O
normalization	O
with	O
α	B-HyperparameterName
=	O
0.6	B-HyperparameterValue
(	O
Wu	O
et	O
al	O
.	O
,	O
2016	O
):	O
lp(y	O
)	O
=	O
(	O
5	O
+	O
|y|	O
)	O
α	O
(	O
5	O
+	O
1	O
)	O
α	O
(	O
12	O
)	O
The	O
decoder	O
is	O
trained	O
to	O
minimize	O
the	O
negative	O
log	O
likelihood	O
of	O
the	O
next	O
token	O
in	O
the	O
target	O
referring	O
expression	O
:	O
J(θ	O
)	O
=	O
−	O
i	O
log	O
p(yi|y	O
<	O
i	O
,	O
X	O
(	O
pre	O
)	O
,	O
x	O
(	O
wiki	O
)	O
,	O
X	O
(	O
pos	O
)	O
)	O
(	O
13	O
)	O
.	O

si	O
=	O
Φ	O
dec	O
(	O
si−1	O
,	O
[	O
ci	O
,	O
Vy	O
i−1	O
,	O
V	O
wiki	O
]	O
)	O
(	O
10	O
)	O
p(y	O
i	O
|y	O
<	O
i	O
,	O
X	O
(	O
pre	O
)	O
,	O
x	O
(	O
wiki	O
)	O
,	O
X	O
(	O
pos	O
)	O
)	O
=	O
softmax(W	O
c	O
s	O
i	O
+	O
b)(11	O
)	O
In	O
Equation	O
10	O
,	O
s	O
0	O
and	O
c	O
0	O
are	O
zero	O
-	O
initialized	O
vectors	O
.	O

In	O
each	O
decoding	O
step	O
i	O
,	O
a	O
final	O
summary	O
-	O
vector	O
for	O
each	O
context	O
c	O
(	O
k	O
)	O
i	O
is	O
computed	O
by	O
summing	O
the	O
encoder	O
states	O
h	O
(	O
k	O
)	O
j	O
weighted	O
by	O
the	O
attention	O
probabilities	O
α	O
(	O
k	O
)	O
i	O
:	O
c	O
(	O
k	O
)	O
i	O
=	O
N	O
j=1	O
α	O
(	O
k	O
)	O
ij	O
h	O
(	O
k	O
)	O
j	O
(	O
6	O
)	O
To	O
combine	O
c	O
HierAtt	O
implements	O
a	O
second	O
attention	O
mechanism	O
inspired	O
by	O
Libovický	O
and	O
Helcl	O
(	O
2017	O
)	O
in	O
order	O
to	O
generate	O
attention	O
weights	O
for	O
the	O
pre	O
-	O
and	O
pos	O
-	O
context	O
summary	O
-	O
vectors	O
c	O
(	O
k	O
)	O
i	O
=	O
v	O
(	O
k)T	O
b	O
tanh(W	O
(	O
k	O
)	O
b	O
si−1	O
+	O
U	O
(	O
k	O
)	O
b	O
c	O
(	O
k	O
)	O
i	O
)	O
(	O
7	O
)	O
β	O
(	O
k	O
)	O
i	O
=	O
exp(e	O
(	O
k	O
)	O
i	O
)	O
n	O
exp(e	O
(	O
n	O
)	O
i	O
)	O
(	O
8)	O
ci	O
=	O
k	O
β	O
(	O
k	O
)	O
i	O
U	O
(	O
k	O
)	O
b	O
c	O
(	O
k	O
)	O
i	O
(	O
9	O
)	O
Decoding	O
Given	O
the	O
summary	O
-	O
vector	O
c	O
i	O
,	O
the	O
embedding	O
of	O
the	O
previous	O
referring	O
expression	O
token	O
V	O
y	O
i−1	O
,	O
the	O
previous	O
decoder	O
state	O
s	O
i−1	O
and	O
the	O
entity	O
-	O
embedding	O
V	O
wiki	O
,	O
the	O
decoders	O
predict	O
their	O
next	O
state	O
which	O
later	O
is	O
used	O
to	O
compute	O
a	O
probability	O
distribution	O
over	O
the	O
tokens	O
in	O
the	O
output	O
vocabulary	O
for	O
the	O
next	O
timestep	O
as	O
Equations	O
10	O
and	O
11	O
show	O
.	O

We	O
compute	O
energies	O
e	O
(	O
k	O
)	O
ij	O
=	O
v	O
(	O
k)T	O
a	O
tanh(W	O
(	O
k	O
)	O
a	O
si−1	O
+	O
U	O
(	O
k	O
)	O
a	O
h	O
(	O
k	O
)	O
j	O
)	O
(	O
4	O
)	O
α	O
(	O
k	O
)	O
ij	O
=	O
exp(e	O
(	O
k	O
)	O
ij	O
)	O
N	O
n=1	O
exp(e	O
(	O
k	O
)	O
in	O
)	O
(	O
5	O
)	O
In	O
general	O
,	O
the	O
attention	O
probability	O
α	O
(	O
k	O
)	O
ij	O
determines	O
the	O
amount	O
of	O
contribution	O
of	O
the	O
jth	O
token	O
of	O
k	O
-	O
context	O
in	O
the	O
generation	O
of	O
the	O
ith	O
token	O
of	O
the	O
referring	O
expression	O
.	O

Seq2Seq	O
models	O
the	O
context	O
vector	O
c	O
i	O
at	O
each	O
timestep	O
i	O
concatenating	O
the	O
pre	O
-	O
and	O
pos	O
-	O
context	O
annotation	O
vectors	O
averaged	O
over	O
time	O
:	O
ĥ(pre	O
)	O
=	O
1	O
N	O
N	O
i	O
h	O
(	O
pre	O
)	O
i	O
(	O
1	O
)	O
ĥ(pos	O
)	O
=	O
1	O
N	O
N	O
i	O
h	O
(	O
pos	O
)	O
i	O
(	O
2	O
)	O
ci	O
=	O
[	O
ĥ(pre	O
)	O
,	O
ĥ(pos	O
)	O
]	O
(	O
3	O
)	O
CAtt	O
is	O
an	O
LSTM	O
decoder	O
augmented	O
with	O
an	O
attention	O
mechanism	O
(	O
Bahdanau	O
et	O
al	O
.	O
,	O
2015	O
)	O
over	O
the	O
pre	O
-	O
and	O
pos	O
-	O
context	O
encodings	O
,	O
which	O
is	O
used	O
to	O
compute	O
c	O
i	O
at	O
each	O
timestep	O
.	O

The	O
difference	O
between	O
the	O
decoder	O
variations	O
is	O
the	O
method	O
to	O
compute	O
c	O
i	O
.	O

All	O
decoders	O
at	O
each	O
timestep	O
i	O
of	O
the	O
generation	O
process	O
take	O
as	O
input	O
features	O
their	O
previous	O
state	O
s	O
i−1	O
,	O
the	O
target	O
entity	O
-	O
embedding	O
V	O
wiki	O
,	O
the	O
embedding	O
of	O
the	O
previous	O
word	O
of	O
the	O
referring	O
expression	O
V	O
y	O
i−1	O
and	O
finally	O
the	O
summary	O
vector	O
of	O
the	O
pre	O
-	O
and	O
poscontexts	O
c	O
i	O
.	O

The	O
referring	B-TaskName
expression	I-TaskName
generation	I-TaskName
module	O
is	O
an	O
LSTM	O
decoder	O
implemented	O
in	O
3	O
different	O
versions	O
:	O
Seq2Seq	O
,	O
CAtt	O
and	O
HierAtt	O
.	O

Decoder	O
.	O

Finally	O
,	O
the	O
encoding	O
of	O
target	O
entity	O
x	O
(	O
wiki	O
)	O
is	O
simply	O
its	O
entry	O
in	O
the	O
shared	O
input	O
word	O
-	O
embedding	O
matrix	O
V	O
wiki	O
.	O

The	O
same	O
process	O
is	O
repeated	O
for	O
the	O
pos	O
-	O
context	O
resulting	O
in	O
representations	O
(	O
−	O
→	O
h	O
(	O
pos	O
)	O
1	O
,	O
•	O
•	O
•	O
,	O
−	O
→	O
h	O
(	O
pos	O
)	O
l	O
)	O
and	O
(	O
←	O
−	O
h	O
(	O
pos	O
)	O
1	O
,	O
•	O
•	O
•	O
,	O
←	O
−	O
h	O
(	O
pos	O
)	O
l	O
)	O
and	O
annotation	O
vectors	O
h	O
(	O
pos	O
)	O
t	O
=	O
[	O
−	O
→	O
h	O
(	O
pos	O
)	O
t	O
,	O
←	O
−	O
h	O
(	O
pos	O
)	O
t	O
]	O
.	O

The	O
final	O
annotation	O
vector	O
for	O
each	O
encoding	O
timestep	O
t	O
is	O
obtained	O
by	O
the	O
concatenation	O
of	O
the	O
forward	O
and	O
backward	O
representations	O
h	O
(	O
pre	O
)	O
t	O
=	O
[	O
−	O
→	O
h	O
(	O
pre	O
)	O
t	O
,	O
←	O
−	O
h	O
(	O
pre	O
)	O
t	O
]	O
.	O

The	O
pre	O
-	O
context	O
X	O
(	O
pre	O
)	O
=	O
{	O
x	O
(	O
pre	O
)	O
1	O
,	O
x	O
(	O
pre	O
)	O
2	O
,	O
...	O
,	O
x(pre	O
)	O
m	O
}	O
is	O
represented	O
by	O
forward	O
and	O
backward	O
hidden	O
-	O
state	O
vectors	O
(	O
−	O
→	O
h	O
(	O
pre	O
)	O
1	O
,	O
•	O
•	O
•	O
,	O
−	O
→	O
h	O
(	O
pre	O
)	O
m	O
)	O
and	O
(	O
←	O
−	O
h	O
(	O
pre	O
)	O
1	O
,	O
•	O
•	O
•	O
,	O
←	O
−	O
h	O
(	O
pre	O
)	O
m	O
)	O
.	O

These	O
modules	O
learn	O
feature	O
representations	O
of	O
the	O
text	O
surrounding	O
the	O
target	O
entity	O
x	O
(	O
wiki	O
)	O
,	O
which	O
are	O
used	O
for	O
the	O
referring	B-TaskName
expression	I-TaskName
generation	I-TaskName
.	O

Our	O
model	O
starts	O
by	O
encoding	O
the	O
pre	O
-	O
and	O
poscontexts	O
with	O
two	O
separate	O
bidirectional	O
LSTM	O
encoders	O
(	O
Schuster	O
and	O
Paliwal	O
,	O
1997;Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
.	O

Context	O
encoders	O
.	O

The	O
model	O
is	O
implemented	O
as	O
a	O
multi	O
-	O
encoder	O
,	O
attentiondecoder	O
network	O
with	O
bidirectional	O
(	O
Schuster	O
and	O
Paliwal	O
,	O
1997	O
)	O
Long	O
-	O
Short	O
Term	O
Memory	O
Layers	O
(	O
LSTM	O
)	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
sharing	O
the	O
same	O
input	O
word	O
-	O
embedding	O
matrix	O
V	O
,	O
as	O
explained	O
further	O
.	O

NeuralREG	B-MethodName
aims	O
to	O
generate	O
a	O
referring	O
expression	O
y	O
=	O
{	O
y	O
1	O
,	O
y	O
2	O
,	O
...	O
,	O
y	O
T	O
}	O
with	O
T	O
tokens	O
to	O
refer	O
to	O
a	O
target	O
entity	O
token	O
x	O
(	O
wiki	O
)	O
given	O
a	O
discourse	O
precontext	O
X	O
(	O
pre	O
)	O
=	O
{	O
x	O
}	O
with	O
m	O
and	O
l	O
tokens	O
,	O
respectively	O
.	O

NeuralREG	B-MethodName
.	O

In	O
the	O
next	O
section	O
,	O
we	O
show	O
in	O
detail	O
how	O
NeuralREG	B-MethodName
models	O
the	O
problem	O
of	O
generating	B-TaskName
a	I-TaskName
referring	I-TaskName
expression	I-TaskName
to	O
a	O
discourse	O
entity	O
.	O

In	O
this	O
context	O
,	O
it	O
is	O
important	O
to	O
observe	O
that	O
the	O
conversion	O
of	O
the	O
general	O
tags	O
to	O
the	O
Wikipedia	O
IDs	O
can	O
be	O
done	O
in	O
constant	O
time	O
during	O
the	O
generation	O
process	O
,	O
since	O
their	O
mapping	O
,	O
like	O
the	O
first	O
representation	O
in	O
Figure	O
2	O
,	O
is	O
the	O
first	O
step	O
of	O
the	O
process	O
.	O

Although	O
the	O
references	O
to	O
discourse	O
entities	O
are	O
represented	O
by	O
general	O
tags	O
in	O
a	O
delexicalized	O
template	O
produced	O
in	O
the	O
generation	O
process	O
(	O
AGENT-1	O
,	O
BRIDGE-1	O
,	O
etc	O
.	O
)	O
,	O
for	O
the	O
purpose	O
of	O
disambiguation	O
,	O
NeuralREG	B-MethodName
's	O
inputs	O
have	O
the	O
references	O
represented	O
by	O
the	O
Wikipedia	O
ID	O
of	O
their	O
entities	O
.	O

References	O
to	O
other	O
discourse	O
entities	O
in	O
the	O
pre	O
-	O
and	O
pos	O
-	O
contexts	O
are	O
represented	O
by	O
their	O
Wikipedia	O
ID	O
,	O
whereas	O
constants	O
(	O
numbers	O
,	O
dates	O
)	O
are	O
represented	O
by	O
a	O
one	O
-	O
word	O
ID	O
removing	O
quotes	O
and	O
replacing	O
white	O
spaces	O
with	O
underscores	O
(	O
e.g.	O
,	O
120	O
million	O
(	O
Australian	O
dollars	O
)	O
for	O
"	O
120	O
million	O
(	O
Australian	O
dollars	O
)	O
"	O
in	O
Figure	O
2	O
)	O
.	O

pieces	O
of	O
text	O
before	O
and	O
after	O
the	O
target	O
reference	O
.	O

AGENT-1	O
has	O
a	O
total	O
of	O
PATIENT-4	O
floors	O
and	O
cost	O
PATIENT-3	O
.	O

Pre	O
-	O
and	O
pos	O
-	O
contexts	O
are	O
the	O
lowercased	O
,	O
tokenized	O
and	O
delexicalized	O
Tag	O
Entity	O
AGENT-1	O
108	O
St	O
Georges	O
Terrace	O
BRIDGE-1	O
Perth	O
PATIENT-1	O
Australia	O
PATIENT-2	O
1988@year	O
PATIENT-3	O
"	O
120	O
million	O
(	O
Australian	O
dollars)"@USD	O
PATIENT-4	O
50@Integer	O
AGENT-1	O
was	O
completed	O
in	O
PATIENT-2	O
in	O
BRIDGE-1	O
,	O
PATIENT-1	O
.	O

Each	O
instance	O
of	O
the	O
final	O
dataset	O
consists	O
of	O
a	O
truecased	O
tokenized	O
referring	O
expression	O
,	O
the	O
target	O
entity	O
(	O
distinguished	O
by	O
its	O
Wikipedia	O
ID	O
)	O
,	O
and	O
the	O
discourse	O
context	O
preceding	O
and	O
following	O
the	O
relevant	O
reference	O
(	O
we	O
refer	O
to	O
these	O
as	O
the	O
pre	O
-	O
and	O
pos	O
-	O
context	O
)	O
.	O

We	O
split	O
this	O
collection	O
in	O
training	O
,	O
developing	O
and	O
test	O
sets	O
,	O
totaling	O
63,061	O
,	O
7,097	O
and	O
8,743	O
referring	O
expressions	O
in	O
each	O
one	O
of	O
them	O
.	O

In	O
total	O
,	O
the	O
final	O
version	O
of	O
our	O
dataset	O
contains	O
78,901	O
referring	O
expressions	O
to	O
1,501	O
Wikipedia	O
entities	O
,	O
in	O
which	O
71.4	O
%	O
(	O
56,321	O
)	O
are	O
proper	O
names	O
,	O
5.6	O
%	O
(	O
4,467	O
)	O
pronouns	O
,	O
22.6	O
%	O
(	O
17,795	O
)	O
descriptions	O
and	O
0.4	O
%	O
(	O
318	O
)	O
demonstrative	O
referring	O
expressions	O
.	O

Using	O
the	O
delexicalized	B-DatasetName
version	I-DatasetName
of	I-DatasetName
the	I-DatasetName
WebNLG	I-DatasetName
corpus	O
,	O
we	O
automatically	O
extracted	O
all	O
referring	O
expressions	O
by	O
tokenizing	O
the	O
original	O
and	O
delexicalized	O
versions	O
of	O
the	O
texts	O
and	O
then	O
finding	O
the	O
non	O
overlapping	O
items	O
.	O

Once	O
all	O
texts	O
were	O
processed	O
and	O
the	O
referring	O
expressions	O
extracted	O
,	O
we	O
filtered	O
only	O
the	O
ones	O
referring	O
to	O
Wikipedia	O
entities	O
,	O
removing	O
references	O
to	O
constants	O
like	O
dates	O
and	O
numbers	O
,	O
for	O
which	O
no	O
references	O
are	O
generated	O
by	O
the	O
model	O
.	O

For	O
instance	O
,	O
by	O
processing	O
the	O
text	O
in	O
Figure	O
1	O
and	O
its	O
delexicalized	O
template	O
in	O
Figure	O
2	O
,	O
we	O
would	O
extract	O
referring	O
expressions	O
like	O
"	O
108	O
St	O
Georges	O
Terrace	O
"	O
and	O
"	O
It	O
"	O
to	O
AGENT-1	O
,	O
108	O
St	O
Georges	O
Terrace	O
,	O
"	O
Perth	O
"	O
to	O
BRIDGE-1	O
,	O
Perth	O
,	O
"	O
Australia	O
"	O
to	O
PATIENT-1	O
,	O
Australia	O
and	O
so	O
on	O
.	O

Referring	O
expression	O
collection	O
.	O

While	O
this	O
dataset	O
(	O
which	O
we	O
make	O
available	O
)	O
has	O
various	O
uses	O
,	O
we	O
used	O
it	O
to	O
extract	O
a	O
collection	O
of	O
referring	O
expressions	O
to	O
Wikipedia	O
entities	O
in	O
order	O
to	O
evaluate	O
how	O
well	O
our	O
REG	B-TaskName
model	O
can	O
produce	O
references	O
to	O
entities	O
throughout	O
a	O
(	O
small	O
)	O
text	O
.	O

Figure	O
2	O
shows	O
the	O
entity	O
mapping	O
and	O
the	O
delexicalized	O
template	O
for	O
the	O
example	O
in	O
Figure	O
1	O
in	O
its	O
versions	O
representing	O
the	O
references	O
with	O
general	O
tags	O
and	O
Wikipedia	O
IDs	O
.	O
We	O
delexicalized	O
20,198	O
distinct	O
texts	O
describing	O
7,812	O
distinct	O
sets	O
of	O
RDF	O
triples	O
,	O
resulting	O
in	O
16,628	O
distinct	O
templates	O
.	O

Once	O
all	O
entities	O
in	O
the	O
text	O
were	O
mapped	O
to	O
different	O
roles	O
,	O
the	O
first	O
two	O
authors	O
of	O
this	O
study	O
manually	O
replaced	O
the	O
referring	O
expressions	O
in	O
the	O
original	O
target	O
texts	O
by	O
their	O
respective	O
tags	O
.	O

Entities	O
which	O
appear	O
on	O
both	O
sides	O
in	O
the	O
relations	O
of	O
a	O
set	O
were	O
represented	O
as	O
BRIDGEs	O
.	O
To	O
distinguish	O
different	O
AGENTs	O
,	O
PATIENTs	O
and	O
BRIDGEs	O
in	O
a	O
set	O
,	O
an	O
ID	O
was	O
given	O
to	O
each	O
entity	O
of	O
each	O
kind	O
(	O
PATIENT-1	O
,	O
PATIENT-2	O
,	O
etc	O
.	O
)	O
.	O

All	O
entities	O
that	O
appear	O
on	O
the	O
left	O
and	O
right	O
side	O
of	O
the	O
triples	O
were	O
mapped	O
to	O
AGENTs	O
and	O
PATIENTs	O
,	O
respectively	O
.	O

We	O
delexicalized	O
the	O
training	O
and	O
development	O
parts	O
of	O
the	O
WebNLG	B-DatasetName
corpus	O
by	O
first	O
automatically	O
mapping	O
each	O
entity	O
in	O
the	O
source	O
representation	O
to	O
a	O
general	O
tag	O
.	O

Delexicalized	B-DatasetName
WebNLG	I-DatasetName
.	O

In	O
order	O
to	O
be	O
able	O
to	O
train	O
and	O
evaluate	O
our	O
models	O
for	O
referring	B-TaskName
expression	I-TaskName
generation	I-TaskName
(	O
the	O
topic	O
of	O
this	O
study	O
)	O
,	O
we	O
produced	O
a	O
delexicalized	O
version	O
of	O
the	O
original	O
corpus	O
.	O

The	O
corpus	O
consists	O
of	O
25,298	O
texts	O
describing	O
9,674	O
sets	O
of	O
up	O
to	O
7	O
RDF	O
triples	O
(	O
an	O
average	O
of	O
2.62	O
texts	O
per	O
set	O
)	O
in	O
15	O
domains	O
(	O
Gardent	O
et	O
al	O
.	O
,	O
2017b	O
)	O
.	O

Figure	O
1	O
depicts	O
an	O
example	O
of	O
a	O
set	O
of	O
5	O
RDF	O
triples	O
and	O
the	O
corresponding	O
text	O
.	O

The	O
target	O
side	O
contains	O
English	O
texts	O
,	O
obtained	O
by	O
crowdsourcing	O
,	O
which	O
describe	O
the	O
source	O
triples	O
.	O

Each	O
RDF	O
triple	O
is	O
formed	O
by	O
a	O
Subject	O
,	O
Predicate	O
and	O
Object	O
,	O
where	O
the	O
Subject	O
and	O
Object	O
are	O
constants	O
or	O
Wikipedia	O
entities	O
,	O
and	O
predicates	O
represent	O
a	O
relation	O
between	O
these	O
two	O
elements	O
in	O
the	O
triple	O
.	O

The	O
source	O
side	O
of	O
the	O
corpus	O
are	O
sets	O
of	O
Resource	O
Description	O
Framework	O
(	O
RDF	O
)	O
triples	O
.	O

In	O
this	O
challenge	O
,	O
participants	O
had	O
to	O
automatically	O
convert	O
non	O
-	O
linguistic	O
data	O
from	O
the	O
Semantic	O
Web	O
into	O
a	O
textual	O
format	O
(	O
Gardent	O
et	O
al	O
.	O
,	O
2017b	O
)	O
.	O

Our	O
data	O
is	O
based	O
on	O
the	O
WebNLG	B-DatasetName
corpus	O
(	O
Gardent	O
et	O
al	O
.	O
,	O
2017a	O
)	O
,	O
which	O
is	O
a	O
parallel	O
resource	O
ini-	O
tially	O
released	O
for	O
the	O
eponymous	O
NLG	O
challenge	O
.	O

WebNLG	B-DatasetName
corpus	O
.	O

3	O
Data	O
and	O
processing	O
.	O

Below	O
we	O
describe	O
our	O
model	O
in	O
more	O
detail	O
,	O
as	O
well	O
as	O
the	O
data	O
on	O
which	O
we	O
develop	O
and	O
evaluate	O
it	O
.	O

In	O
contrast	O
,	O
we	O
introduce	O
NeuralREG	B-MethodName
,	O
an	O
end	O
-	O
to	O
-	O
end	O
approach	O
based	O
on	O
neural	O
networks	O
which	O
generates	O
referring	O
expressions	O
to	O
discourse	O
entities	O
directly	O
from	O
a	O
delexicalized	O
/	O
wikified	O
text	O
fragment	O
,	O
without	O
the	O
use	O
of	O
any	O
feature	O
extraction	O
technique	O
.	O

Moreover	O
,	O
many	O
of	O
these	O
models	O
only	O
address	O
part	O
of	O
the	O
problem	O
,	O
either	O
concentrating	O
on	O
the	O
choice	O
of	O
referential	O
form	O
or	O
on	O
deciding	O
on	O
the	O
contents	O
of	O
,	O
for	O
example	O
,	O
proper	O
names	O
or	O
definite	O
descriptions	O
.	O

Typically	O
,	O
these	O
features	O
are	O
extracted	O
automatically	O
from	O
the	O
context	O
,	O
and	O
engineering	O
relevant	O
ones	O
can	O
be	O
complex	O
.	O

In	O
sum	O
,	O
existing	O
REG	B-TaskName
models	O
for	O
text	O
generation	O
strongly	O
rely	O
on	O
abstract	O
features	O
such	O
as	O
the	O
salience	O
of	O
a	O
referent	O
for	O
deciding	O
on	O
the	O
form	O
or	O
content	O
of	O
a	O
referent	O
.	O

(	O
2010	O
)	O
.	O

More	O
details	O
about	O
the	O
models	O
can	O
be	O
seen	O
on	O
Belz	O
et	O
al	O
.	O

Some	O
participating	O
systems	O
approached	O
this	O
with	O
traditional	O
pipelines	O
for	O
selecting	O
referential	O
form	O
,	O
followed	O
by	O
referential	O
content	O
,	O
while	O
others	O
proposed	O
more	O
integrated	O
methods	O
.	O

The	O
input	O
for	O
the	O
models	O
were	O
texts	O
in	O
which	O
the	O
referring	O
expressions	O
to	O
the	O
topic	O
of	O
the	O
relevant	O
Wikipedia	O
entry	O
were	O
removed	O
and	O
appropriate	O
references	O
throughout	O
the	O
text	O
needed	O
to	O
be	O
generated	O
(	O
by	O
selecting	O
,	O
for	O
each	O
gap	O
,	O
from	O
a	O
list	O
of	O
candidate	O
referring	O
expressions	O
of	O
different	O
forms	O
and	O
with	O
different	O
contents	O
)	O
.	O

This	O
was	O
the	O
case	O
,	O
for	O
instance	O
,	O
in	O
the	O
GREC	B-DatasetName
shared	O
task	O
(	O
Belz	O
et	O
al	O
.	O
,	O
2010	O
)	O
,	O
which	O
aimed	O
to	O
evaluate	O
models	O
for	O
automatically	O
generated	O
referring	O
expressions	O
grounded	O
in	O
discourse	O
.	O

Of	O
course	O
,	O
when	O
texts	O
are	O
generated	O
in	O
practical	O
settings	O
,	O
both	O
form	O
and	O
content	O
need	O
to	O
be	O
chosen	O
.	O

(	O
2011);van	O
Deemter	O
(	O
2016	O
)	O
for	O
proper	O
names	O
.	O

To	O
this	O
end	O
,	O
separate	O
models	O
are	O
typically	O
used	O
,	O
including	O
,	O
for	O
example	O
,	O
Dale	O
and	O
Reiter	O
(	O
1995	O
)	O
for	O
generating	O
descriptions	O
,	O
and	O
Siddharthan	O
et	O
al	O
.	O

Importantly	O
,	O
these	O
models	O
do	O
not	O
specify	O
which	O
contents	O
a	O
particular	O
reference	O
,	O
be	O
it	O
a	O
proper	O
name	O
or	O
description	O
,	O
should	O
have	O
.	O

(	O
2016	O
)	O
proposed	O
a	O
data	O
-	O
driven	O
,	O
non	O
-	O
deterministic	O
model	O
for	O
generating	O
referential	O
forms	O
,	O
taking	O
into	O
account	O
salience	O
features	O
extracted	O
from	O
the	O
discourse	O
such	O
as	O
grammatical	O
position	O
,	O
givenness	O
and	O
recency	O
of	O
the	O
reference	O
.	O

More	O
recently	O
,	O
Castro	O
Ferreira	O
et	O
al	O
.	O

Reiter	O
and	O
Dale	O
(	O
2000	O
)	O
for	O
instance	O
,	O
discussed	O
a	O
straightforward	O
rule	O
-	O
based	O
method	O
based	O
on	O
this	O
notion	O
,	O
stating	O
that	O
full	O
proper	O
names	O
can	O
be	O
used	O
for	O
initial	O
references	O
,	O
typically	O
less	O
salient	O
than	O
subsequent	O
references	O
,	O
which	O
,	O
according	O
to	O
the	O
study	O
,	O
can	O
be	O
realized	O
by	O
a	O
pronoun	O
in	O
case	O
there	O
is	O
no	O
mention	O
to	O
any	O
other	O
entity	O
of	O
same	O
person	O
,	O
gender	O
and	O
number	O
between	O
the	O
reference	O
and	O
its	O
antecedents	O
.	O

Building	O
on	O
these	O
ideas	O
,	O
many	O
REG	B-TaskName
models	O
for	O
generating	O
references	O
in	O
texts	O
also	O
strongly	O
rely	O
on	O
the	O
concept	O
of	O
salience	O
and	O
factors	O
contributing	O
to	O
it	O
.	O

In	O
models	O
such	O
as	O
these	O
,	O
notions	O
like	O
salience	O
play	O
a	O
central	O
role	O
,	O
where	O
it	O
is	O
assumed	O
that	O
entities	O
which	O
are	O
salient	O
in	O
the	O
discourse	O
are	O
more	O
likely	O
to	O
be	O
referred	O
to	O
using	O
shorter	O
referring	O
expressions	O
(	O
like	O
a	O
pronoun	O
)	O
than	O
less	O
salient	O
entities	O
,	O
which	O
are	O
typically	O
referred	O
to	O
using	O
longer	O
expressions	O
(	O
like	O
full	O
proper	O
names	O
)	O
.	O

In	O
psycholinguistic	O
models	O
of	O
reference	O
,	O
various	O
linguistic	O
factors	O
have	O
been	O
proposed	O
as	O
influencing	O
the	O
form	O
of	O
referential	O
expressions	O
,	O
including	O
cognitive	O
status	O
(	O
Gundel	O
et	O
al	O
.	O
,	O
1993	O
)	O
,	O
centering	O
(	O
Grosz	O
et	O
al	O
.	O
,	O
1995	O
)	O
and	O
information	O
density	O
(	O
Jaeger	O
,	O
2010	O
)	O
.	O

There	O
is	O
,	O
however	O
,	O
a	O
lot	O
of	O
earlier	O
work	O
on	O
selecting	O
the	O
form	O
and	O
content	O
of	O
referring	O
expressions	O
,	O
both	O
in	O
psycholinguistics	O
and	O
in	O
computational	O
linguistics	O
.	O

However	O
,	O
the	O
usage	O
of	O
deep	O
neural	O
networks	O
for	O
REG	B-TaskName
has	O
remained	O
limited	O
and	O
we	O
are	O
not	O
aware	O
of	O
any	O
other	O
integrated	O
,	O
end	O
-	O
to	O
-	O
end	O
model	O
for	O
generating	B-TaskName
referring	I-TaskName
expressions	I-TaskName
in	O
discourse	O
.	O

In	O
recent	O
years	O
,	O
we	O
have	O
seen	O
a	O
surge	O
of	O
interest	O
in	O
using	O
(	O
deep	O
)	O
neural	O
networks	O
for	O
a	O
wide	O
range	O
of	O
NLG	O
-	O
related	O
tasks	O
,	O
as	O
the	O
generation	O
of	O
(	O
first	O
sentences	O
of	O
)	O
Wikipedia	O
entries	O
(	O
Lebret	O
et	O
al	O
.	O
,	O
2016	O
)	O
,	O
poetry	O
(	O
Zhang	O
and	O
Lapata	O
,	O
2014	O
)	O
,	O
and	O
texts	O
from	O
abstract	O
meaning	O
representations	O
(	O
e.g.	O
,	O
Konstas	O
et	O
al	O
.	O
,	O
2017;Castro	O
Ferreira	O
et	O
al	O
.	O
,	O
2017a	O
)	O
.	O

Related	O
work	O
.	O

We	O
compare	O
NeuralREG	B-MethodName
against	O
two	O
baselines	O
in	O
an	O
automatic	O
and	O
human	O
evaluation	O
,	O
showing	O
that	O
the	O
integrated	O
neural	O
model	O
is	O
a	O
marked	O
improvement	O
.	O

Both	O
this	O
data	O
set	O
and	O
the	O
model	O
will	O
be	O
made	O
publicly	O
available	O
.	O

While	O
our	O
approach	O
,	O
dubbed	O
as	O
NeuralREG	B-MethodName
,	O
is	O
compatible	O
with	O
different	O
applications	O
of	O
REG	B-TaskName
models	O
,	O
in	O
this	O
paper	O
,	O
we	O
concentrate	O
on	O
the	O
last	O
one	O
,	O
relying	O
on	O
a	O
specifically	O
constructed	O
set	O
of	O
78,901	O
referring	O
expressions	O
to	O
1,501	O
entities	O
in	O
the	O
context	O
of	O
the	O
semantic	O
web	O
,	O
derived	O
from	O
a	O
(	O
delexicalized	O
)	O
version	O
of	O
the	O
WebNLG	B-DatasetName
corpus	O
(	O
Gardent	O
et	O
al	O
.	O
,	O
2017a	O
,	O
b	O
)	O
.	O

Based	O
on	O
the	O
delexicalized	O
input	O
,	O
the	O
model	O
generates	O
outputs	O
which	O
may	O
be	O
likened	O
to	O
templates	O
in	O
which	O
references	O
to	O
the	O
discourse	O
entities	O
are	O
not	O
realized	O
(	O
as	O
in	O
"	O
The	O
ground	O
of	O
ENTITY-1	O
is	O
located	O
in	O
ENTITY-2	O
.	O
"	O
)	O
.	O

Some	O
of	O
these	O
approaches	O
have	O
recently	O
focused	O
on	O
inputs	O
which	O
references	O
to	O
entities	O
are	O
delexicalized	O
to	O
general	O
tags	O
(	O
e.g.	O
,	O
ENTITY-1	O
,	O
ENTITY-2	O
)	O
in	O
order	O
to	O
decrease	O
data	O
sparsity	O
.	O

Besides	O
its	O
use	O
in	O
traditional	O
pipeline	O
NLG	O
systems	O
(	O
Reiter	O
and	O
Dale	O
,	O
2000	O
)	O
,	O
REG	B-TaskName
has	O
also	O
become	O
relevant	O
in	O
modern	O
"	O
end	O
-	O
to	O
-	O
end	O
"	O
NLG	O
approaches	O
,	O
which	O
perform	O
the	O
task	O
in	O
a	O
more	O
integrated	O
manner	O
(	O
see	O
e.g.	O
Konstas	O
et	O
al	O
.	O
,	O
2017;Gardent	O
et	O
al	O
.	O
,	O
2017b	O
)	O
.	O

Our	O
approach	O
is	O
based	O
on	O
neural	O
networks	O
which	O
generate	O
referring	O
expressions	O
to	O
discourse	O
entities	O
relying	O
on	O
the	O
surrounding	O
linguistic	O
context	O
,	O
without	O
the	O
use	O
of	O
any	O
feature	O
extraction	O
technique	O
.	O

Instead	O
,	O
in	O
this	O
paper	O
,	O
we	O
propose	O
NeuralREG	B-MethodName
:	O
an	O
end	O
-	O
to	O
-	O
end	O
approach	O
addressing	O
the	O
full	O
REG	B-TaskName
task	O
,	O
which	O
given	O
a	O
number	O
of	O
entities	O
in	O
a	O
text	O
,	O
produces	O
corresponding	O
referring	O
expressions	O
,	O
simultaneously	O
selecting	O
both	O
form	O
and	O
content	O
.	O

Most	O
of	O
the	O
earlier	O
REG	B-TaskName
approaches	O
focus	O
either	O
on	O
selecting	O
referential	O
form	O
(	O
Orita	O
et	O
al	O
.	O
,	O
2015;Castro	O
Ferreira	O
et	O
al	O
.	O
,	O
2016	O
)	O
,	O
or	O
on	O
selecting	O
referential	O
content	O
,	O
typically	O
zooming	O
in	O
on	O
one	O
specific	O
kind	O
of	O
reference	O
such	O
as	O
a	O
pronoun	O
(	O
e.g.	O
,	O
Henschel	O
et	O
al	O
.	O
,	O
2000;Callaway	O
and	O
Lester	O
,	O
2002	O
)	O
,	O
definite	O
description	O
(	O
e.g.	O
,	O
Dale	O
and	O
Haddock	O
,	O
1991;Dale	O
and	O
Reiter	O
,	O
1995	O
)	O
or	O
proper	O
name	O
generation	O
(	O
e.g.	O
,	O
Siddharthan	O
et	O
al	O
.	O
,	O
2011;van	O
Deemter	O
,	O
2016;Castro	O
Ferreira	O
et	O
al	O
.	O
,	O
2017b	O
)	O
.	O

For	O
example	O
,	O
both	O
"	O
Frida	O
"	O
and	O
1	O
https://github.com/ThiagoCF05/	O
NeuralREG	O
"	O
Kahlo	O
"	O
are	O
name	O
-	O
variants	O
that	O
may	O
occur	O
in	O
a	O
text	O
,	O
and	O
she	O
can	O
alternatively	O
also	O
be	O
described	O
as	O
,	O
say	O
,	O
"	O
the	O
famous	O
female	O
painter	O
"	O
.	O

In	O
addition	O
,	O
the	O
REG	B-TaskName
model	O
must	O
account	O
for	O
the	O
different	O
ways	O
in	O
which	O
a	O
particular	O
referential	O
form	O
can	O
be	O
realized	O
.	O

First	O
,	O
the	O
referential	O
form	O
needs	O
to	O
be	O
decided	O
,	O
asking	O
whether	O
a	O
reference	O
at	O
a	O
given	O
point	O
in	O
the	O
text	O
should	O
assume	O
the	O
form	O
of	O
,	O
for	O
example	O
,	O
a	O
proper	O
name	O
(	O
"	O
Frida	O
Kahlo	O
"	O
)	O
,	O
a	O
pronoun	O
(	O
"	O
she	O
"	O
)	O
or	O
description	O
(	O
"	O
the	O
Mexican	O
painter	O
"	O
)	O
.	O

Referring	B-TaskName
Expression	I-TaskName
Generation	I-TaskName
(	O
REG	B-TaskName
)	O
,	O
the	O
task	O
responsible	O
for	O
generating	O
these	O
references	O
,	O
is	O
typically	O
presented	O
as	O
a	O
twostep	O
procedure	O
.	O

Since	O
the	O
input	O
data	O
will	O
often	O
consist	O
of	O
entities	O
and	O
the	O
relations	O
between	O
them	O
,	O
generating	O
references	O
for	O
these	O
entities	O
is	O
a	O
core	O
task	O
in	O
many	O
NLG	O
systems	O
(	O
Dale	O
and	O
Reiter	O
,	O
1995;Krahmer	O
and	O
van	O
Deemter	O
,	O
2012	O
)	O
.	O

Natural	O
Language	O
Generation	O
(	O
NLG	O
)	O
is	O
the	O
task	O
of	O
automatically	O
converting	O
non	O
-	O
linguistic	O
data	O
into	O
coherent	O
natural	O
language	O
text	O
(	O
Reiter	O
and	O
Dale	O
,	O
2000;Gatt	O
and	O
Krahmer	O
,	O
2018	O
)	O
.	O

Introduction	O
.	O

Data	O
and	O
models	O
are	O
publicly	O
available	O
1	O
.	O

Using	O
a	O
delexicalized	O
version	O
of	O
the	O
WebNLG	B-DatasetName
corpus	O
,	O
we	O
show	O
that	O
the	O
neural	O
model	O
substantially	O
improves	O
over	O
two	O
strong	O
baselines	O
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
a	O
new	O
approach	O
(	O
NeuralREG	B-MethodName
)	O
,	O
relying	O
on	O
deep	O
neural	O
networks	O
,	O
which	O
makes	O
decisions	O
about	O
form	O
and	O
content	O
in	O
one	O
go	O
without	O
explicit	O
feature	O
extraction	O
.	O

Traditionally	O
,	O
Referring	B-TaskName
Expression	I-TaskName
Generation	I-TaskName
(	O
REG	B-TaskName
)	O
models	O
first	O
decide	O
on	O
the	O
form	O
and	O
then	O
on	O
the	O
content	O
of	O
references	O
to	O
discourse	O
entities	O
in	O
text	O
,	O
typically	O
relying	O
on	O
features	O
such	O
as	O
salience	O
and	O
grammatical	O
function	O
.	O

NeuralREG	B-MethodName
:	O
An	O
end	O
-	O
to	O
-	O
end	O
approach	O
to	O
referring	B-TaskName
expression	I-TaskName
generation	I-TaskName
.	O

Acknowledgements	O
.	O

4	O
)	O
For	O
the	O
overall	O
performance	O
indicated	O
by	O
PR	B-MetricName
curves	I-MetricName
,	O
BGRU	B-MethodName
is	O
the	O
most	O
solid	O
relation	O
extractor	O
.	O

We	O
deduce	O
that	O
it	O
is	O
severely	O
affected	O
by	O
the	O
unbalanced	O
instance	O
numbers	O
of	O
different	O
relations	O
,	O
which	O
will	O
make	O
label	O
generator	O
over	O
-	O
fitting	O
to	O
frequent	O
labels	O
.	O

4	O
)	O
The	O
soft	O
-	O
label	O
method	O
greatly	O
improves	O
the	O
accuracy	O
at	O
high	O
confident	O
score	O
but	O
significantly	O
reduces	O
the	O
overall	O
performance	O
.	O

2	O
)	O
The	O
selective	O
attention	O
has	O
limited	O
help	O
in	O
improving	O
the	O
overall	O
performance	O
,	O
even	O
though	O
it	O
may	O
have	O
positive	O
effects	O
at	O
high	O
confident	O
score	O
.	O

From	O
both	O
Table	O
3	O
and	O
Figure	O
5	O
,	O
we	O
are	O
aware	O
of	O
that	O
:	O
1	O
)	O
The	O
relative	O
ranking	O
is	O
quite	O
different	O
from	O
that	O
on	O
held	B-MethodName
-	I-MethodName
out	I-MethodName
evaluation	I-MethodName
according	O
to	O
PR	B-MetricName
curve	I-MetricName
.	O

In	O
this	O
section	O
,	O
we	O
additionally	O
provide	O
PR	B-MetricName
curves	I-MetricName
to	O
show	O
the	O
performance	O
of	O
baselines	O
.	O

C.2	O
Discussion	O
.	O

In	O
.	O

However	O
,	O
part	O
of	O
them	O
are	O
false	O
negative	O
instances	O
in	O
fact	O
and	O
have	O
the	O
corresponding	O
relations	O
,	O
which	O
cause	O
considerable	O
biases	O
between	O
manual	O
and	O
held	B-MethodName
-	I-MethodName
out	I-MethodName
evaluation	I-MethodName
.	O

In	O
held	B-MethodName
-	I-MethodName
out	I-MethodName
evaluation	I-MethodName
,	O
relation	O
predictions	O
for	O
these	O
instances	O
are	O
judged	O
as	O
wrong	O
.	O

These	O
instances	O
are	O
all	O
negative	O
instances	O
and	O
has	O
the	O
automatic	O
label	O
N	O
A	O
in	O
NYT-10	B-DatasetName
.	O

In	O
Figure	O
6	O
,	O
all	O
cases	O
are	O
selected	O
from	O
Top	O
300	O
predictions	O
of	O
PCNN+ATT	B-MethodName
.	O

We	O
present	O
realistic	O
cases	O
in	O
NYT-10	B-DatasetName
to	O
show	O
the	O
effectiveness	O
of	O
our	O
method	O
.	O

B	O
Case	O
Study	O
.	O

When	O
the	O
number	O
is	O
more	O
than	O
100	O
,	O
the	O
distance	O
no	O
longer	O
drops	O
rapidly	O
but	O
begins	O
to	O
fluctuate	O
.	O

With	O
the	O
results	O
,	O
we	O
can	O
observe	O
that	O
the	O
evaluation	O
results	O
obtained	O
by	O
our	O
method	O
become	O
closer	O
to	O
human	O
evaluation	O
when	O
the	O
number	O
of	O
annotated	O
entity	O
pairs	O
is	O
less	O
than	O
100	O
.	O

We	O
have	O
recorded	O
the	O
distance	O
of	O
different	O
iterations	O
between	O
the	O
curves	O
obtained	O
by	O
our	O
method	O
and	O
manual	O
evaluation	O
in	O
Figure	O
4	O
.	O

A.4	O
The	O
result	O
of	O
different	O
iterations	O
.	O

Our	O
method	O
gets	O
the	O
distances	O
0.15	O
to	O
the	O
curve	O
of	O
human	B-MethodName
evaluation	I-MethodName
while	O
corresponding	O
distances	O
for	O
held	B-MethodName
-	I-MethodName
out	I-MethodName
evaluation	I-MethodName
is	O
0.55	O
.	O

The	O
results	O
are	O
shown	O
in	O
Table	O
4	O
,	O
and	O
Figure	O
3	O
.	O

We	O
also	O
evaluate	O
the	O
performance	O
of	O
BGRU+ATT	B-MethodName
with	O
held	B-MethodName
-	I-MethodName
out	I-MethodName
evaluation	I-MethodName
,	O
human	B-MethodName
evaluation	I-MethodName
and	O
our	O
method	O
.	O

A.3	O
Experimental	O
result	O
of	O
BGRU+ATT	B-MethodName
.	O

E	O
p(z	O
i	O
|V	O
)	O
[	O
∆i(z	O
i	O
)	O
]	O
=	O
pi	O
1	O
K	O
|1	O
−	O
pi|	O
+	O
(	O
1	O
−	O
pi	O
)	O
1	O
K	O
|0	O
−	O
pi|	O
=	O
2	O
K	O
pi(1	O
−	O
pi	O
)	O
.	O

Table	O
4	O
:	O
The	O
Precision	B-MetricName
at	O
top	O
K	O
predictions	O
(	O
%	O
)	O
of	O
BGRU+ATT	B-MethodName
upon	O
held	B-MethodName
-	I-MethodName
out	I-MethodName
evaluation	I-MethodName
,	O
our	O
method	O
and	O
human	B-MethodName
evaluation	I-MethodName
on	O
NYT-10	B-DatasetName
.	O

Here	O
we	O
provide	O
the	O
derivation	O
of	O
Equation.8	O
in	O
the	O
main	O
paper	O
.	O

A.2	O
Vetting	O
Strategy	O
.	O

The	O
expression	O
is	O
simplified	O
to	O
:	O
p(z	O
i	O
|y	O
i	O
,	O
s	O
k	O
)	O
=	O
p(y	O
jk	O
|z	O
jk	O
)	O
p(z	O
jk	O
|s	O
jk	O
)	O
v	O
p(y	O
jk	O
|z	O
jk	O
=	O
v)p(z	O
jk	O
=	O
v|s	O
jk	O
)	O
.	O

p(z	O
i	O
|y	O
i	O
,	O
s	O
i	O
)	O
=	O
p(z	O
i	O
,	O
y	O
i	O
,	O
s	O
i	O
)	O
v	O
p(z	O
i	O
=	O
v	O
,	O
y	O
i	O
,	O
s	O
i	O
)	O
=	O
p(z	O
jk	O
,	O
y	O
jk	O
,	O
s	O
jk	O
)	O
v	O
p(z	O
jk	O
=	O
v	O
,	O
y	O
jk	O
,	O
s	O
jk	O
)	O
=	O
p(y	O
jk	O
|z	O
jk	O
,	O
s	O
jk	O
)	O
p(z	O
jk	O
|s	O
jk	O
)	O
v	O
p(y	O
jk	O
|z	O
jk	O
=	O
v	O
,	O
s	O
jk	O
)	O
p(z	O
jk	O
=	O
v|s	O
jk	O
)	O
We	O
assume	O
that	O
given	O
z	O
jk	O
,	O
the	O
observed	O
label	O
y	O
jk	O
is	O
conditionally	O
independent	O
of	O
s	O
jk	O
,	O
which	O
means	O
p(y	O
jk	O
|z	O
jk	O
,	O
s	O
jk	O
)	O
=	O
p(y	O
jk	O
|z	O
jk	O
)	O
.	O

Here	O
we	O
provide	O
the	O
derivation	O
of	O
Equation.6	O
in	O
the	O
main	O
paper	O
.	O

A.1	O
Logistic	O
Regression	O
.	O

A	O
Appendices	O
.	O

Our	O
experiments	O
show	O
that	O
the	O
proposed	O
evaluation	O
method	O
is	O
appropriately	O
unbiased	O
and	O
significant	O
for	O
optimization	O
of	O
distantly	O
relation	B-TaskName
extraction	I-TaskName
in	O
future	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
active	O
testing	O
approach	O
for	O
distantly	B-TaskName
supervised	I-TaskName
relation	I-TaskName
extraction	I-TaskName
,	O
which	O
evaluates	O
performance	O
of	O
relation	O
extractors	O
with	O
both	O
noisy	O
data	O
and	O
a	O
few	O
vetted	O
data	O
.	O

Conclusion	O
.	O

More	O
results	O
and	O
discussions	O
can	O
be	O
found	O
in	O
the	O
Appendix	O
.	O

2018	O
achieves	O
highest	O
precision	O
.	O

3	O
)	O
BGRU	B-MethodName
performs	O
better	O
than	O
any	O
other	O
models	O
,	O
while	O
BGRU	B-MethodName
based	O
method	O
Liu	O
et	O
al	O
.	O

2	O
)	O
Most	O
models	O
make	O
the	O
improvements	O
as	O
they	O
mentioned	O
within	O
papers	O
at	O
high	O
confident	O
score	O
interval	O
.	O

Although	O
GAN	O
and	O
reinforcement	O
learning	O
are	O
helpful	O
to	O
select	O
valuable	O
training	O
instances	O
,	O
they	O
are	O
tendentiously	O
to	O
be	O
overfitted	O
.	O

2018a	O
.	O

2018b	O
andQin	O
et	O
al	O
.	O

From	O
Table	O
3	O
,	O
we	O
can	O
observe	O
that	O
:	O
1	O
)	O
The	O
relative	O
ranking	O
of	O
the	O
models	O
according	O
to	O
precision	O
at	O
top	O
K	O
almost	O
remains	O
the	O
same	O
except	O
Qin	O
et	O
al	O
.	O

All	O
the	O
methods	O
are	O
implemented	O
with	O
the	O
same	O
framework	O
and	O
running	O
in	O
the	O
same	O
run	O
-	O
time	O
environment	O
.	O

With	O
the	O
proposed	O
performance	O
estimator	O
,	O
we	O
reevaluate	O
eight	O
up	O
-	O
to	O
-	O
date	O
distantly	O
supervised	O
rela-	O
Model	O
P@100(%	B-MetricName
)	O
P@200(%	B-MetricName
)	O
P@300(%	B-MetricName
)	O
Table	O
3	O
:	O
The	O
P@N	B-MetricName
precision	B-MetricName
of	O
distantly	O
supervised	O
relation	O
extractors	O
on	O
NYT-10	B-DatasetName
.	O

Re	O
-	O
evaluation	O
of	O
Relation	O
Extractors	O
.	O

With	O
the	O
same	O
vetting	O
budget	O
,	O
MEMC	O
gets	O
more	O
accurate	O
performance	O
estimation	O
at	O
most	O
parts	O
of	O
the	O
range	O
.	O

From	O
the	O
figure	O
,	O
we	O
can	O
conclude	O
that	O
the	O
proposed	O
vetting	O
strategy	O
is	O
much	O
more	O
effective	O
than	O
the	O
random	O
vetting	O
strategy	O
.	O

The	O
distance	O
from	O
curves	O
of	O
different	O
vetting	O
strategies	O
to	O
that	O
of	O
human	O
evaluation	O
is	O
0.176	O
and	O
0.284	O
.	O

We	O
compare	O
our	O
MEMC	O
strategy	O
with	O
a	O
random	O
vetting	O
strategy	O
as	O
shown	O
in	O
Figure	O
2	O
.	O

Effect	O
of	O
Vetting	O
Strategy	O
.	O

Our	O
method	O
obtains	O
at	O
least	O
8.2	B-MetricValue
%	I-MetricValue
closer	O
precision	B-MetricName
to	O
manual	B-MethodName
evaluation	I-MethodName
than	O
the	O
held	B-MethodName
-	I-MethodName
out	I-MethodName
evaluation	I-MethodName
.	O

2	O
)	O
The	O
huge	O
biases	O
caused	O
by	O
wrongly	O
labeled	O
instances	O
are	O
dramatically	O
alleviated	O
by	O
our	O
method	O
.	O

We	O
can	O
observe	O
that	O
1	O
)	O
The	O
performance	O
biases	O
between	O
manual	B-MethodName
evaluation	I-MethodName
and	O
held	B-MethodName
-	I-MethodName
out	I-MethodName
evaluation	I-MethodName
are	O
too	O
significant	O
to	O
be	O
neglected	O
.	O

In	O
this	O
way	O
,	O
our	O
method	O
gets	O
the	O
distances	O
0.17	O
to	O
the	O
curve	O
of	O
human	B-MethodName
evaluation	I-MethodName
while	O
corresponding	O
distances	O
for	O
held	B-MethodName
-	I-MethodName
out	I-MethodName
evaluation	I-MethodName
is	O
0.72	O
.	O

To	O
measure	O
the	O
distance	O
between	O
two	O
curves	O
,	O
we	O
sample	O
20	O
points	O
equidistant	O
on	O
each	O
curve	O
and	O
calculate	O
the	O
Euclidean	O
distance	O
of	O
the	O
two	O
vectors	O
.	O

The	O
results	O
are	O
shown	O
in	O
Table	O
2	O
:	O
The	O
Precision	O
at	O
top	O
K	O
predictions	O
(	O
%	O
)	O
of	O
PCNN+ATT	B-MethodName
upon	O
held	O
-	O
out	O
evaluation	O
,	O
our	O
method	O
and	O
human	O
evaluation	O
on	O
NYT-10	B-DatasetName
.	O

We	O
evaluate	O
the	O
performance	O
of	O
PCNN+ATT	B-MethodName
with	O
held	B-MethodName
-	I-MethodName
out	I-MethodName
evaluation	I-MethodName
,	O
human	B-MethodName
evaluation	I-MethodName
and	O
our	O
method	O
.	O

Effect	O
of	O
Active	B-MethodName
Testing	I-MethodName
.	O

The	O
batch	B-HyperparameterName
size	I-HyperparameterName
for	O
vetting	O
is	O
20	B-HyperparameterValue
and	O
the	O
vetting	B-HyperparameterName
budget	I-HyperparameterName
is	O
set	O
to	O
100	B-HyperparameterValue
entity	O
pairs	O
.	O

The	O
initial	O
state	O
of	O
vetted	O
set	O
includes	O
all	O
the	O
positive	O
entity	O
pairs	O
of	O
the	O
test	O
set	O
in	O
NYT-10	B-DatasetName
and	O
150	O
vetted	O
negative	O
entity	O
pairs	O
.	O

To	O
be	O
more	O
convincing	O
,	O
we	O
provide	O
the	O
experimental	O
results	O
of	O
BGRU+ATT	B-MethodName
in	O
the	O
appendix	O
.	O

We	O
use	O
PCNN+ATT	B-MethodName
(	O
Lin	O
et	O
al	O
.	O
,	O
2016	O
)	O
as	O
baseline	O
relation	O
extractors	O
.	O

Algorithm	O
1	O
Active	B-MethodName
Testing	I-MethodName
Algorithm	O
.	O

The	O
procedure	O
is	O
described	O
in	O
Algorithm	O
1	O
.	O

When	O
the	O
budget	O
is	O
used	O
up	O
,	O
the	O
vetting	O
stops	O
.	O

In	O
this	O
approach	O
,	O
we	O
take	O
it	O
as	O
a	O
hyper	O
parameter	O
.	O

Therefore	O
,	O
vetting	O
budget	O
is	O
the	O
only	O
factor	O
controlling	O
the	O
vetting	O
procedure	O
.	O

With	O
this	O
vetting	O
strategy	O
,	O
the	O
most	O
valuable	O
data	O
is	O
always	O
selected	O
first	O
.	O

Thus	O
,	O
this	O
vetting	O
strategy	O
is	O
also	O
useful	O
for	O
the	O
PR	O
curve	O
.	O

For	O
the	O
PR	O
curve	O
,	O
every	O
point	O
depends	O
on	O
P	O
@K	O
for	O
different	O
K.	O

The	O
change	O
caused	O
by	O
vetting	O
example	O
i	O
can	O
be	O
written	O
as	O
∆	O
i	O
(	O
z	O
i	O
)	O
=	O
|E	O
p(z	O
|V	O
)	O
Q	O
−	O
E	O
p(z	O
|V	O
,	O
z	O
i	O
)	O
Q|(7	O
)	O
For	O
precision	O
at	O
top	O
K	O
,	O
this	O
expected	O
change	O
can	O
be	O
written	O
as	O
E	O
p(z	O
i	O
|V	O
)	O
[	O
∆	O
i	O
(	O
z	O
i	O
)	O
]	O
=	O
2	O
K	O
p	O
i	O
(	O
1	O
−	O
p	O
i	O
)	O
(	O
8)	O
where	O
p	O
i	O
=	O
P	O
(	O
z	O
i	O
=	O
1|Θ	O
)	O
.	O

After	O
vetting	O
example	O
i	O
and	O
updating	O
that	O
estimator	O
,	O
it	O
will	O
become	O
E	O
p(z	O
|V	O
,	O
z	O
i	O
)	O
Q.	O

Let	O
E	O
p(z	O
|V	O
)	O
Q	O
be	O
the	O
expected	O
performance	O
based	O
on	O
the	O
distribution	O
p(z	O
|V	O
)	O
estimated	O
from	O
current	O
vetted	O
set	O
V	O
.	O

The	O
vetting	O
strategy	O
is	O
to	O
select	O
the	O
sample	O
which	O
can	O
yield	O
a	O
largest	O
expected	O
change	O
of	O
performance	O
estimation	O
.	O

In	O
this	O
work	O
,	O
we	O
apply	O
a	O
strategy	O
based	O
on	O
maximum	O
expected	O
model	O
change(MEMC	O
)	O
(	O
Settles	O
,	O
2009	O
)	O
.	O

Vetting	O
Strategy	O
.	O

For	O
each	O
relation	O
,	O
there	O
is	O
a	O
specific	O
logistic	O
regression	O
function	O
to	O
fit	O
.	O

p(z	O
jk	O
|s	O
jk	O
)	O
is	O
fitted	O
by	O
using	O
logistic	O
regression	O
.	O

Given	O
a	O
few	O
vetted	O
data	O
,	O
we	O
fit	O
p(y	O
jk	O
|z	O
jk	O
)	O
by	O
standard	O
maximum	O
likelihood	O
estimation	O
(	O
counting	O
frequencies	O
)	O
.	O

s	O
jk	O
,	O
y	O
jk	O
,	O
z	O
jk	O
are	O
the	O
corresponding	O
elements	O
of	O
s	O
i	O
,	O
y	O
i	O
,	O
z	O
i	O
before	O
sorting	O
confident	O
score	O
.	O

This	O
posterior	O
probability	O
can	O
be	O
derived	O
as	O
(	O
see	O
appendix	O
for	O
proof	O
)	O
p(z	O
i	O
|y	O
i	O
,	O
s	O
i	O
)	O
=	O
p(y	O
jk	O
|z	O
jk	O
)	O
p(z	O
jk	O
|s	O
jk	O
)	O
v	O
p(y	O
jk	O
|z	O
jk	O
=	O
v)p(z	O
jk	O
=	O
v|s	O
jk	O
)	O
(	O
6	O
)	O
where	O
v	O
∈	O
{	O
0	O
,	O
1	O
}	O
.	O

To	O
predict	O
the	O
true	O
latent	O
label	O
z	O
i	O
for	O
a	O
specific	O
relation	O
,	O
we	O
use	O
noisy	O
label	O
y	O
i	O
and	O
confident	O
score	O
s	O
i	O
.	O

Then	O
,	O
the	O
precision	O
and	O
recall	O
equations	O
can	O
be	O
rewritten	O
as	O
E[P	O
@K	O
]	O
=	O
1	O
K	O
(	O
i∈V	O
K	O
z	O
i	O
+	O
i∈U	O
K	O
p(z	O
i	O
=	O
1|Θ	O
)	O
)	O
(	O
4	O
)	O
E[R@K	O
]	O
=	O
i∈V	O
K	O
z	O
i	O
+	O
i∈U	O
K	O
p(z	O
i	O
=	O
1|Θ	O
)	O
i∈V	O
z	O
i	O
+	O
i∈U	O
p(z	O
i	O
=	O
1|Θ	O
)	O
(	O
5	O
)	O
where	O
U	O
K	O
and	O
V	O
K	O
denote	O
the	O
unvetted	O
and	O
vetted	O
subsets	O
of	O
K	O
highest	O
-	O
scoring	O
examples	O
in	O
the	O
total	O
set	O
U	O
∪	O
V	O
.	O

Given	O
posterior	O
estimates	O
p(z	O
i	O
|Θ	O
)	O
,	O
we	O
can	O
compute	O
the	O
expected	O
performance	O
by	O
replacing	O
the	O
true	O
latent	O
label	O
by	O
its	O
probability	O
.	O

We	O
make	O
the	O
assumption	O
that	O
the	O
distribution	O
of	O
true	O
latent	O
labels	O
is	O
conditioned	O
on	O
Θ.	O

In	O
our	O
work	O
,	O
we	O
estimate	O
the	O
probability	O
as	O
p(z	O
i	O
)	O
=	O
i∈U	O
p(z	O
i	O
|Θ	O
)	O
i∈V	O
δ(z	O
i	O
=	O
z	O
i	O
)	O
(	O
3	O
)	O
where	O
Θ	O
represents	O
all	O
available	O
elements	O
such	O
as	O
confident	O
score	O
,	O
noisy	O
labels	O
and	O
so	O
on	O
.	O

The	O
performance	O
evaluation	O
mainly	O
depends	O
on	O
the	O
estimation	O
of	O
z	O
i	O
.	O

We	O
treat	O
the	O
true	O
label	O
z	O
i	O
as	O
a	O
latent	O
variable	O
and	O
z	O
i	O
is	O
its	O
observed	O
value	O
.	O

Our	O
test	O
set	O
consists	O
of	O
two	O
parts	O
:	O
1	O
)	O
a	O
noisy	O
set	O
U	O
in	O
which	O
we	O
only	O
know	O
automatic	O
label	O
y	O
i	O
;	O
2	O
)	O
a	O
vetted	O
set	O
V	O
in	O
which	O
we	O
know	O
both	O
automatic	O
label	O
y	O
i	O
and	O
manual	O
label	O
z	O
i	O
.	O

Metric	O
Estimator	O
.	O

In	O
summary	O
,	O
our	O
method	O
consists	O
of	O
two	O
key	O
components	O
:	O
a	O
vetting	O
strategy	O
and	O
a	O
metric	O
estimator	O
.	O

After	O
a	O
few	O
vetting	O
-	O
evaluating	O
iterations	O
,	O
unbiased	O
performance	O
of	O
relation	B-TaskName
extraction	I-TaskName
is	O
appropriately	O
evaluated	O
.	O

In	O
each	O
iteration	O
there	O
are	O
two	O
steps	O
:	O
1	O
)	O
select	O
a	O
batch	O
of	O
entity	O
pairs	O
with	O
a	O
customized	O
vetting	O
strategy	O
,	O
label	O
them	O
manually	O
,	O
and	O
add	O
them	O
to	O
the	O
vetted	O
set	O
;	O
2	O
)	O
use	O
a	O
new	O
metric	O
estimator	O
to	O
evaluate	O
existing	O
models	O
by	O
the	O
noisy	O
set	O
and	O
the	O
vetted	O
set	O
jointly	O
.	O

A	O
small	O
random	O
sampled	O
set	O
is	O
vetted	O
in	O
the	O
initial	O
state	O
.	O

In	O
this	O
section	O
,	O
we	O
present	O
the	O
general	O
framework	O
of	O
our	O
method	O
.	O

Methodology	O
.	O

z	O
P	O
}	O
=	O
i≤K	O
z	O
i	O
i≤P	O
z	O
i	O
(	O
2	O
)	O
Held	O
-	O
out	O
evaluation	O
replaces	O
z	O
with	O
y	O
to	O
calculate	O
P	O
@K	O
and	O
R@K	O
,	O
which	O
leads	O
to	O
incorrect	O
results	O
obviously	O
.	O

z	O
P	O
}	O
=	O
1	O
K	O
i≤K	O
z	O
i	O
(	O
1	O
)	O
R@K{z	O
1	O
.	O

In	O
summary	O
,	O
P	O
@K	O
and	O
R@K	O
can	O
be	O
described	O
by	O
the	O
following	O
equations	O
,	O
P	O
@K{z	O
1	O
.	O

,	O
z	O
P	O
}	O
.	O

,	O
y	O
P	O
}	O
and	O
z	O
=	O
{	O
z	O
1	O
,	O
.	O

Automatic	O
labels	O
and	O
true	O
labels	O
are	O
denoted	O
as	O
y	O
=	O
{	O
y	O
1	O
,	O
.	O

s	O
P	O
}	O
where	O
P	O
=	O
N	O
p.	O

To	O
compute	O
both	O
metrics	O
,	O
confident	O
score	O
for	O
all	O
entity	O
pairs	O
are	O
sorted	O
in	O
descending	O
order	O
,	O
which	O
is	O
defined	O
as	O
s	O
=	O
{	O
s	O
1	O
,	O
s	O
2	O
.	O

(	O
PR	O
curve	O
)	O
.	O

3	O
An	O
entity	O
pair	O
may	O
have	O
more	O
than	O
one	O
relations	O
.	O

In	O
widely	O
used	O
held	O
-	O
out	O
evaluation	O
,	O
existing	O
methods	O
observe	O
two	O
key	O
metrics	O
which	O
are	O
precision	O
at	O
top	O
K	O
(	O
P	O
@K	O
)	O
and	O
Precision	O
-	O
Recall	O
curve	O
2	O
Confident	O
scores	O
are	O
estimated	O
probabilities	O
for	O
relations	O
.	O

z	O
ip	O
}	O
respectively	O
represent	O
automatic	O
labels	O
and	O
true	O
labels	O
for	O
entity	O
pair	O
i	O
,	O
where	O
y	O
ij	O
and	O
z	O
ij	O
are	O
both	O
in	O
{	O
0	O
,	O
1	O
}	O
3	O
.	O

y	O
ip	O
}	O
and	O
z	O
i	O
=	O
{	O
z	O
i1	O
,	O
z	O
i2	O
.	O

y	O
i	O
=	O
{	O
y	O
i1	O
,	O
y	O
i2	O
.	O

N	O
}	O
,	O
where	O
p	O
is	O
the	O
number	O
of	O
relations	O
,	O
N	O
is	O
the	O
number	O
of	O
entity	O
pairs	O
,	O
and	O
s	O
ij	O
∈	O
(	O
0	O
,	O
1	O
)	O
.	O

s	O
ip	O
}	O
for	O
entity	O
pair	O
i	O
∈	O
{	O
1	O
.	O

Suppose	O
that	O
a	O
distantly	O
supervised	O
model	O
returns	O
confident	O
score	O
2	O
s	O
i	O
=	O
{	O
s	O
i1	O
,	O
s	O
i2	O
.	O

Researchers	O
train	O
a	O
relation	O
extractor	O
based	O
on	O
bags	O
of	O
sentences	O
and	O
then	O
use	O
it	O
to	O
predict	O
relations	O
of	O
entity	O
pairs	O
.	O

In	O
distant	O
supervision	O
paradigm	O
,	O
all	O
sentences	O
containing	O
the	O
same	O
entity	O
pair	O
constitute	O
a	O
bag	O
.	O

Task	O
Definition	O
.	O

Though	O
human	B-MethodName
evaluation	I-MethodName
can	O
yield	O
accurate	O
evaluation	O
results	O
(	O
Zeng	O
et	O
al	O
.	O
,	O
2015;Alt	O
et	O
al	O
.	O
,	O
2019	O
)	O
,	O
labeling	O
all	O
the	O
instances	O
in	O
the	O
test	O
set	O
is	O
too	O
costly	O
.	O

However	O
,	O
none	O
of	O
the	O
above	O
methods	O
pay	O
attention	O
to	O
the	O
biased	O
and	O
inaccurate	O
test	O
set	O
.	O

Then	O
,	O
to	O
alleviate	O
the	O
influence	O
of	O
wrongly	O
labeled	O
instances	O
in	O
distant	O
supervision	O
,	O
those	O
neural	O
relation	O
extractors	O
integrated	O
techniques	O
such	O
as	O
attention	O
mechanism	O
(	O
Lin	O
et	O
al	O
.	O
,	O
2016;Han	O
et	O
al	O
.	O
,	O
2018;Huang	O
and	O
Du	O
,	O
2019	O
)	O
,	O
generative	O
adversarial	O
nets	O
(	O
Qin	O
et	O
al	O
.	O
,	O
2018a;Li	O
et	O
al	O
.	O
,	O
2019	O
)	O
,	O
and	O
reinforcement	O
learning	O
(	O
Feng	O
et	O
al	O
.	O
,	O
2018;Qin	O
et	O
al	O
.	O
,	O
2018b	O
)	O
.	O

In	O
recent	O
years	O
,	O
neural	O
models	O
were	O
widely	O
used	O
to	O
extract	O
semantic	O
meanings	O
accurately	O
without	O
hand	O
-	O
designed	O
features	O
(	O
Zeng	O
et	O
al	O
.	O
,	O
2015;Lin	O
et	O
al	O
.	O
,	O
2017;Zhang	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

A	O
series	O
of	O
studies	O
have	O
been	O
conducted	O
with	O
human	O
-	O
designed	O
features	O
in	O
distantly	B-TaskName
supervised	I-TaskName
relation	I-TaskName
extraction	I-TaskName
(	O
Riedel	O
et	O
al	O
.	O
,	O
2010;Surdeanu	O
et	O
al	O
.	O
,	O
2012;Takamatsu	O
et	O
al	O
.	O
,	O
2012;Angeli	O
et	O
al	O
.	O
,	O
2014;Han	O
and	O
Sun	O
,	O
2016	O
)	O
.	O

Distant	O
supervision	O
(	O
Mintz	O
et	O
al	O
.	O
,	O
2009	O
)	O
was	O
proposed	O
to	O
deal	O
with	O
large	O
-	O
scale	O
relation	B-TaskName
extraction	I-TaskName
with	O
automatic	O
annotations	O
.	O

Related	O
Work	O
.	O

Experimental	O
results	O
demonstrate	O
that	O
the	O
proposed	O
evaluation	O
method	O
yields	O
approximately	O
unbiased	O
estimations	O
for	O
distantly	B-TaskName
supervised	I-TaskName
relation	I-TaskName
extraction	I-TaskName
.	O

With	O
a	O
few	O
vetting	O
-	O
estimating	O
iterations	O
,	O
evaluation	O
results	O
can	O
be	O
dramatically	O
close	O
to	O
that	O
of	O
human	O
evaluation	O
by	O
using	O
limited	O
vetted	O
data	O
and	O
all	O
noisy	O
data	O
.	O

In	O
the	O
estimating	O
stage	O
,	O
a	O
metric	O
estimator	O
is	O
proposed	O
to	O
obtain	O
a	O
more	O
accurate	O
evaluation	O
.	O

In	O
the	O
vetting	O
stage	O
,	O
we	O
adopt	O
an	O
active	O
strategy	O
to	O
select	O
batches	O
of	O
the	O
most	O
valuable	O
entity	O
pairs	O
from	O
the	O
noisy	O
test	O
set	O
for	O
annotating	O
.	O

In	O
our	O
approach	O
,	O
we	O
design	O
an	O
iterative	O
approach	O
,	O
with	O
two	O
stage	O
per	O
iteration	O
:	O
vetting	O
stage	O
and	O
estimating	O
stage	O
.	O

Active	B-MethodName
testing	I-MethodName
has	O
been	O
proved	O
effective	O
in	O
evaluating	O
vision	O
models	O
with	O
large	O
-	O
scale	O
noisy	O
datasets	O
(	O
Nguyen	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
an	O
active	B-MethodName
testing	I-MethodName
approach	O
to	O
estimate	O
the	O
performance	O
of	O
distantly	O
supervised	O
relation	O
extraction	O
.	O

1	O
Clearly	O
,	O
these	O
mislabeled	O
entity	O
pairs	O
yield	O
biased	O
evaluations	O
and	O
lead	O
to	O
inappropriate	O
optimization	O
for	O
distantly	B-TaskName
supervised	I-TaskName
relation	I-TaskName
extraction	I-TaskName
.	O

From	O
a	O
random	O
sampling	O
,	O
we	O
deduce	O
that	O
about	O
8.75	O
%	O
entity	O
pairs	O
in	O
the	O
test	O
set	O
of	O
NYT-10	B-DatasetName
are	O
misclassified	O
as	O
non	O
-	O
relation	O
.	O

For	O
example	O
,	O
over	O
70	O
%	O
of	O
people	O
included	O
in	O
Freebase	O
have	O
no	O
place	O
of	O
birth	O
(	O
Dong	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

This	O
problem	O
is	O
caused	O
by	O
the	O
incompleteness	O
of	O
existing	O
knowledge	O
bases	O
.	O

A	O
false	O
negative	O
instance	O
is	O
an	O
entity	O
pair	O
labeled	O
as	O
non	O
-	O
relation	O
,	O
even	O
if	O
it	O
has	O
at	O
least	O
one	O
relation	O
in	O
reality	O
.	O

Results	O
are	O
obtained	O
by	O
our	O
implementations	O
.	O

(	O
2016	O
)	O
upon	O
held	B-MethodName
-	I-MethodName
out	I-MethodName
evaluation	I-MethodName
and	O
human	B-MethodName
evaluation	I-MethodName
on	O
NYT-10	B-DatasetName
.	O

Evaluations	O
P@100	B-MetricName
P@200	B-MetricName
P@300	B-MetricName
Held	B-MethodName
-	I-MethodName
out	I-MethodName
Evaluation	I-MethodName
83	B-MetricValue
77	B-MetricValue
69	B-MetricValue
Human	B-MethodName
Evaluation	I-MethodName
93(+10	B-MetricValue
)	O
92.5(+15.5	B-MetricValue
)	O
91(+22	B-MetricValue
)	O
Table	O
1	O
:	O
The	O
Precision	B-MetricName
at	I-MetricName
top	I-MetricName
K	I-MetricName
predictions	I-MetricName
(	O
%	O
)	O
of	O
the	O
model	O
Lin	O
et	O
al	O
.	O

The	O
biases	O
between	O
human	B-MethodName
evaluation	I-MethodName
and	O
existing	O
held	B-MethodName
-	I-MethodName
out	I-MethodName
evaluation	I-MethodName
are	O
over	O
10	O
%	O
,	O
which	O
are	O
mainly	O
caused	O
by	O
wrongly	O
labeled	O
instances	O
in	O
the	O
test	O
set	O
,	O
especially	O
false	O
negative	O
instances	O
.	O

As	O
shown	O
in	O
Table	O
1	O
,	O
we	O
compare	O
the	O
results	O
of	O
held	B-MethodName
-	I-MethodName
out	I-MethodName
evaluation	I-MethodName
and	O
human	B-MethodName
evaluation	I-MethodName
for	O
the	O
same	O
model	O
on	O
a	O
widely	O
used	O
*	O
Corresponding	O
author	O
:	O
jiawj@bnu.edu.cn	O
.	O

benchmark	O
dataset	O
NYT-10	B-DatasetName
(	O
Riedel	O
et	O
al	O
.	O
,	O
2010	O
)	O
.	O

Most	O
of	O
them	O
estimate	O
their	O
performance	O
with	O
the	O
held	O
-	O
out	O
evaluation	O
on	O
noisy	O
test	O
sets	O
,	O
which	O
will	O
yield	O
inaccurate	O
evaluations	O
of	O
existing	O
models	O
and	O
seriously	O
mislead	O
the	O
model	O
optimization	O
.	O

However	O
,	O
previous	O
works	O
only	O
focus	O
on	O
wrongly	O
labeled	O
instances	O
in	O
training	O
sets	O
but	O
neglect	O
those	O
in	O
test	O
sets	O
.	O

Clearly	O
,	O
the	O
automatically	O
labeled	O
datasets	O
in	O
distant	O
supervision	O
contain	O
amounts	O
of	O
sentences	O
with	O
wrong	O
relation	O
labels	O
.	O

It	O
assumes	O
that	O
if	O
a	O
pair	O
of	O
entities	O
have	O
a	O
known	O
relation	O
in	O
a	O
knowledge	O
base	O
,	O
all	O
sentences	O
with	O
these	O
two	O
entities	O
may	O
express	O
the	O
same	O
relation	O
.	O

To	O
break	O
the	O
bottleneck	O
of	O
manual	O
labeling	O
,	O
distant	O
supervision	O
(	O
Mintz	O
et	O
al	O
.	O
,	O
2009	O
)	O
automatically	O
labels	O
raw	O
text	O
with	O
knowledge	O
bases	O
.	O

It	O
has	O
been	O
thoroughly	O
researched	O
by	O
supervised	O
methods	O
with	O
hand	O
-	O
labeled	O
data	O
.	O

Relation	B-TaskName
extraction	I-TaskName
aims	O
to	O
identify	O
relations	O
between	O
a	O
pair	O
of	O
entities	O
in	O
a	O
sentence	O
.	O

Introduction	O
.	O

Experiments	O
on	O
a	O
widely	O
used	O
benchmark	O
show	O
that	O
our	O
proposed	O
approach	O
can	O
yield	O
approximately	O
unbiased	O
evaluations	O
for	O
distantly	O
supervised	O
relation	O
extractors	O
.	O

To	O
mitigate	O
this	O
problem	O
,	O
we	O
propose	O
a	O
novel	O
evaluation	O
method	O
named	O
active	B-MethodName
testing	I-MethodName
through	O
utilizing	O
both	O
the	O
noisy	O
test	O
set	O
and	O
a	O
few	O
manual	O
annotations	O
.	O

These	O
biases	O
not	O
only	O
result	O
in	O
unfair	O
evaluations	O
but	O
also	O
mislead	O
the	O
optimization	O
of	O
neural	O
relation	B-TaskName
extraction	I-TaskName
.	O

However	O
,	O
existing	O
works	O
on	O
distantly	B-TaskName
supervised	I-TaskName
relation	I-TaskName
extraction	I-TaskName
suffer	O
from	O
the	O
low	O
quality	O
of	O
test	O
set	O
,	O
which	O
leads	O
to	O
considerable	O
biased	O
performance	O
evaluation	O
.	O

Distant	O
supervision	O
has	O
been	O
a	O
widely	O
used	O
method	O
for	O
neural	O
relation	B-TaskName
extraction	I-TaskName
for	O
its	O
convenience	O
of	O
automatically	O
labeling	O
datasets	O
.	O

Active	B-MethodName
Testing	I-MethodName
:	O
An	O
Unbiased	O
Evaluation	O
Method	O
for	O
Distantly	B-TaskName
Supervised	I-TaskName
Relation	I-TaskName
Extraction	I-TaskName
.	O

Require	O
:	O
unvetted	O
set	O
U	O
,	O
vetted	O
set	O
V	O
,	O
vetting	O
budget	O
T	O
,	O
vetting	O
strategy	O
VS	O
,	O
confident	O
score	O
S	O
,	O
estimator	O
p(z	O
)	O
1	O
:	O
while	O
T	O
>	O
0	O
do	O
Initialization	O
.	O

a	O
black	O
dog	O
plays	O
in	O
the	O
water	O
.	O

a	O
black	O
dog	O
plays	O
around	O
in	O
water	O
looking	O
for	O
fish	O
.	O

Fact	O
→	O
Hum	O
a	O
black	O
dog	O
plays	O
around	O
in	O
water	O
.	O

three	O
kids	O
play	O
on	O
a	O
wall	O
with	O
a	O
green	O
ball	O
fighting	O
for	O
supremacy	O
.	O

three	O
kids	O
on	O
a	O
bar	O
on	O
a	O
field	O
of	O
a	O
date	O
.	O

Fact	O
→	O
Hum	O
three	O
kids	O
play	O
on	O
a	O
wall	O
with	O
a	O
green	O
ball	O
.	O

two	O
dogs	O
play	O
with	O
a	O
tennis	O
ball	O
in	O
the	O
snow	O
celebrating	O
their	O
friendship	O
.	O

two	O
dogs	O
play	O
with	O
a	O
tennis	O
ball	O
in	O
the	O
snow	O
.	O

Fact	O
→	O
Rom	O
two	O
dogs	O
play	O
with	O
a	O
tennis	O
ball	O
in	O
the	O
snow	O
.	O

-i	O
think	O
this	O
would	O
be	O
a	O
good	O
idea	O
if	O
you	O
could	O
not	O
be	O
a	O
statement	O
that	O
harry	O
's	O
signed	O
in	O
one	O
of	O
the	O
schedule	O
.	O

Non	O
-	O
polite	O
Input	O
DRG	O
Our	O
Model	O
jon	O
--please	O
use	O
this	O
resignation	O
letter	O
in	O
lieu	O
of	O
the	O
one	O
sent	O
on	O
friday	O
.	O

We	O
would	O
like	O
to	O
thank	O
Antonis	O
Anastasopoulos	O
,	O
Ritam	O
Dutt	O
,	O
Sopan	O
Khosla	O
,	O
and	O
,	O
Xinyi	O
Wang	O
for	O
the	O
helpful	O
discussions	O
.	O

We	O
would	O
also	O
like	O
to	O
acknowledge	O
NVIDIA	O
's	O
GPU	O
support	O
.	O

This	O
work	O
was	O
also	O
supported	O
in	O
part	O
by	O
ONR	O
Grant	O
N000141812861	O
,	O
NSF	O
IIS1763562	O
,	O
and	O
Apple	O
.	O

The	O
views	O
and	O
conclusions	O
contained	O
herein	O
are	O
those	O
of	O
the	O
authors	O
and	O
should	O
not	O
be	O
interpreted	O
as	O
necessarily	O
representing	O
the	O
official	O
policies	O
or	O
endorsements	O
,	O
either	O
expressed	O
or	O
implied	O
,	O
of	O
the	O
Air	O
Force	O
Research	O
Laboratory	O
or	O
the	O
U.S.	O
Government	O
.	O

The	O
U.S.	O
Government	O
is	O
authorized	O
to	O
reproduce	O
and	O
distribute	O
reprints	O
for	O
Governmental	O
purposes	O
notwithstanding	O
any	O
copyright	O
notation	O
thereon	O
.	O

This	O
material	O
is	O
based	O
on	O
research	O
sponsored	O
in	O
part	O
by	O
the	O
Air	O
Force	O
Research	O
Laboratory	O
under	O
agreement	O
number	O
FA8750	O
-	O
19	O
-	O
2	O
-	O
0200	O
.	O

Acknowledgments	O
.	O

Automatic	O
and	O
human	O
evaluation	O
shows	O
that	O
our	O
approach	O
outperforms	O
other	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
on	O
content	B-MetricName
preservation	I-MetricName
metrics	O
while	O
retaining	O
(	O
or	O
in	O
some	O
cases	O
improving	O
)	O
the	O
transfer	O
accuracies	B-MetricName
.	O

We	O
believe	O
our	O
approach	O
is	O
the	O
first	O
to	O
be	O
robust	O
in	O
cases	O
when	O
the	O
source	O
is	O
style	O
neutral	O
,	O
like	O
the	O
"	O
non	O
-	O
polite	O
"	O
class	O
in	O
the	O
case	O
of	O
politeness	B-TaskName
transfer	I-TaskName
.	O

We	O
extend	O
prior	O
works	O
(	O
Li	O
et	O
al	O
.	O
,	O
2018;Sudhakar	O
et	O
al	O
.	O
,	O
2019	O
)	O
on	O
attribute	B-TaskName
transfer	I-TaskName
by	O
introducing	O
a	O
simple	O
pipeline	O
-tag	B-MethodName
&	I-MethodName
generate	I-MethodName
which	O
is	O
an	O
interpretable	O
two	O
-	O
staged	O
approach	O
for	O
content	O
preserving	O
style	B-TaskName
transfer	I-TaskName
.	O

We	O
introduce	O
the	O
task	O
of	O
politeness	B-TaskName
transfer	I-TaskName
for	O
which	O
we	O
provide	O
a	O
dataset	O
comprised	O
of	O
sentences	O
curated	O
from	O
email	O
exchanges	O
present	O
in	O
the	O
Enron	O
corpus	O
.	O

We	O
conclude	O
that	O
the	O
choice	O
of	O
the	O
tagger	O
variant	O
is	O
dependent	O
on	O
the	O
characterstics	O
of	O
the	O
underlying	O
transfer	B-TaskName
task	I-TaskName
.	O

While	O
the	O
combined	O
tagger	O
learns	O
to	O
use	O
the	O
optimal	O
tagging	O
operation	O
to	O
some	O
extent	O
,	O
a	O
deeper	O
understanding	O
of	O
this	O
phenomenon	O
is	O
an	O
interesting	O
future	O
topic	O
for	O
research	O
.	O

In	O
contrast	O
,	O
on	O
the	O
CAPTIONS	B-DatasetName
dataset	O
,	O
it	O
performs	O
50	O
%	O
more	O
add	O
operations	O
.	O

We	O
find	O
that	O
for	O
Yelp	B-DatasetName
(	O
a	O
polar	O
dataset	O
)	O
the	O
combined	O
tagger	O
performs	O
20	O
%	O
more	O
replace	O
operations	O
(	O
as	O
compared	O
to	O
add	O
operations	O
)	O
.	O

To	O
check	O
if	O
the	O
combined	O
tagger	O
is	O
learning	O
to	O
perform	O
the	O
operation	O
that	O
is	O
more	O
suitable	O
for	O
a	O
dataset	O
,	O
we	O
calculate	O
the	O
fraction	O
of	O
times	O
the	O
combined	O
tagger	O
performs	O
add	O
/	O
replace	O
operations	O
on	O
the	O
Yelp	B-DatasetName
and	O
Captions	B-DatasetName
datasets	O
.	O

Thus	O
,	O
we	O
can	O
use	O
the	O
add	O
-	O
tagger	O
variant	O
for	O
transfer	O
from	O
a	O
polarized	O
class	O
to	O
a	O
neutral	O
class	O
as	O
well	O
.	O

Interestingly	O
,	O
the	O
accuracy	B-MetricName
of	O
the	O
add	O
-	O
tagger	O
is	O
≈	O
50	O
%	O
in	O
the	O
case	O
of	O
Yelp	O
,	O
since	O
adding	O
negative	O
words	O
to	O
a	O
positive	O
sentence	O
or	O
vice	O
-	O
versa	O
neutralizes	O
the	O
classifier	O
scores	O
.	O

On	O
the	O
contrary	O
,	O
for	O
Yelp	B-DatasetName
,	O
where	O
both	O
polarities	O
are	O
clearly	O
defined	O
,	O
the	O
replace	O
-	O
tagger	O
gives	O
the	O
best	O
performance	O
.	O

8	O
We	O
train	O
these	O
tagger	O
variants	O
on	O
the	O
Yelp	B-DatasetName
and	O
Captions	B-DatasetName
datasets	O
and	O
present	O
the	O
results	O
in	O
tagger	O
provides	O
the	O
best	O
accuracy	B-MetricName
with	O
a	O
relatively	O
negligible	O
drop	O
in	O
BLEU	B-MetricName
scores	O
.	O

We	O
also	O
train	O
and	O
compare	O
them	O
with	O
a	O
combined	O
variant	O
.	O

7	O
Ablations	O
We	O
provide	O
a	O
comparison	O
of	O
the	O
two	O
variants	O
of	O
the	O
tagger	O
,	O
namely	O
the	O
replace	O
-	O
tagger	O
and	O
add	O
-	O
tagger	O
on	O
two	O
datasets	O
.	O

Our	O
model	O
follows	O
the	O
strategies	O
prescribed	O
in	O
(	O
Danescu	O
-	O
Niculescu	O
-	O
Mizil	O
et	O
al	O
.	O
,	O
2013	O
)	O
while	O
generating	O
polite	O
sentences	O
.	O

The	O
fourth	O
sentence	O
projects	O
the	O
case	O
where	O
our	O
model	O
uses	O
"	O
thanks	O
"	O
at	O
the	O
end	O
to	O
express	O
gratitude	O
and	O
in	O
turn	O
,	O
makes	O
the	O
sentence	O
more	O
polite	O
.	O

According	O
to	O
the	O
Please	O
Start	O
strategy	O
,	O
it	O
is	O
more	O
direct	O
and	O
insincere	O
to	O
start	O
a	O
sentence	O
with	O
"	O
Please	O
"	O
.	O

The	O
third	O
sentence	O
highlights	O
the	O
ability	O
of	O
the	O
model	O
to	O
add	O
Apologizing	O
words	O
like	O
"	O
Sorry	O
"	O
which	O
helps	O
in	O
deflecting	O
the	O
social	O
threat	O
of	O
the	O
request	O
by	O
attuning	O
to	O
the	O
imposition	O
.	O

The	O
second	O
sentence	O
highlights	O
another	O
subtle	O
concept	O
of	O
politeness	O
of	O
1st	O
Person	O
Plural	O
where	O
adding	O
"	O
we	O
"	O
helps	O
being	O
indirect	O
and	O
creates	O
the	O
sense	O
that	O
the	O
burden	O
of	O
the	O
request	O
is	O
shared	O
between	O
speaker	O
and	O
addressee	O
.	O

The	O
first	O
sentence	O
presents	O
a	O
simple	O
example	O
of	O
the	O
counterfactual	O
modal	O
strategy	O
inducing	O
"	O
Could	O
you	O
please	O
"	O
to	O
make	O
the	O
sentence	O
polite	O
.	O

Our	O
analysis	O
is	O
based	O
on	O
the	O
linguistic	O
strategies	O
for	O
politeness	O
as	O
described	O
in	O
(	O
Danescu	O
-	O
Niculescu	O
-	O
Mizil	O
et	O
al	O
.	O
,	O
2013	O
)	O
.	O

Qualitative	O
Analysis	O
We	O
compare	O
the	O
results	O
of	O
our	O
model	O
with	O
the	O
DRG	B-MethodName
model	O
qualitatively	O
as	O
shown	O
in	O
Table	O
4	O
.	O

Alongside	O
,	O
we	O
also	O
observe	O
consistent	O
improvements	O
of	O
our	O
model	O
on	O
target	B-MetricName
attribute	I-MetricName
matching	I-MetricName
and	O
grammatical	B-MetricName
correctness	I-MetricName
.	O

We	O
observe	O
a	O
significant	O
improvement	O
in	O
content	B-MetricName
preservation	I-MetricName
scores	O
across	O
various	O
datasets	O
(	O
specifically	O
in	O
Politeness	B-DatasetName
domain	O
)	O
highlighting	O
the	O
ability	O
of	O
our	O
model	O
to	O
retain	O
content	O
better	O
than	O
DRG	B-MethodName
.	O

Table	O
3	O
shows	O
the	O
results	O
of	O
human	O
evaluations	O
.	O

Overall	O
,	O
we	O
evaluate	O
both	O
systems	O
on	O
a	O
total	O
of	O
200	O
samples	O
for	O
Politeness	B-DatasetName
and	O
100	O
samples	O
each	O
for	O
Yelp	B-DatasetName
,	O
Gender	B-DatasetName
and	O
Political	B-DatasetName
.	O

(	O
2018	O
)	O
for	O
our	O
human	O
study	O
.	O

We	O
've	O
used	O
the	O
same	O
instructions	O
from	O
Li	O
et	O
al	O
.	O

Instead	O
we	O
rely	O
on	O
the	O
classifier	O
scores	O
for	O
the	O
transfer	O
.	O

Since	O
the	O
judgement	O
of	O
signals	O
that	O
indicate	O
gender	O
and	O
political	O
inclination	O
are	O
prone	O
to	O
personal	O
biases	O
,	O
we	O
do	O
n't	O
annotate	O
these	O
tasks	O
for	O
target	O
attribute	O
match	O
metric	O
.	O

For	O
each	O
of	O
these	O
metrics	O
,	O
the	O
reviewers	O
give	O
a	O
score	O
between	O
1	O
-	O
5	O
to	O
each	O
of	O
the	O
outputs	O
,	O
where	O
1	O
reflects	O
a	O
poor	O
performance	O
on	O
the	O
task	O
and	O
5	O
means	O
a	O
perfect	O
output	O
.	O

(	O
2018	O
)	O
,	O
we	O
select	O
10	O
unbiased	O
human	O
judges	O
to	O
rate	O
the	O
output	O
of	O
our	O
model	O
and	O
DRG	B-MethodName
on	O
three	O
aspects	O
:	O
(	O
1	O
)	O
content	B-MetricName
preservation	I-MetricName
(	O
Con	O
)	O
(	O
2	O
)	O
grammaticality	B-MetricName
of	O
the	O
generated	O
content	O
(	O
Gra	O
)	O
(	O
3	O
)	O
target	B-MetricName
attribute	I-MetricName
match	I-MetricName
of	O
the	O
generations	O
(	O
Att	O
)	O
.	O

Human	O
Evaluation	O
Following	O
Li	O
et	O
al	O
.	O

In	O
some	O
of	O
these	O
cases	O
,	O
we	O
noticed	O
that	O
changing	O
non	O
-	O
tagged	O
words	O
helped	O
in	O
producing	O
outputs	O
that	O
were	O
more	O
natural	O
and	O
fluent	O
.	O

We	O
found	O
that	O
the	O
non	O
-	O
tagged	O
words	O
were	O
changed	O
for	O
only	O
6.9	O
%	O
of	O
the	O
sentences	O
.	O

In	O
order	O
to	O
quantify	O
this	O
,	O
we	O
calculate	O
the	O
fraction	O
of	O
non	O
-	O
tagged	O
words	O
being	O
changed	O
across	O
the	O
datasets	O
.	O

Clearly	O
,	O
replacing	O
content	O
words	O
is	O
not	O
desired	O
since	O
it	O
may	O
drastically	O
change	O
the	O
meaning	O
.	O

Changing	O
Content	O
Words	O
Given	O
that	O
our	O
model	O
is	O
explicitly	O
trained	O
to	O
generate	O
new	O
content	O
only	O
in	O
place	O
of	O
the	O
TAG	O
token	O
,	O
it	O
is	O
expected	O
that	O
a	O
welltrained	O
system	O
will	O
retain	O
most	O
of	O
the	O
non	O
-	O
tagged	O
(	O
content	O
)	O
words	O
.	O

In	O
summary	O
,	O
evaluation	O
via	O
automatic	O
metrics	O
might	O
not	O
truly	O
correlate	O
with	O
task	O
success	O
.	O

Despite	O
high	O
evaluation	O
scores	O
,	O
it	O
does	O
not	O
reflect	O
a	O
high	O
rate	O
of	O
success	O
on	O
the	O
task	O
.	O

This	O
baseline	O
achieves	O
an	O
average	O
accuracy	B-MetricName
score	O
of	O
91.3	B-MetricValue
%	I-MetricValue
and	O
a	O
BLEU	B-MetricName
score	O
of	O
61.44	B-MetricValue
on	O
the	O
Yelp	B-DatasetName
dataset	O
.	O

Similarly	O
,	O
it	O
appends	O
"	O
but	O
overall	O
it	O
was	O
perfect	O
"	O
for	O
transfer	O
into	O
a	O
positive	O
sentiment	O
.	O

This	O
baseline	O
adds	O
"	O
but	O
overall	O
it	O
sucked	O
"	O
at	O
the	O
end	O
of	O
the	O
sentence	O
to	O
transfer	O
it	O
to	O
negative	O
sentiment	O
.	O

We	O
test	O
this	O
hypothesis	O
on	O
the	O
sentiment	B-TaskName
transfer	I-TaskName
task	O
by	O
a	O
Naive	B-MethodName
Baseline	I-MethodName
.	O

BLEU	B-MetricName
relies	O
heavily	O
on	O
n	O
-	O
gram	O
overlap	O
and	O
classifiers	O
can	O
be	O
fooled	O
by	O
certain	O
polarizing	O
keywords	O
.	O

While	O
popular	O
,	O
the	O
metrics	O
of	O
transfer	O
accuracy	B-MetricName
and	O
BLEU	B-MetricName
have	O
significant	O
shortcomings	O
making	O
them	O
susceptible	O
to	O
simple	O
adversaries	O
.	O

Since	O
we	O
do	O
n't	O
make	O
any	O
such	O
assumptions	O
,	O
we	O
perform	O
significantly	O
better	O
on	O
this	O
dataset	O
.	O

Hence	O
,	O
the	O
performance	O
of	O
their	O
model	O
is	O
worse	O
in	O
this	O
case	O
.	O

(	O
2018	O
)	O
,	O
one	O
of	O
the	O
unique	O
aspects	O
of	O
the	O
Amazon	B-DatasetName
dataset	O
is	O
the	O
absence	O
of	O
similar	O
content	O
in	O
both	O
the	O
sentiment	O
polarities	O
.	O

As	O
noted	O
by	O
Li	O
et	O
al	O
.	O

Additionally	O
,	O
we	O
improve	O
the	O
transfer	O
accuracy	B-MetricName
for	O
Amazon	O
by	O
14.2	O
%	O
while	O
achieving	O
accuracies	O
similar	O
to	O
DRG	B-MethodName
on	O
Yelp	B-DatasetName
and	O
Captions	B-DatasetName
.	O

We	O
observe	O
an	O
increase	O
in	O
the	O
BLEU	B-MetricName
-	I-MetricName
reference	I-MetricName
scores	O
by	O
5.25	B-MetricValue
,	O
4.95	B-MetricValue
and	O
3.64	B-MetricValue
on	O
the	O
Yelp	B-DatasetName
,	O
Amazon	B-DatasetName
,	O
and	O
Captions	B-DatasetName
test	O
sets	O
respectively	O
.	O

(	O
2018	O
)	O
.	O

For	O
each	O
of	O
the	O
datasets	O
our	O
test	O
set	O
comprises	O
500	O
samples	O
(	O
with	O
human	O
references	O
)	O
curated	O
by	O
Li	O
et	O
al	O
.	O

In	O
Table	O
2	O
,	O
we	O
compare	O
our	O
model	O
against	O
CAE	B-MethodName
and	O
DRG	B-MethodName
on	O
the	O
Yelp	B-DatasetName
,	O
Amazon	B-DatasetName
,	O
and	O
Captions	B-DatasetName
datasets	O
.	O

The	O
classifier	O
accuracy	B-MetricName
on	O
the	O
generations	O
of	O
our	O
model	O
are	O
comparable	O
(	O
within	O
1	B-MetricValue
%	I-MetricValue
)	O
with	O
that	O
of	O
DRG	B-MethodName
for	O
the	O
Politeness	B-DatasetName
dataset	O
.	O

In	O
general	O
,	O
CAE	B-MethodName
and	O
BST	B-MethodName
achieve	O
high	O
classifier	O
accuracies	B-MetricName
but	O
they	O
fail	O
to	O
retain	O
the	O
original	O
content	O
.	O

The	O
BLEU	B-MetricName
score	O
on	O
the	O
Politeness	B-TaskName
task	I-TaskName
is	O
greater	O
by	O
58.61	B-MetricValue
points	O
with	O
respect	O
to	O
DRG	B-MethodName
.	O

Table	O
1	O
shows	O
that	O
our	O
model	O
achieves	O
significantly	O
higher	O
scores	O
on	O
BLEU	B-MetricName
,	O
ROUGE	B-MetricName
and	O
METEOR	B-MetricName
as	O
compared	O
to	O
the	O
baselines	O
DRG	B-MethodName
,	O
CAE	B-MethodName
and	O
BST	B-MethodName
on	O
the	O
Politeness	B-DatasetName
,	O
Gender	B-DatasetName
and	O
Political	B-DatasetName
datasets	O
.	O

In	O
particular	O
,	O
METEOR	B-MetricName
also	O
uses	O
synonyms	O
and	O
stemmed	O
forms	O
of	O
the	O
words	O
in	O
candidate	O
and	O
reference	O
sentences	O
,	O
and	O
thus	O
may	O
be	O
better	O
at	O
quantifying	O
semantic	O
similarities	O
.	O

We	O
also	O
report	O
ROUGE	B-MetricName
(	O
ROU	B-MetricName
)	O
(	O
Lin	O
,	O
2004	O
)	O
and	O
METEOR	B-MetricName
(	O
MET	B-MetricName
)	O
(	O
Denkowski	O
and	O
Lavie	O
,	O
2011	O
)	O
scores	O
.	O

Additionally	O
,	O
we	O
report	O
the	O
BLEU	B-MetricName
-	I-MetricName
reference	I-MetricName
(	O
BL	B-DatasetName
-	I-DatasetName
r	I-DatasetName
)	O
scores	O
using	O
the	O
human	O
reference	O
sentences	O
on	O
the	O
Yelp	B-DatasetName
,	O
Amazon	B-DatasetName
and	O
Captions	B-DatasetName
datasets	O
(	O
Li	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

The	O
standard	O
metric	O
for	O
measuring	O
content	O
preservation	O
is	O
BLEU	B-MetricName
-	I-MetricName
self	I-MetricName
(	O
BL	B-MetricName
-	I-MetricName
s	I-MetricName
)	O
(	O
Papineni	O
et	O
al	O
.	O
,	O
2002	O
)	O
which	O
is	O
computed	O
with	O
respect	O
to	O
the	O
original	O
sentences	O
.	O

6	O
The	O
metric	O
of	O
transfer	O
accuracy	B-MetricName
(	O
Acc	O
)	O
is	O
defined	O
as	O
the	O
percentage	O
of	O
generated	O
sentences	O
classified	O
to	O
be	O
in	O
the	O
target	O
domain	O
by	O
the	O
classifier	O
.	O

5	O
For	O
politeness	B-TaskName
,	O
we	O
use	O
the	O
classifier	O
trained	O
by	O
(	O
Niu	O
and	O
Bansal	O
,	O
2018	O
)	O
.	O

We	O
use	O
the	O
implementation	O
provided	O
by	O
fastai	O
.	O

The	O
architecture	O
of	O
the	O
classifier	O
is	O
based	O
on	O
AWD	B-MethodName
-	I-MethodName
LSTM	I-MethodName
(	O
Merity	O
et	O
al	O
.	O
,	O
2017	O
)	O
and	O
a	O
softmax	B-HyperparameterValue
layer	B-HyperparameterName
trained	O
via	O
cross	O
-	O
entropy	O
loss	O
.	O

To	O
capture	O
accuracy	B-MetricName
,	O
we	O
use	O
a	O
classifier	O
trained	O
on	O
the	O
nonparallel	O
style	O
corpora	O
for	O
the	O
respective	O
datasets	O
(	O
barring	O
politeness	O
)	O
.	O

Automated	O
Evaluation	O
Following	O
prior	O
work	O
(	O
Li	O
et	O
al	O
.	O
,	O
2018;Shen	O
et	O
al	O
.	O
,	O
2017	O
)	O
,	O
we	O
use	O
automatic	O
metrics	O
for	O
evaluation	O
of	O
the	O
models	O
along	O
two	O
major	O
dimensions	O
:	O
(	O
1	O
)	O
style	O
transfer	O
accuracy	B-MetricName
and	O
(	O
2	O
)	O
content	B-MetricName
preservation	I-MetricName
.	O

For	O
the	O
tagger	O
,	O
we	O
re	O
-	O
rank	O
the	O
final	O
beam	O
search	O
outputs	O
based	O
on	O
the	O
number	O
of	O
[	O
TAG	O
]	O
tokens	O
in	O
the	O
output	O
sequence	O
(	O
favoring	O
more	O
[	O
TAG	O
]	O
tokens	O
)	O
.	O

During	O
inference	O
we	O
use	O
beam	O
search	O
(	O
beam	B-HyperparameterName
size=5	I-HyperparameterName
)	O
to	O
decode	O
tagged	O
sentences	O
and	O
targeted	O
generations	O
for	O
tagger	O
&	O
generator	O
respectively	O
.	O

For	O
Yelp	O
k	B-HyperparameterName
is	O
set	O
to	O
0.97	B-HyperparameterValue
.	O

For	O
all	O
datasets	O
except	O
Yelp	B-DatasetName
we	O
use	O
phrases	O
with	O
p	O
2	O
1	O
(	O
w	O
)	O
≥	O
k	O
=	O
0.9	O
to	O
construct	O
Γ	O
2	O
,	O
Γ	O
1	O
(	O
§	O
4.1	O
)	O
.	O

The	O
value	O
of	O
the	O
smoothing	B-HyperparameterName
parameter	I-HyperparameterName
γ	B-HyperparameterName
in	O
Eq	O
.	O
2	O
is	O
set	O
to	O
0.75	B-HyperparameterValue
.	O

Both	O
modules	O
were	O
also	O
trained	O
with	O
the	O
BPE	O
tokenization	O
(	O
Sennrich	O
et	O
al	O
.	O
,	O
2015	O
)	O
using	O
a	O
vocabulary	O
of	O
size	O
16000	O
for	O
all	O
the	O
datasets	O
except	O
for	O
Captions	B-DatasetName
,	O
which	O
was	O
trained	O
using	O
4000	O
BPE	O
tokens	O
.	O

We	O
empirically	O
observed	O
that	O
these	O
techniques	O
provide	O
an	O
improvement	O
in	O
the	O
fluency	O
and	O
diversity	O
of	O
the	O
generations	O
.	O

For	O
the	O
politeness	O
dataset	O
the	O
generator	O
module	O
is	O
trained	O
with	O
data	O
augmentation	O
techniques	O
like	O
random	O
word	O
shuffle	O
,	O
word	O
drops	O
/	O
replacements	O
as	O
proposed	O
by	O
(	O
I	O
m	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

Dropout	B-HyperparameterName
(	O
Srivastava	O
et	O
al	O
.	O
,	O
2014	O
)	O
with	O
p	B-HyperparameterName
-	I-HyperparameterName
value	I-HyperparameterName
0.3	B-HyperparameterValue
is	O
added	O
for	O
each	O
layer	O
in	O
the	O
transformer	O
.	O

Each	O
transformer	O
has	O
4	B-HyperparameterValue
attention	B-HyperparameterName
heads	I-HyperparameterName
with	O
a	O
512	B-HyperparameterValue
dimensional	I-HyperparameterValue
embedding	B-HyperparameterName
layer	I-HyperparameterName
and	O
hidden	B-HyperparameterName
state	I-HyperparameterName
size	I-HyperparameterName
.	O

Implementation	O
Details	O
We	O
use	O
4	B-HyperparameterValue
-	O
layered	B-HyperparameterName
transformers	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O
2017	O
)	O
to	O
train	O
both	O
tagger	O
and	O
generator	O
modules	O
.	O

DRG	B-MethodName
(	O
Li	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
Style	B-MethodName
Transfer	I-MethodName
Through	I-MethodName
Back	I-MethodName
-	I-MethodName
translation	I-MethodName
(	O
BST	B-MethodName
)	O
(	O
Prabhumoye	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
and	O
Style	B-MethodName
transfer	I-MethodName
from	I-MethodName
nonparallel	I-MethodName
text	I-MethodName
by	I-MethodName
cross	I-MethodName
alignment	I-MethodName
(	O
Shen	O
et	O
al	O
.	O
,	O
2017	O
)	O
(	O
CAE	B-MethodName
)	O
.	O

Baselines	O
We	O
compare	O
our	O
systems	O
against	O
three	O
previous	O
methods	O
.	O

Experiments	O
and	O
Results	O
.	O

For	O
example	O
,	O
in	O
the	O
case	O
of	O
politeness	B-TaskName
transfer	I-TaskName
,	O
the	O
tags	O
added	O
at	O
the	O
beginning	O
(	O
t	O
=	O
0	O
)	O
will	O
almost	O
always	O
be	O
used	O
to	O
generate	O
a	O
token	O
like	O
"	O
Would	O
it	O
be	O
possible	O
...	O
"	O
whereas	O
for	O
a	O
higher	O
t	O
,	O
[	O
TAG	O
]	O
t	O
may	O
be	O
replaced	O
with	O
a	O
token	O
like	O
"	O
thanks	O
"	O
or	O
"	O
sorry	O
.	O
"	O
x	O
(	O
v	O
)	O
i	O
∈	O
X	O
v	O
,	O
v	O
∈	O
{	O
1	O
,	O
.	O

By	O
training	O
both	O
tagger	O
and	O
generator	O
with	O
these	O
positional	O
[	O
TAG	O
]	O
t	O
tokens	O
we	O
enable	O
them	O
to	O
easily	O
realize	O
different	O
distributions	O
of	O
style	O
attributes	O
for	O
different	O
positions	O
in	O
a	O
sentence	O
.	O

T	O
}	O
for	O
a	O
sentence	O
of	O
length	O
T	O
.	O

Hence	O
,	O
instead	O
of	O
using	O
a	O
single	O
[	O
TAG	O
]	O
token	O
,	O
we	O
use	O
a	O
set	O
of	O
positional	O
tokens	O
[	O
TAG	O
]	O
t	O
where	O
t	O
∈	O
{	O
0	O
,	O
1	O
,	O
.	O

L(θ	O
g	O
)	O
=	O
−	O
|Xv|	O
i=1	O
log	O
P	O
θg	O
(	O
x	O
(	O
v	O
)	O
i	O
|z(x	O
i	O
)	O
;	O
θ	O
g	O
)	O
(	O
5	O
)	O
The	O
training	O
data	O
for	O
transfer	O
into	O
style	O
S	O
v	O
comprises	O
of	O
pairs	O
where	O
the	O
input	O
is	O
given	O
by	O
{	O
z(x	O
i	O
)	O
:	O
Finally	O
,	O
we	O
note	O
that	O
the	O
location	O
at	O
which	O
the	O
tags	O
are	O
generated	O
has	O
a	O
significant	O
impact	O
on	O
the	O
distribution	O
over	O
style	O
attributes	O
(	O
in	O
Γ	O
2	O
)	O
that	O
are	O
used	O
to	O
fill	O
the	O
[	O
TAG	O
]	O
token	O
at	O
a	O
particular	O
position	O
.	O

The	O
training	O
for	O
the	O
generator	O
model	O
is	O
complimentary	O
to	O
that	O
of	O
the	O
tagger	O
,	O
in	O
the	O
sense	O
that	O
the	O
generator	O
takes	O
as	O
input	O
the	O
tagged	O
output	O
z(x	O
i	O
)	O
inferred	O
from	O
the	O
source	O
style	O
and	O
modifies	O
the	O
[	O
TAG	O
]	O
tokens	O
to	O
generate	O
the	O
desired	O
sentence	O
x(v	O
)	O
i	O
in	O
the	O
target	O
style	O
S	O
v	O
.	O

Style	O
Targeted	O
Generation	O
.	O

L	O
a	O
(	O
θ	O
t	O
)	O
=	O
−	O
|X	O
1	O
|	O
i=1	O
log	O
P	O
θt	O
(	O
z(x	O
i	O
)	O
|x	O
(	O
2	O
)	O
i	O
\a(x	O
(	O
2	O
)	O
i	O
)	O
;	O
θ	O
t	O
)	O
(	O
4	O
)	O
.	O

The	O
loss	O
objective	O
(	O
L	O
a	O
)	O
given	O
by	O
Eq	O
.	O
4	O
is	O
crucial	O
for	O
tasks	O
like	O
politeness	B-TaskName
transfer	I-TaskName
where	O
one	O
of	O
the	O
styles	O
is	O
poorly	O
defined	O
.	O

In	O
effect	O
,	O
by	O
training	O
in	O
this	O
fashion	O
,	O
the	O
tagger	O
learns	O
to	O
add	O
[	O
TAG	O
]	O
tokens	O
at	O
appropriate	O
locations	O
in	O
a	O
style	O
neutral	O
sentence	O
.	O

For	O
example	O
,	O
in	O
the	O
case	O
of	O
politeness	B-TaskName
transfer	I-TaskName
,	O
we	O
only	O
use	O
the	O
sentences	O
labeled	O
as	O
"	O
polite	O
"	O
for	O
training	O
.	O

Note	O
that	O
we	O
only	O
use	O
samples	O
from	O
X	O
2	O
for	O
training	O
the	O
add	O
-	O
tagger	O
;	O
samples	O
from	O
the	O
style	O
neutral	O
X	O
1	O
are	O
not	O
involved	O
in	O
the	O
training	O
process	O
at	O
all	O
.	O

4	O
,	O
we	O
remove	O
the	O
style	O
phrases	O
"	O
you	O
would	O
like	O
to	O
"	O
and	O
"	O
please	O
"	O
and	O
replace	O
them	O
with	O
[	O
TAG	O
]	O
in	O
the	O
output	O
.	O

As	O
indicated	O
in	O
Fig	O
.	O

For	O
the	O
output	O
we	O
replace	O
the	O
same	O
phrases	O
a	O
(	O
x(2	O
)	O
i	O
)	O
with	O
[	O
TAG	O
]	O
tokens	O
.	O

Essentially	O
,	O
for	O
the	O
input	O
we	O
take	O
samples	O
x	O
(	O
2	O
)	O
i	O
in	O
the	O
target	O
style	O
S	O
2	O
and	O
explicitly	O
remove	O
style	O
phrases	O
a	O
(	O
x(2	O
)	O
i	O
)	O
from	O
it	O
.	O

4	O
)	O
for	O
the	O
add	O
-	O
tagger	O
is	O
given	O
by	O
pairs	O
where	O
the	O
input	O
is	O
{	O
x	O
(	O
2	O
)	O
i	O
\a(x	O
(	O
2	O
)	O
i	O
)	O
:	O
x	O
(	O
2	O
)	O
i	O
∈	O
X	O
2	O
}	O
and	O
the	O
output	O
is	O
{	O
z(x	O
i	O
)	O
:	O
x	O
(	O
2	O
)	O
i	O
∈	O
X	O
2	O
}	O
.	O

The	O
training	O
data	O
(	O
Fig	O
.	O

In	O
such	O
cases	O
,	O
since	O
the	O
source	O
sentences	O
have	O
no	O
attribute	O
markers	O
to	O
remove	O
,	O
the	O
tagger	O
learns	O
to	O
add	O
[	O
TAG	O
]	O
tokens	O
at	O
specific	O
locations	O
suitable	O
for	O
emanating	O
style	O
words	O
in	O
the	O
target	O
style	O
.	O

(	O
2018	O
)	O
)	O
.	O

Examples	O
of	O
such	O
a	O
task	O
include	O
the	O
tasks	O
of	O
politeness	B-TaskName
transfer	I-TaskName
(	O
introduced	O
in	O
this	O
paper	O
)	O
and	O
caption	B-TaskName
style	I-TaskName
transfer	I-TaskName
(	O
used	O
by	O
Li	O
et	O
al	O
.	O

That	O
is	O
,	O
X	O
1	O
consists	O
of	O
style	O
neutral	O
sentences	O
whereas	O
X	O
2	O
consists	O
of	O
sentences	O
in	O
the	O
target	O
style	O
.	O

L	O
r	O
(	O
θ	O
t	O
)	O
=	O
−	O
|X	O
1	O
|	O
i=1	O
log	O
P	O
θt	O
(	O
z(x	O
i	O
)	O
|x	O
(	O
1	O
)	O
i	O
;	O
θ	O
t	O
)	O
(	O
3	O
)	O
The	O
second	O
variant	O
,	O
add	O
-	O
tagger	O
,	O
is	O
designed	O
for	O
cases	O
where	O
the	O
transfer	O
needs	O
to	O
happen	O
from	O
style	O
neutral	O
sentences	O
to	O
the	O
target	O
style	O
.	O

The	O
loss	O
objective	O
for	O
replace	O
-	O
tagger	O
is	O
given	O
by	O
L	O
r	O
(	O
θ	O
t	O
)	O
in	O
Eq	O
.	O
3	O
.	O

In	O
this	O
case	O
the	O
training	O
data	O
comprises	O
of	O
pairs	O
where	O
the	O
input	O
is	O
X	O
1	O
and	O
the	O
output	O
is	O
{	O
z(x	O
i	O
)	O
:	O
x	O
(	O
1	O
)	O
i	O
∈	O
X	O
1	O
}	O
.	O

The	O
first	O
variant	O
,	O
replace	O
-	O
tagger	O
,	O
is	O
suited	O
for	O
a	O
task	O
like	O
sentiment	O
transfer	O
where	O
almost	O
every	O
sentence	O
has	O
some	O
attribute	O
markers	O
a(x	O
(	O
1	O
)	O
i	O
)	O
present	O
in	O
it	O
.	O

Finally	O
,	O
we	O
use	O
the	O
distribution	O
p	O
2	O
1	O
(	O
w)/p	O
1	O
2	O
(	O
w	O
)	O
over	O
Γ	O
2	O
/Γ	O
1	O
(	O
§	O
4.1	O
)	O
to	O
draw	O
samples	O
of	O
attribute	O
-	O
markers	O
that	O
would	O
be	O
replaced	O
with	O
the	O
[	O
TAG	O
]	O
token	O
during	O
the	O
creation	O
of	O
training	O
data	O
.	O

In	O
both	O
the	O
cases	O
,	O
the	O
[	O
TAG	O
]	O
tokens	O
indicate	O
positions	O
where	O
the	O
generator	O
can	O
insert	O
phrases	O
from	O
the	O
target	O
style	O
S	O
2	O
.	O

Depending	O
on	O
the	O
style	B-TaskName
transfer	I-TaskName
task	O
,	O
the	O
tagger	O
is	O
trained	O
to	O
either	O
(	O
1	O
)	O
identify	O
and	O
replace	O
style	O
attributes	O
a	O
(	O
x(1	O
)	O
i	O
)	O
with	O
the	O
token	O
tag	O
[	O
TAG	O
]	O
(	O
replace	O
-	O
tagger	O
)	O
or	O
(	O
2	O
)	O
add	O
the	O
[	O
TAG	O
]	O
token	O
at	O
specific	O
locations	O
in	O
x	O
(	O
1	O
)	O
i	O
(	O
add	O
-	O
tagger	O
)	O
.	O

The	O
tagger	O
model	O
(	O
with	O
parameters	O
θ	O
t	O
)	O
takes	O
as	O
input	O
the	O
sentences	O
in	O
X	O
1	O
and	O
outputs	O
{	O
z(x	O
i	O
)	O
:	O
x	O
i	O
∈	O
X	O
1	O
}	O
.	O

Style	O
Invariant	O
Tagged	O
Sentence	O
.	O

Γ	O
1	O
is	O
computed	O
similarly	O
where	O
we	O
use	O
p	O
1	O
2	O
(	O
w	O
)	O
,	O
η	O
1	O
2	O
(	O
w	O
)	O
.	O

Finally	O
,	O
we	O
estimate	O
Γ	O
2	O
by	O
Γ	O
2	O
=	O
{	O
w	O
:	O
p	O
2	O
1	O
(	O
w	O
)	O
≥	O
k	O
}	O
In	O
other	O
words	O
,	O
Γ	O
2	O
consists	O
of	O
the	O
set	O
of	O
phrases	O
in	O
X	O
2	O
above	O
a	O
given	O
style	O
impact	O
k.	O

We	O
further	O
smooth	O
and	O
normalize	O
η	O
2	O
1	O
(	O
w	O
)	O
to	O
get	O
p	O
2	O
1	O
(	O
w	O
)	O
.	O

Words	O
with	O
higher	O
values	O
for	O
η	O
2	O
1	O
(	O
w	O
)	O
have	O
a	O
higher	O
mean	O
tf	O
-	O
idf	O
in	O
X	O
2	O
vs	O
X	O
1	O
,	O
and	O
thus	O
are	O
more	O
characteristic	O
of	O
S	O
2	O
.	O

η	O
2	O
1	O
(	O
w	O
)	O
=	O
1	O
m	O
m	O
i=1	O
tf	O
-	O
idf(w	O
,	O
x(2	O
)	O
i	O
)	O
1	O
n	O
n	O
j=1	O
tf	O
-	O
idf(w	O
,	O
x(1	O
)	O
j	O
)	O
(	O
1	O
)	O
p	O
2	O
1	O
(	O
w	O
)	O
=	O
η	O
2	O
1	O
(	O
w	O
)	O
γ	O
w	O
η	O
2	O
1	O
(	O
w	O
)	O
γ	O
(	O
2	O
)	O
where	O
,	O
η	O
2	O
1	O
(	O
w	O
)	O
is	O
the	O
ratio	O
of	O
the	O
mean	O
tf	O
-	O
idfs	O
for	O
a	O
given	O
n	O
-	O
gram	O
w	O
present	O
in	O
both	O
X	O
1	O
,	O
X	O
2	O
with	O
|X	O
1	O
|	O
=	O
n	O
and	O
|X	O
2	O
|	O
=	O
m.	O

This	O
is	O
how	O
we	O
define	O
the	O
impactful	O
style	O
markers	O
for	O
style	O
S	O
2	O
.	O

Intuitively	O
,	O
p	O
2	O
1	O
(	O
w	O
)	O
is	O
proportional	O
to	O
the	O
probability	O
of	O
sampling	O
an	O
n	O
-	O
gram	O
present	O
in	O
both	O
X	O
1	O
,	O
X	O
2	O
but	O
having	O
a	O
much	O
higher	O
tf	O
-	O
idf	O
value	O
in	O
X	O
2	O
relative	O
to	O
X	O
1	O
.	O

For	O
a	O
given	O
corpus	O
pair	O
X	O
1	O
,	O
X	O
2	O
in	O
styles	O
S	O
1	O
,	O
S	O
2	O
respectively	O
we	O
first	O
compute	O
a	O
probability	O
distribution	O
p	O
2	O
1	O
(	O
w	O
)	O
over	O
the	O
n	O
-	O
grams	O
w	O
present	O
in	O
both	O
the	O
corpora	O
(	O
Eq	O
.	O
2	O
)	O
.	O

(	O
2018	O
)	O
,	O
we	O
propose	O
a	O
simple	O
approach	O
based	O
on	O
n	O
-	O
gram	O
tf	O
-	O
idfs	O
to	O
estimate	O
the	O
set	O
Γ	O
v	O
,	O
which	O
represents	O
the	O
style	O
markers	O
for	O
style	O
v.	O

Drawing	O
from	O
Li	O
et	O
al	O
.	O

Estimating	O
Style	O
Phrases	O
.	O

In	O
the	O
following	O
sections	O
we	O
discuss	O
in	O
detail	O
the	O
methodologies	O
involved	O
in	O
(	O
1	O
)	O
estimating	O
the	O
relevant	O
attribute	O
markers	O
for	O
a	O
given	O
style	O
,	O
(	O
2	O
)	O
tagger	O
,	O
and	O
(	O
3	O
)	O
generator	O
modules	O
of	O
our	O
approach	O
.	O

The	O
structural	O
bias	O
induced	O
by	O
this	O
two	O
staged	O
approach	O
is	O
helpful	O
in	O
realizing	O
an	O
interpretable	O
style	O
free	O
tagged	O
sentence	O
that	O
explicitly	O
encodes	O
the	O
content	O
.	O

We	O
can	O
also	O
see	O
that	O
the	O
inferred	O
sentence	O
in	O
both	O
the	O
cases	O
is	O
free	O
of	O
the	O
original	O
and	O
target	O
styles	O
.	O

On	O
the	O
contrary	O
,	O
in	O
the	O
second	O
example	O
,	O
the	O
terms	O
"	O
ok	O
"	O
and	O
"	O
bland	O
"	O
are	O
markers	O
of	O
negative	O
sentiment	O
and	O
hence	O
the	O
tagger	O
has	O
replaced	O
them	O
with	O
[	O
TAG	O
]	O
tokens	O
in	O
z(x	O
2	O
)	O
.	O

In	O
the	O
first	O
example	O
x	O
(	O
1	O
)	O
1	O
,	O
where	O
there	O
is	O
no	O
clear	O
style	O
attribute	O
present	O
,	O
our	O
model	O
adds	O
the	O
[	O
TAG	O
]	O
token	O
in	O
z(x	O
1	O
)	O
,	O
indicating	O
that	O
a	O
target	O
style	O
marker	O
should	O
be	O
generated	O
in	O
this	O
position	O
.	O

3	O
shows	O
the	O
overall	O
pipeline	O
of	O
the	O
proposed	O
approach	O
.	O

Training	O
data	O
creation	O
details	O
are	O
given	O
in	O
sections	O
§	O
4.2	O
,	O
§	O
4.3	O
.	O
Fig	O
.	O

tokens	O
.	O

The	O
generator	O
transforms	O
x	O
(	O
1	O
)	O
i	O
into	O
x(2	O
)	O
i	O
which	O
is	O
in	O
target	O
style	O
S	O
2	O
.	O

To	O
create	O
parallel	O
training	O
data	O
,	O
we	O
first	O
estimate	O
the	O
style	O
markers	O
Γ	O
v	O
for	O
a	O
given	O
style	O
S	O
v	O
&	O
then	O
use	O
these	O
to	O
curate	O
style	O
free	O
sentences	O
with	O
[	O
TAG	O
]	O
for	O
an	O
input	O
x	O
(	O
1	O
)	O
i	O
in	O
source	O
style	O
S	O
1	O
.	O

Even	O
though	O
we	O
have	O
non	O
-	O
parallel	O
corpora	O
,	O
both	O
systems	O
are	O
trained	O
in	O
a	O
supervised	O
fashion	O
as	O
sequence	O
-	O
to	O
-	O
sequence	O
models	O
with	O
their	O
own	O
distinct	O
pairs	O
of	O
inputs	O
&	O
outputs	O
.	O

The	O
generator	O
is	O
trained	O
to	O
generate	O
sentences	O
x(2	O
)	O
i	O
in	O
the	O
target	O
style	O
by	O
replacing	O
these	O
[	O
TAG	O
]	O
tokens	O
with	O
stylistically	O
relevant	O
words	O
inferred	O
from	O
target	O
style	O
S	O
2	O
.	O

This	O
is	O
because	O
in	O
such	O
cases	O
we	O
may	O
need	O
to	O
add	O
new	O
phrases	O
to	O
the	O
sentence	O
rather	O
than	O
simply	O
replace	O
existing	O
ones	O
.	O

It	O
is	O
especially	O
critical	O
for	O
tasks	O
like	O
politeness	B-TaskName
transfer	I-TaskName
where	O
the	O
transfer	O
takes	O
place	O
from	O
a	O
non	O
-	O
polite	O
sentence	O
.	O

This	O
is	O
one	O
of	O
the	O
major	O
differences	O
from	O
prior	O
works	O
which	O
mainly	O
focus	O
on	O
removing	O
source	O
style	O
attributes	O
and	O
then	O
replacing	O
them	O
with	O
the	O
target	O
style	O
attributes	O
.	O

This	O
particular	O
capability	O
of	O
the	O
model	O
enables	O
us	O
to	O
generate	O
these	O
tags	O
in	O
an	O
input	O
that	O
is	O
devoid	O
of	O
any	O
attribute	O
marker	O
(	O
i.e.	O
a(x	O
(	O
1	O
)	O
i	O
)	O
=	O
{	O
}	O
)	O
.	O

The	O
former	O
identifies	O
the	O
style	O
attribute	O
markers	O
a(x	O
(	O
1	O
)	O
i	O
)	O
from	O
source	O
style	O
S	O
1	O
and	O
either	O
replaces	O
them	O
with	O
a	O
positional	O
token	O
called	O
[	O
TAG	O
]	O
or	O
merely	O
adds	O
these	O
positional	O
tokens	O
without	O
removing	O
any	O
phrase	O
from	O
the	O
input	O
x	O
(	O
1	O
)	O
i	O
.	O

We	O
train	O
two	O
independent	O
systems	O
for	O
the	O
tagger	O
&	O
generator	O
which	O
have	O
complimentary	O
objectives	O
.	O

The	O
ability	O
of	O
our	O
pipeline	O
to	O
generate	O
observable	O
intermediate	O
outputs	O
z(x	O
i	O
)	O
makes	O
it	O
somewhat	O
more	O
interpretable	O
than	O
those	O
other	O
methods	O
.	O

In	O
these	O
cases	O
z(x	O
i	O
)	O
encodes	O
the	O
input	O
sentence	O
in	O
a	O
continuous	O
latent	O
space	O
whereas	O
for	O
us	O
z(x	O
i	O
)	O
manifests	O
in	O
the	O
surface	O
form	O
.	O

(	O
2017	O
i	O
while	O
being	O
agnostic	O
to	O
style	O
S	O
v	O
.	O

Shen	O
et	O
al	O
.	O

The	O
intermediate	O
variable	O
z(x	O
i	O
)	O
is	O
also	O
seen	O
in	O
other	O
style	O
-	O
transfer	O
methods	O
.	O

Conditioned	O
on	O
z(x	O
i	O
)	O
,	O
we	O
then	O
generate	O
the	O
transferred	O
sentence	O
x(2	O
)	O
i	O
in	O
the	O
target	O
style	O
S	O
2	O
using	O
another	O
model	O
,	O
the	O
generator	O
.	O

The	O
goal	O
of	O
the	O
tagger	O
is	O
to	O
ensure	O
that	O
the	O
sentence	O
z(x	O
i	O
)	O
is	O
agnostic	O
to	O
the	O
original	O
style	O
(	O
S	O
1	O
)	O
of	O
the	O
input	O
sentence	O
.	O

We	O
propose	O
a	O
two	O
staged	O
approach	O
where	O
we	O
first	O
infer	O
a	O
sentence	O
z(x	O
i	O
)	O
from	O
x	O
(	O
1	O
)	O
i	O
using	O
a	O
model	O
,	O
the	O
tagger	O
.	O

For	O
example	O
,	O
phrases	O
like	O
"	O
pretty	O
good	O
"	O
and	O
"	O
worth	O
every	O
penny	O
"	O
are	O
characteristic	O
of	O
the	O
"	O
positive	O
"	O
style	O
in	O
the	O
case	O
of	O
sentiment	O
transfer	O
task	O
.	O

The	O
presence	O
of	O
phrases	O
from	O
Γ	O
v	O
in	O
a	O
sentence	O
x	O
i	O
would	O
asso	O
-	O
ciate	O
the	O
sentence	O
with	O
the	O
style	O
S	O
v	O
.	O

For	O
a	O
style	O
S	O
v	O
where	O
v	O
∈	O
{	O
1	O
,	O
2	O
}	O
,	O
we	O
begin	O
by	O
learning	O
a	O
set	O
of	O
phrases	O
(	O
Γ	O
v	O
)	O
which	O
characterize	O
the	O
style	O
S	O
v	O
.	O

x(2	O
)	O
n	O
}	O
in	O
the	O
target	O
style	O
S	O
2	O
,	O
conditioned	O
on	O
samples	O
in	O
X	O
1	O
.	O

The	O
objective	O
of	O
the	O
task	O
is	O
to	O
efficiently	O
generate	O
samples	O
X1	O
=	O
{	O
x	O
(	O
2	O
)	O
1	O
.	O

x	O
(	O
2	O
)	O
m	O
}	O
from	O
styles	O
S	O
1	O
and	O
S	O
2	O
respectively	O
.	O

x	O
(	O
1	O
)	O
n	O
}	O
and	O
X	O
2	O
=	O
{	O
x	O
(	O
2	O
)	O
1	O
.	O

We	O
are	O
given	O
non	O
-	O
parallel	O
samples	O
of	O
sentences	O
X	O
1	O
=	O
{	O
x	O
(	O
1	O
)	O
1	O
.	O

Methodology	O
.	O

We	O
also	O
use	O
the	O
Amazon	B-DatasetName
dataset	I-DatasetName
of	I-DatasetName
product	I-DatasetName
reviews	I-DatasetName
(	O
He	O
and	O
McAuley	O
,	O
2016	O
)	O
.	O

(	O
2018	O
)	O
.	O

For	O
sentiment	B-DatasetName
transfer	I-DatasetName
,	O
we	O
use	O
the	O
Yelp	B-DatasetName
restaurant	I-DatasetName
review	I-DatasetName
dataset	O
(	O
Shen	O
et	O
al	O
.	O
,	O
2017	O
)	O
to	O
train	O
,	O
and	O
evaluate	O
on	O
a	O
test	O
set	O
of	O
1000	O
sentences	O
released	O
by	O
Li	O
et	O
al	O
.	O

This	O
task	O
parallels	O
the	O
task	O
of	O
politeness	B-TaskName
transfer	I-TaskName
because	O
much	O
like	O
in	O
the	O
case	O
of	O
politeness	B-TaskName
transfer	I-TaskName
,	O
the	O
captions	O
task	O
also	O
involves	O
going	O
from	O
a	O
style	O
neutral	O
(	O
factual	O
)	O
to	O
a	O
style	O
rich	O
(	O
humorous	O
or	O
romantic	O
)	O
parlance	O
.	O

We	O
use	O
this	O
dataset	O
to	O
perform	O
transfer	O
between	O
these	O
styles	O
.	O

The	O
Captions	B-DatasetName
dataset	O
(	O
Gan	O
et	O
al	O
.	O
,	O
2017	O
)	O
has	O
image	O
captions	O
labeled	O
as	O
being	O
factual	O
,	O
romantic	O
or	O
humorous	O
.	O

Other	O
Tasks	O
.	O

This	O
substantiates	O
our	O
claim	O
that	O
the	O
task	O
of	O
politeness	O
transfer	O
is	O
fundamentally	O
different	O
from	O
other	O
attribute	O
transfer	O
tasks	O
like	O
sentiment	O
where	O
both	O
the	O
polarities	O
are	O
clearly	O
defined	O
.	O

We	O
clearly	O
notice	O
that	O
words	O
in	O
the	O
P	O
9	O
bucket	O
are	O
closely	O
linked	O
to	O
polite	O
style	O
,	O
while	O
words	O
in	O
the	O
P	O
0	O
bucket	O
are	O
mostly	O
content	O
words	O
.	O

10	O
of	O
the	O
top	O
30	O
words	O
occurring	O
in	O
each	O
bucket	O
.	O

4	O
The	O
score	O
was	O
calculated	O
for	O
3	O
annotators	O
on	O
a	O
sample	O
set	O
of	O
50	O
sentences	O
.	O

3	O
We	O
used	O
AWD	B-MethodName
-	I-MethodName
LSTM	I-MethodName
based	I-MethodName
classifier	I-MethodName
for	O
classification	O
of	O
action	O
-	O
directive	O
.	O

2	O
We	O
prune	O
the	O
corpus	O
by	O
removing	O
the	O
sentences	O
that	O
1	O
)	O
were	O
less	O
than	O
3	O
words	O
long	O
,	O
2	O
)	O
had	O
more	O
than	O
80	O
%	O
numerical	O
tokens	O
,	O
3	O
)	O
contained	O
email	O
addresses	O
,	O
or	O
4	O
)	O
had	O
repeated	O
occurrences	O
of	O
spurious	O
characters	O
.	O

2	O
,	O
we	O
examine	O
the	O
two	O
extreme	O
buckets	O
with	O
politeness	B-MetricValue
scores	I-MetricValue
of	O
<	O
10	B-MetricValue
%	I-MetricValue
(	O
P	O
0	O
bucket	O
)	O
and	O
>	O
90	B-MetricValue
%	I-MetricValue
(	O
P	O
9	O
bucket	O
)	O
from	O
our	O
corpus	O
by	O
plotting	O
1	O
Pre	O
-	O
processing	O
also	O
involved	O
steps	O
for	O
tokenization	O
(	O
done	O
using	O
spacy	O
(	O
Honnibal	O
and	O
Montani	O
,	O
2017	O
)	O
)	O
and	O
conversion	O
to	O
lower	O
case	O
.	O

In	O
Fig	O
.	O

The	O
annotators	O
had	O
a	O
Fleiss	B-MetricName
's	I-MetricName
Kappa	I-MetricName
score	I-MetricName
(	O
κ	B-MetricValue
)	O
of	O
0.77	B-MetricValue
4	I-MetricValue
and	O
curated	O
a	O
final	O
test	O
set	O
of	O
800	O
sentences	O
.	O

3	O
Further	O
,	O
we	O
use	O
human	O
annotators	O
to	O
manually	O
select	O
the	O
test	O
sentences	O
.	O

We	O
first	O
train	O
a	O
classifier	O
on	O
the	O
switchboard	B-DatasetName
corpus	O
(	O
Jurafsky	O
et	O
al	O
.	O
,	O
1997	O
)	O
to	O
get	O
dialog	O
state	O
tags	O
and	O
filter	O
sentences	O
that	O
have	O
been	O
labeled	O
as	O
either	O
action	O
-	O
directive	O
or	O
quotation	O
.	O

Since	O
the	O
goal	O
of	O
the	O
task	O
is	O
making	O
action	O
directives	O
more	O
polite	O
,	O
we	O
manually	O
curate	O
a	O
test	O
set	O
comprising	O
of	O
such	O
sentences	O
from	O
test	O
splits	O
across	O
the	O
buckets	O
.	O

We	O
use	O
the	O
train	O
-	O
split	O
of	O
the	O
P	O
9	O
bucket	O
of	O
over	O
270	O
K	O
polite	O
sentences	O
as	O
the	O
training	O
data	O
for	O
the	O
politeness	B-TaskName
transfer	I-TaskName
task	I-TaskName
.	O

1	O
)	O
.	O

For	O
our	O
experiments	O
,	O
we	O
assumed	O
all	O
the	O
sentences	O
with	O
a	O
politeness	O
score	O
of	O
over	O
90	B-MetricValue
%	I-MetricValue
by	O
the	O
classifier	O
to	O
be	O
polite	O
,	O
also	O
referred	O
as	O
the	O
P	O
9	O
bucket	O
(	O
marked	O
in	O
green	O
in	O
Fig	O
.	O

All	O
the	O
buckets	O
are	O
further	O
divided	O
into	O
train	O
,	O
test	O
,	O
and	O
dev	O
splits	O
(	O
in	O
a	O
80:10:10	O
ratio	O
)	O
.	O

1	O
)	O
.	O

Finally	O
,	O
we	O
use	O
a	O
politeness	O
classifier	O
(	O
Niu	O
and	O
Bansal	O
,	O
2018	O
)	O
to	O
assign	O
politeness	O
scores	O
to	O
these	O
sentences	O
and	O
filter	O
them	O
into	O
ten	O
buckets	O
based	O
on	O
the	O
score	O
(	O
P	O
0	O
-P	O
9	O
;	O
Fig	O
.	O

Further	O
pruning	O
2	O
led	O
to	O
a	O
cleaned	O
corpus	O
of	O
over	O
1.39	O
million	O
sentences	O
.	O

The	O
first	O
set	O
of	O
pre	O
-	O
processing	O
1	O
steps	O
and	O
de	O
-	O
duplication	O
yielded	O
a	O
corpus	O
of	O
roughly	O
2.5	O
million	O
sentences	O
.	O

We	O
begin	O
by	O
pre	O
-	O
processing	O
the	O
raw	O
Enron	B-TaskName
corpus	O
following	O
Shetty	O
and	O
Adibi	O
(	O
2004	O
)	O
.	O

Emails	O
serve	O
as	O
a	O
medium	O
for	O
exchange	O
of	O
requests	O
,	O
serving	O
as	O
an	O
ideal	O
application	O
for	O
politeness	B-TaskName
transfer	I-TaskName
.	O

The	O
Enron	B-DatasetName
corpus	O
(	O
Klimt	O
and	O
Yang	O
,	O
2004	O
)	O
consists	O
of	O
a	O
large	O
set	O
of	O
email	O
conversations	O
exchanged	O
by	O
the	O
employees	O
of	O
the	O
Enron	B-DatasetName
corporation	O
.	O

Data	O
Preparation	O
.	O

While	O
there	O
can	O
be	O
more	O
than	O
one	O
way	O
of	O
making	O
a	O
sentence	O
polite	O
,	O
for	O
the	O
above	O
examples	O
,	O
adding	O
gratitude	O
(	O
"	O
Thanks	O
and	O
let	O
's	O
stay	O
in	O
touch	O
"	O
)	O
or	O
counterfactuals	O
(	O
"	O
Could	O
you	O
please	O
call	O
me	O
when	O
you	O
get	O
back	O
?	O
"	O
)	O
would	O
make	O
them	O
polite	O
(	O
Danescu	O
-	O
Niculescu	O
-	O
Mizil	O
et	O
al	O
.	O
,	O
2013	O
)	O
.	O

The	O
goal	O
of	O
this	O
task	O
is	O
to	O
convert	O
action	O
-	O
directives	O
to	O
polite	O
requests	O
.	O

(	O
1997	O
)	O
,	O
we	O
use	O
the	O
umbrella	O
term	O
"	O
action	O
-	O
directives	O
"	O
for	O
such	O
sentences	O
.	O

Following	O
Jurafsky	O
et	O
al	O
.	O

Common	O
examples	O
include	O
imperatives	O
"	O
Let	O
's	O
stay	O
in	O
touch	O
"	O
and	O
questions	O
that	O
express	O
a	O
proposal	O
"	O
Can	O
you	O
call	O
me	O
when	O
you	O
get	O
back	O
?	O
"	O
.	O

For	O
the	O
politeness	B-TaskName
transfer	I-TaskName
task	I-TaskName
,	O
we	O
focus	O
on	O
sentences	O
in	O
which	O
the	O
speaker	O
communicates	O
a	O
requirement	O
that	O
the	O
listener	O
needs	O
to	O
fulfill	O
.	O

Politeness	B-TaskName
Transfer	I-TaskName
Task	I-TaskName
.	O

3	O
Tasks	O
and	O
Datasets	O
.	O

Further	O
,	O
our	O
work	O
is	O
more	O
generalizable	O
and	O
we	O
show	O
results	O
on	O
five	O
other	O
style	O
transfer	O
tasks	O
.	O

In	O
contrast	O
,	O
we	O
are	O
capable	O
of	O
generating	O
the	O
entire	O
sentence	O
in	O
the	O
target	O
style	O
.	O

It	O
focuses	O
only	O
on	O
sentiment	O
modification	O
,	O
treating	O
it	O
as	O
a	O
cloze	O
form	O
task	O
of	O
filling	O
in	O
the	O
appropriate	O
words	O
in	O
the	O
target	O
sentiment	O
.	O

(	O
2019	O
)	O
treats	O
style	O
transfer	O
as	O
a	O
conditional	O
language	O
modelling	O
task	O
.	O

Wu	O
et	O
al	O
.	O

This	O
also	O
makes	O
our	O
pipeline	O
faster	O
in	O
addition	O
to	O
being	O
robust	O
to	O
noise	O
.	O

Our	O
methodology	O
differs	O
from	O
these	O
works	O
as	O
it	O
does	O
not	O
require	O
the	O
retrieve	O
stage	O
and	O
makes	O
no	O
assumptions	O
on	O
the	O
existence	O
of	O
similar	O
content	O
phrases	O
in	O
both	O
the	O
styles	O
.	O

However	O
,	O
DRG	B-MethodName
has	O
several	O
limitations	O
:	O
(	O
1	O
)	O
the	O
delete	O
module	O
often	O
marks	O
content	O
words	O
as	O
stylistic	O
markers	O
and	O
deletes	O
them	O
,	O
(	O
2	O
)	O
the	O
retrieve	O
step	O
relies	O
on	O
the	O
presence	O
of	O
similar	O
content	O
in	O
both	O
the	O
source	O
and	O
target	O
styles	O
,	O
(	O
3	O
)	O
the	O
retrieve	O
step	O
is	O
time	O
consuming	O
for	O
large	O
datasets	O
,	O
(	O
4	O
)	O
the	O
pipeline	O
makes	O
the	O
assumption	O
that	O
style	O
can	O
be	O
transferred	O
by	O
deleting	O
stylistic	O
markers	O
and	O
replacing	O
them	O
with	O
target	O
style	O
phrases	O
,	O
(	O
5	O
)	O
the	O
method	O
relies	O
on	O
a	O
fixed	O
corpus	O
of	O
style	O
attribute	O
markers	O
,	O
and	O
is	O
thus	O
limited	O
in	O
its	O
ability	O
to	O
generalize	O
to	O
unseen	O
data	O
during	O
test	O
time	O
.	O

Compared	O
to	O
prior	O
work	O
,	O
"	O
Delete	B-MethodName
,	I-MethodName
Retrieve	I-MethodName
and	I-MethodName
Generate	I-MethodName
"	O
(	O
Li	O
et	O
al	O
.	O
,	O
2018	O
)	O
(	O
referred	O
to	O
as	O
DRG	O
henceforth	O
)	O
and	O
its	O
extension	O
(	O
Sudhakar	O
et	O
al	O
.	O
,	O
2019	O
)	O
are	O
effective	O
methods	O
to	O
generate	O
out	O
-	O
puts	O
in	O
the	O
target	O
style	O
while	O
having	O
a	O
relatively	O
high	O
rate	O
of	O
source	O
content	O
preservation	O
.	O

Current	O
style	B-TaskName
transfer	I-TaskName
techniques	O
(	O
Shen	O
et	O
al	O
.	O
,	O
2017;Hu	O
et	O
al	O
.	O
,	O
2017;Fu	O
et	O
al	O
.	O
,	O
2018;Yang	O
et	O
al	O
.	O
,	O
2018;John	O
et	O
al	O
.	O
,	O
2019	O
)	O
try	O
to	O
disentangle	O
source	O
style	O
from	O
content	O
and	O
then	O
combine	O
the	O
content	O
with	O
the	O
target	O
style	O
to	O
generate	O
the	O
sentence	O
in	O
the	O
target	O
style	O
.	O

We	O
focus	O
our	O
efforts	O
on	O
carving	O
out	O
a	O
task	O
for	O
politeness	B-TaskName
transfer	I-TaskName
and	O
creating	O
a	O
dataset	O
for	O
such	O
a	O
task	O
.	O

Note	O
that	O
formality	O
and	O
politeness	O
are	O
loosely	O
connected	O
but	O
independent	O
styles	O
(	O
Kang	O
and	O
Hovy	O
,	O
2019	O
)	O
.	O

Prior	O
work	O
on	O
style	O
transfer	O
has	O
largely	O
focused	O
on	O
tasks	O
of	O
sentiment	O
modification	O
(	O
Hu	O
et	O
al	O
.	O
,	O
2017;Shen	O
et	O
al	O
.	O
,	O
2017;Li	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
caption	O
transfer	O
(	O
Li	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
persona	O
transfer	O
(	O
Chandu	O
et	O
al	O
.	O
,	O
2019;Zhang	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
gender	O
and	O
political	O
slant	O
transfer	O
(	O
Reddy	O
and	O
Knight	O
,	O
2016;Prabhumoye	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
and	O
formality	O
transfer	O
(	O
Rao	O
and	O
Tetreault	O
,	O
2018;Xu	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

We	O
build	O
upon	O
this	O
body	O
of	O
work	O
by	O
using	O
this	O
corpus	O
as	O
a	O
source	O
for	O
the	O
style	B-TaskName
transfer	I-TaskName
task	I-TaskName
.	O

Prior	O
work	O
on	O
Enron	B-DatasetName
corpus	O
(	O
Yeh	O
and	O
Harnly	O
,	O
2006	O
)	O
has	O
been	O
mostly	O
from	O
a	O
socio	O
-	O
linguistic	O
perspective	O
to	O
observe	O
social	O
power	O
dynamics	O
(	O
Bramsen	O
et	O
al	O
.	O
,	O
2011;McCallum	O
et	O
al	O
.	O
,	O
2007	O
)	O
,	O
formality	O
(	O
Peterson	O
et	O
al	O
.	O
,	O
2011	O
)	O
and	O
politeness	O
(	O
Prabhakaran	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

Their	O
work	O
focuses	O
on	O
contextual	O
dialogue	O
response	O
generation	O
as	O
opposed	O
to	O
content	O
preserving	O
style	O
transfer	O
,	O
while	O
the	O
latter	O
is	O
the	O
central	O
theme	O
of	O
our	O
work	O
.	O

Niu	O
and	O
Bansal	O
(	O
2018	O
)	O
uses	O
this	O
corpus	O
to	O
generate	O
polite	O
dialogues	O
.	O

Recent	O
work	O
(	O
Danescu	O
-	O
Niculescu	O
-	O
Mizil	O
et	O
al	O
.	O
,	O
2013	O
)	O
in	O
computational	O
linguistics	O
has	O
provided	O
a	O
corpus	O
of	O
requests	O
annotated	O
for	O
politeness	O
curated	O
from	O
Wikipedia	B-DatasetName
and	O
StackExchange	B-DatasetName
.	O

Politeness	B-TaskName
and	O
its	O
close	O
relation	O
with	O
power	O
dynamics	O
and	O
social	O
interactions	O
has	O
been	O
well	O
documented	O
(	O
Brown	O
et	O
al	O
.	O
,	O
1987	O
)	O
.	O

Related	O
Work	O
.	O

Finally	O
,	O
we	O
design	O
a	O
"	O
tag	B-MethodName
and	I-MethodName
generate	I-MethodName
"	O
pipeline	O
that	O
is	O
particularly	O
well	O
suited	O
for	O
tasks	O
like	O
politeness	B-TaskName
,	O
while	O
being	O
general	O
enough	O
to	O
match	O
or	O
beat	O
the	O
performance	O
of	O
the	O
existing	O
systems	O
on	O
popular	O
style	O
transfer	O
tasks	O
.	O

In	O
the	O
process	O
,	O
we	O
highlight	O
an	O
important	O
class	O
of	O
problems	O
wherein	O
the	O
transfer	O
involves	O
going	O
from	O
a	O
neutral	O
style	O
to	O
the	O
target	O
style	O
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
we	O
are	O
the	O
first	O
to	O
undertake	O
politeness	B-TaskName
as	O
a	O
style	O
transfer	O
task	O
.	O

Additionally	O
,	O
we	O
hand	O
curate	O
a	O
test	O
set	O
of	O
800	O
samples	O
(	O
from	O
Enron	O
emails	O
)	O
which	O
are	O
annotated	O
as	O
requests	O
.	O

To	O
this	O
end	O
,	O
we	O
provide	O
a	O
large	O
dataset	O
of	O
nearly	O
1.39	O
million	O
sentences	O
labeled	O
for	O
politeness	B-TaskName
(	O
https://github.com/tag-and-generate/	O
politeness	O
-	O
dataset	O
)	O
.	O

Our	O
main	O
contribution	O
is	O
the	O
design	O
of	O
politeness	B-TaskName
transfer	I-TaskName
task	O
.	O

(	O
2018	O
)	O
and	O
improves	O
upon	O
several	O
of	O
its	O
limitations	O
as	O
described	O
in	O
(	O
§	O
2	O
)	O
.	O

Our	O
methodology	O
is	O
inspired	O
by	O
Li	O
et	O
al	O
.	O

The	O
results	O
show	O
that	O
our	O
technique	O
is	O
effective	O
across	O
a	O
broad	O
spectrum	O
of	O
style	O
transfer	O
tasks	O
.	O

Both	O
automatic	O
and	O
human	O
evaluations	O
show	O
that	O
our	O
model	O
beats	O
the	O
stateof	O
-	O
the	O
-	O
art	O
methods	O
in	O
content	O
preservation	O
,	O
while	O
either	O
matching	O
or	O
improving	O
the	O
transfer	O
accuracy	B-MetricName
across	O
six	O
different	O
style	O
transfer	O
tasks	O
(	O
§	O
5	O
)	O
.	O

We	O
evaluate	O
our	O
model	O
on	O
politeness	B-TaskName
transfer	I-TaskName
as	O
well	O
as	O
5	O
additional	O
tasks	O
described	O
in	O
prior	O
work	O
(	O
Shen	O
et	O
al	O
.	O
,	O
2017;Prabhumoye	O
et	O
al	O
.	O
,	O
2018;Li	O
et	O
al	O
.	O
,	O
2018	O
)	O
on	O
content	O
preservation	O
,	O
fluency	O
and	O
style	O
transfer	O
accuracy	O
.	O

Finally	O
,	O
if	O
the	O
input	O
sentence	O
is	O
already	O
in	O
the	O
target	O
style	O
,	O
our	O
model	O
wo	O
n't	O
add	O
any	O
stylistic	O
markers	O
and	O
thus	O
would	O
allow	O
the	O
input	O
to	O
flow	O
as	O
is	O
.	O

Additionally	O
,	O
unlike	O
previous	O
systems	O
,	O
the	O
outputs	O
of	O
the	O
intermediate	O
steps	O
in	O
our	O
system	O
are	O
fully	O
realized	O
,	O
making	O
the	O
whole	O
pipeline	O
interpretable	O
.	O

The	O
generator	O
takes	O
as	O
input	O
the	O
output	O
of	O
the	O
tagger	O
and	O
generates	O
a	O
sentence	O
in	O
the	O
target	O
style	O
.	O

If	O
the	O
sentence	O
has	O
no	O
style	O
attributes	O
,	O
as	O
in	O
the	O
case	O
for	O
politeness	B-TaskName
transfer	I-TaskName
,	O
the	O
tagger	O
adds	O
the	O
tag	O
token	O
in	O
positions	O
where	O
phrases	O
in	O
the	O
target	O
style	O
can	O
be	O
inserted	O
.	O

The	O
tagger	O
identifies	O
the	O
words	O
or	O
phrases	O
which	O
belong	O
to	O
the	O
original	O
style	O
and	O
replaces	O
them	O
with	O
a	O
tag	O
token	O
.	O

We	O
propose	O
a	O
tag	B-MethodName
and	I-MethodName
generate	I-MethodName
pipeline	I-MethodName
to	O
overcome	O
these	O
challenges	O
.	O

Note	O
that	O
this	O
is	O
in	O
stark	O
contrast	O
with	O
the	O
standard	O
style	O
transfer	O
tasks	O
,	O
which	O
involve	O
transferring	O
a	O
sentence	O
from	O
a	O
well	O
-	O
defined	O
style	O
polarity	O
to	O
the	O
other	O
(	O
like	O
positive	O
to	O
negative	O
sentiment	O
)	O
.	O

For	O
our	O
study	O
,	O
we	O
focus	O
on	O
the	O
task	O
of	O
transferring	O
the	O
non	O
-	O
polite	O
sentences	O
to	O
polite	O
sentences	O
,	O
where	O
we	O
simply	O
define	O
non	O
-	O
politeness	O
to	O
be	O
the	O
absence	O
of	O
both	O
politeness	O
and	O
impoliteness	O
.	O

While	O
interesting	O
,	O
such	O
cases	O
can	O
typically	O
be	O
neutralized	O
using	O
lexicons	O
.	O

Further	O
,	O
the	O
other	O
extreme	O
of	O
politeness	B-TaskName
,	O
impolite	O
sentences	O
,	O
are	O
typically	O
riddled	O
with	O
curse	O
words	O
and	O
insulting	O
phrases	O
.	O

However	O
,	O
cues	O
that	O
signal	O
the	O
absence	O
of	O
politeness	B-TaskName
,	O
like	O
direct	O
questions	O
,	O
statements	O
and	O
factuality	O
(	O
Danescu	O
-	O
Niculescu	O
-	O
Mizil	O
et	O
al	O
.	O
,	O
2013	O
)	O
,	O
do	O
not	O
explicitly	O
appear	O
in	O
a	O
sentence	O
,	O
and	O
are	O
thus	O
hard	O
to	O
objectify	O
.	O

It	O
is	O
easy	O
to	O
pinpoint	O
the	O
signals	O
for	O
politeness	B-TaskName
.	O

This	O
example	O
brings	O
out	O
a	O
distinct	O
characteristic	O
of	O
politeness	O
.	O

While	O
the	O
sentence	O
is	O
not	O
impolite	O
,	O
a	O
rephrasing	O
"	O
could	O
you	O
please	O
send	O
me	O
the	O
data	O
"	O
would	O
largely	O
be	O
accepted	O
as	O
a	O
more	O
polite	O
way	O
of	O
phrasing	O
the	O
same	O
statement	O
(	O
Danescu	O
-	O
Niculescu	O
-	O
Mizil	O
et	O
al	O
.	O
,	O
2013	O
)	O
.	O

Consider	O
a	O
common	O
directive	O
in	O
formal	O
communication	O
,	O
"	O
send	O
me	O
the	O
data	O
"	O
.	O

Even	O
after	O
framing	O
politeness	B-TaskName
transfer	I-TaskName
as	O
a	O
task	O
,	O
there	O
are	O
additional	O
challenges	O
involved	O
that	O
differentiate	O
politeness	B-TaskName
from	O
other	O
styles	O
.	O

Thus	O
,	O
we	O
restrict	O
our	O
attention	O
to	O
the	O
notion	O
of	O
politeness	O
as	O
widely	O
accepted	O
by	O
the	O
speakers	O
of	O
North	O
American	O
English	O
in	O
a	O
formal	O
setting	O
.	O

Second	O
,	O
we	O
base	O
our	O
experiments	O
on	O
a	O
dataset	O
derived	O
from	O
the	O
Enron	O
corpus	O
(	O
Klimt	O
and	O
Yang	O
,	O
2004	O
)	O
which	O
consists	O
of	O
email	O
exchanges	O
in	O
an	O
American	O
corporation	O
.	O

We	O
circumscribe	O
the	O
scope	O
of	O
politeness	O
for	O
the	O
purpose	O
of	O
this	O
study	O
as	O
follows	O
:	O
First	O
,	O
we	O
adopt	O
the	O
data	O
driven	O
definition	O
of	O
politeness	O
proposed	O
by	O
(	O
Danescu	O
-	O
Niculescu	O
-	O
Mizil	O
et	O
al	O
.	O
,	O
2013	O
)	O
.	O

For	O
instance	O
,	O
while	O
using	O
"	O
please	O
"	O
in	O
requests	O
made	O
to	O
the	O
closest	O
friends	O
is	O
common	O
amongst	O
the	O
native	O
speakers	O
of	O
North	O
American	O
English	O
,	O
such	O
an	O
act	O
would	O
be	O
considered	O
awkward	O
,	O
if	O
not	O
rude	O
,	O
in	O
the	O
Arab	O
culture	O
(	O
Kádár	O
and	O
Mills	O
,	O
2011	O
)	O
.	O

Second	O
,	O
politeness	O
of	O
a	O
sentence	O
depends	O
on	O
the	O
culture	O
,	O
language	O
,	O
and	O
social	O
structure	O
of	O
both	O
the	O
speaker	O
and	O
the	O
addressed	O
person	O
.	O

First	O
,	O
as	O
noted	O
by	O
(	O
Brown	O
et	O
al	O
.	O
,	O
1987	O
)	O
,	O
the	O
phenomenon	O
of	O
politeness	O
is	O
rich	O
and	O
multifaceted	O
.	O

There	O
are	O
primarily	O
two	O
reasons	O
for	O
this	O
complexity	O
.	O

While	O
native	O
speakers	O
of	O
a	O
language	O
and	O
cohabitants	O
of	O
a	O
region	O
have	O
a	O
good	O
working	O
understanding	O
of	O
the	O
phenomenon	O
of	O
politeness	O
for	O
everyday	O
conversation	O
,	O
pinning	O
it	O
down	O
as	O
a	O
definition	O
is	O
non	O
-	O
trivial	O
(	O
Meier	O
,	O
1995	O
)	O
.	O

It	O
is	O
also	O
imperative	O
to	O
use	O
the	O
appropriate	O
level	O
of	O
politeness	O
for	O
smooth	O
communication	O
in	O
conversations	O
(	O
Coppock	O
,	O
2005	O
)	O
,	O
organizational	O
settings	O
like	O
emails	O
(	O
Peterson	O
et	O
al	O
.	O
,	O
2011	O
)	O
,	O
memos	O
,	O
official	O
documents	O
,	O
and	O
many	O
other	O
settings	O
.	O

The	O
data	O
and	O
code	O
is	O
located	O
at	O
https://	O
github.com/tag-and-generate/Politeness	O
plays	O
a	O
crucial	O
role	O
in	O
social	O
interaction	O
,	O
and	O
is	O
closely	O
tied	O
with	O
power	O
dynamics	O
,	O
social	O
distance	O
between	O
the	O
participants	O
of	O
a	O
conversation	O
,	O
and	O
gender	O
(	O
Brown	O
et	O
al	O
.	O
,	O
1987;Danescu	O
-	O
Niculescu	O
-	O
Mizil	O
et	O
al	O
.	O
,	O
2013	O
)	O
.	O

Additionally	O
,	O
our	O
model	O
surpasses	O
existing	O
methods	O
on	O
human	O
evaluations	O
for	O
grammaticality	O
,	O
meaning	O
preservation	O
and	O
transfer	O
accuracy	O
across	O
all	O
the	O
six	O
style	B-TaskName
transfer	I-TaskName
tasks	O
.	O

For	O
politeness	B-TaskName
as	O
well	O
as	O
five	O
other	O
transfer	O
tasks	O
,	O
our	O
model	O
outperforms	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
on	O
automatic	O
metrics	O
for	O
content	O
preservation	O
,	O
with	O
a	O
comparable	O
or	O
better	O
performance	O
on	O
style	O
transfer	O
accuracy	B-MetricName
.	O

We	O
design	O
a	O
tag	B-MethodName
and	I-MethodName
generate	I-MethodName
pipeline	I-MethodName
that	O
identifies	O
stylistic	O
attributes	O
and	O
subsequently	O
generates	O
a	O
sentence	O
in	O
the	O
target	O
style	O
while	O
preserving	O
most	O
of	O
the	O
source	O
content	O
.	O

We	O
also	O
provide	O
a	O
dataset	O
of	O
more	O
than	O
1.39	O
million	O
instances	O
automatically	O
labeled	O
for	O
politeness	B-TaskName
to	O
encourage	O
benchmark	O
evaluations	O
on	O
this	O
new	O
task	O
.	O

Rao	O
and	O
Tetreault	O
,	O
2018	O
;	O
Xu	O
et	O
al	O
.	O
,	O
2012	O
;	O
Jhamtani	O
et	O
al	O
.	O
,	O
2017	O
)	O
has	O
not	O
focused	O
on	O
politeness	B-TaskName
as	O
a	O
style	B-TaskName
transfer	I-TaskName
task	O
,	O
and	O
we	O
argue	O
that	O
defining	O
it	O
is	O
cumbersome	O
.	O

Motivated	O
by	O
its	O
central	O
importance	O
,	O
in	O
this	O
paper	O
we	O
study	O
the	O
task	O
of	O
converting	O
non	O
-	O
polite	O
sentences	O
to	O
polite	O
sentences	O
while	O
preserving	O
the	O
meaning	O
.	O

Notably	O
,	O
politeness	O
has	O
also	O
been	O
identified	O
as	O
an	O
interpersonal	O
style	O
which	O
can	O
be	O
decoupled	O
from	O
content	O
(	O
Kang	O
and	O
Hovy	O
,	O
2019	O
)	O
.	O

Politeness	B-TaskName
Transfer	I-TaskName
:	O
A	O
Tag	B-MethodName
and	I-MethodName
Generate	I-MethodName
Approach	I-MethodName
.	O

This	O
paper	O
introduces	O
a	O
new	O
task	O
of	O
politeness	B-TaskName
transfer	I-TaskName
which	O
involves	O
converting	O
non	O
-	O
polite	O
sentences	O
to	O
polite	O
sentences	O
while	O
preserving	O
the	O
meaning	O
.	O

Conclusion	O
.	O

Unlike	O
recent	O
language	O
representation	O
models	O
(	O
Peters	O
et	O
al	O
.	O
,	O
2018a;Radford	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
BERT	B-MethodName
is	O
designed	O
to	O
pretrain	O
deep	O
bidirectional	O
representations	O
from	O
unlabeled	O
text	O
by	O
jointly	O
conditioning	O
on	O
both	O
left	O
and	O
right	O
context	O
in	O
all	O
layers	O
.	O

As	O
a	O
result	O
,	O
the	O
pre	O
-	O
trained	O
BERT	B-MethodName
model	O
can	O
be	O
finetuned	O
with	O
just	O
one	O
additional	O
output	O
layer	O
to	O
create	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
for	O
a	O
wide	O
range	O
of	O
tasks	O
,	O
such	O
as	O
question	B-TaskName
answering	I-TaskName
and	O
language	B-TaskName
inference	I-TaskName
,	O
without	O
substantial	O
taskspecific	O
architecture	O
modifications	O
.	O

BERT	B-MethodName
is	O
conceptually	O
simple	O
and	O
empirically	O
powerful	O
.	O

Introduction	O
.	O

Language	O
model	O
pre	O
-	O
training	O
has	O
been	O
shown	O
to	O
be	O
effective	O
for	O
improving	O
many	O
natural	O
language	O
processing	O
tasks	O
(	O
Dai	O
and	O
Le	O
,	O
2015	O
;	O
Peters	O
et	O
al	O
.	O
,	O
2018a;Radford	O
et	O
al	O
.	O
,	O
2018;Howard	O
and	O
Ruder	O
,	O
2018	O
)	O
.	O

These	O
include	O
sentence	O
-	O
level	O
tasks	O
such	O
as	O
natural	B-TaskName
language	I-TaskName
inference	I-TaskName
(	O
Bowman	O
et	O
al	O
.	O
,	O
2015;Williams	O
et	O
al	O
.	O
,	O
2018	O
)	O
and	O
paraphrasing	O
(	O
Dolan	O
and	O
Brockett	O
,	O
2005	O
)	O
,	O
which	O
aim	O
to	O
predict	O
the	O
relationships	O
between	O
sentences	O
by	O
analyzing	O
them	O
holistically	O
,	O
as	O
well	O
as	O
token	O
-	O
level	O
tasks	O
such	O
as	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
and	O
question	B-TaskName
answering	I-TaskName
,	O
where	O
models	O
are	O
required	O
to	O
produce	O
fine	O
-	O
grained	O
output	O
at	O
the	O
token	O
level	O
(	O
Tjong	O
Kim	O
Sang	O
and	O
De	O
Meulder	O
,	O
2003;Rajpurkar	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

There	O
are	O
two	O
existing	O
strategies	O
for	O
applying	O
pre	O
-	O
trained	O
language	O
representations	O
to	O
downstream	O
tasks	O
:	O
feature	O
-	O
based	O
and	O
fine	O
-	O
tuning	O
.	O

The	O
feature	O
-	O
based	O
approach	O
,	O
such	O
as	O
ELMo	B-MethodName
(	O
Peters	O
et	O
al	O
.	O
,	O
2018a	O
)	O
,	O
uses	O
task	O
-	O
specific	O
architectures	O
that	O
include	O
the	O
pre	O
-	O
trained	O
representations	O
as	O
additional	O
features	O
.	O

The	O
fine	O
-	O
tuning	O
approach	O
,	O
such	O
as	O
the	O
Generative	B-MethodName
Pre	I-MethodName
-	I-MethodName
trained	I-MethodName
Transformer	I-MethodName
(	O
OpenAI	B-MethodName
GPT	I-MethodName
)	O
(	O
Radford	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
introduces	O
minimal	O
task	O
-	O
specific	O
parameters	O
,	O
and	O
is	O
trained	O
on	O
the	O
downstream	O
tasks	O
by	O
simply	O
fine	O
-	O
tuning	O
all	O
pretrained	O
parameters	O
.	O

The	O
two	O
approaches	O
share	O
the	O
same	O
objective	O
function	O
during	O
pre	O
-	O
training	O
,	O
where	O
they	O
use	O
unidirectional	O
language	O
models	O
to	O
learn	O
general	O
language	O
representations	O
.	O

We	O
argue	O
that	O
current	O
techniques	O
restrict	O
the	O
power	O
of	O
the	O
pre	O
-	O
trained	O
representations	O
,	O
especially	O
for	O
the	O
fine	O
-	O
tuning	O
approaches	O
.	O

The	O
major	O
limitation	O
is	O
that	O
standard	O
language	O
models	O
are	O
unidirectional	O
,	O
and	O
this	O
limits	O
the	O
choice	O
of	O
architectures	O
that	O
can	O
be	O
used	O
during	O
pre	O
-	O
training	O
.	O

Such	O
restrictions	O
are	O
sub	O
-	O
optimal	O
for	O
sentence	O
-	O
level	O
tasks	O
,	O
and	O
could	O
be	O
very	O
harmful	O
when	O
applying	O
finetuning	O
based	O
approaches	O
to	O
token	O
-	O
level	O
tasks	O
such	O
as	O
question	B-TaskName
answering	I-TaskName
,	O
where	O
it	O
is	O
crucial	O
to	O
incorporate	O
context	O
from	O
both	O
directions	O
.	O

In	O
this	O
paper	O
,	O
we	O
improve	O
the	O
fine	O
-	O
tuning	O
based	O
approaches	O
by	O
proposing	O
BERT	B-MethodName
:	O
Bidirectional	B-MethodName
Encoder	I-MethodName
Representations	I-MethodName
from	I-MethodName
Transformers	I-MethodName
.	O

The	O
masked	O
language	O
model	O
randomly	O
masks	O
some	O
of	O
the	O
tokens	O
from	O
the	O
input	O
,	O
and	O
the	O
objective	O
is	O
to	O
predict	O
the	O
original	O
vocabulary	O
i	O
d	O
of	O
the	O
masked	O
word	O
based	O
only	O
on	O
its	O
context	O
.	O

Unlike	O
left	O
-	O
toright	O
language	O
model	O
pre	O
-	O
training	O
,	O
the	O
MLM	O
objective	O
enables	O
the	O
representation	O
to	O
fuse	O
the	O
left	O
and	O
the	O
right	O
context	O
,	O
which	O
allows	O
us	O
to	O
pretrain	O
a	O
deep	O
bidirectional	O
Transformer	O
.	O

Unlike	O
Radford	O
et	O
al	O
.	O

This	O
is	O
also	O
in	O
contrast	O
to	O
Peters	O
et	O
al	O
.	O

(	O
2018a	O
)	O
,	O
which	O
uses	O
a	O
shallow	O
concatenation	O
of	O
independently	O
trained	O
left	O
-	O
to	O
-	O
right	O
and	O
right	O
-	O
to	O
-	O
left	O
LMs	O
.	O
•	O
We	O
show	O
that	O
pre	O
-	O
trained	O
representations	O
reduce	O
the	O
need	O
for	O
many	O
heavily	O
-	O
engineered	O
taskspecific	O
architectures	O
.	O

BERT	B-MethodName
is	O
the	O
first	O
finetuning	O
based	O
representation	O
model	O
that	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
a	O
large	O
suite	O
of	O
sentence	O
-	O
level	O
and	O
token	O
-	O
level	O
tasks	O
,	O
outperforming	O
many	O
task	O
-	O
specific	O
architectures	O
.	O

•	O
BERT	B-MethodName
advances	O
the	O
state	O
of	O
the	O
art	O
for	O
eleven	O
NLP	O
tasks	O
.	O

Related	O
Work	O
.	O

There	O
is	O
a	O
long	O
history	O
of	O
pre	O
-	O
training	O
general	O
language	O
representations	O
,	O
and	O
we	O
briefly	O
review	O
the	O
most	O
widely	O
-	O
used	O
approaches	O
in	O
this	O
section	O
.	O

Unsupervised	O
Feature	O
-	O
based	O
Approaches	O
.	O

Learning	O
widely	O
applicable	O
representations	O
of	O
words	O
has	O
been	O
an	O
active	O
area	O
of	O
research	O
for	O
decades	O
,	O
including	O
non	O
-	O
neural	O
(	O
Brown	O
et	O
al	O
.	O
,	O
1992;Ando	O
and	O
Zhang	O
,	O
2005;Blitzer	O
et	O
al	O
.	O
,	O
2006	O
)	O
and	O
neural	O
(	O
Mikolov	O
et	O
al	O
.	O
,	O
2013;Pennington	O
et	O
al	O
.	O
,	O
2014	O
)	O
methods	O
.	O

Pre	O
-	O
trained	O
word	O
embeddings	O
are	O
an	O
integral	O
part	O
of	O
modern	O
NLP	O
systems	O
,	O
offering	O
significant	O
improvements	O
over	O
embeddings	O
learned	O
from	O
scratch	O
(	O
Turian	O
et	O
al	O
.	O
,	O
2010	O
)	O
.	O

To	O
pretrain	O
word	O
embedding	O
vectors	O
,	O
left	O
-	O
to	O
-	O
right	O
language	O
modeling	O
objectives	O
have	O
been	O
used	O
(	O
Mnih	O
and	O
Hinton	O
,	O
2009	O
)	O
,	O
as	O
well	O
as	O
objectives	O
to	O
discriminate	O
correct	O
from	O
incorrect	O
words	O
in	O
left	O
and	O
right	O
context	O
(	O
Mikolov	O
et	O
al	O
.	O
,	O
2013	O
)	O
.	O

These	O
approaches	O
have	O
been	O
generalized	O
to	O
coarser	O
granularities	O
,	O
such	O
as	O
sentence	O
embeddings	O
(	O
Kiros	O
et	O
al	O
.	O
,	O
2015;Logeswaran	O
and	O
Lee	O
,	O
2018	O
)	O
or	O
paragraph	O
embeddings	O
(	O
Le	O
and	O
Mikolov	O
,	O
2014	O
)	O
.	O

ELMo	B-MethodName
and	O
its	O
predecessor	O
(	O
Peters	O
et	O
al	O
.	O
,	O
2017(Peters	O
et	O
al	O
.	O
,	O
,	O
2018a	O
)	O
)	O
generalize	O
traditional	O
word	O
embedding	O
research	O
along	O
a	O
different	O
dimension	O
.	O

They	O
extract	O
context	O
-	O
sensitive	O
features	O
from	O
a	O
left	O
-	O
to	O
-	O
right	O
and	O
a	O
right	O
-	O
to	O
-	O
left	O
language	O
model	O
.	O

The	O
contextual	O
representation	O
of	O
each	O
token	O
is	O
the	O
concatenation	O
of	O
the	O
left	O
-	O
to	O
-	O
right	O
and	O
right	O
-	O
to	O
-	O
left	O
representations	O
.	O

Melamud	O
et	O
al	O
.	O

We	O
introduce	O
a	O
new	O
language	O
representation	O
model	O
called	O
BERT	B-MethodName
,	O
which	O
stands	O
for	O
Bidirectional	B-MethodName
Encoder	I-MethodName
Representations	I-MethodName
from	I-MethodName
Transformers	I-MethodName
.	O

For	O
example	O
,	O
in	O
OpenAI	B-MethodName
GPT	I-MethodName
,	O
the	O
authors	O
use	O
a	O
left	O
-	O
toright	O
architecture	O
,	O
where	O
every	O
token	O
can	O
only	O
attend	O
to	O
previous	O
tokens	O
in	O
the	O
self	O
-	O
attention	O
layers	O
of	O
the	O
Transformer	B-MethodName
(	O
Vaswani	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

(	O
2018	O
)	O
,	O
which	O
uses	O
unidirectional	O
language	O
models	O
for	O
pre	O
-	O
training	O
,	O
BERT	B-MethodName
uses	O
masked	O
language	O
models	O
to	O
enable	O
pretrained	O
deep	O
bidirectional	O
representations	O
.	O

When	O
integrating	O
contextual	O
word	O
embeddings	O
with	O
existing	O
task	O
-	O
specific	O
architectures	O
,	O
ELMo	B-MethodName
advances	O
the	O
state	O
of	O
the	O
art	O
for	O
several	O
major	O
NLP	O
benchmarks	O
(	O
Peters	O
et	O
al	O
.	O
,	O
2018a	O
)	O
including	O
question	B-TaskName
answering	I-TaskName
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O
2016	O
)	O
,	O
sentiment	B-TaskName
analysis	I-TaskName
(	O
Socher	O
et	O
al	O
.	O
,	O
2013	O
)	O
,	O
and	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
Tjong	O
Kim	O
Sang	O
and	O
De	O
Meulder	O
,	O
2003	O
)	O
.	O

(	O
2016	O
)	O
proposed	O
learning	O
contextual	O
representations	O
through	O
a	O
task	O
to	O
predict	O
a	O
single	O
word	O
from	O
both	O
left	O
and	O
right	O
context	O
using	O
LSTMs	B-MethodName
.	O
Similar	O
to	O
ELMo	B-MethodName
,	O
their	O
model	O
is	O
feature	O
-	O
based	O
and	O
not	O
deeply	O
bidirectional	O
.	O

Fedus	O
et	O
al	O
.	O

Unsupervised	O
Fine	O
-	O
tuning	O
Approaches	O
.	O

As	O
with	O
the	O
feature	O
-	O
based	O
approaches	O
,	O
the	O
first	O
works	O
in	O
this	O
direction	O
only	O
pre	O
-	O
trained	O
word	O
embedding	O
parameters	O
from	O
unlabeled	O
text	O
(	O
Collobert	O
and	O
Weston	O
,	O
2008	O
)	O
.	O

At	O
least	O
partly	O
due	O
to	O
this	O
advantage	O
,	O
OpenAI	B-MethodName
GPT	I-MethodName
(	O
Radford	O
et	O
al	O
.	O
,	O
2018	O
)	O
achieved	O
previously	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
many	O
sentencelevel	O
tasks	O
from	O
the	O
GLUE	B-DatasetName
benchmark	O
(	O
Wang	O
et	O
al	O
.	O
,	O
2018a	O
)	O
.	O

The	O
advantage	O
of	O
these	O
approaches	O
is	O
that	O
few	O
parameters	O
need	O
to	O
be	O
learned	O
from	O
scratch	O
.	O

Left	O
-	O
to	O
-	O
right	O
language	O
model	O
-	O
BERT	B-MethodName
BERT	B-MethodName
E	O
[	O
CLS	O
]	O
E	O
1	O
E	O
[	O
SEP	O
]	O
...	O

E	O
N	O
E	O
1	O
'	O
...	O

E	O
M	O
'	O
C	O
T	O
1	O
T	O
[	O
SEP	O
]	O
...	O

...	O

...	O

E	O
N	O
E	O
1	O
'	O
...	O

E	O
M	O
'	O
C	O
T	O
1	O
T	O
[	O
SEP	O
]	O
...	O

ing	O
and	O
auto	B-MethodName
-	I-MethodName
encoder	I-MethodName
objectives	O
have	O
been	O
used	O
for	O
pre	O
-	O
training	O
such	O
models	O
(	O
Howard	O
and	O
Ruder	O
,	O
2018;Radford	O
et	O
al	O
.	O
,	O
2018;Dai	O
and	O
Le	O
,	O
2015	O
)	O
.	O

Transfer	O
Learning	O
from	O
Supervised	O
Data	O
.	O

There	O
has	O
also	O
been	O
work	O
showing	O
effective	O
transfer	O
from	O
supervised	O
tasks	O
with	O
large	O
datasets	O
,	O
such	O
as	O
natural	B-TaskName
language	I-TaskName
inference	I-TaskName
(	O
Conneau	O
et	O
al	O
.	O
,	O
2017	O
)	O
and	O
machine	B-TaskName
translation	I-TaskName
(	O
McCann	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

BERT	B-MethodName
.	O

Computer	O
vision	O
research	O
has	O
also	O
demonstrated	O
the	O
importance	O
of	O
transfer	O
learning	O
from	O
large	O
pre	O
-	O
trained	O
models	O
,	O
where	O
an	O
effective	O
recipe	O
is	O
to	O
fine	O
-	O
tune	O
models	O
pre	O
-	O
trained	O
with	O
Ima	B-DatasetName
-	I-DatasetName
geNet	I-DatasetName
(	O
Deng	O
et	O
al	O
.	O
,	O
2009;Yosinski	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

We	O
introduce	O
BERT	B-MethodName
and	O
its	O
detailed	O
implementation	O
in	O
this	O
section	O
.	O

There	O
are	O
two	O
steps	O
in	O
our	O
framework	O
:	O
pre	O
-	O
training	O
and	O
fine	O
-	O
tuning	O
.	O

During	O
pre	O
-	O
training	O
,	O
the	O
model	O
is	O
trained	O
on	O
unlabeled	O
data	O
over	O
different	O
pre	O
-	O
training	O
tasks	O
.	O

For	O
finetuning	O
,	O
the	O
BERT	B-MethodName
model	O
is	O
first	O
initialized	O
with	O
the	O
pre	O
-	O
trained	O
parameters	O
,	O
and	O
all	O
of	O
the	O
parameters	O
are	O
fine	O
-	O
tuned	O
using	O
labeled	O
data	O
from	O
the	O
downstream	O
tasks	O
.	O

Each	O
downstream	O
task	O
has	O
separate	O
fine	O
-	O
tuned	O
models	O
,	O
even	O
though	O
they	O
are	O
initialized	O
with	O
the	O
same	O
pre	O
-	O
trained	O
parameters	O
.	O

A	O
distinctive	O
feature	O
of	O
BERT	B-MethodName
is	O
its	O
unified	O
architecture	O
across	O
different	O
tasks	O
.	O

The	O
question	B-TaskName
-	I-TaskName
answering	I-TaskName
example	O
in	O
Figure	O
1	O
will	O
serve	O
as	O
a	O
running	O
example	O
for	O
this	O
section	O
.	O

There	O
is	O
mini	O
-	O
mal	O
difference	O
between	O
the	O
pre	O
-	O
trained	O
architecture	O
and	O
the	O
final	O
downstream	O
architecture	O
.	O

(	O
2017	O
)	O
and	O
released	O
in	O
the	O
tensor2tensor	O
library	O
.	O

(	O
2017	O
)	O
as	O
well	O
as	O
excellent	O
guides	O
such	O
as	O
"	O
The	O
Annotated	O
Transformer	O
.	O
"	O
2	O
In	O
this	O
work	O
,	O
we	O
denote	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
layers	I-HyperparameterName
(	O
i.e.	O
,	O
Transformer	O
blocks	O
)	O
as	O
L	B-HyperparameterName
,	O
the	O
hidden	B-HyperparameterName
size	I-HyperparameterName
as	O
H	B-HyperparameterName
,	O
and	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
self	I-HyperparameterName
-	I-HyperparameterName
attention	I-HyperparameterName
heads	I-HyperparameterName
as	O
A.	B-HyperparameterName

3	O
We	O
primarily	O
report	O
results	O
on	O
two	O
model	O
sizes	O
:	O
BERT	B-MethodName
BASE	I-MethodName
(	O
L=12	B-HyperparameterName
,	O
H=768	B-HyperparameterName
,	O
A=12	B-HyperparameterValue
,	O
Total	B-HyperparameterName
Param	I-HyperparameterName
-	I-HyperparameterName
eters=110	I-HyperparameterName
M	B-HyperparameterValue
)	O
and	O
BERT	B-MethodName
LARGE	I-MethodName
(	O
L=24	B-HyperparameterName
,	O
H=1024	B-HyperparameterName
,	O
A=16	B-HyperparameterName
,	O
Total	B-HyperparameterName
Parameters=340	I-HyperparameterName
M	B-HyperparameterValue
)	O
.	O

BERT	B-MethodName
BASE	I-MethodName
was	O
chosen	O
to	O
have	O
the	O
same	O
model	O
size	O
as	O
OpenAI	B-MethodName
GPT	I-MethodName
for	O
comparison	O
purposes	O
.	O

Critically	O
,	O
however	O
,	O
the	O
BERT	B-MethodName
Transformer	O
uses	O
bidirectional	O
self	O
-	O
attention	O
,	O
while	O
the	O
GPT	B-MethodName
Transformer	O
uses	O
constrained	O
self	O
-	O
attention	O
where	O
every	O
token	O
can	O
only	O
attend	O
to	O
context	O
to	O
its	O
left	O
.	O

4	O
Input	O
/	O
Output	O
Representations	O
To	O
make	O
BERT	B-MethodName
handle	O
a	O
variety	O
of	O
down	O
-	O
stream	O
tasks	O
,	O
our	O
input	O
representation	O
is	O
able	O
to	O
unambiguously	O
represent	O
both	O
a	O
single	O
sentence	O
and	O
a	O
pair	O
of	O
sentences	O
(	O
e.g.	O
,	O
Question	O
,	O
Answer	O
)	O
in	O
one	O
token	O
sequence	O
.	O

Throughout	O
this	O
work	O
,	O
a	O
"	O
sentence	O
"	O
can	O
be	O
an	O
arbitrary	O
span	O
of	O
contiguous	O
text	O
,	O
rather	O
than	O
an	O
actual	O
linguistic	O
sentence	O
.	O

A	O
"	O
sequence	O
"	O
refers	O
to	O
the	O
input	O
token	O
sequence	O
to	O
BERT	B-MethodName
,	O
which	O
may	O
be	O
a	O
single	O
sentence	O
or	O
two	O
sentences	O
packed	O
together	O
.	O

We	O
use	O
WordPiece	O
embeddings	O
(	O
Wu	O
et	O
al	O
.	O
,	O
2016	O
)	O
with	O
a	O
30,000	O
token	O
vocabulary	O
.	O

The	O
first	O
token	O
of	O
every	O
sequence	O
is	O
always	O
a	O
special	O
classification	O
token	O
(	O
[	O
CLS	O
]	O
)	O
.	O

The	O
final	O
hidden	O
state	O
corresponding	O
to	O
this	O
token	O
is	O
used	O
as	O
the	O
aggregate	O
sequence	O
representation	O
for	O
classification	O
tasks	O
.	O

Sentence	O
pairs	O
are	O
packed	O
together	O
into	O
a	O
single	O
sequence	O
.	O

We	O
differentiate	O
the	O
sentences	O
in	O
two	O
ways	O
.	O

First	O
,	O
we	O
separate	O
them	O
with	O
a	O
special	O
token	O
(	O
[	O
SEP	O
]	O
)	O
.	O

Second	O
,	O
we	O
add	O
a	O
learned	O
embedding	O
to	O
every	O
token	O
indicating	O
whether	O
it	O
belongs	O
to	O
sentence	O
A	O
or	O
sentence	O
B.	O

As	O
shown	O
in	O
Figure	O
1	O
,	O
we	O
denote	O
input	O
embedding	O
as	O
E	O
,	O
the	O
final	O
hidden	O
vector	O
of	O
the	O
special	O
[	O
CLS	O
]	O
token	O
as	O
C	O
∈	O
R	O
H	O
,	O
and	O
the	O
final	O
hidden	O
vector	O
for	O
the	O
i	O
th	O
input	O
token	O
as	O
T	O
i	O
∈	O
R	O
H	O
.	O

For	O
a	O
given	O
token	O
,	O
its	O
input	O
representation	O
is	O
constructed	O
by	O
summing	O
the	O
corresponding	O
token	O
,	O
segment	O
,	O
and	O
position	O
embeddings	O
.	O

A	O
visualization	O
of	O
this	O
construction	O
can	O
be	O
seen	O
in	O
Figure	O
2	O
.	O

Pre	O
-	O
training	O
BERT	B-MethodName
.	O

Unlike	O
Peters	O
et	O
al	O
.	O

(	O
2018a	O
)	O
and	O
Radford	O
et	O
al	O
.	O

(	O
2018	O
)	O
,	O
we	O
do	O
not	O
use	O
traditional	O
left	O
-	O
to	O
-	O
right	O
or	O
right	O
-	O
to	O
-	O
left	O
language	O
models	O
to	O
pre	O
-	O
train	O
BERT	B-MethodName
.	O

Instead	O
,	O
we	O
pre	O
-	O
train	O
BERT	B-MethodName
using	O
two	O
unsupervised	O
tasks	O
,	O
described	O
in	O
this	O
section	O
.	O

This	O
step	O
is	O
presented	O
in	O
the	O
left	O
part	O
of	O
Figure	O
1	O
.	O

Unfortunately	O
,	O
standard	O
conditional	O
language	O
models	O
can	O
only	O
be	O
trained	O
left	O
-	O
to	O
-	O
right	O
or	O
right	O
-	O
to	O
-	O
left	O
,	O
since	O
bidirectional	O
conditioning	O
would	O
allow	O
each	O
word	O
to	O
indirectly	O
"	O
see	O
itself	O
"	O
,	O
and	O
the	O
model	O
could	O
trivially	O
predict	O
the	O
target	O
word	O
in	O
a	O
multi	O
-	O
layered	O
context	O
.	O

former	O
is	O
often	O
referred	O
to	O
as	O
a	O
"	O
Transformer	O
encoder	O
"	O
while	O
the	O
left	O
-	O
context	O
-	O
only	O
version	O
is	O
referred	O
to	O
as	O
a	O
"	O
Transformer	O
decoder	O
"	O
since	O
it	O
can	O
be	O
used	O
for	O
text	O
generation	O
.	O

In	O
order	O
to	O
train	O
a	O
deep	O
bidirectional	O
representation	O
,	O
we	O
simply	O
mask	O
some	O
percentage	O
of	O
the	O
input	O
tokens	O
at	O
random	O
,	O
and	O
then	O
predict	O
those	O
masked	O
tokens	O
.	O

In	O
this	O
case	O
,	O
the	O
final	O
hidden	O
vectors	O
corresponding	O
to	O
the	O
mask	O
tokens	O
are	O
fed	O
into	O
an	O
output	O
softmax	O
over	O
the	O
vocabulary	O
,	O
as	O
in	O
a	O
standard	O
LM	O
.	O

In	O
contrast	O
to	O
denoising	B-MethodName
auto	I-MethodName
-	I-MethodName
encoders	I-MethodName
(	O
Vincent	O
et	O
al	O
.	O
,	O
2008	O
)	O
,	O
we	O
only	O
predict	O
the	O
masked	O
words	O
rather	O
than	O
reconstructing	O
the	O
entire	O
input	O
.	O

Although	O
this	O
allows	O
us	O
to	O
obtain	O
a	O
bidirectional	O
pre	O
-	O
trained	O
model	O
,	O
a	O
downside	O
is	O
that	O
we	O
are	O
creating	O
a	O
mismatch	O
between	O
pre	O
-	O
training	O
and	O
fine	O
-	O
tuning	O
,	O
since	O
the	O
[	O
MASK	O
]	O
token	O
does	O
not	O
appear	O
during	O
fine	O
-	O
tuning	O
.	O

To	O
mitigate	O
this	O
,	O
we	O
do	O
not	O
always	O
replace	O
"	O
masked	O
"	O
words	O
with	O
the	O
actual	O
[	O
MASK	O
]	O
token	O
.	O

The	O
training	O
data	O
generator	O
chooses	O
15	O
%	O
of	O
the	O
token	O
positions	O
at	O
random	O
for	O
prediction	O
.	O

If	O
the	O
i	O
-	O
th	O
token	O
is	O
chosen	O
,	O
we	O
replace	O
the	O
i	O
-	O
th	O
token	O
with	O
(	O
1	O
)	O
the	O
[	O
MASK	O
]	O
token	O
80	O
%	O
of	O
the	O
time	O
(	O
2	O
)	O
a	O
random	O
token	O
10	O
%	O
of	O
the	O
time	O
(	O
3	O
)	O
the	O
unchanged	O
i	O
-	O
th	O
token	O
10	O
%	O
of	O
the	O
time	O
.	O

In	O
all	O
of	O
our	O
experiments	O
,	O
we	O
mask	O
15	O
%	O
of	O
all	O
WordPiece	O
tokens	O
in	O
each	O
sequence	O
at	O
random	O
.	O

Then	O
,	O
T	O
i	O
will	O
be	O
used	O
to	O
predict	O
the	O
original	O
token	O
with	O
cross	O
entropy	O
loss	O
.	O

In	O
order	O
to	O
train	O
a	O
model	O
that	O
understands	O
sentence	O
relationships	O
,	O
we	O
pre	O
-	O
train	O
for	O
a	O
binarized	O
next	O
sentence	O
prediction	O
task	O
that	O
can	O
be	O
trivially	O
generated	O
from	O
any	O
monolingual	O
corpus	O
.	O

Specifically	O
,	O
when	O
choosing	O
the	O
sentences	O
A	O
and	O
B	O
for	O
each	O
pretraining	O
example	O
,	O
50	O
%	O
of	O
the	O
time	O
B	O
is	O
the	O
actual	O
next	O
sentence	O
that	O
follows	O
A	O
(	O
labeled	O
as	O
IsNext	O
)	O
,	O
and	O
50	O
%	O
of	O
the	O
time	O
it	O
is	O
a	O
random	O
sentence	O
from	O
the	O
corpus	O
(	O
labeled	O
as	O
NotNext	O
)	O
.	O

5	O
Despite	O
its	O
simplicity	O
,	O
we	O
demonstrate	O
in	O
Section	O
5.1	O
that	O
pre	O
-	O
training	O
towards	O
this	O
task	O
is	O
very	O
beneficial	O
to	O
both	O
QA	B-TaskName
and	O
NLI	B-TaskName
.	O

(	O
2017	O
)	O
and	O
Logeswaran	O
and	O
Lee	O
(	O
2018	O
)	O
.	O

However	O
,	O
in	O
prior	O
work	O
,	O
only	O
sentence	O
embeddings	O
are	O
transferred	O
to	O
down	O
-	O
stream	O
tasks	O
,	O
where	O
BERT	B-MethodName
transfers	O
all	O
parameters	O
to	O
initialize	O
end	O
-	O
task	O
model	O
parameters	O
.	O

Pre	O
-	O
training	O
data	O
.	O

The	O
pre	O
-	O
training	O
procedure	O
largely	O
follows	O
the	O
existing	O
literature	O
on	O
language	O
model	O
pre	O
-	O
training	O
.	O

For	O
the	O
pre	O
-	O
training	O
corpus	O
we	O
use	O
the	O
BooksCorpus	B-DatasetName
(	O
800	O
M	O
words	O
)	O
(	O
Zhu	O
et	O
al	O
.	O
,	O
2015	O
)	O
and	O
English	B-DatasetName
Wikipedia	I-DatasetName
(	O
2,500	O
M	O
words	O
)	O
.	O

For	O
Wikipedia	O
we	O
extract	O
only	O
the	O
text	O
passages	O
and	O
ignore	O
lists	O
,	O
tables	O
,	O
and	O
headers	O
.	O

It	O
is	O
critical	O
to	O
use	O
a	O
document	O
-	O
level	O
corpus	O
rather	O
than	O
a	O
shuffled	O
sentence	O
-	O
level	O
corpus	O
such	O
as	O
the	O
Billion	B-DatasetName
Word	I-DatasetName
Benchmark	I-DatasetName
(	O
Chelba	O
et	O
al	O
.	O
,	O
2013	O
)	O
in	O
order	O
to	O
extract	O
long	O
contiguous	O
sequences	O
.	O

Fine	O
-	O
tuning	O
BERT	O
.	O

Fine	O
-	O
tuning	O
is	O
straightforward	O
since	O
the	O
selfattention	O
mechanism	O
in	O
the	O
Transformer	B-MethodName
allows	O
BERT	B-MethodName
to	O
model	O
many	O
downstream	O
taskswhether	O
they	O
involve	O
single	O
text	O
or	O
text	O
pairs	O
-	O
by	O
swapping	O
out	O
the	O
appropriate	O
inputs	O
and	O
outputs	O
.	O

For	O
applications	O
involving	O
text	O
pairs	O
,	O
a	O
common	O
pattern	O
is	O
to	O
independently	O
encode	O
text	O
pairs	O
before	O
applying	O
bidirectional	O
cross	O
attention	O
,	O
such	O
as	O
Parikh	O
et	O
al	O
.	O

(	O
2016	O
)	O
;	O
Seo	O
et	O
al	O
.	O

(	O
2017	O
)	O
.	O

BERT	B-MethodName
instead	O
uses	O
the	O
self	O
-	O
attention	O
mechanism	O
to	O
unify	O
these	O
two	O
stages	O
,	O
as	O
encoding	O
a	O
concatenated	O
text	O
pair	O
with	O
self	O
-	O
attention	O
effectively	O
includes	O
bidirectional	O
cross	O
attention	O
between	O
two	O
sentences	O
.	O

At	O
the	O
input	O
,	O
sentence	O
A	O
and	O
sentence	O
B	O
from	O
pre	O
-	O
training	O
are	O
analogous	O
to	O
(	O
1	O
)	O
sentence	O
pairs	O
in	O
paraphrasing	O
,	O
(	O
2	O
)	O
hypothesis	O
-	O
premise	O
pairs	O
in	O
entailment	B-TaskName
,	O
(	O
3	O
)	O
question	O
-	O
passage	O
pairs	O
in	O
question	B-TaskName
answering	I-TaskName
,	O
and	O
(	O
4	O
)	O
a	O
degenerate	O
text-∅	O
pair	O
in	O
text	B-TaskName
classification	I-TaskName
or	O
sequence	B-TaskName
tagging	I-TaskName
.	O

For	O
each	O
task	O
,	O
we	O
simply	O
plug	O
in	O
the	O
taskspecific	O
inputs	O
and	O
outputs	O
into	O
BERT	B-MethodName
and	O
finetune	O
all	O
the	O
parameters	O
end	O
-	O
to	O
-	O
end	O
.	O

At	O
the	O
output	O
,	O
the	O
token	O
representations	O
are	O
fed	O
into	O
an	O
output	O
layer	O
for	O
tokenlevel	O
tasks	O
,	O
such	O
as	O
sequence	B-TaskName
tagging	I-TaskName
or	O
question	B-TaskName
answering	I-TaskName
,	O
and	O
the	O
[	O
CLS	O
]	O
representation	O
is	O
fed	O
into	O
an	O
output	O
layer	O
for	O
classification	O
,	O
such	O
as	O
entailment	B-TaskName
or	O
sentiment	B-TaskName
analysis	I-TaskName
.	O

Compared	O
to	O
pre	O
-	O
training	O
,	O
fine	O
-	O
tuning	O
is	O
relatively	O
inexpensive	O
.	O

All	O
of	O
the	O
results	O
in	O
the	O
paper	O
can	O
be	O
replicated	O
in	O
at	O
most	O
1	O
hour	O
on	O
a	O
single	O
Cloud	O
TPU	O
,	O
or	O
a	O
few	O
hours	O
on	O
a	O
GPU	O
,	O
starting	O
from	O
the	O
exact	O
same	O
pre	O
-	O
trained	O
model	O
.	O

7	O
We	O
describe	O
the	O
task	O
-	O
specific	O
details	O
in	O
the	O
corresponding	O
subsections	O
of	O
Section	O
4	O
.	O

More	O
details	O
can	O
be	O
found	O
in	O
Appendix	O
A.5	O
.	O
Experiments	O
.	O

In	O
this	O
section	O
,	O
we	O
present	O
BERT	B-MethodName
fine	O
-	O
tuning	O
results	O
on	O
11	O
NLP	O
tasks	O
.	O

GLUE	B-DatasetName
.	O

Detailed	O
descriptions	O
of	O
GLUE	B-DatasetName
datasets	O
are	O
included	O
in	O
Appendix	O
B.1	O
.	O
To	O
fine	O
-	O
tune	O
on	O
GLUE	B-DatasetName
,	O
we	O
represent	O
the	O
input	O
sequence	O
(	O
for	O
single	O
sentence	O
or	O
sentence	O
pairs	O
)	O
as	O
described	O
in	O
Section	O
3	O
,	O
and	O
use	O
the	O
final	O
hidden	O
vector	O
C	O
∈	O
R	O
H	O
corresponding	O
to	O
the	O
first	O
input	O
token	O
(	O
[	O
CLS	O
]	O
)	O
as	O
the	O
aggregate	O
representation	O
.	O

The	O
only	O
new	O
parameters	O
introduced	O
during	O
fine	O
-	O
tuning	O
are	O
classification	O
layer	O
weights	O
W	O
∈	O
R	O
K×H	O
,	O
where	O
K	O
is	O
the	O
number	O
of	O
labels	O
.	O

We	O
compute	O
a	O
standard	O
classification	O
loss	O
with	O
C	O
and	O
W	O
,	O
i.e.	O
,	O
log(softmax(CW	O
T	O
)	O
)	O
.	O

We	O
use	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
32	B-HyperparameterValue
and	O
fine	O
-	O
tune	O
for	O
3	O
epochs	O
over	O
the	O
data	O
for	O
all	O
GLUE	B-DatasetName
tasks	O
.	O

For	O
each	O
task	O
,	O
we	O
selected	O
the	O
best	O
fine	O
-	O
tuning	O
learning	B-HyperparameterName
rate	I-HyperparameterName
(	O
among	O
5e-5	B-HyperparameterValue
,	O
4e-5	B-HyperparameterValue
,	O
3e-5	B-HyperparameterValue
,	O
and	O
2e-5	B-HyperparameterValue
)	O
on	O
the	O
Dev	O
set	O
.	O

Additionally	O
,	O
for	O
BERT	B-MethodName
LARGE	I-MethodName
we	O
found	O
that	O
finetuning	O
was	O
sometimes	O
unstable	O
on	O
small	O
datasets	O
,	O
so	O
we	O
ran	O
several	O
random	O
restarts	O
and	O
selected	O
the	O
best	O
model	O
on	O
the	O
Dev	O
set	O
.	O

With	O
random	O
restarts	O
,	O
we	O
use	O
the	O
same	O
pre	O
-	O
trained	O
checkpoint	O
but	O
perform	O
different	O
fine	O
-	O
tuning	O
data	O
shuffling	O
and	O
classifier	O
layer	O
initialization	O
.	O

9	O
Results	O
are	O
presented	O
in	O
Table	O
1	O
.	O

The	O
General	B-DatasetName
Language	I-DatasetName
Understanding	I-DatasetName
Evaluation	I-DatasetName
(	O
GLUE	B-DatasetName
)	O
benchmark	O
(	O
Wang	O
et	O
al	O
.	O
,	O
2018a	O
)	O
is	O
a	O
collection	O
of	O
diverse	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
tasks	O
.	O

Both	O
BERT	B-MethodName
BASE	I-MethodName
and	O
BERT	B-MethodName
LARGE	I-MethodName
outperform	O
all	O
systems	O
on	O
all	O
tasks	O
by	O
a	O
substantial	O
margin	O
,	O
obtaining	O
4.5	B-MetricValue
%	I-MetricValue
and	O
7.0	B-MetricValue
%	I-MetricValue
respective	O
average	O
accuracy	B-MethodName
improvement	O
over	O
the	O
prior	O
state	O
of	O
the	O
art	O
.	O

Note	O
that	O
BERT	B-MethodName
BASE	I-MethodName
and	O
OpenAI	B-MethodName
GPT	I-MethodName
are	O
nearly	O
identical	O
in	O
terms	O
of	O
model	O
architecture	O
apart	O
from	O
the	O
attention	O
masking	O
.	O

On	O
the	O
official	O
GLUE	B-DatasetName
leaderboard	O
10	O
,	O
BERT	B-MethodName
LARGE	I-MethodName
obtains	O
a	O
score	B-MetricName
of	O
80.5	B-MetricValue
,	O
compared	O
to	O
OpenAI	B-MethodName
GPT	I-MethodName
,	O
which	O
obtains	O
72.8	B-MetricValue
as	O
of	O
the	O
date	O
of	O
writing	O
.	O

For	O
the	O
largest	O
and	O
most	O
widely	O
reported	O
GLUE	B-DatasetName
task	O
,	O
MNLI	B-DatasetName
,	O
BERT	B-MethodName
obtains	O
a	O
4.6	B-MetricValue
%	I-MetricValue
absolute	O
accuracy	B-MetricName
improvement	O
.	O

We	O
find	O
that	O
BERT	B-MethodName
LARGE	I-MethodName
significantly	O
outperforms	O
BERT	B-MethodName
BASE	I-MethodName
across	O
all	O
tasks	O
,	O
especially	O
those	O
with	O
very	O
little	O
training	O
data	O
.	O

The	O
effect	O
of	O
model	O
size	O
is	O
explored	O
more	O
thoroughly	O
in	O
Section	O
5.2	O
.	O
SQuAD	B-DatasetName
v1.1	I-DatasetName
.	O
The	O
Stanford	B-DatasetName
Question	I-DatasetName
Answering	I-DatasetName
Dataset	I-DatasetName
(	O
SQuAD	B-DatasetName
v1.1	I-DatasetName
)	O
is	O
a	O
collection	O
of	O
100k	O
crowdsourced	O
question	O
/	O
answer	O
pairs	O
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

Given	O
a	O
question	O
and	O
a	O
passage	O
from	O
9	O
The	O
GLUE	B-DatasetName
data	O
set	O
distribution	O
does	O
not	O
include	O
the	O
Test	O
labels	O
,	O
and	O
we	O
only	O
made	O
a	O
single	O
GLUE	B-DatasetName
evaluation	O
server	O
submission	O
for	O
each	O
of	O
BERTBASE	B-MethodName
and	O
BERTLARGE	B-MethodName
.	O

10	O
https://gluebenchmark.com/leaderboard	O
Wikipedia	O
containing	O
the	O
answer	O
,	O
the	O
task	O
is	O
to	O
predict	O
the	O
answer	O
text	O
span	O
in	O
the	O
passage	O
.	O

As	O
shown	O
in	O
Figure	O
1	O
,	O
in	O
the	O
question	B-TaskName
answering	I-TaskName
task	O
,	O
we	O
represent	O
the	O
input	O
question	O
and	O
passage	O
as	O
a	O
single	O
packed	O
sequence	O
,	O
with	O
the	O
question	O
using	O
the	O
A	O
embedding	O
and	O
the	O
passage	O
using	O
the	O
B	O
embedding	O
.	O

We	O
only	O
introduce	O
a	O
start	O
vector	O
S	O
∈	O
R	O
H	O
and	O
an	O
end	O
vector	O
E	O
∈	O
R	O
H	O
during	O
fine	O
-	O
tuning	O
.	O

The	O
probability	O
of	O
word	O
i	O
being	O
the	O
start	O
of	O
the	O
answer	O
span	O
is	O
computed	O
as	O
a	O
dot	O
product	O
between	O
T	O
i	O
and	O
S	O
followed	O
by	O
a	O
softmax	O
over	O
all	O
of	O
the	O
words	O
in	O
the	O
paragraph	O
:	O
P	O
i	O
=	O
e	O
S•T	O
i	O
j	O
e	O
S•T	O
j	O
.	O

The	O
analogous	O
formula	O
is	O
used	O
for	O
the	O
end	O
of	O
the	O
answer	O
span	O
.	O

The	O
score	O
of	O
a	O
candidate	O
span	O
from	O
position	O
i	O
to	O
position	O
j	O
is	O
defined	O
as	O
S•T	O
i	O
+	O
E•T	O
j	O
,	O
and	O
the	O
maximum	O
scoring	O
span	O
where	O
j	O
≥	O
i	O
is	O
used	O
as	O
a	O
prediction	O
.	O

The	O
training	O
objective	O
is	O
the	O
sum	O
of	O
the	O
log	O
-	O
likelihoods	O
of	O
the	O
correct	O
start	O
and	O
end	O
positions	O
.	O

We	O
fine	O
-	O
tune	O
for	O
3	O
epochs	O
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
5e-5	B-HyperparameterValue
and	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
32	B-HyperparameterValue
.	O

Table	O
2	O
shows	O
top	O
leaderboard	O
entries	O
as	O
well	O
as	O
results	O
from	O
top	O
published	O
systems	O
(	O
Seo	O
et	O
al	O
.	O
,	O
2017;Clark	O
and	O
Gardner	O
,	O
2018;Peters	O
et	O
al	O
.	O
,	O
2018a;Hu	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

The	O
top	O
results	O
from	O
the	O
SQuAD	B-DatasetName
leaderboard	O
do	O
not	O
have	O
up	O
-	O
to	O
-	O
date	O
public	O
system	O
descriptions	O
available	O
,	O
11	O
and	O
are	O
allowed	O
to	O
use	O
any	O
public	O
data	O
when	O
training	O
their	O
systems	O
.	O

We	O
therefore	O
use	O
modest	O
data	O
augmentation	O
in	O
our	O
system	O
by	O
first	O
fine	O
-	O
tuning	O
on	O
TriviaQA	B-DatasetName
(	O
Joshi	O
et	O
al	O
.	O
,	O
2017	O
)	O
befor	O
fine	O
-	O
tuning	O
on	O
SQuAD	B-DatasetName
.	O

Our	O
best	O
performing	O
system	O
outperforms	O
the	O
top	O
leaderboard	O
system	O
by	O
+1.5	B-MetricValue
F1	B-MetricName
in	O
ensembling	O
and	O
+1.3	B-MetricValue
F1	B-MetricName
as	O
a	O
single	O
system	O
.	O

In	O
fact	O
,	O
our	O
single	O
BERT	B-MethodName
model	O
outperforms	O
the	O
top	O
ensemble	O
system	O
in	O
terms	O
of	O
F1	B-MetricName
score	O
.	O

tuning	O
data	O
,	O
we	O
only	O
lose	O
0.1	B-MetricValue
-	O
0.4	B-MetricValue
F1	B-MetricName
,	O
still	O
outperforming	O
all	O
existing	O
systems	O
by	O
a	O
wide	O
margin	O
.	O

12	O
4.3	O
SQuAD	B-DatasetName
v2.0	I-DatasetName
The	O
SQuAD	B-DatasetName
2.0	I-DatasetName
task	O
extends	O
the	O
SQuAD	B-DatasetName
1.1	I-DatasetName
problem	O
definition	O
by	O
allowing	O
for	O
the	O
possibility	O
that	O
no	O
short	O
answer	O
exists	O
in	O
the	O
provided	O
paragraph	O
,	O
making	O
the	O
problem	O
more	O
realistic	O
.	O

We	O
use	O
a	O
simple	O
approach	O
to	O
extend	O
the	O
SQuAD	B-DatasetName
v1.1	I-DatasetName
BERT	B-MethodName
model	O
for	O
this	O
task	O
.	O

We	O
treat	O
questions	O
that	O
do	O
not	O
have	O
an	O
answer	O
as	O
having	O
an	O
answer	O
span	O
with	O
start	O
and	O
end	O
at	O
the	O
[	O
CLS	O
]	O
token	O
.	O

The	O
probability	O
space	O
for	O
the	O
start	O
and	O
end	O
answer	O
span	O
positions	O
is	O
extended	O
to	O
include	O
the	O
position	O
of	O
the	O
[	O
CLS	O
]	O
token	O
.	O

For	O
prediction	O
,	O
we	O
compare	O
the	O
score	O
of	O
the	O
no	O
-	O
answer	O
span	O
:	O
s	O
null	O
=	O
S•C	O
+	O
E•C	O
to	O
the	O
score	O
of	O
the	O
best	O
non	O
-	O
null	O
span	O
12	O
The	O
TriviaQA	B-DatasetName
data	O
we	O
used	O
consists	O
of	O
paragraphs	O
from	O
TriviaQA	B-DatasetName
-	I-DatasetName
Wiki	I-DatasetName
formed	O
of	O
the	O
first	O
400	O
tokens	O
in	O
documents	O
,	O
that	O
contain	O
at	O
least	O
one	O
of	O
the	O
provided	O
possible	O
answers	O
.	O

ŝ	O
i	O
,	O
j	O
=	O
max	O
j≥i	O
S•T	O
i	O
+	O
E•T	O
j	O
.	O

We	O
predict	O
a	O
non	O
-	O
null	O
answer	O
when	O
ŝ	O
i	O
,	O
j	O
>	O
s	O
null	O
+	O
τ	O
,	O
where	O
the	O
threshold	O
τ	O
is	O
selected	O
on	O
the	O
dev	O
set	O
to	O
maximize	O
F1	B-MetricName
.	O

We	O
did	O
not	O
use	O
TriviaQA	B-DatasetName
data	O
for	O
this	O
model	O
.	O

We	O
fine	O
-	O
tuned	O
for	O
2	O
epochs	O
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
5e-5	B-HyperparameterValue
and	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
48	B-HyperparameterValue
.	O

System	O
.	O

The	O
results	O
compared	O
to	O
prior	O
leaderboard	O
entries	O
and	O
top	O
published	O
work	O
(	O
Sun	O
et	O
al	O
.	O
,	O
2018;Wang	O
et	O
al	O
.	O
,	O
2018b	O
)	O
are	O
shown	O
in	O
Table	O
3	O
,	O
excluding	O
systems	O
that	O
use	O
BERT	B-MethodName
as	O
one	O
of	O
their	O
components	O
.	O

We	O
observe	O
a	O
+5.1	B-MetricValue
F1	B-MetricName
improvement	O
over	O
the	O
previous	O
best	O
system	O
.	O

SWAG	B-DatasetName
.	O

The	O
Situations	B-DatasetName
With	I-DatasetName
Adversarial	I-DatasetName
Generations	I-DatasetName
(	O
SWAG	B-DatasetName
)	O
dataset	O
contains	O
113k	O
sentence	O
-	O
pair	O
completion	O
examples	O
that	O
evaluate	O
grounded	O
commonsense	O
inference	O
(	O
Zellers	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

Given	O
a	O
sentence	O
,	O
the	O
task	O
is	O
to	O
choose	O
the	O
most	O
plausible	O
continuation	O
among	O
four	O
choices	O
.	O

When	O
fine	O
-	O
tuning	O
on	O
the	O
SWAG	B-DatasetName
dataset	O
,	O
we	O
construct	O
four	O
input	O
sequences	O
,	O
each	O
containing	O
the	O
concatenation	O
of	O
the	O
given	O
sentence	O
(	O
sentence	O
A	O
)	O
and	O
a	O
possible	O
continuation	O
(	O
sentence	O
B	O
)	O
.	O

The	O
only	O
task	O
-	O
specific	O
parameters	O
introduced	O
is	O
a	O
vector	O
whose	O
dot	O
product	O
with	O
the	O
[	O
CLS	O
]	O
token	O
representation	O
C	O
denotes	O
a	O
score	O
for	O
each	O
choice	O
which	O
is	O
normalized	O
with	O
a	O
softmax	O
layer	O
.	O

We	O
fine	O
-	O
tune	O
the	O
model	O
for	O
3	O
epochs	O
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
2e-5	B-HyperparameterValue
and	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
16	B-HyperparameterValue
.	O

Results	O
are	O
presented	O
in	O
Table	O
4	O
.	O

BERT	B-MethodName
LARGE	I-MethodName
outperforms	O
the	O
authors	O
'	O
baseline	O
ESIM+ELMo	B-MethodName
system	O
by	O
+27.1	B-MetricValue
%	I-MetricValue
and	O
OpenAI	B-MethodName
GPT	I-MethodName
by	O
8.3	B-MetricValue
%	I-MetricValue
.	O

Ablation	O
Studies	O
.	O

In	O
this	O
section	O
,	O
we	O
perform	O
ablation	O
experiments	O
over	O
a	O
number	O
of	O
facets	O
of	O
BERT	B-MethodName
in	O
order	O
to	O
better	O
understand	O
their	O
relative	O
importance	O
.	O

Additional	O
ablation	O
studies	O
can	O
be	O
found	O
in	O
Appendix	O
C.	O

Effect	O
of	O
Pre	O
-	O
training	O
Tasks	O
.	O

We	O
demonstrate	O
the	O
importance	O
of	O
the	O
deep	O
bidirectionality	O
of	O
BERT	B-MethodName
by	O
evaluating	O
two	O
pretraining	O
objectives	O
using	O
exactly	O
the	O
same	O
pretraining	O
data	O
,	O
fine	O
-	O
tuning	O
scheme	O
,	O
and	O
hyperparameters	O
as	O
BERT	B-MethodName
BASE	I-MethodName
:	O
No	O
NSP	O
:	O
A	O
bidirectional	O
model	O
which	O
is	O
trained	O
using	O
the	O
"	O
masked	O
LM	O
"	O
(	O
MLM	O
)	O
but	O
without	O
the	O
"	O
next	B-TaskName
sentence	I-TaskName
prediction	I-TaskName
"	O
(	O
NSP	B-TaskName
)	O
task	O
.	O

LTR	O
&	O
No	O
NSP	B-TaskName
:	O
.	O

A	O
left	O
-	O
context	O
-	O
only	O
model	O
which	O
is	O
trained	O
using	O
a	O
standard	O
Left	O
-	O
to	O
-	O
Right	O
(	O
LTR	O
)	O
LM	O
,	O
rather	O
than	O
an	O
MLM	O
.	O

The	O
left	O
-	O
only	O
constraint	O
was	O
also	O
applied	O
at	O
fine	O
-	O
tuning	O
,	O
because	O
removing	O
it	O
introduced	O
a	O
pre	O
-	O
train	O
/	O
fine	O
-	O
tune	O
mismatch	O
that	O
degraded	O
downstream	O
performance	O
.	O

Additionally	O
,	O
this	O
model	O
was	O
pre	O
-	O
trained	O
without	O
the	O
NSP	B-TaskName
task	O
.	O

This	O
is	O
directly	O
comparable	O
to	O
OpenAI	B-MethodName
GPT	I-MethodName
,	O
but	O
using	O
our	O
larger	O
training	O
dataset	O
,	O
our	O
input	O
representation	O
,	O
and	O
our	O
fine	O
-	O
tuning	O
scheme	O
.	O

We	O
first	O
examine	O
the	O
impact	O
brought	O
by	O
the	O
NSP	B-TaskName
task	O
.	O

In	O
Table	O
5	O
,	O
we	O
show	O
that	O
removing	O
NSP	B-TaskName
hurts	O
performance	O
significantly	O
on	O
QNLI	B-DatasetName
,	O
MNLI	B-DatasetName
,	O
and	O
SQuAD	B-DatasetName
1.1	I-DatasetName
.	O
Next	O
,	O
we	O
evaluate	O
the	O
impact	O
of	O
training	O
bidirectional	O
representations	O
by	O
comparing	O
"	O
No	O
NSP	B-TaskName
"	O
to	O
"	O
LTR	O
&	O
No	O
NSP	B-TaskName
"	O
.	O

The	O
LTR	O
model	O
performs	O
worse	O
than	O
the	O
MLM	O
model	O
on	O
all	O
tasks	O
,	O
with	O
large	O
drops	O
on	O
MRPC	B-DatasetName
and	O
SQuAD	B-DatasetName
.	O

For	O
SQuAD	B-DatasetName
it	O
is	O
intuitively	O
clear	O
that	O
a	O
LTR	O
model	O
will	O
perform	O
poorly	O
at	O
token	O
predictions	O
,	O
since	O
the	O
token	O
-	O
level	O
hidden	O
states	O
have	O
no	O
rightside	O
context	O
.	O

In	O
order	O
to	O
make	O
a	O
good	O
faith	O
attempt	O
at	O
strengthening	O
the	O
LTR	O
system	O
,	O
we	O
added	O
a	O
randomly	O
initialized	O
BiLSTM	B-MethodName
on	O
top	O
.	O

This	O
does	O
significantly	O
improve	O
results	O
on	O
SQuAD	B-DatasetName
,	O
but	O
the	O
results	O
are	O
still	O
far	O
worse	O
than	O
those	O
of	O
the	O
pretrained	O
bidirectional	O
models	O
.	O

The	O
BiLSTM	B-MethodName
hurts	O
performance	O
on	O
the	O
GLUE	B-DatasetName
tasks	O
.	O

However	O
:	O
(	O
a	O
)	O
this	O
is	O
twice	O
as	O
expensive	O
as	O
a	O
single	O
bidirectional	O
model	O
;	O
(	O
b	O
)	O
this	O
is	O
non	O
-	O
intuitive	O
for	O
tasks	O
like	O
QA	B-TaskName
,	O
since	O
the	O
RTL	O
model	O
would	O
not	O
be	O
able	O
to	O
condition	O
the	O
answer	O
on	O
the	O
question	O
;	O
(	O
c	O
)	O
this	O
it	O
is	O
strictly	O
less	O
powerful	O
than	O
a	O
deep	O
bidirectional	O
model	O
,	O
since	O
it	O
can	O
use	O
both	O
left	O
and	O
right	O
context	O
at	O
every	O
layer	O
.	O

We	O
recognize	O
that	O
it	O
would	O
also	O
be	O
possible	O
to	O
train	O
separate	O
LTR	O
and	O
RTL	O
models	O
and	O
represent	O
each	O
token	O
as	O
the	O
concatenation	O
of	O
the	O
two	O
models	O
,	O
as	O
ELMo	B-MethodName
does	O
.	O

Effect	O
of	O
Model	O
Size	O
.	O

In	O
this	O
section	O
,	O
we	O
explore	O
the	O
effect	O
of	O
model	O
size	O
on	O
fine	O
-	O
tuning	O
task	O
accuracy	O
.	O

Results	O
on	O
selected	O
GLUE	B-DatasetName
tasks	O
are	O
shown	O
in	O
Table	O
6	O
.	O

In	O
this	O
table	O
,	O
we	O
report	O
the	O
average	O
Dev	O
Set	O
accuracy	B-MetricName
from	O
5	O
random	O
restarts	O
of	O
fine	O
-	O
tuning	O
.	O

We	O
can	O
see	O
that	O
larger	O
models	O
lead	O
to	O
a	O
strict	O
accuracy	O
improvement	O
across	O
all	O
four	O
datasets	O
,	O
even	O
for	O
MRPC	B-DatasetName
which	O
only	O
has	O
3,600	O
labeled	O
training	O
examples	O
,	O
and	O
is	O
substantially	O
different	O
from	O
the	O
pre	O
-	O
training	O
tasks	O
.	O

It	O
is	O
also	O
perhaps	O
surprising	O
that	O
we	O
are	O
able	O
to	O
achieve	O
such	O
significant	O
improvements	O
on	O
top	O
of	O
models	O
which	O
are	O
already	O
quite	O
large	O
relative	O
to	O
the	O
existing	O
literature	O
.	O

For	O
example	O
,	O
the	O
largest	O
Transformer	B-MethodName
explored	O
in	O
Vaswani	O
et	O
al	O
.	O

(	O
2017	O
)	O
is	O
(	O
L=6	B-HyperparameterName
,	O
H=1024	B-HyperparameterName
,	O
A=16	B-HyperparameterName
)	O
with	O
100	O
M	O
parameters	O
for	O
the	O
encoder	O
,	O
and	O
the	O
largest	O
Transformer	O
we	O
have	O
found	O
in	O
the	O
literature	O
is	O
(	O
L=64	B-HyperparameterName
,	O
H=512	B-HyperparameterName
,	O
A=2	B-HyperparameterName
)	O
with	O
235	O
M	O
parameters	O
(	O
Al	O
-	O
Rfou	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

By	O
contrast	O
,	O
BERT	B-MethodName
BASE	I-MethodName
contains	O
110	O
M	O
parameters	O
and	O
BERT	B-MethodName
LARGE	I-MethodName
contains	O
340	O
M	O
parameters	O
.	O

It	O
has	O
long	O
been	O
known	O
that	O
increasing	O
the	O
model	O
size	O
will	O
lead	O
to	O
continual	O
improvements	O
on	O
large	O
-	O
scale	O
tasks	O
such	O
as	O
machine	B-TaskName
translation	I-TaskName
and	O
language	O
modeling	O
,	O
which	O
is	O
demonstrated	O
by	O
the	O
LM	O
perplexity	O
of	O
held	O
-	O
out	O
training	O
data	O
shown	O
in	O
Table	O
6	O
.	O

However	O
,	O
we	O
believe	O
that	O
this	O
is	O
the	O
first	O
work	O
to	O
demonstrate	O
convincingly	O
that	O
scaling	O
to	O
extreme	O
model	O
sizes	O
also	O
leads	O
to	O
large	O
improvements	O
on	O
very	O
small	O
scale	O
tasks	O
,	O
provided	O
that	O
the	O
model	O
has	O
been	O
sufficiently	O
pre	O
-	O
trained	O
.	O

Peters	O
et	O
al	O
.	O

(	O
2018b	O
)	O
presented	O
mixed	O
results	O
on	O
the	O
downstream	O
task	O
impact	O
of	O
increasing	O
the	O
pre	O
-	O
trained	O
bi	O
-	O
LM	O
size	O
from	O
two	O
to	O
four	O
layers	O
and	O
Melamud	O
et	O
al	O
.	O

(	O
2016	O
)	O
mentioned	O
in	O
passing	O
that	O
increasing	O
hidden	B-HyperparameterName
dimension	I-HyperparameterName
size	I-HyperparameterName
from	O
200	B-HyperparameterValue
to	O
600	B-HyperparameterValue
helped	O
,	O
but	O
increasing	O
further	O
to	O
1,000	B-HyperparameterValue
did	O
not	O
bring	O
further	O
improvements	O
.	O

Both	O
of	O
these	O
prior	O
works	O
used	O
a	O
featurebased	O
approach	O
-we	O
hypothesize	O
that	O
when	O
the	O
model	O
is	O
fine	O
-	O
tuned	O
directly	O
on	O
the	O
downstream	O
tasks	O
and	O
uses	O
only	O
a	O
very	O
small	O
number	O
of	O
randomly	O
initialized	O
additional	O
parameters	O
,	O
the	O
taskspecific	O
models	O
can	O
benefit	O
from	O
the	O
larger	O
,	O
more	O
expressive	O
pre	O
-	O
trained	O
representations	O
even	O
when	O
downstream	O
task	O
data	O
is	O
very	O
small	O
.	O

Feature	O
-	O
based	O
Approach	O
with	O
BERT	B-MethodName
.	O

In	O
Section	O
C.2	O
,	O
we	O
evaluate	O
the	O
impact	O
this	O
procedure	O
.	O

Additionally	O
,	O
because	O
random	O
replacement	O
only	O
occurs	O
for	O
1.5	O
%	O
of	O
all	O
tokens	O
(	O
i.e.	O
,	O
10	O
%	O
of	O
15	O
%	O
)	O
,	O
this	O
does	O
not	O
seem	O
to	O
harm	O
the	O
model	O
's	O
language	O
understanding	O
capability	O
.	O

Compared	O
to	O
standard	O
langauge	O
model	O
training	O
,	O
the	O
masked	O
LM	O
only	O
make	O
predictions	O
on	O
15	O
%	O
of	O
tokens	O
in	O
each	O
batch	O
,	O
which	O
suggests	O
that	O
more	O
pre	O
-	O
training	O
steps	O
may	O
be	O
required	O
for	O
the	O
model	O
.	O

The	O
purpose	O
of	O
this	O
is	O
to	O
bias	O
the	O
representation	O
towards	O
the	O
actual	O
observed	O
word	O
.	O

Masked	O
LM	O
and	O
the	O
Masking	O
Procedure	O
Assuming	O
the	O
unlabeled	O
sentence	O
is	O
my	O
dog	O
is	O
hairy	O
,	O
and	O
during	O
the	O
random	O
masking	O
procedure	O
we	O
chose	O
the	O
4	O
-	O
th	O
token	O
(	O
which	O
corresponding	O
to	O
hairy	O
)	O
,	O
our	O
masking	O
procedure	O
can	O
be	O
further	O
illustrated	O
by	O
•	O
10	O
%	O
of	O
the	O
time	O
:	O
Replace	O
the	O
word	O
with	O
a	O
random	O
word	O
,	O
e.g.	O
,	O
my	O
dog	O
is	O
hairy	O
→	O
my	O
dog	O
is	O
apple	O
•	O
10	O
%	O
of	O
the	O
time	O
:	O
Keep	O
the	O
word	O
unchanged	O
,	O
e.g.	O
,	O
my	O
dog	O
is	O
hairy	O
→	O
my	O
dog	O
is	O
hairy	O
.	O

A.1	O
Illustration	O
of	O
the	O
Pre	O
-	O
training	O
Tasks	O
We	O
provide	O
examples	O
of	O
the	O
pre	O
-	O
training	O
tasks	O
in	O
the	O
following	O
.	O

A	O
Additional	O
Details	O
for	O
BERT	B-MethodName
.	O

Appendix	O
for	O
"	O
BERT	B-MethodName
:	O
Pre	O
-	O
training	O
of	O
Deep	O
Bidirectional	B-MethodName
Transformers	I-MethodName
for	O
Language	O
Understanding	O
"	O
We	O
organize	O
the	O
appendix	O
into	O
three	O
sections	O
:	O
•	O
Additional	O
implementation	O
details	O
for	O
BERT	B-MethodName
are	O
presented	O
in	O
Appendix	O
A	O
;	O
.	O

Interestingly	O
,	O
using	O
only	O
the	O
RND	O
strategy	O
performs	O
much	O
worse	O
than	O
our	O
strategy	O
as	O
well	O
.	O

However	O
,	O
as	O
expected	O
,	O
using	O
only	O
the	O
MASK	O
strategy	O
was	O
problematic	O
when	O
applying	O
the	O
featurebased	O
approach	O
to	O
NER	B-TaskName
.	O

All	O
of	O
the	O
BERT	B-MethodName
results	O
presented	O
so	O
far	O
have	O
used	O
the	O
fine	O
-	O
tuning	O
approach	O
,	O
where	O
a	O
simple	O
classification	O
layer	O
is	O
added	O
to	O
the	O
pre	O
-	O
trained	O
model	O
,	O
and	O
all	O
parameters	O
are	O
jointly	O
fine	O
-	O
tuned	O
on	O
a	O
downstream	O
task	O
.	O

However	O
,	O
the	O
feature	O
-	O
based	O
approach	O
,	O
where	O
fixed	O
features	O
are	O
extracted	O
from	O
the	O
pretrained	O
model	O
,	O
has	O
certain	O
advantages	O
.	O

First	O
,	O
not	O
all	O
tasks	O
can	O
be	O
easily	O
represented	O
by	O
a	O
Transformer	B-MethodName
encoder	O
architecture	O
,	O
and	O
therefore	O
require	O
a	O
task	O
-	O
specific	O
model	O
architecture	O
to	O
be	O
added	O
.	O

Second	O
,	O
there	O
are	O
major	O
computational	O
benefits	O
to	O
pre	O
-	O
compute	O
an	O
expensive	O
representation	O
of	O
the	O
training	O
data	O
once	O
and	O
then	O
run	O
many	O
experiments	O
with	O
cheaper	O
models	O
on	O
top	O
of	O
this	O
representation	O
.	O

In	O
this	O
section	O
,	O
we	O
compare	O
the	O
two	O
approaches	O
by	O
applying	O
BERT	B-MethodName
to	O
the	O
CoNLL-2003	O
Named	B-TaskName
Entity	I-TaskName
Recognition	I-TaskName
(	O
NER	B-TaskName
)	O
task	O
(	O
Tjong	O
Kim	O
Sang	O
and	O
De	O
Meulder	O
,	O
2003	O
)	O
.	O

In	O
the	O
input	O
to	O
BERT	B-MethodName
,	O
we	O
use	O
a	O
case	O
-	O
preserving	O
WordPiece	O
model	O
,	O
and	O
we	O
include	O
the	O
maximal	O
document	O
context	O
provided	O
by	O
the	O
data	O
.	O

Following	O
standard	O
practice	O
,	O
we	O
formulate	O
this	O
as	O
a	O
tagging	O
task	O
but	O
do	O
not	O
use	O
a	O
CRF	O
layer	O
in	O
the	O
output	O
.	O

We	O
use	O
the	O
representation	O
of	O
the	O
first	O
sub	O
-	O
token	O
as	O
the	O
input	O
to	O
the	O
token	O
-	O
level	O
classifier	O
over	O
the	O
NER	B-TaskName
label	O
set	O
.	O

To	O
ablate	O
the	O
fine	O
-	O
tuning	O
approach	O
,	O
we	O
apply	O
the	O
feature	O
-	O
based	O
approach	O
by	O
extracting	O
the	O
activations	O
from	O
one	O
or	O
more	O
layers	O
without	O
fine	O
-	O
tuning	O
any	O
parameters	O
of	O
BERT	B-MethodName
.	O

These	O
contextual	O
embeddings	O
are	O
used	O
as	O
input	O
to	O
a	O
randomly	O
initialized	O
two	O
-	O
layer	O
768	O
-	O
dimensional	O
BiLSTM	B-MethodName
before	O
the	O
classification	O
layer	O
.	O

Results	O
are	O
presented	O
in	O
Table	O
7	O
.	O

BERT	B-MethodName
LARGE	I-MethodName
performs	O
competitively	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
.	O

The	O
best	O
performing	O
method	O
concatenates	O
the	O
token	O
representations	O
from	O
the	O
top	O
four	O
hidden	O
layers	O
of	O
the	O
pre	O
-	O
trained	O
Transformer	B-MethodName
,	O
which	O
is	O
only	O
0.3	B-MetricValue
F1	B-MetricName
behind	O
fine	O
-	O
tuning	O
the	O
entire	O
model	O
.	O

This	O
demonstrates	O
that	O
BERT	B-MethodName
is	O
effective	O
for	O
both	O
finetuning	O
and	O
feature	O
-	O
based	O
approaches	O
.	O

Conclusion	O
.	O

Recent	O
empirical	O
improvements	O
due	O
to	O
transfer	O
learning	O
with	O
language	O
models	O
have	O
demonstrated	O
that	O
rich	O
,	O
unsupervised	O
pre	O
-	O
training	O
is	O
an	O
integral	O
part	O
of	O
many	O
language	O
understanding	O
systems	O
.	O

In	O
particular	O
,	O
these	O
results	O
enable	O
even	O
low	O
-	O
resource	O
tasks	O
to	O
benefit	O
from	O
deep	O
unidirectional	O
architectures	O
.	O

Our	O
major	O
contribution	O
is	O
further	O
generalizing	O
these	O
findings	O
to	O
deep	O
bidirectional	O
architectures	O
,	O
allowing	O
the	O
same	O
pre	O
-	O
trained	O
model	O
to	O
successfully	O
tackle	O
a	O
broad	O
set	O
of	O
NLP	O
tasks	O
.	O

To	O
generate	O
each	O
training	O
input	O
sequence	O
,	O
we	O
sample	O
two	O
spans	O
of	O
text	O
from	O
the	O
corpus	O
,	O
which	O
we	O
refer	O
to	O
as	O
"	O
sentences	O
"	O
even	O
though	O
they	O
are	O
typically	O
much	O
longer	O
than	O
single	O
sentences	O
(	O
but	O
can	O
be	O
shorter	O
also	O
)	O
.	O

The	O
first	O
sentence	O
receives	O
the	O
A	O
embedding	O
and	O
the	O
second	O
receives	O
the	O
B	O
embedding	O
.	O

50	O
%	O
of	O
the	O
time	O
B	O
is	O
the	O
actual	O
next	O
sentence	O
that	O
follows	O
A	O
and	O
50	O
%	O
of	O
the	O
time	O
it	O
is	O
a	O
random	O
sentence	O
,	O
which	O
is	O
done	O
for	O
the	O
"	B-TaskName
next	I-TaskName
sentence	I-TaskName
prediction	I-TaskName
"	O
task	O
.	O

They	O
are	O
sampled	O
such	O
that	O
the	O
combined	O
length	O
is	O
≤	O
512	O
tokens	O
.	O

The	O
LM	O
masking	O
is	O
applied	O
after	O
WordPiece	O
tokenization	O
with	O
a	O
uniform	O
masking	O
rate	O
of	O
15	O
%	O
,	O
and	O
no	O
special	O
consideration	O
given	O
to	O
partial	O
word	O
pieces	O
.	O

We	O
train	O
with	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
256	B-HyperparameterValue
sequences	O
(	O
256	O
sequences	O
*	O
512	O
tokens	O
=	O
128,000	O
tokens	O
/	O
batch	O
)	O
for	O
1,000,000	B-HyperparameterValue
steps	B-HyperparameterName
,	O
which	O
is	O
approximately	O
40	O
epochs	O
over	O
the	O
3.3	O
billion	O
word	O
corpus	O
.	O

We	O
use	O
Adam	O
with	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
1e-4	B-HyperparameterValue
,	O
β	B-HyperparameterName
1	I-HyperparameterName
=	O
0.9	B-HyperparameterValue
,	O
β	B-HyperparameterName
2	I-HyperparameterName
=	O
0.999	B-HyperparameterValue
,	O
L2	B-HyperparameterName
weight	I-HyperparameterName
decay	I-HyperparameterName
of	O
0.01	B-HyperparameterValue
,	O
learning	O
rate	O
warmup	O
over	O
the	O
first	O
10,000	O
steps	O
,	O
and	O
linear	O
decay	O
of	O
the	O
learning	O
rate	O
.	O

We	O
use	O
a	O
dropout	B-HyperparameterName
probability	I-HyperparameterName
of	O
0.1	B-HyperparameterValue
on	O
all	O
layers	O
.	O

We	O
use	O
a	O
gelu	O
activation	O
(	O
Hendrycks	O
and	O
Gimpel	O
,	O
2016	O
)	O
rather	O
than	O
the	O
standard	O
relu	O
,	O
following	O
OpenAI	B-MethodName
GPT	I-MethodName
.	O

The	O
training	O
loss	O
is	O
the	O
sum	O
of	O
the	O
mean	O
masked	O
LM	O
likelihood	O
and	O
the	O
mean	O
next	O
sentence	O
prediction	O
likelihood	O
.	O

E	O
N	O
...	O

T	O
1	O
T	O
2	O
T	O
N	O
...	O

E	O
1	O
E	O
2	O
E	O
N	O
...	O

T	O
1	O
T	O
2	O
T	O
N	O
...	O

E	O
1	O
E	O
2	O
E	O
N	O
...	O

Training	O
of	O
BERT	B-MethodName
BASE	I-MethodName
was	O
performed	O
on	O
4	O
Cloud	O
TPUs	O
in	O
Pod	O
configuration	O
(	O
16	O
TPU	O
chips	O
total	O
)	O
.	O

13	O
Training	O
of	O
BERT	B-MethodName
LARGE	I-MethodName
was	O
performed	O
on	O
16	O
Cloud	O
TPUs	O
(	O
64	O
TPU	O
chips	O
total	O
)	O
.	O

Each	O
pretraining	O
took	O
4	O
days	O
to	O
complete	O
.	O

Longer	O
sequences	O
are	O
disproportionately	O
expensive	O
because	O
attention	O
is	O
quadratic	O
to	O
the	O
sequence	O
length	O
.	O

Then	O
,	O
we	O
train	O
the	O
rest	O
10	O
%	O
of	O
the	O
steps	O
of	O
sequence	O
of	O
512	O
to	O
learn	O
the	O
positional	O
embeddings	O
.	O

A.3	O
Fine	O
-	O
tuning	O
Procedure	O
.	O

For	O
fine	O
-	O
tuning	O
,	O
most	O
model	O
hyperparameters	O
are	O
the	O
same	O
as	O
in	O
pre	O
-	O
training	O
,	O
with	O
the	O
exception	O
of	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
,	O
learning	B-HyperparameterName
rate	I-HyperparameterName
,	O
and	O
number	B-HyperparameterName
of	I-HyperparameterName
training	I-HyperparameterName
epochs	I-HyperparameterName
.	O

The	O
dropout	B-HyperparameterName
probability	I-HyperparameterName
was	O
always	O
kept	O
at	O
0.1	B-HyperparameterValue
.	O
The	O
optimal	O
hyperparameter	O
values	O
are	O
task	O
-	O
specific	O
,	O
but	O
we	O
found	O
the	O
following	O
range	O
of	O
possible	O
values	O
to	O
work	O
well	O
across	O
all	O
tasks	O
:	O
•	O
Batch	B-HyperparameterName
size	I-HyperparameterName
:	O
16	B-HyperparameterValue
,	O
32	B-HyperparameterValue
We	O
also	O
observed	O
that	O
large	O
data	O
sets	O
(	O
e.g.	O
,	O
100k+	O
labeled	O
training	O
examples	O
)	O
were	O
far	O
less	O
sensitive	O
to	O
hyperparameter	O
choice	O
than	O
small	O
data	O
sets	O
.	O

Fine	O
-	O
tuning	O
is	O
typically	O
very	O
fast	O
,	O
so	O
it	O
is	O
reasonable	O
to	O
simply	O
run	O
an	O
exhaustive	O
search	O
over	O
the	O
above	O
parameters	O
and	O
choose	O
the	O
model	O
that	O
performs	O
best	O
on	O
the	O
development	O
set	O
.	O

A.4	O
Comparison	O
of	O
BERT	B-MethodName
,	O
ELMo	B-MethodName
,	O
and	O
OpenAI	B-MethodName
GPT	I-MethodName
Here	O
we	O
studies	O
the	O
differences	O
in	O
recent	O
popular	O
representation	O
learning	O
models	O
including	O
ELMo	B-MethodName
,	O
OpenAI	B-MethodName
GPT	I-MethodName
and	O
BERT	B-MethodName
.	O

The	O
comparisons	O
between	O
the	O
model	O
architectures	O
are	O
shown	O
visually	O
in	O
Figure	O
3	O
.	O

Note	O
that	O
in	O
addition	O
to	O
the	O
architecture	O
differences	O
,	O
BERT	B-MethodName
and	O
OpenAI	B-MethodName
GPT	I-MethodName
are	O
finetuning	O
approaches	O
,	O
while	O
ELMo	B-MethodName
is	O
a	O
feature	O
-	O
based	O
approach	O
.	O

The	O
most	O
comparable	O
existing	O
pre	O
-	O
training	O
method	O
to	O
BERT	B-MethodName
is	O
OpenAI	B-MethodName
GPT	I-MethodName
,	O
which	O
trains	O
a	O
left	O
-	O
to	O
-	O
right	O
Transformer	O
LM	O
on	O
a	O
large	O
text	O
corpus	O
.	O

In	O
fact	O
,	O
many	O
of	O
the	O
design	O
decisions	O
in	O
BERT	B-MethodName
were	O
intentionally	O
made	O
to	O
make	O
it	O
as	O
close	O
to	O
GPT	B-MethodName
as	O
possible	O
so	O
that	O
the	O
two	O
methods	O
could	O
be	O
minimally	O
compared	O
.	O

The	O
core	O
argument	O
of	O
this	O
work	O
is	O
that	O
the	O
bi	O
-	O
directionality	O
and	O
the	O
two	O
pretraining	O
tasks	O
presented	O
in	O
Section	O
3.1	O
account	O
for	O
the	O
majority	O
of	O
the	O
empirical	O
improvements	O
,	O
but	O
we	O
do	O
note	O
that	O
there	O
are	O
several	O
other	O
differences	O
between	O
how	O
BERT	B-MethodName
and	O
GPT	B-MethodName
were	O
trained	O
:	O
•	O
GPT	B-MethodName
is	O
trained	O
on	O
the	O
BooksCorpus	B-DatasetName
(	O
800	O
M	O
words	O
)	O
;	O
BERT	B-MethodName
is	O
trained	O
on	O
the	O
BooksCorpus	B-DatasetName
(	O
800	O
M	O
words	O
)	O
and	O
Wikipedia	B-DatasetName
(	O
2,500	O
M	O
words	O
)	O
.	O

•	O
GPT	B-MethodName
uses	O
a	O
sentence	O
separator	O
(	O
[	O
SEP	O
]	O
)	O
and	O
classifier	O
token	O
(	O
[	O
CLS	O
]	O
)	O
which	O
are	O
only	O
introduced	O
at	O
fine	O
-	O
tuning	O
time	O
;	O
BERT	B-MethodName
learns	O
[	O
SEP	O
]	O
,	O
[	O
CLS	O
]	O
and	O
sentence	O
A	O
/	O
B	O
embeddings	O
during	O
pre	O
-	O
training	O
.	O

•	O
GPT	B-MethodName
used	O
the	O
same	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
5e-5	B-HyperparameterValue
for	O
all	O
fine	O
-	O
tuning	O
experiments	O
;	O
BERT	B-MethodName
chooses	O
a	O
task	O
-	O
specific	O
fine	O
-	O
tuning	O
learning	O
rate	O
which	O
performs	O
the	O
best	O
on	O
the	O
development	O
set	O
.	O

To	O
isolate	O
the	O
effect	O
of	O
these	O
differences	O
,	O
we	O
perform	O
ablation	O
experiments	O
in	O
Section	O
5.1	O
which	O
demonstrate	O
that	O
the	O
majority	O
of	O
the	O
improvements	O
are	O
in	O
fact	O
coming	O
from	O
the	O
two	O
pre	O
-	O
training	O
tasks	O
and	O
the	O
bidirectionality	O
they	O
enable	O
.	O

A.5	O
Illustrations	O
of	O
Fine	O
-	O
tuning	O
on	O
Different	O
Tasks	O
.	O

The	O
illustration	O
of	O
fine	O
-	O
tuning	O
BERT	B-MethodName
on	O
different	O
tasks	O
can	O
be	O
seen	O
in	O
Figure	O
4	O
.	O

Our	O
task	O
-	O
specific	O
models	O
are	O
formed	O
by	O
incorporating	O
BERT	B-MethodName
with	O
one	O
additional	O
output	O
layer	O
,	O
so	O
a	O
minimal	O
number	O
of	O
parameters	O
need	O
to	O
be	O
learned	O
from	O
scratch	O
.	O

The	O
GLUE	B-DatasetName
benchmark	O
includes	O
the	O
following	O
datasets	O
,	O
the	O
descriptions	O
of	O
which	O
were	O
originally	O
summarized	O
in	O
Wang	O
et	O
al	O
.	O

Given	O
a	O
pair	O
of	O
sentences	O
,	O
the	O
goal	O
is	O
to	O
predict	O
whether	O
the	O
second	O
sentence	O
is	O
an	O
entailment	O
,	O
contradiction	O
,	O
or	O
neutral	O
with	O
respect	O
to	O
the	O
first	O
one	O
.	O

QNLI	B-DatasetName
Question	B-DatasetName
Natural	I-DatasetName
Language	I-DatasetName
Inference	I-DatasetName
is	O
a	O
version	O
of	O
the	O
Stanford	B-DatasetName
Question	I-DatasetName
Answering	I-DatasetName
Dataset	I-DatasetName
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O
2016	O
)	O
which	O
has	O
been	O
converted	O
to	O
a	O
binary	B-TaskName
classification	I-TaskName
task	O
(	O
Wang	O
et	O
al	O
.	O
,	O
2018a	O
...	O

E	O
N	O
E	O
1	O
'	O
...	O

E	O
M	O
'	O
C	O
T	O
1	O
T	O
[	O
SEP	O
]	O
...	O

T	O
N	O
T	O
1	O
'	O
...	O

T	O
M	O
'	O
[	O
CLS	O
]	O
Tok	O
1	O
[	O
SEP	O
]	O
...	O

...	O

[	O
CLS	O
]	O
E	O
[	O
CLS	O
]	O
E	O
1	O
E	O
2	O
E	O
N	O
C	O
T	O
1	O
T	O
2	O
T	O
N	O
Single	O
Sentence	O
B	O
-	O
PER	O
O	O
O	O
...	O

...	O

E	O
[	O
CLS	O
]	O
E	O
1	O
E	O
[	O
SEP	O
]	O
Class	O
Label	O
...	O

E	O
N	O
E	O
1	O
'	O
...	O

E	O
M	O
'	O
C	O
T	O
1	O
T	O
[	O
SEP	O
]	O
...	O

with	O
human	O
annotations	O
of	O
their	O
sentiment	O
(	O
Socher	O
et	O
al	O
.	O
,	O
2013	O
)	O
.	O

STS	O
-	O
B.	O

The	O
Semantic	B-DatasetName
Textual	I-DatasetName
Similarity	I-DatasetName
Benchmark	I-DatasetName
is	O
a	O
collection	O
of	O
sentence	O
pairs	O
drawn	O
from	O
news	O
headlines	O
and	O
other	O
sources	O
(	O
Cer	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

They	O
were	O
annotated	O
with	O
a	O
score	O
from	O
1	O
to	O
5	O
denoting	O
how	O
similar	O
the	O
two	O
sentences	O
are	O
in	O
terms	O
of	O
semantic	O
meaning	O
.	O

MRPC	B-DatasetName
Microsoft	B-DatasetName
Research	I-DatasetName
Paraphrase	I-DatasetName
Corpus	I-DatasetName
consists	O
of	O
sentence	O
pairs	O
automatically	O
extracted	O
from	O
online	O
news	O
sources	O
,	O
with	O
human	O
annotations	O
for	O
whether	O
the	O
sentences	O
in	O
the	O
pair	O
are	O
semantically	O
equivalent	O
(	O
Dolan	O
and	O
Brockett	O
,	O
2005	O
)	O
.	O

RTE	B-DatasetName
Recognizing	B-DatasetName
Textual	I-DatasetName
Entailment	I-DatasetName
is	O
a	O
binary	B-TaskName
entailment	I-TaskName
task	O
similar	O
to	O
MNLI	B-DatasetName
,	O
but	O
with	O
much	O
less	O
training	O
data	O
(	O
Bentivogli	O
et	O
al	O
.	O
,	O
2009	O
)	O
.	O

14	O
WNLI	B-DatasetName
Winograd	B-DatasetName
NLI	I-DatasetName
is	O
a	O
small	O
natural	B-TaskName
language	I-TaskName
inference	I-TaskName
dataset	O
(	O
Levesque	O
et	O
al	O
.	O
,	O
2011	O
)	O
.	O

For	O
our	O
GLUE	B-DatasetName
submission	O
,	O
we	O
always	O
predicted	O
the	O
majority	O
class	O
.	O

C	O
Additional	O
Ablation	O
Studies	O
.	O

C.2	O
Ablation	O
for	O
Different	O
Masking	O
Procedures	O
.	O

In	O
Section	O
3.1	O
,	O
we	O
mention	O
that	O
BERT	B-MethodName
uses	O
a	O
mixed	O
strategy	O
for	O
masking	O
the	O
target	O
tokens	O
when	O
pre	O
-	O
training	O
with	O
the	O
masked	O
language	O
model	O
(	O
MLM	O
)	O
objective	O
.	O

The	O
following	O
is	O
an	O
ablation	O
study	O
to	O
evaluate	O
the	O
effect	O
of	O
different	O
masking	O
strategies	O
.	O

Note	O
that	O
the	O
purpose	O
of	O
the	O
masking	O
strategies	O
is	O
to	O
reduce	O
the	O
mismatch	O
between	O
pre	O
-	O
training	O
and	O
fine	O
-	O
tuning	O
,	O
as	O
the	O
[	O
MASK	O
]	O
symbol	O
never	O
appears	O
during	O
the	O
fine	O
-	O
tuning	O
stage	O
.	O

We	O
report	O
the	O
Dev	O
results	O
for	O
both	O
MNLI	B-DatasetName
and	O
NER	B-TaskName
.	O

For	O
NER	B-TaskName
,	O
we	O
report	O
both	O
fine	O
-	O
tuning	O
and	O
feature	O
-	O
based	O
approaches	O
,	O
as	O
we	O
expect	O
the	O
mismatch	O
will	O
be	O
amplified	O
for	O
the	O
feature	O
-	O
based	O
approach	O
as	O
the	O
model	O
will	O
not	O
have	O
the	O
chance	O
to	O
adjust	O
the	O
representations	O
.	O

The	O
results	O
are	O
presented	O
in	O
Table	O
8	O
.	O

In	O
the	O
table	O
,	O
MASK	O
means	O
that	O
we	O
replace	O
the	O
target	O
token	O
with	O
the	O
[	O
MASK	O
]	O
symbol	O
for	O
MLM	O
;	O
SAME	O
means	O
that	O
we	O
keep	O
the	O
target	O
token	O
as	O
is	O
;	O
RND	O
means	O
that	O
we	O
replace	O
the	O
target	O
token	O
with	O
another	O
random	O
token	O
.	O

The	O
right	O
part	O
of	O
the	O
paper	O
represents	O
the	O
Dev	O
set	O
results	O
.	O

For	O
the	O
feature	O
-	O
based	O
approach	O
,	O
we	O
concatenate	O
the	O
last	O
4	O
layers	O
of	O
BERT	B-MethodName
as	O
the	O
features	O
,	O
which	O
was	O
shown	O
to	O
be	O
the	O
best	O
approach	O
in	O
Section	O
5.3	O
.	O
From	O
the	O
table	O
it	O
can	O
be	O
seen	O
that	O
fine	O
-	O
tuning	O
is	O
surprisingly	O
robust	O
to	O
different	O
masking	O
strategies	O
.	O

BERT	B-MethodName
alleviates	O
the	O
previously	O
mentioned	O
unidirectionality	O
constraint	O
by	O
using	O
a	O
"	O
masked	O
language	O
model	O
"	O
(	O
MLM	O
)	O
pre	O
-	O
training	O
objective	O
,	O
inspired	O
by	O
the	O
Cloze	O
task	O
(	O
Taylor	O
,	O
1953	O
)	O
.	O

BERT	B-MethodName
:	O
Pre	O
-	O
training	O
of	O
Deep	O
Bidirectional	O
Transformers	O
for	O
Language	O
Understanding	O
.	O

It	O
obtains	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
eleven	O
natural	O
language	O
processing	O
tasks	O
,	O
including	O
pushing	O
the	O
GLUE	B-MetricName
score	O
to	O
80.5	B-MetricValue
%	I-MetricValue
(	O
7.7	B-MetricValue
%	I-MetricValue
point	O
absolute	O
improvement	O
)	O
,	O
MultiNLI	B-DatasetName
accuracy	B-MetricName
to	O
86.7	B-MetricValue
%	I-MetricValue
(	O
4.6	B-MetricValue
%	I-MetricValue
absolute	O
improvement	O
)	O
,	O
SQuAD	B-DatasetName
v1.1	I-DatasetName
question	B-TaskName
answering	I-TaskName
Test	O
F1	B-MetricName
to	O
93.2	B-MetricValue
(	O
1.5	B-MetricValue
point	O
absolute	O
improvement	O
)	O
and	O
SQuAD	B-DatasetName
v2.0	I-DatasetName
Test	O
F1	B-MetricName
to	O
83.1	B-MetricValue
(	O
5.1	B-MetricValue
point	O
absolute	O
improvement	O
)	O
.	O

The	O
contributions	O
of	O
our	O
paper	O
are	O
as	O
follows	O
:	O
•	O
We	O
demonstrate	O
the	O
importance	O
of	O
bidirectional	O
pre	O
-	O
training	O
for	O
language	O
representations	O
.	O

In	O
addition	O
to	O
the	O
masked	O
language	O
model	O
,	O
we	O
also	O
use	O
a	O
"	O
next	O
sentence	O
prediction	O
"	O
task	O
that	O
jointly	O
pretrains	O
text	O
-	O
pair	O
representations	O
.	O

The	O
code	O
and	O
pre	O
-	O
trained	O
models	O
are	O
available	O
at	O
https://github.com/	O
google	O
-	O
research	O
/	O
bert	O
.	O

To	O
train	O
sentence	O
representations	O
,	O
prior	O
work	O
has	O
used	O
objectives	O
to	O
rank	O
candidate	O
next	O
sentences	O
(	O
Jernite	O
et	O
al	O
.	O
,	O
2017;Logeswaran	O
and	O
Lee	O
,	O
2018	O
)	O
,	O
left	O
-	O
to	O
-	O
right	O
generation	O
of	O
next	O
sentence	O
words	O
given	O
a	O
representation	O
of	O
the	O
previous	O
sentence	O
(	O
Kiros	O
et	O
al	O
.	O
,	O
2015	O
)	O
,	O
or	O
denoising	B-MethodName
autoencoder	I-MethodName
derived	O
objectives	O
(	O
Hill	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

(	O
2018	O
)	O
shows	O
that	O
the	O
cloze	O
task	O
can	O
be	O
used	O
to	O
improve	O
the	O
robustness	O
of	O
text	O
generation	O
models	O
.	O

More	O
recently	O
,	O
sentence	O
or	O
document	O
encoders	O
which	O
produce	O
contextual	O
token	O
representations	O
have	O
been	O
pre	O
-	O
trained	O
from	O
unlabeled	O
text	O
and	O
fine	O
-	O
tuned	O
for	O
a	O
supervised	O
downstream	O
task	O
(	O
Dai	O
and	O
Le	O
,	O
2015;Howard	O
and	O
Ruder	O
,	O
2018;Radford	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

Model	O
Architecture	O
BERT	B-MethodName
's	O
model	O
architecture	O
is	O
a	O
multi	O
-	O
layer	O
bidirectional	O
Transformer	O
encoder	O
based	O
on	O
the	O
original	O
implementation	O
described	O
in	O
Vaswani	O
et	O
al	O
.	O

1	O
Because	O
the	O
use	O
of	O
Transformers	O
has	O
become	O
common	O
and	O
our	O
implementation	O
is	O
almost	O
identical	O
to	O
the	O
original	O
,	O
we	O
will	O
omit	O
an	O
exhaustive	O
background	O
description	O
of	O
the	O
model	O
architecture	O
and	O
refer	O
readers	O
to	O
Vaswani	O
et	O
al	O
.	O

Task	O
#	O
1	O
:	O
Masked	O
LM	O
Intuitively	O
,	O
it	O
is	O
reasonable	O
to	O
believe	O
that	O
a	O
deep	O
bidirectional	O
model	O
is	O
strictly	O
more	O
powerful	O
than	O
either	O
a	O
left	O
-	O
to	O
-	O
right	O
model	O
or	O
the	O
shallow	O
concatenation	O
of	O
a	O
left	O
-	O
toright	O
and	O
a	O
right	O
-	O
to	O
-	O
left	O
model	O
.	O

We	O
refer	O
to	O
this	O
procedure	O
as	O
a	O
"	O
masked	O
LM	O
"	O
(	O
MLM	O
)	O
,	O
although	O
it	O
is	O
often	O
referred	O
to	O
as	O
a	O
Cloze	O
task	O
in	O
the	O
literature	O
(	O
Taylor	O
,	O
1953	O
)	O
.	O

We	O
compare	O
variations	O
of	O
this	O
procedure	O
in	O
Appendix	O
C.2	O
.	O
Task	O
#	O
2	O
:	O
Next	O
Sentence	O
Prediction	O
(	O
NSP	O
)	O
Many	O
important	O
downstream	O
tasks	O
such	O
as	O
Question	B-TaskName
Answering	I-TaskName
(	O
QA	B-TaskName
)	O
and	O
Natural	B-TaskName
Language	I-TaskName
Inference	I-TaskName
(	O
NLI	B-TaskName
)	O
are	O
based	O
on	O
understanding	O
the	O
relationship	O
between	O
two	O
sentences	O
,	O
which	O
is	O
not	O
directly	O
captured	O
by	O
language	O
modeling	O
.	O

As	O
we	O
show	O
in	O
Figure	O
1	O
,	O
C	O
is	O
used	O
for	O
next	O
sentence	O
prediction	O
(	O
NSP	O
)	O
.	O

6	O
he	O
likes	O
play	O
#	O
#	O
ing	O
[	O
SEP	O
]	O
my	O
dog	O
is	O
cute	O
[	O
SEP	O
]	O
Input	O
E	O
[	O
CLS	O
]	O
E	O
he	O
E	O
likes	O
E	O
play	O
E	O
#	O
#	O
ing	O
E	O
[	O
SEP	O
]	O
E	O
my	O
E	O
dog	O
E	O
is	O
E	O
cute	O
E	O
[	O
SEP	O
]	O
Token	O
Embeddings	O
E	O
A	O
E	O
B	O
E	O
B	O
E	O
B	O
E	O
B	O
E	O
B	O
E	O
A	O
E	O
A	O
E	O
A	O
E	O
A	O
E	O
A	O
Segment	O
Embeddings	O
E	O
0	O
E	O
6	O
E	O
7	O
E	O
8	O
E	O
9	O
E	O
10	O
E	O
1	O
E	O
2	O
E	O
3	O
E	O
4	O
E	O
5	O
Position	O
Embeddings	O
The	O
NSP	O
task	O
is	O
closely	O
related	O
to	O
representationlearning	O
objectives	O
used	O
in	O
Jernite	O
et	O
al	O
.	O

•	O
GPT	B-MethodName
was	O
trained	O
for	O
1	B-HyperparameterValue
M	I-HyperparameterValue
steps	B-HyperparameterName
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
32,000	B-HyperparameterValue
words	I-HyperparameterValue
;	O
BERT	B-MethodName
was	O
trained	O
for	O
1	B-HyperparameterValue
M	I-HyperparameterValue
steps	B-HyperparameterName
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
128,000	B-HyperparameterValue
words	I-HyperparameterValue
.	O

(	O
2018a	O
):	O
MNLI	B-DatasetName
Multi	B-DatasetName
-	I-DatasetName
Genre	I-DatasetName
Natural	I-DatasetName
Language	I-DatasetName
Inference	I-DatasetName
is	O
a	O
large	O
-	O
scale	O
,	O
crowdsourced	O
entailment	B-TaskName
classification	I-TaskName
task	O
(	O
Williams	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

QQP	B-DatasetName
Quora	B-DatasetName
Question	I-DatasetName
Pairs	I-DatasetName
is	O
a	O
binary	B-TaskName
classification	I-TaskName
task	O
where	O
the	O
goal	O
is	O
to	O
determine	O
if	O
two	O
questions	O
asked	O
on	O
Quora	O
are	O
semantically	O
equivalent	O
(	O
Chen	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

Tok	O
M	O
Question	O
Paragraph	O
BERT	B-MethodName
E	O
[	O
CLS	O
]	O
E	O
1	O
E	O
2	O
E	O
N	O
C	O
T	O
1	O
T	O
2	O
T	O
N	O
Single	O
Sentence	O
...	O

BERT	B-MethodName
Tok	O
1	O
Tok	O
2	O
Tok	O
N	O
...	O

CoLA	B-DatasetName
The	O
Corpus	B-DatasetName
of	I-DatasetName
Linguistic	I-DatasetName
Acceptability	I-DatasetName
is	O
a	O
binary	B-TaskName
single	I-TaskName
-	I-TaskName
sentence	I-TaskName
classification	I-TaskName
task	O
,	O
where	O
the	O
goal	O
is	O
to	O
predict	O
whether	O
an	O
English	O
sentence	O
is	O
linguistically	O
"	O
acceptable	O
"	O
or	O
not	O
(	O
Warstadt	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

We	O
therefore	O
exclude	O
this	O
set	O
to	O
be	O
fair	O
to	O
OpenAI	B-MethodName
GPT	I-MethodName
.	O

The	O
GLUE	B-DatasetName
webpage	O
notes	O
that	O
there	O
are	O
issues	O
with	O
the	O
construction	O
of	O
this	O
dataset	O
,	O
15	O
and	O
every	O
trained	O
system	O
that	O
's	O
been	O
submitted	O
to	O
GLUE	B-DatasetName
has	O
performed	O
worse	O
than	O
the	O
65.1	B-MetricValue
baseline	O
accuracy	B-MetricName
of	O
predicting	O
the	O
majority	O
class	O
.	O

The	O
numbers	O
in	O
the	O
left	O
part	O
of	O
the	O
table	O
represent	O
the	O
probabilities	O
of	O
the	O
specific	O
strategies	O
used	O
during	O
MLM	O
pre	O
-	O
training	O
(	O
BERT	B-MethodName
uses	O
80	B-HyperparameterValue
%	I-HyperparameterValue
,	O
10	B-HyperparameterValue
%	I-HyperparameterValue
,	O
10	B-HyperparameterValue
%	I-HyperparameterValue
)	O
.	O

The	O
advantage	O
of	O
this	O
procedure	O
is	O
that	O
the	O
Transformer	O
encoder	O
does	O
not	O
know	O
which	O
words	O
it	O
will	O
be	O
asked	O
to	O
predict	O
or	O
which	O
have	O
been	O
replaced	O
by	O
random	O
words	O
,	O
so	O
it	O
is	O
forced	O
to	O
keep	O
a	O
distributional	O
contextual	O
representation	O
of	O
every	O
input	O
token	O
.	O

