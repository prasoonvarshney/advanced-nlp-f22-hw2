G	O
denotes	O
a	O
feed	O
-	O
forward	O
neural	O
network	O
,	O
and	O
[	O
;	O
]	O
denotes	O
concatenation	O
.	O

As	O
explained	O
in	O
section	O
3	O
,	O
the	O
Predictor	O
uses	O
Decomposable	B-MethodName
Attention	I-MethodName
for	O
prediction	O
.	O

Decomposable	B-MethodName
Attention	I-MethodName
computes	O
a	O
two	O
-	O
dimensional	O
attention	O
matrix	O
,	O
computed	O
by	O
two	O
sets	O
of	O
vectors	O
,	O
and	O
thus	O
,	O
captures	O
detailed	O
information	O
useful	O
for	O
prediction	O
.	O

We	O
verified	O
these	O
hypotheses	O
with	O
the	O
Reddit	B-DatasetName
TIFU	I-DatasetName
dataset	O
,	O
but	O
not	O
with	O
the	O
email	O
datasets	O
,	O
because	O
few	O
emails	O
included	O
annotated	O
summaries	O
,	O
and	O
those	O
emails	O
did	O
not	O
have	O
replies	O
with	O
quotes	O
.	O

We	O
evaluated	O
our	O
model	O
with	O
two	O
mail	B-DatasetName
datasets	O
,	O
ECS	B-DatasetName
and	O
EPS	B-DatasetName
,	O
and	O
one	O
social	B-DatasetName
media	I-DatasetName
dataset	O
TIFU	B-DatasetName
,	O
using	O
ROUGE	B-MetricName
as	O
an	O
evaluation	O
metric	O
,	O
and	O
validated	O
that	O
our	O
model	O
is	O
useful	O
for	O
summarization	O
.	O

This	O
paper	O
proposes	O
Implicit	B-MethodName
Quote	I-MethodName
Extractor	I-MethodName
,	O
a	O
model	O
that	O
extracts	O
implicit	O
quotes	O
as	O
summaries	O
.	O

appear	O
only	O
once	O
in	O
the	O
source	O
text	O
;	O
thus	O
TextRank	B-MethodName
fails	O
to	O
capture	O
the	O
salient	O
sentences	O
.	O

The	O
sample	O
is	O
from	O
the	O
EPS	B-DatasetName
dataset	O
.	O

Figure	O
3	O
shows	O
the	O
correlation	O
between	O
the	O
maximum	B-MetricName
PageRank	I-MetricName
in	O
each	O
post	O
of	O
ECS	B-MethodName
/	O
EPS	B-MethodName
and	O
ROUGE-1	B-MetricName
-	O
F	B-MetricName
scores	O
Table	O
8	O
shows	O
a	O
demonstrative	O
example	O
of	O
extracted	O
summaries	O
of	O
IQE	B-TaskName
and	O
TextRank	B-MetricName
.	O

Comparing	O
with	O
TextRank	B-DatasetName
,	O
we	O
verify	O
that	O
our	O
method	O
can	O
capture	O
salient	O
sentences	O
that	O
the	O
centrality	O
-	O
based	O
method	O
fails	O
to	O
.	O

TextRank	B-MethodName
is	O
a	O
typical	O
example	O
;	O
TextRank	B-MethodName
is	O
a	O
centrality	O
-	O
based	O
method	O
that	O
extracts	O
sentences	O
with	O
high	O
PageRank	O
as	O
the	O
summary	O
.	O

As	O
explained	O
in	O
the	O
Introduction	O
,	O
most	O
conventional	O
unsupervised	B-TaskName
summarization	I-TaskName
methods	O
are	O
based	O
on	O
the	O
assumption	O
that	O
important	O
topics	O
appear	O
frequently	O
in	O
a	O
document	O
.	O

Without	O
pretraining	O
,	O
the	O
accuracy	O
decreased	O
.	O

However	O
,	O
on	O
the	O
Reddit	B-DatasetName
TIFU	I-DatasetName
dataset	O
,	O
NER	B-TaskName
did	O
not	O
affect	O
the	O
accuracy	B-MetricName
.	O

To	O
validate	O
the	O
effect	O
of	O
NER	B-TaskName
,	O
we	O
experiment	O
without	O
replacing	O
named	O
entities	O
.	O

Effect	O
of	O
replacing	O
named	O
entities	O
As	O
explained	O
in	O
the	O
section	O
4.3	O
,	O
our	O
models	O
shown	O
in	O
Tables	O
2	O
,	O
3	O
and	O
4	O
all	O
use	O
the	O
Stanford	B-MethodName
NER	I-MethodName
.	O

The	O
results	O
of	O
the	O
two	O
analyses	O
support	O
the	O
claim	O
that	O
our	O
model	O
is	O
more	O
likely	O
to	O
extract	O
quotes	O
and	O
that	O
the	O
ability	O
of	O
extracting	O
quotes	O
leads	O
to	O
better	O
summarization	O
.	O

The	O
result	O
in	O
the	O
Table	O
6	O
shows	O
ROUGE	B-MetricName
scores	O
are	O
higher	O
when	O
the	O
extracted	O
sentence	O
coincides	O
with	O
a	O
quote	O
.	O

IQEquote	O
indicates	O
the	O
data	O
where	O
the	O
extracted	O
sentence	O
coincides	O
with	O
a	O
quote	O
,	O
and	O
IQEnonquote	O
vice	O
versa	O
.	O

We	O
compute	O
ROUGE	B-MetricName
scores	O
when	O
our	O
model	O
succeeds	O
or	O
fails	O
in	O
quote	O
extraction	O
(	O
which	O
means	O
when	O
MRR	B-MetricName
equals	O
1	B-MetricValue
or	O
otherwise	O
)	O
.	O

Next	O
,	O
we	O
validate	O
whether	O
the	O
ROUGE	B-MetricName
scores	O
become	O
better	O
when	O
our	O
model	O
succeeded	O
in	O
extracting	O
quotes	O
.	O

IQE	B-MethodName
is	O
more	O
likely	O
to	O
extract	O
quotes	O
than	O
TextRank	B-MethodName
,	O
LexRank	B-MethodName
and	O
Random	B-MethodName
.	O

For	O
each	O
data	O
,	O
we	O
compute	O
MRR	B-MetricName
and	O
use	O
the	O
mean	O
value	O
as	O
a	O
result	O
.	O

Thus	O
we	O
set	O
the	O
threshold	O
at	O
four	B-HyperparameterValue
;	O
if	O
R(q	B-HyperparameterName
)	O
is	O
larger	O
than	O
4	B-HyperparameterValue
we	O
set	O
MRR	B-HyperparameterName
0	B-HyperparameterValue
.	O

Therefore	O
,	O
the	O
MRR	B-MetricName
in	O
our	O
study	O
indicates	O
the	O
capability	O
of	O
a	O
model	O
to	O
extract	O
quotes	O
.	O

MRR	B-MetricName
=	O
1	O
R(q	O
)	O
(	O
R(q	O
)	O
≤	O
4	O
)	O
0	O
(	O
R(q	O
)	O
>	O
4)(12	O
)	O
The	O
function	O
R	O
denotes	O
the	O
rank	O
of	O
the	O
saliency	O
scores	O
a	O
model	O
computes	O
;	O
our	O
model	O
does	O
not	O
compute	O
the	O
scores	O
but	O
sequentially	O
extracts	O
sentences	O
,	O
and	O
the	O
order	O
is	O
regarded	O
as	O
the	O
rank	O
here	O
.	O

We	O
compute	O
MRR	B-MetricName
as	O
follows	O
.	O

To	O
assess	O
the	O
ability	O
of	O
quote	B-TaskName
extraction	I-TaskName
,	O
we	O
regard	O
the	O
extraction	O
of	O
quotes	O
as	O
an	O
information	B-TaskName
retrieval	O
task	O
and	O
evaluate	O
with	O
Mean	B-MetricName
Reciprocal	I-MetricName
Rank	I-MetricName
(	O
MRR	B-MetricName
)	O
.	O

297	O
Model	O
ROUGE-1	B-MetricName
-	I-MetricName
F	I-MetricName
ROUGE-2	B-MetricName
-	I-MetricName
F	I-MetricName
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
-	I-MetricName
F	I-MetricName
#	O
of	O
For	O
the	O
experiments	O
,	O
we	O
use	O
the	O
Reddit	B-DatasetName
TIFU	I-DatasetName
dataset	O
and	O
replies	O
extracted	O
via	O
praw	O
as	O
described	O
in	O
4.2	O
.	O
From	O
the	O
dataset	O
,	O
we	O
extract	O
replies	O
that	O
contain	O
quotes	O
,	O
which	O
start	O
with	O
the	O
symbol	O
"	O
>	O
"	O
.	O

Second	O
,	O
following	O
Carenini	O
's	O
work	O
(	O
Carenini	O
et	O
al	O
.	O
,	O
2007;Oya	O
and	O
Carenini	O
,	O
2014	O
)	O
,	O
we	O
assumed	O
quotes	O
were	O
useful	O
for	O
summarization	O
but	O
it	O
is	O
not	O
clear	O
whether	O
the	O
quote	O
extraction	O
leads	O
to	O
better	O
results	O
of	O
summarization	O
.	O

The	O
Performance	O
of	O
Summarization	B-TaskName
and	O
Quote	B-TaskName
Extraction	I-TaskName
.	O

IQE	B-MethodName
did	O
not	O
outperform	O
TextRank	B-MethodName
on	O
TIFU	B-DatasetName
dataset	O
.	O

Baseline	O
models	O
such	O
as	O
LexRank	B-MethodName
and	O
TextRank	B-MethodName
compute	O
similarity	O
of	O
sentences	O
using	O
the	O
co	O
-	O
occurrence	O
of	O
words	O
.	O

The	O
average	O
number	O
of	O
words	O
each	O
sentence	O
has	O
is	O
smaller	O
in	O
EPS	O
.	O

Our	O
model	O
outperforms	O
the	O
baseline	O
models	O
more	O
with	O
the	O
EPS	B-DatasetName
dataset	O
than	O
the	O
ECS	B-DatasetName
dataset	O
.	O

IQE	B-MethodName
-	I-MethodName
TextRank	I-MethodName
performed	O
worse	O
than	O
IQE	B-MethodName
with	O
the	O
mail	B-DatasetName
datasets	O
.	O

PacSum	B-MethodName
significantly	O
outperformed	O
TextRank	B-MethodName
on	O
the	O
news	B-DatasetName
article	I-DatasetName
dataset	O
(	O
Zheng	O
and	O
Lapata	O
,	O
2019	O
)	O
but	O
does	O
not	O
work	O
well	O
on	O
our	O
datasets	O
where	O
the	O
sentence	O
position	O
is	O
not	O
an	O
important	O
factor	O
.	O

Reranking	B-MethodName
improves	O
the	O
accuracy	B-MetricName
on	O
ECS	B-TaskName
and	O
TIFU	B-DatasetName
but	O
not	O
on	O
EPS	B-TaskName
.	O

On	O
Reddit	B-DatasetName
TIFU	I-DatasetName
dataset	O
,	O
IQE	B-MethodName
with	I-MethodName
reranking	I-MethodName
outperforms	O
most	O
baseline	O
models	O
except	O
TextRank	B-MethodName
.	O

Our	O
model	O
outperforms	O
baseline	O
models	O
on	O
the	O
mail	B-DatasetName
datasets	O
(	O
ECS	B-MethodName
and	O
EPS	B-MethodName
)	O
in	O
most	O
metrics	O
.	O

As	O
another	O
baseline	O
,	O
we	O
employ	O
IQETextRank	B-MethodName
;	O
the	O
TextRank	B-MethodName
model	O
that	O
leverages	O
cosine	O
similarities	O
of	O
sentence	O
vectors	O
of	O
IQE	B-MethodName
's	O
Encoder	O
as	O
similarities	O
between	O
sentences	O
.	O

PacSum	B-MethodName
and	O
LexRank	B-MethodName
leverage	O
idf	O
.	O

KLSum	B-MethodName
employs	O
the	O
Kullbuck	O
-	O
Leibler	O
divergence	O
to	O
constrain	O
extracted	O
sentences	O
and	O
the	O
source	O
text	O
to	O
have	O
the	O
similar	O
word	O
distribution	O
.	O

296	O
Model	O
ROUGE-1	B-MetricName
-	I-MetricName
F	I-MetricName
ROUGE-2	B-MetricName
-	I-MetricName
F	I-MetricName
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
-	I-MetricName
F	I-MetricName
#	B-MetricName
of	I-MetricName
PacSum	B-DatasetName
is	O
an	O
improved	O
model	O
of	O
TextRank	B-TaskName
,	O
which	O
harnesses	O
the	O
position	O
of	O
sentences	O
as	O
a	O
feature	O
.	O

TextRank	B-MethodName
and	O
LexRank	B-MethodName
are	O
graph	O
-	O
centrality	O
based	O
methods	O
that	O
have	O
long	O
been	O
considered	O
as	O
strong	O
methods	O
for	O
unsupervised	B-TaskName
summarization	I-TaskName
.	O

As	O
baseline	O
models	O
,	O
we	O
employ	O
TextRank	B-MethodName
(	O
Mihalcea	O
and	O
Tarau	O
,	O
2004	O
)	O
,	O
LexRank	B-MethodName
(	O
Erkan	O
and	O
Radev	O
,	O
2004	O
)	O
,	O
KLSum	B-MethodName
(	O
Haghighi	O
and	O
Vanderwende	O
,	O
2009	O
)	O
,	O
PacSum	B-MethodName
(	O
Zheng	O
and	O
Lapata	O
,	O
2019	O
)	O
,	O
Lead	B-MethodName
,	O
and	O
Random	B-MethodName
.	O

As	O
a	O
validation	O
metric	O
,	O
we	O
use	O
an	O
average	O
of	O
ROUGE-1	B-MetricName
-	I-MetricName
F	I-MetricName
,	O
ROUGE-2	B-MetricName
-	I-MetricName
F	I-MetricName
,	O
and	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
-	I-MetricName
F.	I-MetricName

For	O
ROUGE	B-DatasetName
computation	O
,	O
we	O
use	O
ROUGE	B-DatasetName
2.0	I-DatasetName
(	O
Ganesan	O
,	O
2015	O
)	O
.	O

We	O
use	O
the	O
first	O
20	O
,	O
40	B-HyperparameterValue
,	O
and	O
60	B-HyperparameterValue
words	O
of	O
the	O
extracted	O
sentences	O
.	O

Following	O
previous	O
work	O
,	O
we	O
report	O
the	O
average	O
F1	B-MetricName
of	O
ROUGE-1	B-MetricName
,	O
ROUGE-2	B-MetricName
,	O
and	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
for	O
the	O
evaluation	O
(	O
Lin	O
,	O
2004	O
)	O
.	O

We	O
pretrain	O
word	O
embeddings	O
of	O
the	O
model	O
with	O
Skipgram	B-DatasetName
,	O
using	O
the	O
same	O
data	O
as	O
the	O
training	O
.	O

We	O
replace	O
the	O
named	O
entities	O
on	O
the	O
text	O
data	O
with	O
tags	O
(	O
person	O
,	O
location	O
,	O
and	O
organization	O
)	O
using	O
the	O
Stanford	O
Named	O
Entity	I-MethodName
Recognizer	I-MethodName
(	O
NER	B-TaskName
)	O
4	O
,	O
to	O
prevent	O
the	O
model	O
from	O
simply	O
using	O
named	O
entities	O
as	O
a	O
hint	O
for	O
the	O
prediction	O
.	O

During	O
training	O
,	O
L	O
,	O
the	O
number	O
of	O
sentences	O
the	O
Extractor	O
extracts	O
is	O
randomly	O
set	O
from	O
1	O
to	O
4	O
,	O
so	O
that	O
the	O
model	O
can	O
extract	O
an	O
arbitrary	O
number	O
of	O
sentences	O
.	O

We	O
set	O
this	O
threshold	O
as	O
4	O
.	O

The	O
epoch	B-HyperparameterName
size	I-HyperparameterName
is	O
10	B-HyperparameterValue
,	O
and	O
we	O
use	O
Adam	B-HyperparameterValue
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
as	O
an	O
optimizer	B-HyperparameterName
.	O

The	O
upper	O
limit	O
of	O
the	O
number	O
of	O
sentences	O
is	O
set	O
to	O
30	O
,	O
and	O
that	O
of	O
words	O
in	O
each	O
sentence	O
is	O
set	O
to	O
200	O
.	O

We	O
tokenize	O
each	O
email	O
or	O
post	O
into	O
sentences	O
and	O
each	O
sentence	O
into	O
words	O
using	O
the	O
nltk	O
tokenizer	O
3	O
.	O

The	O
size	O
of	O
the	O
vocabulary	O
is	O
set	O
to	O
30,000	O
.	O

The	O
dimensions	O
of	O
the	O
embedding	O
layers	O
and	O
hidden	O
layers	O
of	O
the	O
LSTM	B-MethodName
are	O
100	B-HyperparameterValue
.	O

An	O
overview	O
of	O
the	O
TIFU	B-DatasetName
evaluation	O
dataset	O
is	O
also	O
summarized	O
in	O
Table	O
1	O
.	O

Because	O
the	O
TIFU	B-DatasetName
dataset	O
does	O
not	O
include	O
replies	O
,	O
we	O
collected	O
replies	O
of	O
the	O
posts	O
included	O
in	O
the	O
TIFU	B-DatasetName
dataset	O
using	O
praw	O
2	O
.	O

We	O
preprocess	O
the	O
TIFU	B-DatasetName
dataset	O
similarly	O
as	O
the	O
mail	B-DatasetName
datasets	O
.	O

On	O
the	O
discussion	O
forum	O
Reddit	O
TIFU	B-DatasetName
,	O
users	O
post	O
a	O
tldr	O
along	O
with	O
the	O
post	O
.	O

The	O
Reddit	B-DatasetName
TIFU	I-DatasetName
dataset	O
(	O
Kim	O
et	O
al	O
.	O
,	O
2019	O
)	O
is	O
a	O
dataset	O
that	O
leverages	O
tldr	O
tags	O
for	O
the	O
summarization	O
task	O
,	O
which	O
is	O
the	O
abbreviation	O
of	O
"	O
too	O
long	O
did	O
n't	O
read	O
"	O
.	O

Reddit	B-DatasetName
TIFU	I-DatasetName
Dataset	O
.	O

For	O
evaluation	O
,	O
we	O
employ	O
the	O
Enron	B-DatasetName
Summarization	I-DatasetName
dataset	O
(	O
Loza	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

The	O
Avocado	B-DatasetName
collection	I-DatasetName
is	O
a	O
public	O
dataset	O
that	O
comprises	O
emails	O
obtained	O
from	O
279	O
custodians	O
of	O
a	O
defunct	O
information	O
technology	O
company	O
.	O

We	O
use	O
Avocado	B-DatasetName
collection	I-DatasetName
1	O
for	O
the	O
training	O
.	O

One	O
is	O
a	O
mail	B-DatasetName
dataset	O
,	O
and	O
the	O
other	O
is	O
a	O
dataset	O
from	O
the	O
social	O
media	O
platform	O
,	O
Reddit	O
.	O

To	O
compute	O
the	O
relation	O
between	O
the	O
post	O
and	O
the	O
reply	O
candidate	O
,	O
we	O
employ	O
Decomposable	B-MethodName
Attention	I-MethodName
(	O
Parikh	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

The	O
discretized	O
attention	O
weights	O
α	O
are	O
computed	O
as	O
follows	O
:	O
u	O
i	O
∼	O
Uniform(0	O
,	O
1	O
)	O
(	O
3	O
)	O
g	O
i	O
=	O
−	O
log	O
(	O
−	O
log	O
u	O
i	O
)	O
(	O
4	O
)	O
a	O
ti	O
=	O
c	O
T	O
tanh(h	O
ext	O
t	O
+	O
h	O
p	O
i	O
)	O
(	O
5	O
)	O
π	O
ti	O
=	O
exp	O
a	O
ti	O
N	O
k=1	O
exp	O
a	O
tk	O
(	O
6	O
)	O
α	O
ti	O
=	O
exp	O
(	O
log	O
π	O
ti	O
+	O
g	O
i	O
)	O
/τ	O
N	O
k=1	O
exp	O
(	O
log	O
π	O
tk	O
+	O
g	O
k	O
)	O
/τ	O
(	O
7	O
)	O
c	O
is	O
a	O
parameter	O
vector	O
,	O
and	O
the	O
temperature	B-HyperparameterName
τ	O
is	O
set	O
to	O
0.1	B-HyperparameterValue
.	O
We	O
input	O
the	O
linear	O
sum	O
of	O
the	O
attention	O
weights	O
α	O
and	O
the	O
sentence	O
vectors	O
h	O
p	O
i	O
to	O
LSTM	B-MethodName
and	O
update	O
the	O
hidden	O
state	O
of	O
the	O
Extractor	O
.	O

We	O
employ	O
LSTM	B-MethodName
to	O
sequentially	O
compute	O
features	O
on	O
the	O
Extractor	O
.	O

IQE	B-TaskName
requires	O
replies	O
only	O
during	O
the	O
training	O
and	O
can	O
induce	O
summaries	O
without	O
replies	O
during	O
the	O
evaluation	O
.	O

We	O
compute	O
the	O
features	O
of	O
each	O
sentence	O
h	O
p	O
i	O
by	O
inputting	O
embedded	O
vectors	O
to	O
Bidirectional	B-MethodName
Long	I-MethodName
Short	I-MethodName
-	I-MethodName
Term	I-MethodName
Memory	I-MethodName
(	O
BiLSTM	B-MethodName
)	O
and	O
concatenating	O
the	O
last	O
two	O
hidden	O
layers	O
:	O
h	O
p	O
i	O
=	O
BiLSTM(X	O
p	O
i	O
)	O
(	O
1	O
)	O
Extractor	O
The	O
Extractor	O
extracts	O
a	O
few	O
sentences	O
of	O
a	O
post	O
for	O
prediction	O
.	O

We	O
propose	O
Implicit	B-MethodName
Quote	I-MethodName
Extractor	I-MethodName
(	O
IQE	B-MethodName
)	O
,	O
an	O
unsupervised	B-TaskName
extractive	I-TaskName
summarization	I-TaskName
model	O
.	O

A	O
few	O
studies	O
used	O
these	O
quotes	O
as	O
features	O
for	O
summarization	O
.	O

Quotes	O
are	O
also	O
important	O
factors	O
of	O
summarization	O
.	O

Despite	O
the	O
rise	O
of	O
neural	O
summarization	O
models	O
,	O
most	O
research	O
on	O
conversation	B-TaskName
summarization	I-TaskName
is	O
based	O
on	O
non	O
-	O
neural	O
models	O
.	O

However	O
,	O
these	O
methods	O
use	O
pretrained	O
neural	O
network	O
models	O
as	O
a	O
feature	O
extractor	O
,	O
whereas	O
we	O
propose	O
an	O
end	O
-	O
to	O
-	O
end	O
neural	O
extractive	O
summarization	I-TaskName
model	O
.	O

A	O
few	O
neural	O
-	O
network	O
-	O
based	O
unsupervised	B-TaskName
extractive	I-TaskName
summarization	I-TaskName
methods	O
were	O
proposed	O
(	O
Kågebäck	O
et	O
al	O
.	O
,	O
2014;Yin	O
and	O
Pei	O
,	O
2015;Ma	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

Despite	O
the	O
rise	O
of	O
neural	O
networks	O
,	O
conventional	O
non	O
-	O
neural	O
methods	O
are	O
still	O
powerful	O
in	O
the	O
field	O
of	O
unsupervised	B-TaskName
extractive	I-TaskName
summarization	I-TaskName
.	O

Most	O
unsupervised	B-TaskName
summarization	I-TaskName
methods	O
proposed	O
are	O
extractive	O
methods	O
.	O

Summarization	B-TaskName
methods	O
can	O
be	O
roughly	O
grouped	O
into	O
two	O
methods	O
:	O
extractive	B-TaskName
summarization	I-TaskName
and	O
abstractive	B-TaskName
summarization	I-TaskName
.	O

•	O
Using	O
the	O
Reddit	B-DatasetName
dataset	O
,	O
we	O
verified	O
that	O
quote	B-TaskName
extraction	I-TaskName
leads	O
to	O
a	O
high	O
performance	O
of	O
summarization	O
.	O

•	O
We	O
proposed	O
an	O
unsupervised	B-TaskName
extractive	I-TaskName
neural	I-TaskName
summarization	I-TaskName
model	O
,	O
Implicit	B-MethodName
Quote	I-MethodName
Extractor	I-MethodName
(	O
IQE	B-MethodName
)	O
,	O
and	O
demonstrated	O
that	O
the	O
model	O
outperformed	O
or	O
achieved	O
results	O
competitive	O
to	O
baseline	O
models	O
on	O
two	O
mail	B-DatasetName
datasets	O
and	O
a	O
Reddit	B-DatasetName
dataset	O
.	O

Using	O
the	O
Reddit	B-DatasetName
dataset	O
where	O
quotes	O
are	O
abundant	O
,	O
we	O
obtain	O
results	O
that	O
supports	O
the	O
hypothesis	O
.	O

We	O
also	O
evaluated	O
our	O
model	O
with	O
Reddit	B-DatasetName
TIFU	I-DatasetName
dataset	O
(	O
Kim	O
et	O
al	O
.	O
,	O
2019	O
)	O
and	O
achieved	O
results	O
competitive	O
with	O
those	O
of	O
the	O
baseline	O
models	O
.	O

We	O
evaluate	O
our	O
model	O
with	O
two	O
datasets	O
of	O
Enron	B-TaskName
mail	O
(	O
Loza	O
et	O
al	O
.	O
,	O
2014	O
)	O
,	O
corporate	O
and	O
private	O
mails	O
,	O
and	O
verify	O
that	O
our	O
model	O
outperforms	O
baseline	O
models	O
.	O

Summaries	O
should	O
not	O
depend	O
on	O
replies	O
,	O
so	O
IQE	O
does	O
not	O
use	O
reply	O
features	O
to	O
extract	O
sentences	O
.	O

To	O
predict	O
accurately	O
,	O
IQE	B-MethodName
has	O
to	O
extract	O
sentences	O
that	O
replies	O
frequently	O
refer	O
to	O
.	O

IQE	B-MethodName
extracts	O
a	O
few	O
sentences	O
of	O
the	O
post	O
as	O
a	O
feature	O
for	O
prediction	O
.	O

The	O
aim	O
of	O
our	O
model	O
is	O
to	O
extract	O
these	O
implicit	O
quotes	O
for	O
extractive	O
summarization	O
.	O

The	O
model	O
is	O
Implicit	B-MethodName
Quote	I-MethodName
Extractor	I-MethodName
(	O
IQE	B-MethodName
)	O
.	O

Graph	O
-	O
centrality	O
based	O
on	O
the	O
similarity	O
of	O
sentences	O
(	O
Mihalcea	O
and	O
Tarau	O
,	O
2004;Erkan	O
and	O
Radev	O
,	O
2004;Zheng	O
and	O
Lapata	O
,	O
2019	O
)	O
has	O
long	O
been	O
a	O
strong	O
feature	O
for	O
unsupervised	B-TaskName
summarization	I-TaskName
,	O
and	O
is	O
also	O
used	O
to	O
summarize	O
conversations	O
(	O
Mehdad	O
et	O
al	O
.	O
,	O
2014;Shang	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

Previous	O
research	O
proposed	O
diverse	O
methods	O
of	O
unsupervised	B-TaskName
summarization	I-TaskName
.	O

Neuralnetwork	B-MethodName
-	O
based	O
models	O
have	O
achieved	O
great	O
performance	O
on	O
supervised	B-TaskName
summarization	I-TaskName
,	O
but	O
its	O
application	O
to	O
unsupervised	B-TaskName
summarization	I-TaskName
is	O
not	O
sufficiently	O
explored	O
.	O

As	O
the	O
amount	O
of	O
information	O
exchanged	O
via	O
online	O
conversations	O
is	O
growing	O
rapidly	O
,	O
automated	O
summarization	O
of	O
conversations	O
is	O
in	O
demand	O
.	O

We	O
evaluate	O
our	O
model	O
on	O
two	O
email	O
datasets	O
and	O
one	O
social	O
media	O
dataset	O
,	O
and	O
confirm	O
that	O
our	O
model	O
is	O
useful	O
for	O
extractive	B-TaskName
summarization	I-TaskName
.	O

Implicit	O
Quote	I-MethodName
Extractor	I-MethodName
aims	O
to	O
extract	O
implicit	O
quotes	O
as	O
summaries	O
.	O

We	O
propose	O
Implicit	B-MethodName
Quote	I-MethodName
Extractor	I-MethodName
,	O
an	O
endto	O
-	O
end	O
unsupervised	B-TaskName
extractive	O
neural	O
summarization	I-TaskName
model	O
for	O
conversational	O
texts	O
.	O

Identifying	O
Implicit	O
Quotes	O
for	O
Unsupervised	B-TaskName
Extractive	I-TaskName
Summarization	I-TaskName
of	O
Conversations	O
.	O

Finally	O
,	O
we	O
concatenate	O
v	O
1	O
and	O
v	O
2	O
and	O
obtain	O
binary	O
-	O
classification	O
result	O
y	O
through	O
a	O
linear	O
layer	O
H	O
and	O
the	O
sigmoid	O
function	O
.	O

Next	O
,	O
we	O
separately	O
compare	O
the	O
aligned	O
phrases	O
β	O
t	O
and	O
x	O
ext	O
t	O
,	O
α	O
j	O
and	O
h	O
r	O
j	O
,	O
using	O
a	O
function	O
G.	O

β	O
i	O
is	O
a	O
linear	O
sum	O
of	O
reply	O
features	O
h	O
r	O
j	O
that	O
is	O
aligned	O
to	O
x	O
ext	O
t	O
and	O
vice	O
versa	O
for	O
α	O
j	O
.	O

The	O
weights	O
of	O
the	O
co	O
-	O
attention	O
matrix	O
are	O
normalized	O
row	O
-	O
wise	O
and	O
column	O
-	O
wise	O
in	O
the	O
equations	O
(	O
14	O
)	O
and	O
(	O
15	O
)	O
.	O

First	O
,	O
we	O
compute	O
a	O
co	O
-	O
attention	O
matrix	O
E	O
as	O
in	O
(	O
13	O
)	O
.	O

The	O
computation	O
uses	O
the	O
following	O
equations	O
:	O
The	O
computation	O
of	O
x	O
ext	O
t	O
and	O
h	O
r	O
j	O
are	O
explained	O
in	O
section	O
3	O
.	O

A.1	O
Decomposable	O
Attention	O
.	O

A	O
Appendices	O
.	O

For	O
future	O
work	O
,	O
we	O
will	O
examine	O
whether	O
our	O
hypotheses	O
are	O
valid	O
for	O
emails	O
and	O
other	O
datasets	O
.	O

We	O
hypothesized	O
that	O
our	O
model	O
is	O
more	O
likely	O
to	O
extract	O
quotes	O
and	O
that	O
ability	O
improved	O
the	O
performance	O
of	O
our	O
model	O
.	O

Conclusion	O
.	O

Our	O
model	O
,	O
by	O
contrast	O
,	O
can	O
capture	O
them	O
because	O
they	O
are	O
topics	O
that	O
replies	O
often	O
refer	O
to	O
.	O

She	O
run	O
into	O
heather	O
evans	O
which	O
she	O
had	O
n't	O
talked	O
in	O
10	O
years	O
.	O

Rachel	O
is	O
coming	O
to	O
visit	O
her	O
in	O
couple	O
of	O
weeks	O
and	O
she	O
is	O
asking	O
if	O
he	O
/	O
she	O
will	O
join	O
for	O
any	O
of	O
the	O
rodeo	O
stuff	O
.	O

She	O
is	O
scared	O
of	O
being	O
a	O
mother	O
but	O
also	O
pretty	O
exited	O
about	O
it	O
.	O

She	O
is	O
having	O
a	O
baby	O
due	O
in	O
June	O
.	O

The	O
sender	O
just	O
move	O
out	O
to	O
Katy	O
few	O
months	O
ago	O
.	O

The	O
sender	O
wants	O
to	O
congratulate	O
the	O
recipient	O
for	O
his	O
/	O
her	O
new	O
promotion	O
,	O
as	O
well	O
as	O
,	O
updating	O
him	O
/	O
her	O
about	O
her	O
life	O
.	O

Summary	O
(	O
Gold	O
)	O
.	O

Looking	O
forward	O
to	O
hearing	O
back	O
from	O
ya	O
.	O

Got	O
ta	O
get	O
back	O
to	O
work	O
.	O

Anyway	O
,	O
I	O
'll	O
let	O
you	O
go	O
.	O

Seems	O
like	O
she	O
's	O
doing	O
well	O
but	O
I	O
can	O
never	O
really	O
tell	O
with	O
her	O
.	O

I	O
had	O
n't	O
talked	O
to	O
her	O
in	O
about	O
10	O
years	O
.	O

It	O
was	O
the	O
weirdest	O
thing	O
-heather	O
evans	O
.	O

You	O
'll	O
never	O
guess	O
who	O
I	O
got	O
in	O
touch	O
with	O
about	O
a	O
month	O
ago	O
.	O

You	O
planning	O
on	O
coming	O
in	O
for	O
any	O
of	O
the	O
rodeo	O
stuff	O
?	O

Rachel	O
is	O
coming	O
to	O
visit	O
me	O
in	O
a	O
couple	O
of	O
weeks	O
.	O

I	O
'm	O
really	O
excited	O
though	O
.	O

The	O
thought	O
of	O
me	O
being	O
a	O
mother	O
is	O
downright	O
scary	O
but	O
I	O
figure	O
since	O
I	O
'm	O
almost	O
30	O
,	O
I	O
probably	O
need	O
to	O
start	O
growing	O
up	O
.	O

I	O
ca	O
n't	O
even	O
believe	O
it	O
myself	O
.	O

New	O
news	O
from	O
me	O
-I'm	O
having	O
a	O
baby	O
-due	O
in	O
June	O
.	O

I	O
love	O
it	O
there	O
-my	O
parents	O
live	O
about	O
10	O
minutes	O
away	O
.	O

My	O
hubby	O
and	O
'	O
I	O
moved	O
out	O
to	O
Katy	O
a	O
few	O
months	O
ago	O
.	O

I	O
'm	O
sure	O
it	O
's	O
going	O
to	O
be	O
alot	O
different	O
for	O
you	O
but	O
it	O
sounds	O
like	O
a	O
great	O
deal	O
.	O

Congrats	O
on	O
your	O
promotion	O
.	O

However	O
,	O
those	O
words	O
Source	O
Text	O
Just	O
got	O
your	O
email	O
address	O
from	O
Rachel	O
.	O

The	O
summary	O
includes	O
descriptions	O
regarding	O
a	O
promotion	O
and	O
that	O
the	O
sender	O
is	O
having	O
a	O
baby	O
.	O

We	O
suspected	O
that	O
important	O
topics	O
are	O
not	O
always	O
referred	O
to	O
frequently	O
,	O
and	O
suggested	O
another	O
criterion	O
:	O
the	O
frequency	O
of	O
being	O
referred	O
to	O
in	O
replies	O
.	O

A	O
sentence	O
having	O
high	O
PageRank	B-MetricName
indicates	O
that	O
the	O
sentence	O
has	O
high	O
similarity	O
with	O
many	O
other	O
sentences	O
,	O
meaning	O
that	O
many	O
sentences	O
refer	O
to	O
the	O
same	O
topic	O
.	O

Difference	O
from	O
Conventional	O
Methods	O
.	O

This	O
shows	O
the	O
importance	O
of	O
the	O
separate	O
training	O
of	O
each	O
component	O
.	O

Table	O
7	O
shows	O
the	O
effect	O
of	O
pretraining	O
.	O

As	O
explained	O
in	O
the	O
section	O
4.3	O
,	O
we	O
pretrained	O
the	O
Predictor	O
in	O
the	O
first	O
few	O
epochs	O
so	O
that	O
the	O
model	O
can	O
learn	O
the	O
extraction	O
and	O
the	O
prediction	O
separately	O
.	O

Effect	O
of	O
pretraining	O
Predictor	O
.	O

Thus	O
,	O
named	O
entities	O
will	O
not	O
be	O
hints	O
to	O
predict	O
reply	O
-	O
relation	O
.	O

Reddit	O
is	O
an	O
anonymized	O
social	O
media	O
platform	O
,	O
and	O
the	O
posts	O
are	O
less	O
likely	O
to	O
refer	O
to	O
people	O
's	O
names	O
.	O

Ablation	O
Tests	O
.	O

Does	O
extracting	O
quotes	O
lead	O
to	O
good	O
summarization	O
?	O

Table	O
5	O
shows	O
the	O
results	O
.	O

As	O
explained	O
in	O
the	O
section	O
4.3	O
,	O
we	O
trained	O
our	O
model	O
to	O
extract	O
up	O
to	O
four	O
sentences	O
.	O

If	O
a	O
model	O
extracts	O
quotes	O
as	O
salient	O
sentences	O
,	O
the	O
rank	O
becomes	O
higher	O
.	O

How	O
well	O
our	O
model	O
extracts	O
quotes	O
?	O
.	O

We	O
label	O
sentences	O
of	O
the	O
posts	O
that	O
are	O
quoted	O
by	O
the	O
replies	O
and	O
verify	O
how	O
accurately	O
our	O
model	O
can	O
extract	O
the	O
quoted	O
sentences	O
.	O

In	O
total	O
,	O
1,969	O
posts	O
have	O
replies	O
that	O
include	O
quotes	O
.	O

To	O
answer	O
these	O
questions	O
,	O
we	O
conduct	O
two	O
experiments	O
.	O

First	O
,	O
because	O
we	O
did	O
not	O
use	O
quotes	O
as	O
supervision	O
,	O
it	O
is	O
not	O
clear	O
how	O
well	O
our	O
model	O
extracts	O
quotes	O
.	O

Our	O
model	O
performed	O
well	O
on	O
the	O
Mail	B-DatasetName
datasets	O
but	O
two	O
questions	O
remain	O
unclear	O
.	O

It	O
is	O
conceivable	O
that	O
Reddit	O
users	O
are	O
less	O
likely	O
to	O
refer	O
to	O
important	O
topics	O
on	O
the	O
post	O
,	O
given	O
that	O
anyone	O
can	O
reply	O
.	O

Thus	O
,	O
if	O
the	O
lengths	O
of	O
sentences	O
are	O
short	O
,	O
it	O
fails	O
to	O
build	O
decent	O
co	O
-	O
occurrence	O
networks	O
and	O
to	O
capture	O
the	O
saliency	O
of	O
the	O
sentences	O
.	O

The	O
overview	O
of	O
the	O
datasets	O
in	O
Table	O
1	O
explains	O
the	O
reason	O
.	O

This	O
indicates	O
that	O
the	O
performance	O
of	O
our	O
model	O
does	O
not	O
result	O
from	O
the	O
use	O
of	O
neural	O
networks	O
.	O

Experimental	O
results	O
for	O
each	O
evaluation	O
dataset	O
are	O
listed	O
in	O
Table	O
2	O
,	O
3	O
and	O
4	O
.	O

Results	O
and	O
Discussion	O
.	O

This	O
is	O
added	O
to	O
verify	O
that	O
the	O
success	O
of	O
our	O
model	O
is	O
not	O
only	O
because	O
our	O
model	O
uses	O
neural	O
networks	O
.	O

We	O
compute	O
idf	O
using	O
the	O
validation	O
data	O
.	O

Lead	O
is	O
a	O
simple	O
method	O
that	O
extracts	O
the	O
first	O
few	O
sentences	O
from	O
the	O
source	O
text	O
but	O
is	O
considered	O
as	O
a	O
strong	O
baseline	O
for	O
the	O
summarization	O
of	O
news	O
articles	O
.	O

Baseline	O
.	O

Each	O
model	O
extracts	O
3	O
sentences	O
as	O
a	O
summary	O
.	O

In	O
the	O
evaluation	O
phase	O
,	O
we	O
only	O
use	O
the	O
Encoder	O
and	O
Extractor	O
and	O
do	O
not	O
use	O
the	O
Predictor	O
.	O

Evaluation	O
.	O

We	O
conduct	O
the	O
same	O
experiment	O
five	O
times	O
and	O
use	O
the	O
average	O
of	O
the	O
results	O
to	O
mitigate	O
the	O
effect	O
of	O
randomness	O
rooting	O
in	O
initialization	O
and	O
optimization	O
.	O

Thus	O
,	O
we	O
train	O
the	O
Predictor	O
in	O
the	O
first	O
few	O
epochs	O
before	O
training	O
the	O
Extractor	O
.	O

Models	O
with	O
several	O
components	O
generally	O
achieve	O
better	O
results	O
if	O
each	O
component	O
is	O
pretrained	O
separately	O
(	O
Hashimoto	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

The	O
Extractor	O
learns	O
to	O
extract	O
proper	O
sentences	O
and	O
the	O
Predictor	O
learns	O
to	O
predict	O
the	O
relation	O
between	O
a	O
post	O
and	O
a	O
reply	O
candidate	O
.	O

This	O
is	O
to	O
train	O
the	O
Extractor	O
and	O
the	O
Predictor	O
efficiently	O
.	O

In	O
the	O
first	O
few	O
epochs	O
,	O
we	O
do	O
not	O
use	O
the	O
Extractor	O
;	O
all	O
the	O
post	O
sentences	O
are	O
used	O
for	O
the	O
prediction	O
of	O
post	O
-	O
reply	O
relations	O
.	O

Training	O
.	O

We	O
use	O
3,000	O
posts	O
and	O
tldrs	O
that	O
are	O
not	O
included	O
in	O
the	O
training	O
dataset	O
as	O
the	O
validation	O
dataset	O
,	O
and	O
the	O
same	O
number	O
of	O
posts	O
and	O
tldrs	O
as	O
the	O
evaluation	O
dataset	O
.	O

We	O
use	O
that	O
367,000	O
pairs	O
of	O
posts	O
and	O
replies	O
as	O
the	O
training	O
dataset	O
.	O

As	O
a	O
consequence	O
,	O
we	O
obtained	O
183,500	O
correct	O
pairs	O
of	O
posts	O
and	O
replies	O
and	O
the	O
same	O
number	O
of	O
wrong	O
pairs	O
.	O

tldr	O
briefly	O
explains	O
what	O
is	O
written	O
in	O
the	O
original	O
post	O
and	O
thus	O
can	O
be	O
regarded	O
as	O
a	O
summary	O
.	O

Therefore	O
,	O
we	O
have	O
112,348	O
pairs	O
in	O
total	O
.	O

The	O
number	O
of	O
positive	O
labels	O
and	O
negative	O
labels	O
are	O
equal	O
.	O

We	O
labeled	O
a	O
pair	O
with	O
an	O
actual	O
reply	O
as	O
positive	O
and	O
a	O
pair	O
with	O
a	O
wrong	O
reply	O
that	O
is	O
randomly	O
sampled	O
from	O
the	O
whole	O
dataset	O
as	O
negative	O
.	O

After	O
the	O
preprocessing	O
,	O
we	O
have	O
56,174	O
pairs	O
.	O

We	O
exclude	O
pairs	O
where	O
the	O
number	O
of	O
words	O
in	O
a	O
post	O
or	O
a	O
reply	O
is	O
smaller	O
than	O
50	O
or	O
25	O
.	O

From	O
this	O
dataset	O
,	O
we	O
use	O
post	O
-	O
and	O
-	O
reply	O
pairs	O
to	O
train	O
our	O
model	O
.	O

Mail	B-DatasetName
Dataset	I-DatasetName
.	O

We	O
train	O
and	O
evaluate	O
the	O
model	O
on	O
two	O
domains	O
of	O
datasets	O
.	O

Experiment	O
.	O

To	O
take	O
advantage	O
of	O
our	O
method	O
and	O
conventional	O
methods	O
,	O
we	O
employ	O
reranking	O
;	O
we	O
simply	O
reorder	O
summaries	O
(	O
3	O
sentences	O
)	O
extracted	O
by	O
our	O
model	O
based	O
on	O
the	O
ranking	O
of	O
TextRank	B-MetricName
(	O
Mihalcea	O
and	O
Tarau	O
,	O
2004	O
)	O
.	O

L	O
rep	O
=	O
−t	O
rep	O
log	O
y	O
−	O
(	O
1	O
−	O
t	O
rep	O
)	O
log	O
(	O
1	O
−	O
y	O
)	O
(	O
11	O
)	O
Reranking	O
As	O
we	O
mentioned	O
in	O
the	O
Introduction	O
,	O
we	O
are	O
seeking	O
for	O
a	O
criterion	O
that	O
is	O
different	O
from	O
conventional	O
methods	O
.	O

The	O
loss	O
of	O
this	O
classification	O
L	O
rep	O
is	O
obtained	O
by	O
cross	O
entropy	O
as	O
follows	O
where	O
t	O
rep	O
is	O
1	O
when	O
a	O
reply	O
candidate	O
is	O
an	O
actual	O
reply	O
,	O
and	O
otherwise	O
0	O
.	O

The	O
detail	O
of	O
the	O
computation	O
is	O
described	O
in	O
Appendix	O
A.1	O
.	O
Decomposable	O
Attention	O
.	O

y	O
=	O
sigmoid(DA(x	O
ext	O
1	O
,	O
...	O
,	O
x	O
ext	O
L−1	O
,	O
h	O
r	O
1	O
,	O
...	O
,	O
h	O
r	O
M	O
)	O
)	O
(	O
10	O
)	O
where	O
DA	O
denotes	O
Decomposable	O
Attention	O
.	O

From	O
this	O
architecture	O
,	O
we	O
obtain	O
the	O
probability	O
of	O
binary	O
-	O
classification	O
y	O
through	O
the	O
sigmoid	O
function	O
.	O

Sentence	O
vectors	O
{	O
h	O
r	O
j	O
}	O
of	O
each	O
sentence	O
{	O
s	O
r	O
j	O
}	O
on	O
the	O
reply	O
are	O
computed	O
similarly	O
to	O
the	O
equation	O
1	O
.	O

Suppose	O
a	O
reply	O
candidate	O
R	O
=	O
{	O
s	O
r	O
1	O
,	O
s	O
r	O
2	O
,	O
...	O
,	O
s	O
r	O
M	O
}	O
has	O
M	O
sentences	O
.	O

We	O
labeled	O
actual	O
replies	O
as	O
positive	O
,	O
and	O
randomly	O
sampled	O
posts	O
as	O
negative	O
.	O

Predictor	O
Then	O
,	O
using	O
only	O
the	O
extracted	O
sentences	O
and	O
a	O
reply	O
candidate	O
,	O
the	O
Predictor	O
predicts	O
whether	O
the	O
candidate	O
is	O
an	O
actual	O
reply	O
or	O
not	O
.	O

x	O
ext	O
t	O
=	O
N	O
i=1	O
α	O
ti	O
h	O
p	O
i	O
(	O
1	O
≤	O
t	O
≤	O
L	O
)	O
(	O
8)	O
h	O
ext	O
t+1	O
=	O
LSTM(x	B-MethodName
ext	O
t	O
)	O
(	O
0	O
≤	O
t	O
≤	O
L	O
−	O
1	O
)	O
(	O
9	O
)	O
The	O
initial	O
input	O
vector	O
x	O
ext	O
0	O
of	O
the	O
Extractor	O
is	O
a	O
parameter	O
,	O
and	O
L	O
is	O
defined	O
by	O
a	O
user	O
depending	O
on	O
the	O
number	O
of	O
sentences	O
required	O
for	O
a	O
summary	O
.	O

We	O
repeat	O
this	O
step	O
L	O
times	O
.	O

By	O
adding	O
Gumbel	O
noise	O
g	O
using	O
noise	O
u	O
from	O
a	O
uniform	O
distribution	O
,	O
the	O
attention	O
weights	O
a	O
become	O
a	O
one	O
-	O
hot	O
vector	O
.	O

During	O
the	O
training	O
,	O
we	O
use	O
Gumbel	B-MethodName
Softmax	O
(	O
Jang	O
et	O
al	O
.	O
,	O
2017	O
)	O
to	O
make	O
this	O
discrete	O
process	O
differentiable	O
.	O

The	O
sentence	O
with	O
the	O
highest	O
attention	O
weight	O
is	O
extracted	O
.	O

h	O
ext	O
0	O
=	O
1	O
N	O
N	O
i=1	O
h	O
p	O
i	O
(	O
2	O
)	O
The	O
Extractor	O
computes	O
attention	O
weights	O
using	O
the	O
hidden	O
states	O
of	O
the	O
Extractor	O
h	O
ext	O
t	O
and	O
the	O
sentence	O
features	O
h	O
p	O
i	O
computed	O
on	O
the	O
Encoder	O
.	O

We	O
set	O
the	O
mean	O
vector	O
of	O
the	O
sentence	O
features	O
of	O
the	O
Encoder	O
h	O
p	O
i	O
as	O
the	O
initial	O
hidden	O
state	O
of	O
the	O
Extractor	O
h	O
ext	O
0	O
.	O

This	O
is	O
because	O
summaries	O
should	O
not	O
depend	O
on	O
replies	O
.	O

Note	O
that	O
the	O
Extractor	O
does	O
not	O
use	O
reply	O
features	O
for	O
extraction	O
.	O

For	O
accurate	O
prediction	O
,	O
the	O
Extractor	O
learns	O
to	O
extract	O
sentences	O
that	O
replies	O
frequently	O
refer	O
to	O
.	O

..	O
,	O
x	O
p	O
iK	O
i	O
}	O
through	O
word	O
embedding	O
layers	O
.	O

Words	O
are	O
embedded	O
to	O
continuous	O
vectors	O
X	O
p	O
i	O
=	O
{	O
x	O
p	O
i1	O
,	O
x	O
p	O
i2	O
,	O
.	O

Each	O
sentence	O
s	O
p	O
i	O
comprises	O
K	O
i	O
words	O
W	O
p	O
i	O
=	O
{	O
w	O
p	O
i1	O
,	O
w	O
p	O
i2	O
,	O
...	O
,	O
w	O
p	O
iK	O
i	O
}	O
.	O

First	O
,	O
the	O
post	O
is	O
split	O
into	O
N	O
sentences	O
{	O
s	O
p	O
1	O
,	O
s	O
p	O
2	O
,	O
...	O
,	O
s	O
p	O
N	O
}	O
.	O

Encoder	O
The	O
Encoder	O
computes	O
features	O
of	O
posts	O
.	O

We	O
describe	O
each	O
component	O
below	O
.	O

The	O
Encoder	O
computes	O
features	O
of	O
posts	O
,	O
the	O
Extractor	O
extracts	O
sentences	O
of	O
a	O
post	O
to	O
use	O
for	O
prediction	O
,	O
and	O
the	O
Predictor	O
predicts	O
whether	O
a	O
reply	O
candidate	O
is	O
an	O
actual	O
reply	O
or	O
not	O
.	O

The	O
model	O
comprises	O
an	O
Encoder	O
,	O
an	O
Extractor	O
,	O
and	O
a	O
Predictor	O
.	O

The	O
training	O
task	O
of	O
the	O
model	O
is	O
to	O
predict	O
whether	O
a	O
reply	O
candidate	O
is	O
true	O
or	O
not	O
.	O

A	O
reply	O
candidate	O
can	O
be	O
either	O
a	O
true	O
or	O
a	O
false	O
reply	O
to	O
the	O
post	O
.	O

The	O
inputs	O
to	O
the	O
model	O
during	O
training	O
are	O
a	O
post	O
and	O
reply	O
candidate	O
.	O

Figure	O
2	O
shows	O
the	O
structure	O
of	O
the	O
model	O
.	O

Model	O
.	O

In	O
our	O
research	O
,	O
we	O
solely	O
focus	O
on	O
quotes	O
,	O
and	O
do	O
not	O
directly	O
use	O
quotes	O
as	O
supervision	O
;	O
rather	O
,	O
we	O
aim	O
to	O
extract	O
implicit	O
quotes	O
.	O

The	O
previous	O
research	O
used	O
quotes	O
as	O
auxiliary	O
features	O
.	O

Some	O
previous	O
work	O
(	O
Carenini	O
et	O
al	O
.	O
,	O
2007;Oya	O
and	O
Carenini	O
,	O
2014	O
)	O
assigned	O
weights	O
to	O
words	O
that	O
appeared	O
in	O
quotes	O
,	O
and	O
improved	O
the	O
conventional	O
centroidbased	O
methods	O
.	O

When	O
we	O
reply	O
to	O
a	O
post	O
or	O
an	O
email	O
and	O
when	O
we	O
want	O
to	O
emphasize	O
a	O
certain	O
part	O
of	O
it	O
,	O
we	O
quote	O
the	O
original	O
text	O
.	O

Dialogue	B-TaskName
act	I-TaskName
classification	I-TaskName
is	O
a	O
classification	O
task	O
that	O
classifies	O
sentences	O
depending	O
on	O
what	O
their	O
functions	O
are	O
(	O
e.g.	O
:	O
questions	O
,	O
answers	O
,	O
greetings	O
)	O
,	O
and	O
has	O
also	O
been	O
applied	O
for	O
summarization	O
(	O
Bhatia	O
et	O
al	O
.	O
,	O
2014;Oya	O
and	O
Carenini	O
,	O
2014	O
)	O
.	O

A	O
few	O
used	O
path	O
scores	O
of	O
word	O
graphs	O
(	O
Mehdad	O
et	O
al	O
.	O
,	O
2014;Shang	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

!	O
"	O
!	O
!	O
#	O
!	O
!	O
$	O
!	O
!	O
"	O
%	O
&	O
'	O
!	O
#	O
%	O
&	O
'	O
!	O
(	O
%	O
&	O
'	O
…	O
!	O
"	O
)	O
!	O
#	O
)	O
!	O
*	O
)	O
…	O
…	O
…	O
"	O
"	O
!	O
Split	O
to	O
sentences	O
Attention	O
&	O
Gumbel	O
Softmax	O
#	O
"	O
%	O
&	O
'	O
≓	O
!	O
+	O
!	O
#	O
,	O
%	O
&	O
'	O
#	O
(	O
%	O
&	O
'	O
≓	O
!	O
-	O
!	O
"	O
#	O
!	O
"	O
$	O
!	O
"	O
"	O
)	O
"	O
#	O
)	O
"	O
*	O
)	O
BiLSTM	O
BiLSTM	O
BiLSTM	O
…	O
Research	O
on	O
the	O
summarization	O
of	O
online	O
conversations	O
such	O
as	O
mail	O
,	O
chat	O
,	O
social	O
media	O
,	O
and	O
online	O
discussion	O
fora	O
has	O
been	O
conducted	O
for	O
a	O
long	O
time	O
.	O

Another	O
research	O
employed	O
a	O
task	O
to	O
reconstruct	O
masked	O
sentences	O
for	O
summarization	O
(	O
Laban	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

(	O
2019	O
)	O
(	O
2019	O
)	O
generated	O
summaries	O
from	O
mean	O
vectors	O
of	O
review	O
vectors	O
,	O
and	O
Amplayo	O
and	O
Lapata	O
(	O
2020	O
)	O
employed	O
the	O
prior	O
distribution	O
of	O
Variational	O
Auto	O
-	I-MethodName
Encoder	O
to	O
induce	O
summaries	O
.	O

For	O
review	B-TaskName
abstractive	I-TaskName
summarization	I-TaskName
,	O
Isonuma	O
et	O
al	O
.	O

(	O
2019	O
)	O
employed	O
the	O
reconstruction	O
task	O
of	O
the	O
original	O
sentence	O
from	O
a	O
compressed	O
one	O
.	O

Baziotis	O
et	O
al	O
.	O

For	O
sentence	O
compression	O
,	O
Fevry	O
and	O
Phang	O
(	O
2018	O
)	O
employed	O
the	O
task	O
to	O
reorder	O
the	O
shuffled	O
word	O
order	O
of	O
sentences	O
.	O

As	O
for	O
end	O
-	O
to	O
-	O
end	O
unsupervised	O
neural	O
models	O
,	O
a	O
few	O
abstractive	O
models	O
have	O
been	O
proposed	O
.	O

That	O
is	O
,	O
our	O
model	O
can	O
extract	O
salient	O
sentences	O
that	O
conventional	O
methods	O
fail	O
to	O
.	O

These	O
methods	O
assume	O
that	O
important	O
topics	O
appear	O
frequently	O
in	O
a	O
document	O
,	O
but	O
our	O
model	O
focuses	O
on	O
a	O
different	O
aspect	O
of	O
texts	O
:	O
the	O
probability	O
of	O
being	O
quoted	O
.	O

Other	O
models	O
use	O
reconstruction	O
loss	O
(	O
He	O
et	O
al	O
.	O
,	O
2012;Liu	O
et	O
al	O
.	O
,	O
2015;Ma	O
et	O
al	O
.	O
,	O
2016	O
)	O
,	O
Kullback	B-HyperparameterValue
-	I-HyperparameterValue
Leibler	I-HyperparameterValue
divergence	I-MethodName
(	O
Haghighi	O
and	O
Vanderwende	O
,	O
2009	O
)	O
or	O
path	O
score	O
calculation	O
(	O
Mehdad	O
et	O
al	O
.	O
,	O
2014;Shang	O
et	O
al	O
.	O
,	O
2018	O
)	O
based	O
on	O
multi	O
-	O
sentence	O
compression	O
algorithm	O
(	O
Filippova	O
,	O
2010	O
)	O
.	O

The	O
graph	B-MethodName
-	I-MethodName
centrality	I-MethodName
-	I-MethodName
based	I-MethodName
method	I-MethodName
(	O
Mihalcea	O
and	O
Tarau	O
,	O
2004;Erkan	O
and	O
Radev	O
,	O
2004;Zheng	O
and	O
Lapata	O
,	O
2019	O
)	O
and	O
centroid	B-MethodName
-	I-MethodName
based	I-MethodName
method	I-MethodName
(	O
Gholipour	O
Ghalandari	O
,	O
2017	O
)	O
have	O
been	O
major	O
methods	O
in	O
this	O
field	O
.	O

Related	O
Works	O
.	O

The	O
contributions	O
of	O
our	O
research	O
are	O
as	O
follows	O
:	O
•	O
We	O
verified	O
that	O
"	O
the	O
possibility	O
of	O
being	O
quoted	O
"	O
is	O
useful	O
for	O
summarization	O
,	O
and	O
demonstrated	O
that	O
it	O
reflects	O
an	O
important	O
aspect	O
of	O
saliency	O
that	O
conventional	O
methods	O
do	O
not	O
.	O

Furthermore	O
,	O
we	O
both	O
quantitatively	O
and	O
qualitatively	O
analyzed	O
that	O
our	O
model	O
can	O
capture	O
salient	O
sentences	O
that	O
conventional	O
frequency	O
-	O
based	O
methods	O
can	O
not	O
.	O

Our	O
model	O
is	O
based	O
on	O
a	O
hypothesis	O
that	O
the	O
ability	O
of	O
extracting	O
quotes	O
leads	O
to	O
a	O
good	O
result	O
.	O

The	O
model	O
requires	O
replies	O
only	O
during	O
the	O
training	O
and	O
not	O
during	O
the	O
evaluation	O
.	O

The	O
training	O
task	O
of	O
the	O
model	O
is	O
to	O
predict	O
if	O
a	O
reply	O
candidate	O
is	O
an	O
actual	O
reply	O
to	O
the	O
post	O
.	O

We	O
use	O
pairs	O
of	O
a	O
post	O
and	O
reply	O
candidate	O
to	O
train	O
the	O
model	O
.	O

As	O
shown	O
in	O
Figure	O
1	O
,	O
implicit	O
quotes	O
are	O
sentences	O
of	O
posts	O
that	O
are	O
not	O
explicitly	O
quoted	O
in	O
replies	O
,	O
but	O
are	O
those	O
the	O
replies	O
most	O
likely	O
refer	O
to	O
.	O

We	O
propose	O
a	O
model	O
that	O
can	O
be	O
trained	O
without	O
explicit	O
labels	O
of	O
quotes	O
.	O

However	O
,	O
most	O
replies	O
do	O
not	O
include	O
quotes	O
,	O
so	O
it	O
is	O
difficult	O
to	O
use	O
quotes	O
as	O
the	O
training	O
labels	O
of	O
neural	O
models	O
.	O

Previous	O
research	O
assigned	O
weights	O
to	O
words	O
that	O
appear	O
in	O
quotes	O
,	O
and	O
improved	O
the	O
centroidbased	B-MethodName
summarization	I-TaskName
(	O
Carenini	O
et	O
al	O
.	O
,	O
2007;Oya	O
and	O
Carenini	O
,	O
2014	O
)	O
.	O

Thus	O
,	O
we	O
aim	O
to	O
extract	O
quotes	O
as	O
summaries	O
.	O

If	O
we	O
can	O
predict	O
quoted	O
parts	O
,	O
we	O
can	O
extract	O
important	O
sentences	O
irrespective	O
of	O
how	O
frequently	O
the	O
same	O
topic	O
appears	O
in	O
the	O
text	O
.	O

The	O
reply	O
on	O
the	O
bottom	O
includes	O
a	O
quote	O
,	O
which	O
generally	O
starts	O
with	O
a	O
symbol	O
"	O
>	O
"	O
.	O

When	O
one	O
replies	O
to	O
an	O
email	O
or	O
a	O
post	O
,	O
a	O
quote	O
is	O
used	O
to	O
highlight	O
the	O
important	O
parts	O
of	O
the	O
text	O
;	O
an	O
example	O
is	O
shown	O
in	O
Figure	O
1	O
.	O

As	O
an	O
alternative	O
aspect	O
,	O
we	O
propose	O
"	O
the	O
probability	O
of	O
being	O
quoted	O
"	O
.	O

For	O
more	O
accurate	O
summarization	O
,	O
relying	O
solely	O
on	O
the	O
frequency	O
is	O
not	O
sufficient	O
and	O
we	O
need	O
to	O
focus	O
on	O
other	O
aspects	O
of	O
texts	O
.	O

Therefore	O
,	O
if	O
important	O
topics	O
appear	O
only	O
a	O
few	O
times	O
,	O
these	O
methods	O
fail	O
to	O
capture	O
salient	O
sentences	O
.	O

The	O
premise	O
of	O
these	O
methods	O
is	O
that	O
important	O
topics	O
appear	O
frequently	O
in	O
a	O
document	O
.	O

Apart	O
from	O
centrality	O
,	O
centroid	O
of	O
vectors	O
(	O
Gholipour	O
Ghalandari	O
,	O
2017	O
)	O
,	O
Kullback	O
-	O
Leibler	O
divergence	O
(	O
Haghighi	O
and	O
Vanderwende	O
,	O
2009	O
)	O
,	O
reconstruction	O
loss	O
(	O
He	O
et	O
al	O
.	O
,	O
2012;Liu	O
et	O
al	O
.	O
,	O
2015;Ma	O
et	O
al	O
.	O
,	O
2016	O
)	O
,	O
and	O
path	O
scores	O
of	O
word	O
graphs	O
(	O
Mehdad	O
et	O
al	O
.	O
,	O
2014;Shang	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
are	O
leveraged	O
for	O
summarization	O
.	O

Because	O
it	O
is	O
not	O
realistic	O
to	O
prepare	O
such	O
large	O
datasets	O
for	O
every	O
domain	O
,	O
there	O
is	O
a	O
growing	O
requirement	O
for	O
unsupervised	O
methods	O
.	O

Supervised	B-TaskName
summarization	I-TaskName
requires	O
tens	O
of	O
thousands	O
of	O
human	O
-	O
annotated	O
summaries	O
.	O

Introduction	O
.	O

We	O
further	O
discuss	O
two	O
topics	O
;	O
one	O
is	O
whether	O
quote	B-TaskName
extraction	I-TaskName
is	O
an	O
important	O
factor	O
for	O
summarization	B-TaskName
,	O
and	O
the	O
other	O
is	O
whether	O
our	O
model	O
can	O
capture	O
salient	O
sentences	O
that	O
conventional	O
methods	O
can	O
not	O
.	O

To	O
predict	O
accurately	O
,	O
the	O
model	O
learns	O
to	O
extract	O
sentences	O
that	O
replies	O
frequently	O
refer	O
to	O
.	O

For	O
prediction	O
,	O
the	O
model	O
has	O
to	O
choose	O
a	O
few	O
sentences	O
from	O
the	O
post	O
.	O

The	O
training	O
task	O
of	O
the	O
model	O
is	O
to	O
predict	O
whether	O
a	O
reply	O
candidate	O
is	O
a	O
true	O
reply	O
to	O
a	O
post	O
.	O

However	O
,	O
even	O
if	O
it	O
is	O
not	O
explicitly	O
shown	O
,	O
replies	O
always	O
refer	O
to	O
certain	O
parts	O
of	O
texts	O
;	O
we	O
call	O
them	O
implicit	O
quotes	O
.	O

Most	O
replies	O
do	O
not	O
explicitly	O
include	O
quotes	O
,	O
so	O
it	O
is	O
difficult	O
to	O
use	O
quotes	O
as	O
supervision	O
.	O

We	O
aim	O
to	O
extract	O
quoted	O
sentences	O
as	O
summaries	O
.	O

When	O
we	O
reply	O
to	O
posts	O
,	O
quotes	O
are	O
used	O
to	O
highlight	O
important	O
part	O
of	O
texts	O
.	O

Importantly	O
,	O
when	O
q	O
t	O
is	O
chosen	O
such	O
that	O
q	O
t	O
=	O
x	O
t	O
,	O
we	O
can	O
parallelize	O
the	O
computation	O
of	O
the	O
attention	O
mechanism	O
for	O
all	O
time	O
-	O
steps	O
before	O
running	O
a	O
forward	O
pass	O
through	O
the	O
model	O
.	O

The	O
geo	O
-	O
hash	O
information	O
2	O
associated	O
with	O
each	O
utterance	O
encodes	O
a	O
very	O
rough	O
estimate	O
of	O
the	O
geolocation	O
of	O
a	O
user	O
's	O
device	O
.	O

We	O
learn	O
embeddings	O
to	O
represent	O
both	O
the	O
geohash	O
and	O
the	O
dialogue	O
prompt	O
information	O
.	O

We	O
ingest	O
both	O
types	O
of	O
contexts	O
via	O
the	O
concatenationbased	O
approach	O
,	O
using	O
word	O
-	O
embeddings	O
as	O
the	O
attention	O
queries	O
.	O

While	O
we	O
focus	O
on	O
datetime	O
information	O
,	O
we	O
demonstrate	O
that	O
our	O
approach	O
can	O
be	O
applied	O
to	O
any	O
type	O
of	O
non	O
-	O
linguistic	O
context	O
,	O
such	O
as	O
geolocation	O
and	O
dialogue	O
prompts	O
.	O

Moreover	O
,	O
the	O
attention	O
mechanism	O
we	O
propose	O
can	O
improve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
contextual	O
LM	O
models	O
by	O
over	O
2.8	B-MetricValue
%	I-MetricValue
relative	O
in	O
terms	O
of	O
perplexity	B-MetricName
.	O

We	O
find	O
that	O
incorporating	O
datetime	O
context	O
into	O
a	O
LM	O
can	O
yield	O
a	O
relative	O
reduction	O
in	O
perplexity	B-MetricName
of	O
9.0	B-MetricValue
%	I-MetricValue
over	O
a	O
model	O
that	O
does	O
not	O
incorporate	O
context	O
.	O

The	O
proposed	O
model	O
dynamically	O
builds	O
up	O
a	O
representation	O
of	O
contextual	O
information	O
that	O
can	O
be	O
ingested	O
into	O
a	O
RNN	B-MethodName
-	O
LM	O
via	O
a	O
concatenation	O
-	O
based	O
or	O
factorization	O
-	O
based	O
approach	O
.	O

In	O
this	O
paper	O
,	O
we	O
introduce	O
an	O
attention	B-MethodName
-	I-MethodName
based	O
mechanism	O
to	O
condition	O
neural	O
LMs	O
for	O
ASR	B-TaskName
on	O
non	O
-	O
linguistic	O
contextual	O
information	O
.	O

Conclusion	O
.	O

While	O
attention	O
-	O
based	O
models	O
have	O
been	O
used	O
to	O
condition	O
neural	O
models	O
on	O
particular	O
aspects	O
or	O
traits	O
(	O
Zheng	O
et	O
al	O
.	O
,	O
2019;Tang	O
et	O
al	O
.	O
,	O
2016	O
)	O
,	O
we	O
focus	O
on	O
contextual	O
information	O
that	O
benefits	O
ASR	B-TaskName
systems	O
.	O

(	O
2015	O
)	O
.	O

The	O
attention	O
mechanism	O
we	O
propose	O
builds	O
on	O
the	O
global	O
attention	O
model	O
proposed	O
by	O
Luong	O
et	O
al	O
.	O

Our	O
contribution	O
lies	O
first	O
in	O
the	O
application	O
of	O
these	O
models	O
to	O
ASR	B-TaskName
,	O
and	O
secondly	O
their	O
extension	O
with	O
an	O
attention	O
mechanism	O
.	O

Methods	O
that	O
apply	O
low	O
-	O
rank	O
matrix	O
factorization	O
to	O
RNNs	B-MethodName
are	O
somewhat	O
newer	O
,	O
and	O
were	O
first	O
explored	O
by	O
Kuchaiev	O
and	O
Ginsburg	O
(	O
2017	O
)	O
.	O

The	O
concatenation	O
-	O
based	O
ap	O
-	O
proach	O
has	O
been	O
adopted	O
as	O
a	O
common	O
method	O
for	O
incorporating	O
non	O
-	O
linguistic	O
context	O
into	O
a	O
neural	O
LM	O
(	O
Yogatama	O
et	O
al	O
.	O
,	O
2017;Wen	O
et	O
al	O
.	O
,	O
2013;Ma	O
et	O
al	O
.	O
,	O
2018;Ghosh	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

Outside	O
of	O
ASR	B-TaskName
,	O
our	O
work	O
directly	O
builds	O
upon	O
the	O
concatenation	B-MethodName
-	I-MethodName
based	I-MethodName
(	O
Mikolov	O
and	O
Zweig	O
,	O
2012	O
)	O
and	O
factorization	B-MethodName
-	I-MethodName
based	I-MethodName
(	O
Jaech	O
and	O
Ostendorf	O
,	O
2018a	O
)	O
approaches	O
to	O
condition	O
RNN	B-MethodName
-	I-MethodName
LMs	I-MethodName
on	O
sentence	O
context	O
.	O

Related	O
Work	O
.	O

Another	O
related	O
line	O
of	O
research	O
has	O
explored	O
learning	O
utterance	O
embeddings	O
for	O
dialogue	O
systems	O
using	O
Gaussian	B-MethodName
mixture	I-MethodName
models	I-MethodName
that	O
are	O
enhanced	O
with	O
utterance	O
-	O
level	O
context	O
,	O
such	O
as	O
intent	O
(	O
Yan	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

Overall	O
,	O
the	O
behavior	O
of	O
the	O
attention	O
mechanism	O
is	O
consistent	O
with	O
our	O
initial	O
hypothesis	O
that	O
certain	O
types	O
of	O
datetime	O
information	O
can	O
benefit	O
a	O
contextual	O
LM	O
model	O
more	O
than	O
others	O
over	O
the	O
course	O
of	O
an	O
utterance	O
.	O

This	O
shift	O
might	O
indicate	O
that	O
the	O
model	O
has	O
learned	O
to	O
condition	O
the	O
type	O
of	O
music	O
users	O
listen	O
to	O
to	O
the	O
hour	O
of	O
the	O
day	O
.	O

Finally	O
when	O
the	O
word	O
"	O
songs	O
"	O
is	O
ingested	O
,	O
the	O
model	O
substantially	O
reduces	O
the	O
weight	O
placed	O
on	O
month	O
information	O
and	O
in	O
turn	O
increases	O
the	O
weight	O
on	O
hour	O
information	O
.	O

Once	O
the	O
model	O
observes	O
the	O
word	O
"	O
christmas	O
"	O
,	O
it	O
places	O
all	O
of	O
the	O
attention	O
on	O
the	O
month	O
information	O
,	O
indicating	O
the	O
model	O
has	O
successfully	O
learned	O
that	O
"	O
christmas	O
"	O
is	O
a	O
word	O
strongly	O
associated	O
with	O
a	O
particular	O
month	O
(	O
i.e.	O
,	O
December	O
)	O
.	O

This	O
would	O
suggest	O
that	O
conditioning	O
on	O
the	O
fact	O
that	O
the	O
utterance	O
was	O
spoken	O
in	O
December	O
can	O
help	O
the	O
model	O
predict	O
the	O
type	O
of	O
media	O
to	O
play	O
.	O

However	O
as	O
the	O
model	O
processes	O
the	O
subsequent	O
words	O
"	O
play	O
me	O
best	O
"	O
,	O
the	O
attention	O
begins	O
to	O
shift	O
towards	O
using	O
more	O
of	O
the	O
month	O
information	O
(	O
i.e.	O
,	O
that	O
this	O
utterance	O
was	O
spoken	O
in	O
December	O
)	O
,	O
and	O
away	O
from	O
hour	O
and	O
day	O
information	O
.	O

When	O
the	O
model	O
processes	O
the	O
start	O
-	O
of	O
-	O
sentence	O
token	O
,	O
the	O
attention	O
mechanism	O
weights	O
each	O
of	O
the	O
datetime	O
information	O
roughly	O
equally	O
.	O

Figure	O
4	O
shows	O
this	O
analysis	O
.	O

For	O
a	O
given	O
utterance	O
like	O
"	O
play	O
me	O
best	O
christmas	O
songs	O
"	O
spoken	O
in	O
December	O
,	O
we	O
highlight	O
the	O
changing	O
weight	O
placed	O
on	O
each	O
of	O
the	O
datetime	O
information	O
.	O

To	O
do	O
so	O
,	O
we	O
visualize	O
the	O
weights	O
of	O
the	O
attention	O
mechanism	O
as	O
an	O
utterance	O
is	O
processed	O
by	O
the	O
model	O
.	O

We	O
next	O
seek	O
to	O
understand	O
how	O
the	O
attention	O
mechanism	O
constructs	O
a	O
dynamic	O
representations	O
of	O
datetime	O
context	O
.	O

Attention	O
Weights	O
.	O

This	O
analysis	O
further	O
corroborates	O
that	O
the	O
trained	O
contextual	O
LMs	O
successfully	O
condition	O
their	O
predictions	O
on	O
datetime	O
information	O
.	O

The	O
horizontal	O
blue	O
dashed	O
line	O
indicates	O
the	O
conditional	O
probability	O
of	O
the	O
word	O
'	O
snooze	O
'	O
following	O
the	O
start	O
-	O
of	O
-	O
sentence	O
token	O
when	O
evaluated	O
with	O
a	O
LM	O
that	O
does	O
not	O
ingest	O
datetime	O
information	O
.	O

As	O
we	O
would	O
expect	O
,	O
the	O
probability	O
of	O
this	O
"	O
snooze	O
"	O
is	O
highest	O
in	O
the	O
morning	O
(	O
between	O
5	O
and	O
6	O
am	O
)	O
,	O
as	O
users	O
are	O
waking	O
up	O
and	O
snoozing	O
their	O
alarms	O
.	O

In	O
Figure	O
3	O
,	O
we	O
evaluate	O
the	O
conditional	O
probability	O
of	O
the	O
word	O
"	O
snooze	O
"	O
in	O
an	O
utterance	O
following	O
the	O
start	O
-	O
of	O
-	O
sentence	O
token	O
,	O
as	O
we	O
vary	O
the	O
hour	O
of	O
day	O
information	O
associated	O
with	O
this	O
utterance	O
.	O

For	O
a	O
given	O
utterance	O
,	O
we	O
can	O
evaluate	O
the	O
probability	O
of	O
the	O
words	O
in	O
the	O
utterance	O
as	O
we	O
vary	O
the	O
datetime	O
information	O
associated	O
with	O
the	O
utterance	O
.	O

In	O
addition	O
to	O
these	O
results	O
,	O
we	O
visualize	O
how	O
the	O
contextual	O
LMs	O
leverage	O
datetime	O
contexts	O
.	O

Visual	O
Analysis	O
.	O

Method	O
.	O

Recall	O
that	O
when	O
trained	O
on	O
correct	O
datetime	O
information	O
this	O
was	O
our	O
best	O
-	O
performing	O
model	O
overall	O
in	O
terms	O
of	O
both	O
perplexity	B-MetricName
and	O
WER	B-MetricName
,	O
indicating	O
that	O
the	O
performance	O
of	O
this	O
model	O
can	O
be	O
attributed	O
in	O
part	O
to	O
its	O
use	O
of	O
contextual	O
information	O
.	O

We	O
observed	O
the	O
overall	O
largest	O
relative	O
degradation	O
in	O
perplexity	B-MetricName
,	O
when	O
using	O
the	O
concatenationbased	O
model	O
.	O

In	O
general	O
,	O
if	O
a	O
model	O
uses	O
datetime	O
information	O
as	O
an	O
additional	O
signal	O
,	O
we	O
would	O
expect	O
the	O
performance	O
of	O
the	O
model	O
to	O
decrease	O
when	O
the	O
datetime	O
context	O
is	O
shuffled	O
.	O

In	O
Table	O
4	O
,	O
we	O
report	O
the	O
relative	O
degradation	O
(	O
i.e.	O
,	O
a	O
negative	O
reduction	O
)	O
in	O
perplexity	B-MetricName
resulting	O
from	O
evaluating	O
these	O
models	O
on	O
the	O
shuffled	O
datetime	O
contexts	O
.	O

For	O
each	O
of	O
our	O
best	O
-	O
performing	O
models	O
in	O
a	O
given	O
category	O
of	O
method	O
(	O
prepend	O
,	O
concat	O
,	O
or	O
factor	O
)	O
,	O
we	O
retrained	O
and	O
evaluated	O
those	O
models	O
on	O
the	O
dataset	O
containing	O
shuffled	O
datetime	O
information	O
.	O

terance	O
in	O
our	O
training	O
and	O
test	O
sets	O
.	O

To	O
answer	O
this	O
question	O
,	O
we	O
randomly	O
shuffled	O
the	O
datetime	O
information	O
associated	O
with	O
each	O
ut-2	O
We	O
use	O
a	O
two	O
integer	O
precision	O
geo	O
-	O
hash	O
.	O

The	O
first	O
question	O
we	O
hope	O
to	O
answer	O
is	O
:	O
to	O
what	O
extent	O
can	O
the	O
relative	O
improvements	O
in	O
perplexity	B-MetricName
and	O
WER	B-MetricName
in	O
the	O
models	O
that	O
incorporate	O
datetime	O
context	O
be	O
explained	O
by	O
the	O
additional	O
signal	O
from	O
the	O
context	O
versus	O
the	O
additional	O
parameters	O
that	O
these	O
models	O
contain	O
?	O

Datetime	O
Context	O
Signal	O
.	O

In	O
this	O
section	O
,	O
we	O
focus	O
once	O
again	O
on	O
datetime	O
information	O
to	O
better	O
understand	O
how	O
contextual	O
LMs	O
use	O
datetime	O
signal	O
.	O

Analysis	O
.	O

In	O
general	O
,	O
we	O
find	O
that	O
conditioning	O
neural	O
LMs	O
on	O
each	O
of	O
the	O
different	O
types	O
of	O
context	O
reduces	O
perplexity	B-MetricName
and	O
WER	B-MetricName
.	O

Table	O
3	O
summarizes	O
the	O
results	O
.	O

We	O
evaluate	O
these	O
models	O
on	O
a	O
test	O
set	O
of	O
de	O
-	O
identified	O
utterances	O
representative	O
of	O
user	O
interactions	O
with	O
Alexa	O
.	O

Dialogue	O
prompts	O
indicate	O
whether	O
a	O
transcribed	O
utterance	O
was	O
an	O
initial	O
query	O
to	O
the	O
dialog	O
system	O
or	O
if	O
it	O
was	O
a	O
follow	O
-	O
up	O
turn	O
.	O

Relative	B-MetricName
PPL	I-MetricName
Reduction	I-MetricName
(	O
%	B-MetricName
)	O
WERR	B-MetricName
(	O
%	O
)	O
Full	O
Tail	O
Full	O
Tail	O
Default	O
0.0	O
0.0	O
0.0	O
0.0	O
Datetime	O
11.4	O
11.6	O
1.6	O
1.7	O
Geo	O
-	O
hash	O
12.4	O
12.5	O
0.5	O
1.0	O
Dialogue	O
Prompt	O
13.9	O
14.1	O
0.3	O
0.6	O
We	O
train	O
the	O
LMs	O
on	O
a	O
subset	O
of	O
the	O
utterances	O
of	O
the	O
initial	O
dataset	O
which	O
also	O
contain	O
utterancelevel	O
geo	O
-	O
hash	O
information	O
and	O
dialogue	O
prompt	O
information	O
.	O

Context	O
Type	O
.	O

To	O
illustrate	O
this	O
point	O
we	O
train	O
two	O
neural	O
LMs	O
using	O
two	O
other	O
types	O
of	O
context	O
:	O
geolocation	O
information	O
and	O
dialogue	O
prompts	O
.	O

We	O
underscore	O
,	O
however	O
,	O
that	O
the	O
contextual	O
mechanism	O
we	O
introduce	O
can	O
be	O
applied	O
to	O
any	O
type	O
of	O
contextual	O
information	O
that	O
can	O
be	O
represented	O
as	O
embeddings	O
.	O

So	O
far	O
,	O
our	O
experiments	O
have	O
focused	O
exclusively	O
on	O
conditioning	O
neural	O
LMs	O
on	O
datetime	O
context	O
.	O

Other	O
Non	O
-	O
Linguistic	O
Context	O
.	O

Method	O
.	O

As	O
in	O
Table	O
1	O
,	O
the	O
concatenationbased	B-MethodName
model	I-MethodName
with	I-MethodName
attention	I-MethodName
mechanism	I-MethodName
achieved	O
the	O
largest	O
reductions	O
in	O
WER	B-TaskName
.	O

We	O
evaluated	O
the	O
relative	O
WER	B-MetricName
reduction(WERR	I-MetricName
)	O
on	O
a	O
large	O
test	O
set	O
of	O
de	O
-	O
identified	O
,	O
transcribed	O
utterances	O
representative	O
of	O
general	O
user	O
interactions	O
with	O
Alexa	O
,	O
as	O
well	O
as	O
on	O
the	O
tail	O
of	O
this	O
dataset	O
.	O

Table	O
2	O
summarizes	O
the	O
results	O
.	O

As	O
the	O
LM	O
component	O
of	O
this	O
system	O
,	O
we	O
used	O
the	O
bestperforming	O
models	O
within	O
each	O
category	O
of	O
method	O
that	O
we	O
report	O
in	O
Table	O
1	O
.	O

In	O
addition	O
to	O
evaluating	O
the	O
models	O
on	O
relative	O
reductions	O
in	O
perplexity	B-MetricName
,	O
we	O
also	O
validated	O
the	O
downstream	O
performance	O
of	O
a	O
hybrid	B-MethodName
CTC	B-MethodName
-	I-MethodName
HMM	I-MethodName
(	O
Graves	O
et	O
al	O
.	O
,	O
2006	O
)	O
ASR	B-TaskName
system	O
that	O
incorporated	O
contextual	O
information	O
in	O
its	O
LM	O
component	O
.	O

In	O
this	O
instance	O
,	O
perplexity	B-MetricName
was	O
reduced	O
by	O
2.8	B-MetricValue
%	I-MetricValue
on	O
the	O
tail	O
of	O
our	O
evaluation	O
set	O
,	O
and	O
by	O
2.1	B-MetricValue
%	I-MetricValue
on	O
the	O
full	O
dataset	O
.	O

The	O
use	O
of	O
attention	O
led	O
to	O
the	O
largest	O
relative	O
improvement	O
in	O
the	O
factorization	O
-	O
based	O
approach	O
when	O
using	O
learned	O
context	O
embeddings	O
.	O

In	O
nearly	O
every	O
experiment	O
we	O
ran	O
,	O
we	O
found	O
that	O
our	O
attention	O
mechanism	O
further	O
reduced	O
perplexity	O
.	O

Again	O
,	O
we	O
found	O
that	O
on	O
the	O
full	O
dataset	O
the	O
improvement	O
in	O
perplexity	B-MetricName
by	O
using	O
our	O
attention	O
mechanism	O
was	O
statistically	O
significant	O
.	O

In	O
the	O
case	O
of	O
the	O
factorization	B-MethodName
-	I-MethodName
based	I-MethodName
approach	O
,	O
we	O
achieved	O
the	O
lowest	O
perplexity	B-MetricName
when	O
the	O
attention	O
mechanism	O
used	O
the	O
hidden	O
state	O
of	O
the	O
RNN	B-MethodName
model	O
as	O
the	O
query	O
vector	O
and	O
datetime	O
information	O
was	O
represented	O
as	O
a	O
feature	O
-	O
engineered	O
vector	O
.	O

Figure	O
2	O
visualizes	O
the	O
intervals	O
.	O

Confidence	O
intervals	O
were	O
calculated	O
by	O
running	O
the	O
training	O
algorithm	O
10	O
times	O
for	O
each	O
model	O
type	O
.	O

We	O
corroborated	O
these	O
results	O
by	O
computing	O
95	B-MetricValue
%	I-MetricValue
confidence	O
intervals	O
for	O
the	O
best	O
-	O
performing	O
concatenation	O
-	O
based	O
models	O
with	O
and	O
without	O
attention	O
.	O

We	O
obtained	O
the	O
best	O
results	O
when	O
representing	O
datetime	O
information	O
as	O
learned	O
embeddings	O
and	O
using	O
the	O
input	O
embedding	O
at	O
a	O
given	O
time	O
-	O
step	O
as	O
the	O
query	O
vector	O
.	O

For	O
the	O
concatenation	B-MethodName
-	I-MethodName
based	I-MethodName
model	I-MethodName
,	O
we	O
found	O
that	O
adding	O
our	O
attention	O
mechanism	O
led	O
to	O
further	O
reductions	O
in	O
perplexity	O
,	O
regardless	O
of	O
the	O
type	O
of	O
query	O
vector	O
or	O
context	O
representation	O
used	O
.	O

We	O
also	O
differentiate	O
between	O
the	O
two	O
variants	O
of	O
encoding	O
the	O
query	O
vector	O
used	O
by	O
the	O
attention	O
mechanism	O
:	O
either	O
by	O
using	O
the	O
hidden	O
state	O
vector	O
,	O
or	O
the	O
input	O
embedding	O
at	O
a	O
given	O
time	O
-	O
step	O
.	O

In	O
reporting	O
our	O
results	O
,	O
we	O
distinguish	O
between	O
the	O
two	O
forms	O
of	O
representing	O
contextual	O
information	O
:	O
either	O
as	O
learned	O
embeddings	O
or	O
as	O
a	O
featureengineered	O
representation	O
.	O

We	O
additionally	O
trained	O
a	O
simple	O
baseline	O
,	O
Prepend	B-MethodName
,	O
which	O
was	O
comprised	O
of	O
a	O
standard	O
LSTM	O
model	O
that	O
treated	O
datetime	O
context	O
as	O
input	O
tokens	O
that	O
were	O
prepended	O
to	O
the	O
input	O
texts	O
.	O

In	O
Table	O
1	O
,	O
we	O
report	O
the	O
relative	O
decrease	O
in	O
perplexity	B-MetricName
of	O
models	O
that	O
leveraged	O
datetime	O
context	O
compared	O
to	O
a	O
baseline	B-MethodName
LSTM	I-MethodName
model	O
that	O
did	O
not	O
use	O
any	O
contextual	O
information	O
.	O

In	O
practice	O
,	O
these	O
two	O
statistics	O
have	O
been	O
shown	O
to	O
be	O
correlated	O
by	O
a	O
power	O
law	O
relationship	O
(	O
Klakow	O
and	O
Peters	O
,	O
2002	O
)	O
.	O

Word	B-MetricName
error	I-MetricName
rate	I-MetricName
,	O
on	O
the	O
other	O
hand	O
,	O
measures	O
the	O
Levenshtein	B-MetricName
(	O
minimum	O
edit	O
)	O
distance	O
between	O
a	O
recognized	O
word	O
sequence	O
and	O
a	O
reference	O
word	O
sequence	O
.	O

Perplexity	B-MetricName
is	O
a	O
common	O
statistic	O
widely	O
used	O
in	O
language	O
modeling	O
and	O
speech	O
recognition	O
to	O
measure	O
how	O
well	O
a	O
language	O
model	O
predicts	O
a	O
sample	O
of	O
text	O
(	O
Jelinek	O
et	O
al	O
.	O
,	O
1977	O
)	O
.	O

We	O
used	O
two	O
metrics	O
for	O
our	O
evaluations	O
:	O
perplexity	B-MetricName
and	O
word	B-MetricName
error	I-MetricName
rate	I-MetricName
.	O

We	O
also	O
defined	O
the	O
head	O
and	O
tail	O
subsets	O
of	O
our	O
development	O
set	O
,	O
representing	O
,	O
respectively	O
,	O
the	O
top	O
5	O
%	O
most	O
frequently	O
occurring	O
utterances	O
,	O
and	O
utterances	O
occurring	O
only	O
once	O
.	O

The	O
utterances	O
in	O
our	O
training	O
and	O
evaluation	O
set	O
were	O
collected	O
in	O
the	O
same	O
time	O
-	O
range	O
.	O

We	O
evaluated	O
our	O
models	O
on	O
a	O
heldout	O
set	O
of	O
utterances	O
that	O
were	O
randomly	O
sampled	O
from	O
the	O
full	O
dataset	O
.	O

Datetime	O
.	O

6	O
Results	O
.	O

We	O
initialized	O
random	O
weights	O
using	O
Xavier	B-HyperparameterValue
-	I-HyperparameterValue
He	I-HyperparameterValue
weight	I-HyperparameterValue
initialization	O
(	O
He	O
et	O
al	O
.	O
,	O
2015	O
)	O
.	O

Other	O
hyperparameters	O
,	O
such	O
as	O
the	O
initial	B-HyperparameterName
learning	B-HyperparameterName
rate	I-HyperparameterName
,	O
were	O
selected	O
via	O
random	O
search	O
.	O

In	O
practice	O
,	O
we	O
found	O
that	O
the	O
larger	O
the	O
rank	O
size	O
the	O
less	O
stable	O
the	O
training	O
procedure	O
became	O
.	O

We	O
set	O
the	O
rank	B-HyperparameterName
of	I-HyperparameterName
the	O
basis	I-HyperparameterName
tensors	I-HyperparameterName
in	O
the	O
factorization	B-MethodName
-	I-MethodName
approach	I-MethodName
to	O
5	B-HyperparameterValue
,	O
after	O
experimenting	O
with	O
rank	B-HyperparameterName
sizes	I-HyperparameterName
2	B-HyperparameterValue
,	O
3	B-HyperparameterValue
,	O
10	B-HyperparameterValue
,	O
15	B-HyperparameterValue
,	O
20	B-HyperparameterValue
.	O

We	O
initially	O
experimented	O
with	O
smaller	O
and	O
larger	O
embedding	B-HyperparameterName
sizes	I-HyperparameterName
(	O
50	B-HyperparameterValue
,	O
100	B-HyperparameterValue
,	O
1024	B-HyperparameterValue
)	O
,	O
but	O
found	O
that	O
512	B-HyperparameterValue
generally	O
provided	O
a	O
good	O
tradeoff	O
between	O
model	O
performance	O
and	O
compute	O
resources	O
required	O
to	O
train	O
a	O
model	O
.	O

We	O
used	O
a	O
fixed	O
dimensionality	O
of	O
512	B-HyperparameterValue
for	O
word	O
,	O
context	O
and	O
hidden	O
state	O
embeddings	O
.	O

Implementation	O
of	O
the	O
model	O
and	O
training	O
procedure	O
was	O
written	O
in	O
PyTorch	O
and	O
native	O
PyTorch	O
libraries	O
.	O

The	O
training	O
of	O
each	O
model	O
was	O
conducted	O
on	O
a	O
single	O
V100	O
GPU	O
,	O
with	O
16	O
GB	O
of	O
memory	O
on	O
a	O
Linux	O
cluster	O
,	O
and	O
took	O
roughly	O
6	O
hours	O
to	O
train	O
.	O

400,000	B-HyperparameterValue
batch	B-HyperparameterName
update	I-HyperparameterName
steps	I-HyperparameterName
,	O
using	O
a	O
batch	B-HyperparameterName
-	I-HyperparameterName
size	I-HyperparameterName
of	O
256	B-HyperparameterValue
.	O

Models	O
were	O
trained	O
using	O
the	O
Adam	B-HyperparameterValue
optimizer	B-HyperparameterName
with	O
an	O
initial	B-HyperparameterName
learning	I-HyperparameterName
rate	I-HyperparameterName
of	O
0.001	B-HyperparameterValue
,	O
and	O
a	O
standard	O
cross	O
entropy	O
loss	B-HyperparameterName
function	I-HyperparameterName
.	O

Both	O
the	O
concatenationbased	O
and	O
factorization	O
-	O
based	O
methods	O
can	O
be	O
easily	O
adapted	O
to	O
use	O
an	O
LSTM	O
cell	O
.	O

We	O
used	O
a	O
1	B-HyperparameterValue
-	I-MethodName
recurrent	B-HyperparameterName
-	I-MethodName
layer	B-HyperparameterName
LSTM	I-MethodName
model	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
as	O
the	O
base	O
model	O
in	O
all	O
of	O
our	O
experiments	O
.	O

Experimental	O
Setup	O
.	O

For	O
an	O
utterance	O
like	O
"	O
turn	O
alarm	O
off	O
"	O
,	O
we	O
showcase	O
how	O
the	O
model	O
builds	O
a	O
dynamic	O
representation	O
of	O
the	O
datetime	O
context	O
,	O
at	O
a	O
given	O
time	O
-	O
step	O
t	O
(	O
t	O
=	O
2	O
in	O
the	O
figure	O
)	O
.	O

In	O
Figure	O
1	O
we	O
illustrate	O
how	O
the	O
attention	O
mechanism	O
augments	O
the	O
concatenation	O
-	O
based	O
approach	O
.	O

m	O
t	O
=	O
|M	O
|	O
i=1	O
α	O
i	O
,	O
t	O
m	O
i	O
We	O
can	O
now	O
use	O
this	O
constructed	O
context	O
,	O
as	O
the	O
context	O
input	O
to	O
either	O
the	O
concatenation	O
-	O
based	O
or	O
the	O
factorization	O
-	O
based	O
approach	O
.	O

We	O
then	O
define	O
the	O
alignment	O
score	O
as	O
α	O
i	O
,	O
t	O
=	O
sof	O
tmax(score(m	O
i	O
,	O
q	O
t	O
)	O
)	O
The	O
alignment	O
scores	O
are	O
finally	O
used	O
to	O
build	O
up	O
a	O
dynamic	O
representation	O
of	O
the	O
context	O
,	O
m	O
t	O
,	O
for	O
a	O
given	O
time	O
-	O
step	O
.	O

For	O
a	O
given	O
m	O
i	O
∈	O
M	O
and	O
q	O
t	O
,	O
we	O
calculate	O
a	O
score	O
as	O
score(m	O
i	O
,	O
q	O
t	O
)	O
=	O
m	O
T	O
i	O
W	O
a	O
q	O
t	O
.	O

The	O
size	O
of	O
W	O
a	O
is	O
R	O
f	O
×e	O
if	O
q	O
t	O
=	O
x	O
t	O
,	O
or	O
R	O
f	O
×d	O
if	O
q	O
t	O
=	O
h	O
t	O
.	O

To	O
compute	O
this	O
score	O
,	O
we	O
learn	O
a	O
weight	O
matrix	O
W	O
a	O
.	O

Regardless	O
of	O
the	O
choice	O
of	O
q	O
t	O
,	O
the	O
attention	O
mechanism	O
first	O
computes	O
a	O
score	O
for	O
each	O
context	O
embedding	O
m	O
i	O
∈	O
M	O
for	O
a	O
given	O
q	O
t	O
.	O

This	O
can	O
not	O
be	O
done	O
when	O
q	O
t	O
=	O
h	O
t	O
,	O
as	O
the	O
attention	O
mechanism	O
can	O
only	O
be	O
computed	O
sequentially	O
for	O
each	O
hidden	O
state	O
of	O
the	O
model	O
.	O

Let	O
q	O
t	O
=	O
h	O
t	O
,	O
where	O
h	O
t	O
is	O
the	O
hidden	O
state	O
of	O
the	O
RNN	B-MethodName
model	O
at	O
time	O
-	O
step	O
t.	O

2	O
.	O

Let	O
q	O
t	O
=	O
x	O
t	O
,	O
where	O
x	O
t	O
is	O
the	O
embedding	O
for	O
the	O
input	O
word	O
at	O
time	O
-	O
step	O
t.	O

We	O
propose	O
two	O
methods	O
for	O
defining	O
this	O
query	O
vector	O
1	O
.	O

In	O
addition	O
to	O
the	O
set	O
M	O
,	O
the	O
attention	O
mechanism	O
takes	O
in	O
a	O
query	O
vector	O
,	O
q	O
t	O
,	O
for	O
each	O
time	O
-	O
step	O
t.	O

We	O
also	O
experiment	O
with	O
adding	O
a	O
similar	O
vector	O
of	O
all	O
0s	O
in	O
the	O
case	O
where	O
context	O
embeddings	O
are	O
learned	O
,	O
but	O
find	O
no	O
improvement	O
.	O

Thus	O
,	O
the	O
attention	O
mechanism	O
can	O
act	O
as	O
a	O
learnable	O
gate	O
to	O
limit	O
the	O
non	O
-	O
linguistic	O
context	O
passed	O
into	O
the	O
model	O
.	O

We	O
do	O
so	O
because	O
our	O
attention	O
mechanism	O
builds	O
a	O
dynamic	O
representation	O
of	O
the	O
context	O
by	O
interpolating	O
over	O
multiple	O
context	O
embeddings	O
.	O

However	O
,	O
in	O
the	O
case	O
where	O
datetime	O
information	O
is	O
represented	O
as	O
a	O
feature	O
-	O
engineered	O
vector	O
,	O
we	O
augment	O
M	O
to	O
include	O
an	O
8	O
-	O
dimensional	O
vector	O
of	O
all	O
0s	O
:	O
M	O
=	O
{	O
m	O
,	O
0	O
}	O
.	O

We	O
assume	O
as	O
input	O
to	O
the	O
attention	O
mechanism	O
the	O
same	O
set	O
M	O
of	O
context	O
representations	O
.	O

Using	O
an	O
attention	O
mechanism	O
enables	O
us	O
to	O
dynamically	O
weight	O
the	O
importance	O
that	O
the	O
model	O
places	O
on	O
particular	O
datetime	O
context	O
as	O
the	O
model	O
processes	O
an	O
utterance	O
.	O

By	O
the	O
time	O
the	O
model	O
has	O
observed	O
the	O
words	O
"	O
what	O
temperature	O
will	O
"	O
,	O
we	O
would	O
expect	O
the	O
model	O
to	O
condition	O
the	O
predictions	O
of	O
the	O
remaining	O
words	O
primarily	O
on	O
the	O
hour	O
and	O
day	O
information	O
.	O

For	O
instance	O
,	O
assume	O
a	O
LM	O
is	O
given	O
the	O
phrase	O
"	O
what	O
temperature	O
will	O
it	O
be	O
on	O
friday	O
"	O
.	O

We	O
hypothesize	O
that	O
at	O
certain	O
time	O
-	O
steps	O
within	O
an	O
utterance	O
,	O
attending	O
to	O
particular	O
datetime	O
information	O
will	O
facilitate	O
the	O
model	O
's	O
predictions	O
more	O
than	O
other	O
information	O
.	O

We	O
apply	O
this	O
mechanism	O
to	O
the	O
context	O
embeddings	O
at	O
each	O
time	O
-	O
step	O
of	O
the	O
RNN	B-MethodName
model	O
,	O
in	O
order	O
to	O
adapt	O
the	O
context	O
representation	O
dynamically	O
.	O

We	O
propose	O
an	O
attention	O
mechanism	O
that	O
augments	O
both	O
the	O
concatenation	B-MethodName
-	I-MethodName
based	I-MethodName
and	O
factorizationbased	O
approaches	O
.	O

Attention	O
Mechanism	O
.	O

A	O
prediction	O
,	O
ŷt	O
,	O
is	O
generated	O
in	O
the	O
same	O
manner	O
as	O
in	O
the	O
concatenation	B-MethodName
-	O
based	O
model	O
.	O

The	O
resulting	O
matrices	O
W	O
x	O
,	O
W	O
h	O
are	O
now	O
used	O
as	O
the	O
weights	O
in	O
the	O
RNN	B-MethodName
cell	O
.	O

We	O
can	O
now	O
use	O
the	O
contextual	O
representation	O
to	O
interpolate	O
the	O
two	O
sets	O
of	O
basis	O
tensors	O
to	O
produce	O
two	O
new	O
weight	O
matrices	O
,	O
W	O
x	O
and	O
W	O
h	O
,	O
where	O
W	O
x	O
=	O
W	O
x	O
+	O
(	O
W	O
(	O
L	O
)	O
x	O
T	O
m	O
)	O
T	O
(	O
W	O
(	O
R	O
)	O
x	O
T	O
m	O
)	O
W	O
h	O
=	O
W	O
h	O
+	O
(	O
W	O
(	O
L	O
)	O
h	O
T	O
m	O
)	O
T	O
(	O
W	O
(	O
R	O
)	O
h	O
T	O
m	O
)	O
.	O

The	O
right	O
adaptation	O
tensors	O
,	O
W	O
(	O
R	O
)	O
x	O
,	O
W	O
(	O
R	O
)	O
h	O
are	O
both	O
of	O
dimensionality	O
R	O
r×d×f	O
.	O

The	O
left	O
adaptation	O
tensors	O
,	O
W	O
(	O
L	O
)	O
x	O
,	O
W	O
(	O
L	O
)	O
h	O
,	O
are	O
of	O
dimensionality	O
R	O
f	O
×e×r	O
,	O
and	O
R	O
f	O
×d×r	O
,	O
respectively	O
.	O

These	O
basis	O
tensors	O
are	O
of	O
fixed	O
rank	O
r	O
,	O
where	O
r	O
is	O
a	O
tuned	O
hyperparameter	O
.	O

The	O
adaption	O
process	O
involves	O
learning	O
basis	O
tensors	O
W	O
(	O
L	O
)	O
x	O
,	O
W	O
(	O
R	O
)	O
x	O
and	O
W	O
(	O
L	O
)	O
h	O
,	O
W	O
(	O
R	O
)	O
h	O
.	O

Compared	O
to	O
the	O
concatenation	B-MethodName
-	I-MethodName
based	I-MethodName
architecture	I-MethodName
,	O
this	O
approach	O
adapts	O
a	O
larger	O
fraction	O
of	O
the	O
RNN	B-MethodName
model	O
's	O
parameters	O
.	O

Unlike	O
the	O
concatenation	B-MethodName
-	I-MethodName
based	I-MethodName
approach	O
,	O
which	O
directly	O
inserts	O
contextual	O
information	O
into	O
the	O
RNN	B-MethodName
cell	O
,	O
the	O
factorization	B-MethodName
-	O
based	O
method	O
adapts	O
the	O
weight	O
matrices	O
W	O
x	O
,	O
W	O
h	O
of	O
the	O
RNN	B-MethodName
model	O
.	O

Factorization	B-MethodName
-	I-MethodName
based	I-MethodName
LM	I-MethodName
Adaptation	I-MethodName
.	O

To	O
generate	O
a	O
prediction	O
,	O
ŷt	O
for	O
a	O
word	O
at	O
time	O
-	O
step	O
t	O
,	O
h	O
t	O
is	O
passed	O
through	O
a	O
projection	O
layer	O
,	O
W	O
v	O
∈	O
R	O
d×|V	O
|	O
to	O
match	O
the	O
dimension	O
of	O
the	O
vocabulary	O
size	O
|V	O
|	O
,	O
before	O
applying	O
a	O
softmax	O
layer	O
ŷt	O
=	O
sof	O
tmax(W	O
v	O
h	O
t	O
)	O
.	O

Notice	O
that	O
the	O
expression	O
above	O
can	O
be	O
equivalently	O
calculated	O
by	O
concatenating	O
the	O
matrices	O
W	O
m	O
and	O
W	O
x	O
,	O
as	O
well	O
as	O
the	O
vectors	O
m	O
and	O
x	O
t	O
h	O
t	O
=	O
σ([W	O
x	O
;	O
W	O
m	O
]	O
[	O
x	O
t	O
;	O
m	O
]	O
+	O
W	O
h	O
h	O
t−1	O
+	O
b	O
)	O
.	O

In	O
the	O
concatenation	O
-	O
based	O
approach	O
,	O
this	O
hidden	O
-	O
state	O
is	O
adapted	O
in	O
the	O
following	O
manner	O
h	O
t	O
=	O
σ(W	O
m	O
m	O
+	O
W	O
x	O
x	O
t	O
+	O
W	O
h	O
h	O
t−1	O
+	O
b	O
)	O
.	O

A	O
standard	O
RNN	B-MethodName
model	O
without	O
contextual	O
information	O
keeps	O
track	O
of	O
a	O
hidden	O
-	O
state	O
at	O
time	O
-	O
step	O
t	O
,	O
h	O
t	O
,	O
that	O
is	O
calculated	O
as	O
h	O
t	O
=	O
σ(W	O
x	O
x	O
t	O
+	O
W	O
h	O
h	O
t−1	O
+	O
b	O
)	O
,	O
where	O
x	O
t	O
represents	O
the	O
word	O
embedding	O
at	O
timestep	O
t	O
,	O
b	O
is	O
a	O
bias	O
vector	O
,	O
W	O
x	O
∈	O
R	O
e×d	O
,	O
and	O
W	O
h	O
∈	O
R	O
d×d	O
.	O

In	O
this	O
case	O
,	O
f	B-HyperparameterName
is	O
four	O
-	O
times	O
the	O
size	O
of	O
each	O
individual	O
context	O
embedding	O
.	O

When	O
representing	O
contextual	O
information	O
as	O
learned	O
embeddings	O
,	O
recall	O
that	O
we	O
first	O
concatenate	O
the	O
embeddings	O
together	O
before	O
passing	O
these	O
into	O
the	O
model	O
.	O

In	O
practice	O
,	O
f	O
is	O
either	O
a	O
hyperparameter	O
when	O
datetime	O
context	O
is	O
represented	O
as	O
learned	O
embeddings	O
,	O
or	O
f	O
=	O
8	O
when	O
this	O
context	O
is	O
represented	O
as	O
a	O
feature	O
-	O
engineered	O
vector	O
.	O

The	O
concatenation	B-MethodName
-	O
based	O
approach	O
learns	O
a	O
weight	O
matrix	O
W	O
m	O
of	O
dimensionality	O
R	O
f	O
×d	O
,	O
where	O
f	O
represents	O
the	O
size	O
of	O
the	O
context	O
representation	O
and	O
d	O
represents	O
the	O
hidden	O
-	O
dimensionality	O
of	O
the	O
RNN	B-MethodName
model	O
.	O

Concatenation	B-MethodName
-	I-MethodName
based	I-MethodName
LM	I-MethodName
Adaptation	I-MethodName
.	O

The	O
methods	O
we	O
discuss	O
,	O
however	O
,	O
can	O
be	O
applied	O
to	O
each	O
layer	O
of	O
a	O
multi	O
-	O
layer	O
RNN	B-MethodName
model	O
.	O

The	O
notation	O
we	O
use	O
to	O
describe	O
architectures	O
assumes	O
a	O
1	B-HyperparameterValue
-	O
layer	B-HyperparameterName
RNN	B-MethodName
model	O
.	O

We	O
then	O
introduce	O
our	O
attentionmechanism	O
that	O
can	O
be	O
used	O
to	O
augment	O
both	O
of	O
these	O
approaches	O
.	O

In	O
this	O
section	O
,	O
we	O
first	O
describe	O
the	O
architecture	O
of	O
the	O
concatenation	B-MethodName
-	I-MethodName
based	I-MethodName
and	O
factorization	B-MethodName
-	I-MethodName
based	I-MethodName
approaches	O
.	O

Model	O
.	O

A	O
set	O
,	O
M	O
,	O
containing	O
a	O
single	O
embedding	O
M	O
=	O
{	O
m	O
}	O
,	O
where	O
m	O
represents	O
an	O
8dimensional	O
feature	O
-	O
engineered	O
contextual	O
datetime	O
representation	O
,	O
as	O
described	O
in	O
the	O
previous	O
section	O
.	O

2	O
.	O

When	O
using	O
the	O
concatenationbased	B-MethodName
or	O
factorization	B-MethodName
-	O
based	O
approaches	O
without	O
attention	O
,	O
we	O
first	O
concatenate	O
the	O
embeddings	O
together	O
,	O
m	O
=	O
[	O
m	O
1	O
;	O
m	O
2	O
;	O
m	O
3	O
;	O
m	O
4	O
]	O
,	O
and	O
use	O
the	O
resulting	O
vector	O
as	O
input	O
to	O
the	O
model	O
.	O

A	O
set	O
,	O
M	O
,	O
of	O
four	O
learned	O
context	O
embeddings	O
M	O
=	O
{	O
m	O
1	O
,	O
m	O
2	O
,	O
m	O
3	O
,	O
m	O
4	O
}	O
,	O
where	O
m	O
1	O
is	O
an	O
encoding	O
of	O
the	O
month	O
information	O
,	O
m	O
2	O
is	O
an	O
encoding	O
of	O
the	O
week	O
information	O
,	O
m	O
3	O
is	O
an	O
encoding	O
of	O
the	O
day	O
of	O
the	O
week	O
information	O
,	O
and	O
m	O
4	O
is	O
an	O
encoding	O
of	O
the	O
hour	O
of	O
the	O
day	O
information	O
.	O

We	O
additionally	O
represent	O
the	O
contextual	O
information	O
as	O
either	O
:	O
1	O
.	O

,	O
n	O
}	O
,	O
where	O
n	O
is	O
the	O
length	O
of	O
the	O
input	O
sequence	O
and	O
e	O
is	O
the	O
dimensionality	O
of	O
the	O
word	O
embeddings	O
.	O

,	O
n	O
}	O
,	O
that	O
are	O
converted	O
by	O
an	O
embedding	O
layer	O
into	O
embeddings	O
x	O
i	O
∈	O
R	O
e	O
for	O
i	O
∈	O
{	O
1	O
,	O
.	O

We	O
assume	O
as	O
input	O
to	O
a	O
model	O
a	O
sequence	O
of	O
either	O
word	O
or	O
subword	O
tokens	O
,	O
w	O
i	O
for	O
i	O
∈	O
{	O
1	O
,	O
.	O

Input	O
Representation	O
.	O

Feature	O
-	O
engineered	O
representation	O
:	O
Additionally	O
,	O
we	O
consider	O
transforming	O
the	O
datetime	O
information	O
into	O
a	O
single	O
8	O
-	O
dimensional	O
feature	O
-	O
engineered	O
vector	O
,	O
where	O
the	O
dimensions	O
of	O
the	O
vector	O
are	O
defined	O
as	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
sin	O
(	O
2π•hour	O
24	O
)	O
cos	O
(	O
2π•hour	O
24	O
)	O
sin	O
(	O
2π•day	O
7	O
)	O
cos	O
(	O
2π•day	O
7	O
)	O
sin	O
(	O
2π•week	O
53	O
)	O
cos	O
(	O
2π•week	O
)	O
sin	O
(	O
2π•month	O
12	O
)	O
cos	O
(	O
2π•month	O
12	O
)	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
Since	O
the	O
datetime	O
context	O
is	O
continuous	O
and	O
cyclical	O
,	O
this	O
approach	O
explicitly	O
encodes	O
tem	O
-	O
poral	O
proximity	O
in	O
the	O
date	O
and	O
time	O
information	O
.	O

2	O
.	O

We	O
experiment	O
with	O
different	O
ways	O
of	O
parsing	O
the	O
information	O
,	O
such	O
as	O
encoding	O
weekday	O
versus	O
weekend	O
,	O
or	O
morning	O
versus	O
evening	O
,	O
but	O
find	O
this	O
information	O
is	O
largely	O
entailed	O
within	O
our	O
method	O
for	O
processing	O
datetime	O
information	O
.	O

These	O
embeddings	O
are	O
initialized	O
as	O
random	O
vectors	O
,	O
and	O
trained	O
along	O
with	O
the	O
rest	O
of	O
the	O
model	O
.	O

These	O
tokens	O
are	O
subsequently	O
used	O
as	O
input	O
to	O
the	O
model	O
,	O
where	O
they	O
are	O
passed	O
through	O
an	O
embedding	O
layer	O
to	O
generate	O
context	O
embeddings	O
.	O

In	O
the	O
example	O
above	O
,	O
we	O
would	O
transform	O
the	O
datetime	O
information	O
into	O
tokens	O
representing	O
:	O
month-12	O
,	O
week-52	O
,	O
wednesday	O
,	O
7	O
am	O
.	O

Learned	O
embeddings	O
:	O
We	O
first	O
consider	O
creating	O
tokens	O
for	O
the	O
month	O
number	O
,	O
week	O
number	O
,	O
day	O
of	O
the	O
week	O
and	O
hour	O
that	O
an	O
utterance	O
was	O
spoken	O
.	O

In	O
order	O
to	O
condition	O
a	O
LM	O
on	O
this	O
datetime	O
information	O
,	O
we	O
consider	O
two	O
methods	O
for	O
transforming	O
the	O
contextual	O
information	O
into	O
a	O
continuous	O
vector	O
representation	O
:	O
1	O
.	O

In	O
the	O
example	O
above	O
,	O
we	O
can	O
infer	O
that	O
the	O
utterance	O
"	O
play	O
christmas	O
music	O
"	O
was	O
spoken	O
on	O
December	O
23	O
,	O
2020	O
at	O
7	O
in	O
the	O
morning	O
local	O
time	O
.	O

A	O
typical	O
utterance	O
in	O
our	O
dataset	O
might	O
look	O
like	O
this	O
:	O
2020	O
-	O
12	O
-	O
23	O
07:00	O
play	O
christmas	O
music	O
.	O

Context	O
Representation	O
.	O

We	O
randomly	O
split	O
our	O
dataset	O
into	O
a	O
training	O
set	O
,	O
development	O
set	O
and	O
test	O
set	O
,	O
using	O
a	O
partition	B-HyperparameterName
ratio	I-HyperparameterName
of	O
90/5/5	B-HyperparameterValue
and	O
we	O
ensure	O
that	O
each	O
partition	O
contains	O
more	O
than	O
500	O
hours	O
worth	O
of	O
data	O
.	O

Any	O
information	O
about	O
the	O
device	O
or	O
the	O
speaker	O
from	O
which	O
an	O
utterance	O
originates	O
has	O
been	O
removed	O
.	O

The	O
datetime	O
information	O
is	O
reported	O
according	O
to	O
the	O
local	O
time	O
zone	O
of	O
each	O
given	O
user	O
.	O

Each	O
utterance	O
also	O
contains	O
associated	O
information	O
about	O
the	O
year	O
,	O
month	O
,	O
day	O
,	O
and	O
hour	O
that	O
the	O
utterance	O
was	O
spoken	O
.	O

We	O
use	O
a	O
corpus	O
of	O
over	O
5,000	O
hours	O
of	O
deidentified	O
,	O
transcribed	O
English	O
utterances	O
,	O
collected	O
over	O
several	O
years	O
.	O

Data	O
.	O

Moreover	O
,	O
our	O
attention	O
mechanism	O
can	O
improve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
for	O
conditional	O
LMs	O
by	O
over	O
2.8	B-MetricValue
%	I-MetricValue
relative	O
in	O
terms	O
of	O
perplexity	B-MetricName
.	O

Compared	O
to	O
a	O
standard	O
model	O
that	O
does	O
not	O
include	O
contextual	O
information	O
,	O
using	O
our	O
method	O
to	O
contextualize	O
a	O
neural	O
LM	O
on	O
datetime	O
information	O
achieves	O
a	O
relative	O
reduction	O
in	O
perplexity	B-MetricName
of	O
7.0	B-MetricValue
%	I-MetricValue
,	O
and	O
a	O
relative	O
reduction	O
in	O
perplexity	B-MetricName
of	O
9.0	B-MetricValue
%	I-MetricValue
when	O
evaluated	O
on	O
the	O
tail	O
of	O
this	O
dataset	O
.	O

We	O
evaluate	O
our	O
method	O
on	O
a	O
large	O
de	O
-	O
identified	O
dataset	O
of	O
transcribed	O
utterances	O
.	O

To	O
underscore	O
this	O
point	O
we	O
also	O
provide	O
results	O
for	O
conditioning	O
LMs	O
on	O
geolocation	O
information	O
and	O
dialogue	O
prompts	O
that	O
are	O
commonly	O
available	O
in	O
ASR	O
systems	O
.	O

Our	O
approach	O
,	O
however	O
,	O
can	O
generalized	O
to	O
any	O
type	O
of	O
context	O
.	O

We	O
concentrate	O
on	O
datetime	O
information	O
because	O
of	O
its	O
widespread	O
availability	O
in	O
many	O
ASR	O
systems	O
.	O

Our	O
experiments	O
focus	O
primarily	O
on	O
conditioning	O
neural	O
LMs	O
on	O
datetime	O
context	O
.	O

The	O
resulting	O
embedding	O
can	O
be	O
used	O
as	O
an	O
additional	O
input	O
to	O
either	O
the	O
concatenation	O
-	O
based	O
or	O
factorization	O
-	O
based	O
model	O
.	O

The	O
attention	O
mechanism	O
that	O
we	O
propose	O
builds	O
up	O
a	O
dynamic	O
context	O
representation	O
over	O
the	O
course	O
of	O
processing	O
an	O
utterance	O
.	O

We	O
introduce	O
an	O
attention	O
mechanism	O
that	O
augments	O
both	O
the	O
concatenation	O
-	O
based	O
and	O
factorization	O
-	O
based	O
approaches	O
to	O
condition	O
a	O
neural	O
LM	O
on	O
context	O
.	O

This	O
factorization	O
-	O
based	O
approach	O
has	O
proven	O
effective	O
in	O
generating	O
automatic	O
completions	O
of	O
sentences	O
that	O
are	O
personalized	O
for	O
particular	O
users	O
(	O
Jaech	O
and	O
Ostendorf	O
,	O
2018b	O
)	O
.	O

Factorizing	O
the	O
weight	O
-	O
matrix	O
enables	O
a	O
larger	O
fraction	O
of	O
a	O
model	O
's	O
parameters	O
to	O
adjust	O
to	O
a	O
given	O
contextual	O
signal	O
.	O

The	O
authors	O
propose	O
decomposing	O
the	O
weight	O
-	O
matrix	O
into	O
a	O
set	O
of	O
left	O
and	O
right	O
basis	O
tensors	O
which	O
are	O
then	O
multiplied	O
by	O
a	O
learned	O
context	O
embedding	O
to	O
produce	O
a	O
new	O
weight	O
-	O
matrix	O
.	O

In	O
contrast	O
to	O
these	O
methods	O
,	O
Jaech	O
and	O
Ostendorf	O
(	O
2018a	O
)	O
adapt	O
the	O
weight	B-HyperparameterName
-	I-HyperparameterName
matrix	I-HyperparameterName
used	O
in	O
an	O
RNN	B-MethodName
model	O
to	O
a	O
given	O
contextual	O
input	O
.	O

(	O
2019	O
)	O
use	O
an	O
attention	O
mechanism	O
over	O
learned	O
personality	O
trait	O
embeddings	O
,	O
in	O
order	O
to	O
generate	O
personalized	O
dialogue	O
responses	O
.	O

The	O
aforementioned	O
approaches	O
learn	O
a	O
representation	O
of	O
the	O
context	O
that	O
is	O
directly	O
used	O
as	O
input	O
to	O
a	O
neural	O
LM	O
.	O

Similarly	O
,	O
Zheng	O
et	O
al	O
.	O

(	O
2016	O
)	O
use	O
an	O
attention	O
module	O
that	O
attends	O
to	O
wordlocation	O
information	O
to	O
predict	O
the	O
polarity	O
of	O
a	O
sentence	O
.	O

Tang	O
et	O
al	O
.	O

Recently	O
,	O
attention	O
mechanisms	O
,	O
initially	O
developed	O
for	O
machine	O
translation	O
(	O
Bahdanau	O
et	O
al	O
.	O
,	O
2014;Luong	O
et	O
al	O
.	O
,	O
2015	O
)	O
,	O
have	O
been	O
used	O
by	O
neural	O
LMs	O
to	O
adaptively	O
condition	O
their	O
predictions	O
on	O
certain	O
non	O
-	O
linguistic	O
contexts	O
.	O

This	O
concatenation	O
-	O
based	O
approach	O
has	O
been	O
used	O
in	O
a	O
variety	O
of	O
domains	O
including	O
text	B-TaskName
classification	I-TaskName
(	O
Yogatama	O
et	O
al	O
.	O
,	O
2017	O
)	O
,	O
personalized	O
conversational	O
agents	O
(	O
Wen	O
et	O
al	O
.	O
,	O
2013	O
)	O
,	O
and	O
voice	O
search	O
queries	O
(	O
Ma	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

The	O
most	O
common	O
method	O
for	O
incorporating	O
nonlinguistic	O
information	O
into	O
a	O
RNN	B-MethodName
-	O
LM	O
is	O
to	O
learn	O
a	O
representation	O
of	O
the	O
context	O
that	O
is	O
concatenated	O
with	O
word	O
embeddings	O
as	O
input	O
to	O
the	O
model	O
.	O

While	O
,	O
outside	O
of	O
ASR	O
,	O
transformer	O
-	O
based	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O
2017	O
)	O
language	O
models	O
have	O
largely	O
replaced	O
RNN	B-MethodName
-	I-MethodName
LMs	I-MethodName
,	O
RNNs	B-MethodName
remain	O
dominant	O
in	O
ASR	O
architectures	O
such	O
as	O
connectionist	B-TaskName
temporal	I-MethodName
classification	O
(	O
Graves	O
et	O
al	O
.	O
,	O
2006	O
)	O
,	O
and	O
RNN	B-MethodName
-	I-MethodName
T	I-MethodName
(	O
Graves	O
,	O
2012;He	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

In	O
this	O
paper	O
,	O
we	O
focus	O
on	O
adapting	O
recurrent	B-MethodName
neural	I-MethodName
network	I-MethodName
language	I-MethodName
models	I-MethodName
(	O
RNN	B-MethodName
-	I-MethodName
LMs	I-MethodName
)	O
to	O
use	O
both	O
text	O
and	O
non	O
-	O
linguistic	O
contextual	O
data	O
for	O
speech	B-TaskName
recognition	I-TaskName
in	O
general	O
.	O

These	O
past	O
efforts	O
,	O
however	O
,	O
have	O
largely	O
focused	O
on	O
improving	O
a	O
particular	O
skill	O
of	O
an	O
ASR	O
system	O
,	O
and	O
not	O
the	O
system	O
's	O
speech	B-TaskName
recognition	I-TaskName
in	O
general	O
.	O

To	O
-	O
date	O
,	O
some	O
voice	O
assistants	O
have	O
leveraged	O
coarse	O
geographic	O
information	O
for	O
improving	O
location	O
search	O
queries	O
(	O
Bocchieri	O
and	O
Caseiro	O
,	O
2010;Lloyd	O
and	O
Kristjansson	O
,	O
2012	O
)	O
.	O

As	O
an	O
example	O
,	O
knowing	O
that	O
an	O
utterance	O
was	O
spoken	O
on	O
December	O
25th	O
,	O
a	O
LM	O
should	O
learn	O
that	O
the	O
word	O
"	O
christmas	O
"	O
rather	O
than	O
"	O
easter	O
"	O
is	O
more	O
likely	O
to	O
follow	O
the	O
phrase	O
"	O
lookup	O
cookie	O
recipes	O
for	O
"	O
.	O

We	O
hypothesize	O
that	O
these	O
additional	O
data	O
,	O
such	O
as	O
the	O
time	O
at	O
which	O
an	O
utterance	O
was	O
spoken	O
,	O
provide	O
a	O
useful	O
input	O
signal	O
for	O
a	O
LM	O
.	O

They	O
often	O
collect	O
utterances	O
spoken	O
by	O
users	O
,	O
along	O
with	O
associated	O
de	O
-	O
identified	O
contextual	O
information	O
.	O

Voice	O
assistants	O
have	O
become	O
ubiquitous	O
and	O
crucially	O
rely	O
on	O
ASR	O
systems	O
to	O
convert	O
user	O
inputs	O
to	O
text	O
.	O

The	O
LM	O
component	O
is	O
trained	O
separately	O
,	O
typically	O
on	O
large	O
amounts	O
of	O
transcribed	O
utterances	O
that	O
have	O
been	O
collected	O
by	O
an	O
existing	O
speech	O
recognition	O
system	O
.	O

Conventional	O
automatic	B-TaskName
speech	I-TaskName
recognition	I-TaskName
(	O
ASR	B-TaskName
)	O
systems	O
include	O
a	O
language	O
model	O
(	O
LM	O
)	O
and	O
an	O
acoustic	O
model	O
(	O
AM	O
)	O
.	O

Introduction	O
.	O

When	O
evaluated	O
on	O
utterances	O
extracted	O
from	O
the	O
long	O
tail	O
of	O
the	O
dataset	O
,	O
our	O
method	O
improves	O
perplexity	B-MetricName
by	O
9.0	B-MetricValue
%	I-MetricValue
relative	O
over	O
a	O
standard	O
LM	O
and	O
by	O
over	O
2.8	B-MetricValue
%	I-MetricValue
relative	O
when	O
compared	O
to	O
a	O
state	O
-	O
of	O
-	O
theart	O
model	O
for	O
contextual	O
LM	O
.	O

When	O
applied	O
to	O
a	O
large	O
de	O
-	O
identified	O
dataset	O
of	O
utterances	O
collected	O
by	O
a	O
popular	O
voice	O
assistant	O
platform	O
,	O
our	O
method	O
reduces	O
perplexity	B-MetricName
by	O
7.0	B-MetricValue
%	I-MetricValue
relative	O
over	O
a	O
standard	O
LM	O
that	O
does	O
not	O
incorporate	O
contextual	O
information	O
.	O

We	O
introduce	O
an	O
attention	O
mechanism	O
for	O
training	O
neural	O
speech	B-TaskName
recognition	I-TaskName
language	I-TaskName
models	O
on	O
both	O
text	O
and	O
nonlinguistic	O
contextual	O
data	O
1	O
.	O

For	O
some	O
domains	O
like	O
voice	O
assistants	O
,	O
however	O
,	O
additional	O
context	O
,	O
such	O
as	O
time	O
at	O
which	O
an	O
utterance	O
was	O
spoken	O
,	O
provides	O
a	O
rich	O
input	O
signal	O
.	O

Language	O
modeling	O
(	O
LM	O
)	O
for	O
automatic	B-TaskName
speech	I-TaskName
recognition	I-TaskName
(	O
ASR	B-TaskName
)	O
does	O
not	O
usually	O
incorporate	O
utterance	O
level	O
contextual	O
information	O
.	O

Attention	B-MethodName
-	I-MethodName
based	I-MethodName
Contextual	I-MethodName
Language	I-MethodName
Model	I-MethodName
Adaptation	I-MethodName
for	O
Speech	B-TaskName
Recognition	I-TaskName
.	O

As	O
we	O
move	O
away	O
from	O
the	O
morning	O
hours	O
,	O
the	O
conditional	O
probability	O
of	O
the	O
word	O
"	O
snooze	O
"	O
decreases	O
substantially	O
,	O
reaching	O
a	O
low	O
-	O
point	O
by	O
the	O
afternoon	O
and	O
evening	O
.	O

arship	O
scheme	O
and	O
by	O
the	O
ADAPT	O
Centre	O
for	O
Digital	O
Content	O
Technology	O
which	O
is	O
funded	O
under	O
the	O
SFI	O
Research	O
Centres	O
Programme	O
(	O
Grant	O
13	O
/	O
RC/2106	O
)	O
and	O
is	O
co	O
-	O
funded	O
under	O
the	O
European	O
Regional	O
Development	O
Fund	O
.	O

Furthermore	O
,	O
we	O
would	O
like	O
to	O
experiment	O
with	O
larger	O
datasets	O
to	O
verify	O
whether	O
the	O
positive	O
effect	O
of	O
the	O
linguistic	O
features	O
remains	O
.	O

In	O
the	O
next	O
step	O
,	O
we	O
will	O
let	O
the	O
models	O
converge	O
,	O
create	O
the	O
ensemble	O
models	O
for	O
the	O
different	O
systems	O
and	O
compute	O
whether	O
the	O
increase	O
in	O
BLEU	B-MetricName
score	O
is	O
significant	O
.	O

In	O
the	O
future	O
,	O
we	O
would	O
like	O
to	O
perform	O
manual	O
evaluations	O
on	O
the	O
outputs	O
of	O
our	O
systems	O
to	O
see	O
whether	O
they	O
correlate	O
with	O
the	O
BLEU	B-MetricName
scores	O
.	O

Furthermore	O
,	O
the	O
benefit	O
of	O
the	O
additional	O
features	O
is	O
more	O
clear	O
on	O
a	O
dissimilar	O
test	O
set	O
which	O
is	O
in	O
accordance	O
with	O
our	O
original	O
hypothesis	O
stating	O
that	O
semantic	O
and	O
syntactic	O
features	O
(	O
and	O
their	O
combination	O
)	O
can	O
be	O
beneficial	O
for	O
generalization	O
.	O

For	O
both	O
language	O
pairs	O
we	O
observe	O
that	O
adding	O
extra	O
semantic	O
and/or	O
syntactic	O
features	O
leads	O
to	O
faster	O
convergence	O
.	O

In	O
this	O
work	O
we	O
experimented	O
with	O
EN	O
-	O
FR	O
and	O
EN	O
-	O
DE	O
data	O
augmented	O
with	O
semantic	O
and	O
syntactic	O
features	O
.	O

Conclusions	O
and	O
Future	O
Work	O
.	O

1	O
0	O
0	O
0	O
0	O
2	O
0	O
0	O
0	O
0	O
3	O
0	O
0	O
0	O
0	O
4	O
0	O
0	O
0	O
0	O
5	O
0	O
0	O
0	O
0	O
6	O
0	O
0	O
0	O
0	O
7	O
0	O
0	O
0	O
0	O
8	O
0	O
0	O
0	O
0	O
9	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
1	O
1	O
0	O
0	O
0	O
0	O
1	O
2	O
0	O
0	O
0	O
0	O
1	O
3	O
0	O
0	O
0	O
0	O
1	O
4	O
0	O
0	O
0	O
0	O
.	O

However	O
,	O
their	O
combination	O
(	O
SST	B-MethodName
-	I-MethodName
CCG	I-MethodName
)	O
leads	O
to	O
a	O
more	O
robust	O
NMT	B-TaskName
system	O
with	O
a	O
higher	O
BLEU	B-MetricName
(	O
see	O
Table	O
2	O
)	O
.	O

In	O
the	O
last	O
iterations	O
,	O
see	O
Figure	O
4	O
,	O
we	O
see	O
how	O
the	O
two	O
systems	O
enriched	O
with	O
supersense	O
tags	O
and	O
CCG	O
tags	O
lead	O
to	O
small	O
improvements	O
over	O
the	O
baseline	O
.	O

Figure	O
3	O
compares	O
the	O
BPE	B-MethodName
-	I-MethodName
ed	I-MethodName
baseline	I-MethodName
system	O
(	O
BPE	B-MethodName
)	O
with	O
the	O
NMT	B-TaskName
system	O
enriched	O
with	O
SST	O
and	O
CCG	O
tags	O
(	O
SST	B-MethodName
-	I-MethodName
CCG	I-MethodName
)	O
.	O

The	O
results	O
for	O
the	O
EN	B-DatasetName
-	I-DatasetName
DE	I-DatasetName
system	O
are	O
very	O
similar	O
to	O
the	O
EN	B-MethodName
-	I-MethodName
FR	I-MethodName
system	O
:	O
the	O
model	O
converges	O
faster	O
and	O
we	O
observe	O
the	O
same	O
trends	O
with	O
respect	O
to	O
the	O
BLEU	B-MetricName
scores	O
of	O
the	O
different	O
systems	O
.	O

English	O
-	O
German	O
.	O

This	O
supports	O
our	O
hypothesis	O
that	O
semantic	O
and	O
syntactic	O
features	O
are	O
particularly	O
useful	O
when	O
combined	O
.	O

Moreover	O
,	O
the	O
benefit	O
of	O
syntactic	O
and	O
semantic	O
features	O
seems	O
to	O
be	O
more	O
than	O
cumulative	O
at	O
some	O
points	O
,	O
confirming	O
the	O
idea	O
that	O
providing	O
both	O
information	O
sources	O
can	O
help	O
the	O
NMT	B-TaskName
system	O
learn	O
semantico	O
-	O
syntactic	O
patterns	O
.	O

The	O
best	O
CCG	B-MethodName
-	I-MethodName
SST	I-MethodName
model	O
(	O
23.21	B-MetricValue
BLEU	B-MetricName
)	O
outperforms	O
the	O
best	O
BPE	B-MethodName
-	I-MethodName
ed	I-MethodName
baseline	I-MethodName
model	O
(	O
22.54	B-MetricValue
BLEU	B-MetricName
)	O
with	O
0.67	B-MetricValue
BLEU	B-MetricName
(	O
see	O
Table	O
1	O
)	O
.	O

Although	O
Sennrich	O
and	O
Haddow	O
(	O
2016	O
)	O
observe	O
that	O
features	O
are	O
not	O
necessarily	O
cumulative	O
(	O
possibly	O
since	O
the	O
information	O
from	O
the	O
syntactic	O
features	O
partially	O
overlapped	O
)	O
,	O
the	O
system	O
enriched	O
with	O
both	O
semantic	O
and	O
syntactic	O
features	O
outperforms	O
the	O
two	O
separate	O
systems	O
as	O
well	O
as	O
the	O
baseline	O
system	O
.	O

1	O
0	O
0	O
0	O
0	O
2	O
0	O
0	O
0	O
0	O
3	O
0	O
0	O
0	O
0	O
4	O
0	O
0	O
0	O
0	O
5	O
0	O
0	O
0	O
0	O
6	O
0	O
0	O
0	O
0	O
7	O
0	O
0	O
0	O
0	O
8	O
0	O
0	O
0	O
0	O
9	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
1	O
1	O
0	O
0	O
0	O
0	O
1	O
2	O
0	O
0	O
0	O
0	O
1	O
3	O
0	O
0	O
0	O
0	O
1	O
4	O
0	O
0	O
0	O
0	O
1	O
5	O
0	O
0	O
0	O
0	O
1	O
6	O
0	O
0	O
0	O
0	O
To	O
see	O
in	O
more	O
detail	O
how	O
our	O
semantically	O
enriched	O
SST	B-MethodName
system	O
compares	O
to	O
an	O
NMT	B-TaskName
system	O
with	O
syntactic	O
CCG	O
supertags	O
and	O
how	O
a	O
system	O
that	O
integrates	O
both	O
semantic	O
features	O
and	O
syntactic	O
features	O
(	O
SST	B-MethodName
-	I-MethodName
CCG	I-MethodName
)	O
performs	O
,	O
a	O
more	O
detailed	O
graph	O
is	O
provided	O
in	O
Figure	O
2	O
where	O
we	O
zoom	O
in	O
on	O
later	O
stages	O
of	O
the	O
learning	O
process	O
.	O

From	O
the	O
graph	O
,	O
it	O
can	O
also	O
be	O
observed	O
that	O
the	O
system	O
has	O
a	O
more	O
robust	O
,	O
consistent	O
learning	O
curve	O
.	O

Figure	O
1	O
compares	O
the	O
BPE	B-MethodName
-	I-MethodName
ed	I-MethodName
baseline	I-MethodName
system	O
(	O
BPE	B-MethodName
)	O
with	O
the	O
supertag	O
-	O
supersensetag	O
system	O
(	O
CCG	B-MethodName
-	I-MethodName
SST	I-MethodName
)	O
automatically	O
evaluated	O
on	O
the	O
newstest2013	B-DatasetName
(	O
in	O
terms	O
of	O
BLEU	B-MetricName
(	O
Papineni	O
et	O
al	O
.	O
,	O
2002	O
)	O
)	O
over	O
all	O
150,000	O
iterations	O
.	O

As	O
we	O
hypothesized	O
,	O
the	O
benefits	O
of	O
the	O
features	O
added	O
,	O
was	O
more	O
clear	O
on	O
the	O
newstest2013	B-DatasetName
than	O
on	O
the	O
Europarl	B-DatasetName
test	O
set	O
.	O

For	O
both	O
test	O
sets	O
,	O
the	O
NMT	B-TaskName
system	O
with	O
supersenses	O
(	O
SST	B-MethodName
)	O
converges	O
faster	O
than	O
the	O
baseline	O
(	O
BPE	B-MethodName
)	O
NMT	B-TaskName
system	O
.	O

English	O
-	O
French	O
.	O

Results	O
.	O

All	O
systems	O
are	O
trained	O
for	O
150,000	O
iterations	O
and	O
evaluated	O
after	O
every	O
10,000	O
iterations	O
.	O

We	O
trained	O
all	O
our	O
BPE	B-MethodName
-	I-MethodName
ed	I-MethodName
NMT	I-TaskName
systems	O
with	O
CCG	O
tag	O
features	O
,	O
supersensetags	O
(	O
SST	O
)	O
,	O
POS	O
tags	O
and	O
the	O
combination	O
of	O
syntactic	O
features	O
(	O
POS	O
or	O
CCG	O
)	O
with	O
the	O
semantic	O
ones	O
(	O
SST	O
)	O
.	O

We	O
ran	O
the	O
BPE	B-MethodName
algorithm	I-MethodName
with	O
89	O
,	O
500	O
operations	O
.	O

In	O
order	O
to	O
by	O
-	O
pass	O
the	O
OOV	B-TaskName
problem	O
and	O
reduce	O
the	O
number	O
of	O
dictionary	O
entries	O
we	O
use	O
word	O
-	O
segmentation	O
with	O
BPE	O
(	O
Sennrich	O
,	O
2015	O
)	O
.	O

We	O
keep	O
the	O
embedding	B-HyperparameterName
layer	I-HyperparameterName
fixed	O
to	O
700	B-HyperparameterValue
for	O
all	O
models	O
in	O
order	O
to	O
ensure	O
that	O
the	O
improvements	O
are	O
not	O
simply	O
due	O
to	O
an	O
increase	O
of	O
the	O
parameters	O
in	O
the	O
embedding	O
layer	O
.	O

We	O
used	O
the	O
nematus	B-MethodName
toolkit	I-MethodName
(	O
Sennrich	O
et	O
al	O
.	O
,	O
2017	O
)	O
to	O
train	O
encoder	O
-	O
decoder	O
NMT	B-TaskName
models	O
with	O
the	O
following	O
parameters	O
:	O
vocabulary	B-HyperparameterName
size	I-HyperparameterName
:	O
35000	B-HyperparameterValue
,	O
maximum	B-HyperparameterName
sentence	I-HyperparameterName
length	I-HyperparameterName
:	O
60	B-HyperparameterValue
,	O
vector	B-HyperparameterName
dimension	I-HyperparameterName
:	O
1024	B-HyperparameterValue
,	O
word	B-HyperparameterName
embedding	I-HyperparameterName
layer	I-HyperparameterName
:	O
700	B-HyperparameterValue
,	O
learning	B-HyperparameterName
optimizer	I-HyperparameterName
:	O
adadelta	B-HyperparameterValue
.	O

Description	O
of	O
the	O
NMT	B-TaskName
system	O
.	O

Two	O
different	O
test	O
sets	O
are	O
used	O
in	O
order	O
to	O
show	O
how	O
additional	O
semantic	O
and	O
syntactic	O
features	O
can	O
help	O
the	O
NMT	B-TaskName
system	O
translate	O
different	O
types	O
of	O
test	O
sets	O
and	O
thus	O
evaluate	O
the	O
general	O
effect	O
of	O
our	O
improvement	O
.	O

We	O
test	O
the	O
systems	O
on	O
5	O
K	O
sentences	O
(	O
different	O
from	O
the	O
training	O
data	O
)	O
extracted	O
from	O
Europarl	B-DatasetName
and	O
the	O
newstest2013	B-DatasetName
.	O

An	O
example	O
of	O
a	O
CCGtagged	O
sentence	O
is	O
given	O
in	O
(	O
6	O
):	O
Our	O
NMT	B-TaskName
systems	O
are	O
trained	O
on	O
1	O
M	O
parallel	O
sentences	O
of	O
the	O
Europarl	B-DatasetName
corpus	O
for	O
EN	O
-	O
FR	O
and	O
EN	O
-	O
DE	O
(	O
Koehn	O
,	O
2005	O
)	O
.	O

This	O
kind	O
of	O
information	O
can	O
help	O
resolve	O
ambiguity	O
in	O
terms	O
of	O
prepositional	O
attachment	O
,	O
among	O
others	O
.	O

CCG	O
tags	O
provide	O
global	O
syntactic	O
information	O
on	O
the	O
lexical	O
level	O
.	O

The	O
POS	O
tags	O
are	O
generated	O
by	O
the	O
Stanford	O
POS	O
-	O
tagger	O
(	O
Toutanova	O
et	O
al	O
.	O
,	O
2003	O
)	O
;	O
for	O
the	O
supertags	O
we	O
used	O
the	O
EasySRL	B-MethodName
tool	O
(	O
Lewis	O
et	O
al	O
.	O
,	O
2015	O
)	O
which	O
annotates	O
words	O
with	O
CCG	O
tags	O
.	O

To	O
support	O
our	O
hypothesis	O
we	O
also	O
experimented	O
with	O
syntactic	O
features	O
(	O
separately	O
and	O
in	O
combination	O
with	O
the	O
semantic	O
ones	O
):	O
POS	O
tags	O
and	O
CCG	O
supertags	O
.	O

We	O
hypothesize	O
that	O
more	O
general	O
semantic	O
information	O
can	O
be	O
particularly	O
useful	O
for	O
NMT	B-TaskName
in	O
combination	O
with	O
more	O
detailed	O
syntactic	O
information	O
.	O

Supertags	O
and	O
POS	O
-	O
tags	O
.	O

If	O
the	O
MWE	O
did	O
not	O
receive	O
a	O
particular	O
tag	O
,	O
we	O
added	O
the	O
tag	O
mwe	O
to	O
all	O
its	O
components	O
,	O
as	O
in	O
example	O
(	O
5	O
)	O
(	O
4	O
)	O
Input	O
:	O
"	O
EU	O
citizens	O
"	O
SST	O
:	O
"	O
EU	O
citizens|GROUP	O
"	O
Output	O
:	O
"	O
EU|GROUP	O
citizens|GROUP	O
"	O
(	O
5	O
)	O
Input	O
:	O
"	O
a	O
number	O
of	O
"	O
SST	O
:	O
"	O
a	O
number	O
of	O
"	O
Output	O
:	O
"	O
a|mwe	O
number|mwe	O
of|mwe	O
"	O
.	O

op@@|GROUP	O
ers|GROUP	O
"	O
For	O
the	O
MWEs	O
we	O
decided	O
to	O
copy	O
the	O
supersense	O
tag	O
to	O
all	O
the	O
words	O
of	O
the	O
MWE	O
(	O
if	O
provided	O
by	O
the	O
tagger	O
)	O
,	O
as	O
in	O
(	O
4	O
)	O
.	O

(	O
3	O
)	O
Input	O
:	O
"	O
the	O
stormtroopers	O
"	O
SST	O
:	O
"	O
the	O
stormtroopers|GROUP	O
"	O
BPE	O
:	O
"	O
the	O
stor@@	O
m@@	O
tro@@	O
op@@	O
ers	O
"	O
Output	O
:	O
"	O
the|none	O
stor@@|GROUP	O
...	O

Furthermore	O
,	O
we	O
add	O
a	O
none	O
tag	O
to	O
all	O
words	O
that	O
did	O
not	O
receive	O
a	O
supersense	O
tag	O
.	O

It	O
is	O
split	O
into	O
5	O
subword	O
units	O
so	O
the	O
supersense	O
tag	O
feature	O
is	O
copied	O
to	O
all	O
its	O
five	O
subword	O
units	O
.	O

In	O
(	O
3	O
)	O
,	O
we	O
give	O
an	O
example	O
with	O
the	O
word	O
'	O
stormtroopers	O
'	O
that	O
is	O
tagged	O
with	O
the	O
supersense	O
tag	O
'	O
GROUP	O
'	O
.	O

For	O
each	O
word	O
that	O
is	O
split	O
into	O
subword	O
units	O
,	O
we	O
copy	O
the	O
features	O
of	O
the	O
word	O
in	O
question	O
to	O
its	O
subword	O
units	O
.	O

(	O
2016	O
)	O
using	O
a	O
variant	O
of	O
BPE	O
for	O
word	O
segmentation	O
capable	O
of	O
encoding	O
open	O
vocabularies	O
with	O
a	O
compact	O
symbol	O
vocabulary	O
of	O
variablelength	O
subword	O
units	O
.	O

To	O
reduce	O
the	O
number	O
of	O
out	O
-	O
of	O
-	O
vocabulary	O
(	O
OOV	O
)	O
words	O
,	O
we	O
follow	O
the	O
approach	O
of	O
Sennrich	O
et	O
al	O
.	O

These	O
embedding	O
vectors	O
are	O
then	O
concatenated	O
into	O
one	O
embedding	O
vector	O
and	O
used	O
in	O
the	O
model	O
instead	O
of	O
the	O
simple	O
word	O
embedding	O
one	O
(	O
Sennrich	O
and	O
Haddow	O
,	O
2016	O
)	O
.	O

A	O
separate	O
embedding	O
is	O
learned	O
for	O
every	O
source	O
-	O
side	O
feature	O
provided	O
(	O
the	O
word	O
itself	O
,	O
POS	O
-	O
tag	O
,	O
supersense	O
tag	O
etc	O
.	O
)	O
.	O

(	O
2014	O
)	O
.	O

We	O
add	O
this	O
semantico	O
-	O
syntactic	O
information	O
in	O
the	O
source	O
as	O
an	O
extra	O
feature	O
in	O
the	O
embedding	O
layer	O
following	O
the	O
approach	O
of	O
Sennrich	O
and	O
Haddow	O
(	O
2016	O
)	O
,	O
who	O
extended	O
the	O
model	O
of	O
Bahdanau	O
et	O
al	O
.	O

In	O
Example	O
(	O
2	O
)	O
,	O
the	O
1	O
https://github.com/nschneid/	O
pysupersensetagger	O
2	O
All	O
the	O
examples	O
are	O
extracted	O
from	O
our	O
data	O
used	O
later	O
on	O
to	O
train	O
the	O
NMT	B-TaskName
systems	O
MWEs	O
in	O
fact	O
,	O
a	O
number	O
of	O
and	O
EU	O
citizens	O
are	O
retrieved	O
by	O
the	O
tagger	O
.	O

(	O
2	O
)	O
Since	O
MWEs	O
and	O
supersenses	O
naturally	O
complement	O
each	O
other	O
,	O
Schneider	O
and	O
Smith	O
(	O
2015	O
)	O
integrated	O
the	O
MWE	B-TaskName
identification	I-TaskName
task	O
(	O
Schneider	O
et	O
al	O
.	O
,	O
2014	O
)	O
with	O
the	O
supersense	O
tagging	O
task	O
of	O
Ciaramita	O
and	O
Altun	O
(	O
2006	O
)	O
.	O

Furthermore	O
,	O
there	O
is	O
a	O
separate	O
tag	O
to	O
distinguish	O
auxiliary	O
verbs	O
from	O
main	O
verbs	O
.	O

This	O
way	O
,	O
the	O
supersenses	O
also	O
provide	O
syntactic	O
information	O
useful	O
for	O
disambiguation	O
as	O
in	O
(	O
2	O
)	O
,	O
where	O
the	O
word	O
work	O
is	O
correctly	O
tagged	O
as	O
a	O
noun	O
(	O
with	O
its	O
capitalized	O
supersense	O
tag	O
ACT	O
)	O
in	O
the	O
first	O
part	O
of	O
the	O
sentence	O
and	O
as	O
a	O
verb	O
(	O
with	O
the	O
lowercased	O
supersense	O
tag	O
social	O
)	O
.	O

However	O
,	O
the	O
supersense	O
tags	O
for	O
verbs	O
are	O
always	O
lowercased	O
while	O
the	O
ones	O
for	O
nouns	O
are	O
capitalized	O
.	O

To	O
obtain	O
the	O
supersense	O
tags	O
we	O
used	O
the	O
AMALGrAM	B-MethodName
(	O
A	O
Machine	O
Analyzer	O
of	O
Lexical	O
Groupings	O
and	O
Meanings	O
)	O
2.0	O
tool	O
1	O
which	O
in	O
addition	O
to	O
the	O
noun	O
and	O
verb	O
supersenses	O
analyzes	O
English	O
input	O
sentences	O
for	O
MWEs	O
.	O
An	O
example	O
of	O
a	O
sentence	O
annotated	O
with	O
the	O
AMALGrAM	B-MethodName
tool	O
is	O
given	O
in	O
(	O
1	O
):	O
2	O
(	O
1	O
)	O
(	O
a	O
)	O
"	O
He	O
seemed	O
to	O
have	O
little	O
faith	O
in	O
our	O
democratic	O
structures	O
,	O
suggesting	O
that	O
various	O
articles	O
could	O
be	O
misused	O
by	O
governments	O
.	O
"	O
(	O
b	O
)	O
"	O
He	O
seemed|cognition	O
to	O
have|stative	O
little	O
faith|COGNITION	O
in	O
our	O
democratic	O
structures|ARTIFACT	O
,	O
suggesting|communication	O
that	O
various	O
articles|COMMUNICATION	O
could	O
be|'a	O
misused|social	O
by	O
governments|GROUP	O
.	O
"	O
As	O
can	O
be	O
noted	O
in	O
(	O
1	O
)	O
,	O
some	O
supersenses	O
,	O
such	O
as	O
cognition	O
exist	O
for	O
both	O
nouns	O
and	O
verbs	O
.	O

The	O
supersenses	O
cover	O
all	O
nouns	O
and	O
verbs	O
with	O
a	O
total	O
of	O
41	O
supersense	O
categories	O
,	O
26	O
for	O
nouns	O
and	O
15	O
for	O
verbs	O
.	O

Supersenses	O
are	O
a	O
term	O
which	O
refers	O
to	O
the	O
top	O
-	O
level	O
hypernyms	O
in	O
the	O
WordNet	O
(	O
Miller	O
,	O
1995	O
)	O
taxonomy	O
,	O
sometimes	O
also	O
referred	O
to	O
as	O
semantic	O
fields	O
(	O
Schneider	O
and	O
Smith	O
,	O
2015	O
)	O
.	O

The	O
novelty	O
of	O
our	O
work	O
is	O
the	O
integration	O
of	O
explicit	O
semantic	O
features	O
supersenses	O
into	O
an	O
NMT	B-TaskName
system	O
.	O

Supersense	O
Tags	O
.	O

3	O
Semantics	O
and	O
Syntax	O
in	O
NMT	B-TaskName
.	O

Similarly	O
to	O
syntactic	O
features	O
,	O
we	O
hypothesize	O
that	O
semantic	O
features	O
in	O
the	O
form	O
of	O
semantic	O
'	O
classes	O
'	O
can	O
be	O
beneficial	O
for	O
NMT	B-TaskName
providing	O
it	O
with	O
an	O
extra	O
ability	O
to	O
generalize	O
and	O
thus	O
better	O
learn	O
more	O
complex	O
semanticosyntactic	O
patters	O
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
there	O
has	O
not	O
been	O
any	O
work	O
on	O
explicitly	O
integrating	O
semantic	O
information	O
in	O
NMT	B-TaskName
.	O

(	O
2017	O
)	O
focus	O
on	O
incorporating	O
sourceside	O
long	O
distance	O
dependencies	O
by	O
enriching	O
each	O
source	O
state	O
with	O
global	O
dependency	O
structure	O
.	O

Wu	O
et	O
al	O
.	O

They	O
focus	O
on	O
source	O
-	O
side	O
syntactic	O
information	O
based	O
on	O
Head	B-MethodName
-	I-MethodName
Driven	I-MethodName
Phrase	I-MethodName
Structure	I-MethodName
Grammar	I-MethodName
(	O
Sag	O
et	O
al	O
.	O
,	O
1999	O
)	O
where	O
target	O
words	O
are	O
aligned	O
not	O
only	O
with	O
the	O
corresponding	O
source	O
words	O
but	O
with	O
the	O
entire	O
source	O
phrase	O
.	O

(	O
2016	O
)	O
integrated	O
syntactic	O
information	O
in	O
the	O
form	O
of	O
linearized	O
parse	O
trees	O
by	O
using	O
an	O
encoder	O
that	O
computes	O
vector	O
representations	O
for	O
each	O
phrase	O
in	O
the	O
source	O
tree	O
.	O

Similarly	O
,	O
Eriguchi	O
et	O
al	O
.	O

A	O
similar	O
observation	O
was	O
made	O
by	O
Li	O
et	O
al	O
(	O
2017	O
2017	O
)	O
observe	O
that	O
the	O
mixed	B-MethodName
RNN	I-MethodName
(	O
the	O
simplest	O
RNN	O
encoder	O
)	O
,	O
where	O
words	O
and	O
label	O
annotation	O
vectors	O
are	O
simply	O
stitched	O
together	O
in	O
the	O
input	O
sequences	O
,	O
yields	O
the	O
best	O
performance	O
with	O
a	O
significant	O
improvement	O
(	O
1.4	B-MetricValue
BLEU	B-MetricName
)	O
.	O

Moreover	O
,	O
they	O
experiment	O
with	O
serializing	O
and	O
multitasking	O
and	O
show	O
that	O
tightly	O
coupling	O
the	O
words	O
with	O
their	O
syntactic	O
features	O
leads	O
to	O
improvements	O
in	O
translation	O
quality	O
(	O
measured	O
by	O
BLEU	B-MetricName
)	O
while	O
a	O
multitask	O
approach	O
(	O
where	O
the	O
NMT	B-TaskName
predicts	O
CCG	O
supertags	O
and	O
words	O
independently	O
)	O
does	O
not	O
perform	O
better	O
than	O
the	O
baseline	O
system	O
.	O

(	O
2017	O
)	O
extend	O
their	O
work	O
by	O
including	O
CCG	O
supertags	O
as	O
explicit	O
features	O
in	O
a	O
factored	O
NMT	B-TaskName
systems	O
.	O

Nadejde	O
et	O
al	O
.	O

When	O
evaluating	O
the	O
gains	O
from	O
the	O
features	O
individually	O
,	O
it	O
results	O
that	O
the	O
gain	O
from	O
different	O
features	O
is	O
not	O
fully	O
cumulative	O
.	O

Sennrich	O
and	O
Haddow	O
(	O
2016	O
)	O
show	O
that	O
the	O
inclusion	O
of	O
linguistic	O
fea	O
-	O
tures	O
leads	O
to	O
improvements	O
over	O
the	O
NMT	B-TaskName
baseline	O
for	O
EN	O
-	I-MetricName
DE	O
(	O
0.6	B-MetricValue
BLEU	B-MetricName
)	O
,	O
DE	B-DatasetName
-	I-MetricName
EN	I-MetricName
(	O
1.5	B-MetricValue
BLEU	B-MetricName
)	O
and	O
EN	B-DatasetName
-	I-MetricName
RO	I-MetricName
(	O
1.0	B-MetricValue
BLEU	B-MetricName
)	O
.	O

Furthermore	O
,	O
sometimes	O
information	O
is	O
present	O
in	O
the	O
encoding	O
vectors	O
but	O
is	O
lost	O
during	O
the	O
decoding	O
phase	O
(	O
Vanmassenhove	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

(	O
2016	O
)	O
show	O
that	O
although	O
NMT	B-TaskName
systems	O
are	O
able	O
to	O
partially	O
learn	O
syntactic	O
information	O
,	O
more	O
complex	O
patterns	O
remain	O
problematic	O
.	O

Similarly	O
,	O
on	O
the	O
syntax	O
level	O
,	O
Shi	O
et	O
al	O
.	O

They	O
then	O
investigated	O
whether	O
features	O
such	O
as	O
lemmas	O
,	O
subword	O
tags	O
,	O
morphological	O
features	O
,	O
POS	O
tags	O
and	O
dependency	O
labels	O
could	O
be	O
useful	O
for	O
NMT	B-TaskName
systems	O
or	O
whether	O
their	O
inclusion	O
is	O
redundant	O
.	O

By	O
representing	O
the	O
encoder	O
input	O
as	O
a	O
combination	O
of	O
features	O
(	O
Alexandrescu	O
and	O
Kirchhoff	O
,	O
2006	O
)	O
,	O
Sennrich	O
and	O
Haddow	O
(	O
2016	O
)	O
generalized	O
the	O
embedding	O
layer	O
in	O
such	O
a	O
way	O
that	O
an	O
arbitrary	O
number	O
of	O
linguistic	O
features	O
can	O
be	O
explicitly	O
integrated	O
.	O

Furthermore	O
,	O
the	O
encoder	O
and	O
attention	O
layers	O
can	O
be	O
shared	O
between	O
features	O
.	O

The	O
integration	O
of	O
linguistic	O
features	O
can	O
be	O
handled	O
in	O
a	O
flexible	O
way	O
without	O
creating	O
sparsity	O
issues	O
or	O
limiting	O
context	O
information	O
(	O
within	O
the	O
same	O
sentence	O
)	O
.	O

One	O
of	O
the	O
main	O
strengths	O
of	O
NMT	B-TaskName
is	O
its	O
strong	O
ability	O
to	O
generalize	O
.	O

However	O
,	O
adding	O
syntactic	O
features	O
to	O
SMT	B-TaskName
systems	O
led	O
to	O
improvements	O
with	O
respect	O
to	O
word	B-MetricName
order	I-MetricName
and	O
morphological	B-MetricName
agreement	I-MetricName
(	O
Williams	O
and	O
Koehn	O
,	O
2012;Sennrich	O
,	O
2015	O
)	O
.	O

Compared	O
to	O
factored	B-MethodName
NMT	B-TaskName
models	O
,	O
factored	B-MethodName
SMT	B-TaskName
models	O
have	O
some	O
disadvantages	O
:	O
(	O
a	O
)	O
adding	O
factors	O
increases	O
the	O
sparsity	O
of	O
the	O
models	O
,	O
(	O
b	O
)	O
the	O
n	B-HyperparameterName
-	I-HyperparameterName
grams	I-HyperparameterName
limit	O
the	O
size	O
of	O
context	O
that	O
is	O
taken	O
into	O
account	O
,	O
and	O
(	O
c	O
)	O
features	O
are	O
assumed	O
to	O
be	O
independent	O
of	O
each	O
other	O
.	O

In	O
SMT	B-TaskName
,	O
various	O
linguistic	O
features	O
such	O
as	O
stems	O
(	O
Toutanova	O
et	O
al	O
.	O
,	O
2008	O
)	O
lemmas	O
(	O
Mareček	O
et	O
al	O
.	O
,	O
2011;Fraser	O
et	O
al	O
.	O
,	O
2012	O
)	O
,	O
POStags	O
(	O
Avramidis	O
and	O
Koehn	O
,	O
2008	O
)	O
,	O
dependency	O
labels	O
(	O
Avramidis	O
and	O
Koehn	O
,	O
2008	O
)	O
and	O
supertags	O
(	O
Hassan	O
et	O
al	O
.	O
,	O
2007;Haque	O
et	O
al	O
.	O
,	O
2009	O
)	O
are	O
integrated	O
using	O
pre	O
-	O
or	O
post	O
-	O
processing	O
techniques	O
often	O
involving	O
factored	O
phrase	O
-	O
based	O
models	O
(	O
Koehn	O
and	O
Hoang	O
,	O
2007	O
)	O
.	O

Related	O
Work	O
.	O

Finally	O
,	O
We	O
conclude	O
and	O
present	O
some	O
of	O
the	O
ideas	O
for	O
future	O
work	O
in	O
Section	O
6	O
.	O

The	O
experimental	O
set	O
-	O
up	O
is	O
described	O
in	O
Section	O
4	O
followed	O
by	O
the	O
results	O
in	O
Section	O
5	O
.	O

Next	O
,	O
Section	O
3	O
presents	O
the	O
semantic	O
and	O
syntactic	O
features	O
used	O
.	O

The	O
remainder	O
of	O
this	O
paper	O
is	O
structured	O
as	O
follows	O
:	O
First	O
,	O
in	O
Section	O
2	O
,	O
the	O
related	O
work	O
is	O
discussed	O
.	O

We	O
further	O
experiment	O
with	O
a	O
combination	O
of	O
semantic	O
supersenses	O
and	O
syntactic	O
supertag	O
features	O
(	O
CCG	O
syntactic	O
categories	O
(	O
Steedman	O
,	O
2000	O
)	O
using	O
EasySRL	B-MethodName
(	O
Lewis	O
et	O
al	O
.	O
,	O
2015	O
)	O
)	O
and	O
less	O
complex	O
features	O
such	O
as	O
POS	O
-	O
tags	O
,	O
assuming	O
that	O
supersense	O
-	O
tags	O
have	O
the	O
potential	O
to	O
be	O
useful	O
especially	O
in	O
combination	O
with	O
syntactic	O
information	O
.	O

(	O
2016	O
)	O
,	O
replicating	O
the	O
tags	O
for	O
every	O
subword	O
unit	O
obtained	O
by	O
byte	O
-	O
pair	O
encoding	O
(	O
BPE	O
)	O
.	O

The	O
features	O
are	O
integrated	O
using	O
the	O
framework	O
of	O
Sennrich	O
et	O
al	O
.	O

To	O
obtain	O
these	O
features	O
,	O
we	O
used	O
the	O
AMALGrAM	B-MethodName
2.0	I-DatasetName
tool	O
(	O
Schneider	O
et	O
al	O
.	O
,	O
2014;Schneider	O
and	O
Smith	O
,	O
2015	O
)	O
which	O
analyses	O
the	O
input	O
sentence	O
for	O
Multi	O
-	O
Word	O
Expressions	O
as	O
well	O
as	O
noun	O
and	O
verb	O
supersenses	O
.	O

We	O
investigate	O
the	O
effect	O
of	O
integrating	O
supersense	O
features	O
(	O
26	O
for	O
nouns	O
,	O
15	O
for	O
verbs	O
)	O
into	O
an	O
NMT	B-TaskName
system	O
.	O

Inspired	O
by	O
Named	B-TaskName
Entity	I-TaskName
Recognition	I-TaskName
which	O
provides	O
such	O
abstractions	O
for	O
a	O
limited	O
set	O
of	O
words	O
,	O
supersense	O
-	O
tagging	O
uses	O
an	O
inventory	O
of	O
more	O
general	O
semantic	O
classes	O
for	O
domain	O
-	O
independent	O
settings	O
(	O
Schneider	O
and	O
Smith	O
,	O
2015	O
)	O
.	O

To	O
apply	O
semantic	O
abstractions	O
at	O
the	O
word	O
-	O
level	O
that	O
enable	O
a	O
characterisation	O
beyond	O
that	O
what	O
can	O
be	O
superficially	O
derived	O
,	O
coarse	O
-	O
grained	O
semantic	O
classes	O
can	O
be	O
used	O
.	O

Furthermore	O
,	O
a	O
combination	O
of	O
both	O
syntactic	O
and	O
semantic	O
features	O
would	O
provide	O
the	O
NMT	B-TaskName
system	O
with	O
a	O
way	O
of	O
learning	O
semantico	O
-	O
syntactic	O
patterns	O
.	O

However	O
,	O
making	O
some	O
level	O
of	O
semantics	O
more	O
explicitly	O
available	O
at	O
the	O
word	O
level	O
can	O
provide	O
the	O
translation	O
system	O
with	O
a	O
higher	O
level	O
of	O
abstraction	O
beneficial	O
to	O
learn	O
more	O
complex	O
constructions	O
.	O

This	O
might	O
be	O
explained	O
by	O
the	O
fact	O
that	O
NMT	B-TaskName
models	O
already	O
have	O
means	O
of	O
learning	O
semantic	O
similarities	O
through	O
word	O
-	O
embeddings	O
,	O
where	O
words	O
are	O
represented	O
in	O
a	O
common	O
vector	O
space	O
(	O
Mikolov	O
et	O
al	O
.	O
,	O
2013	O
)	O
.	O

Although	O
there	O
has	O
been	O
some	O
work	O
on	O
semantic	O
features	O
for	O
SMT	B-TaskName
(	O
Banchs	O
and	O
Costa	O
-	O
Jussà	O
,	O
2011	O
)	O
,	O
so	O
far	O
,	O
no	O
work	O
has	O
been	O
done	O
on	O
enriching	O
NMT	B-TaskName
systems	O
with	O
more	O
general	O
semantic	O
features	O
at	O
the	O
word	O
-	O
level	O
.	O

When	O
integrating	O
linguistic	O
information	O
into	O
an	O
MT	B-TaskName
system	O
,	O
following	O
the	O
central	O
role	O
assigned	O
to	O
syntax	O
by	O
many	O
linguists	O
,	O
the	O
focus	O
has	O
been	O
mainly	O
on	O
the	O
integration	O
of	O
syntactic	O
features	O
.	O

More	O
recent	O
work	O
showed	O
that	O
explicitly	O
(	O
Sennrich	O
and	O
Haddow	O
,	O
2016;Nadejde	O
et	O
al	O
.	O
,	O
2017;Bastings	O
et	O
al	O
.	O
,	O
2017;Aharoni	O
and	O
Goldberg	O
,	O
2017	O
)	O
or	O
implicitly	O
(	O
Eriguchi	O
et	O
al	O
.	O
,	O
2017	O
)	O
modeling	O
extra	O
syntactic	O
information	O
into	O
an	O
NMT	B-TaskName
system	O
on	O
the	O
source	O
(	O
and/or	O
target	O
)	O
side	O
could	O
lead	O
to	O
improvements	O
in	O
translation	O
quality	O
.	O

Although	O
NMT	B-TaskName
seems	O
to	O
partially	O
'	O
learn	O
'	O
or	O
generalize	O
some	O
patterns	O
related	O
to	O
syntax	O
from	O
the	O
raw	O
,	O
sentence	O
-	O
aligned	O
parallel	O
data	O
,	O
more	O
complex	O
phenomena	O
(	O
e.g.	O
prepositional	O
-	O
phrase	O
attachment	O
)	O
remain	O
problematic	O
(	O
Bentivogli	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

Compared	O
to	O
Statistical	B-TaskName
Machine	I-TaskName
Translation	I-TaskName
(	O
SMT	B-TaskName
)	O
,	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
,	O
NMT	B-TaskName
performs	O
particularly	O
well	O
when	O
it	O
comes	O
to	O
word	O
-	O
reorderings	O
and	O
translations	O
involving	O
morphologically	O
rich	O
languages	O
(	O
Bentivogli	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

Neural	B-TaskName
Machine	I-TaskName
Translation	I-TaskName
(	O
NMT	B-TaskName
)	O
models	O
have	O
recently	O
become	O
the	O
state	O
-	O
of	O
-	O
the	O
art	O
in	O
the	O
field	O
of	O
Machine	B-TaskName
Translation	I-TaskName
(	O
Bahdanau	O
et	O
al	O
.	O
,	O
2014;Cho	O
et	O
al	O
.	O
,	O
2014;Kalchbrenner	O
et	O
al	O
.	O
,	O
2014;Sutskever	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

Introduction	O
.	O

In	O
experiments	O
on	O
various	O
test	O
sets	O
,	O
we	O
observe	O
that	O
such	O
features	O
(	O
and	O
particularly	O
when	O
combined	O
)	O
help	O
the	O
NMT	B-TaskName
model	O
training	O
to	O
converge	O
faster	O
and	O
improve	O
the	O
model	O
quality	O
according	O
to	O
the	O
BLEU	B-MetricName
scores	O
.	O

In	O
this	O
paper	O
we	O
incorporate	O
semantic	O
supersensetags	O
and	O
syntactic	O
supertag	O
features	O
into	O
EN	O
-	O
FR	O
and	O
EN	O
-	O
DE	O
factored	O
NMT	B-TaskName
systems	O
.	O

SuperNMT	B-MethodName
:	O
Neural	B-TaskName
Machine	I-TaskName
Translation	I-TaskName
with	O
Semantic	O
Supersenses	O
and	O
Syntactic	O
Supertags	O
.	O

We	O
have	O
used	O
unsupervised	B-TaskName
NMT	B-TaskName
approach	O
of	O
MASS	B-MethodName
(	O
Song	O
et	O
al	O
.	O
,	O
2019	O
)	O
to	O
build	O
a	O
single	O
model	O
that	O
can	O
translate	O
in	O
both	O
the	O
directions	O
i.e.	O
Russian	O
to	O
Hindi	O
and	O
vice	O
-	O
versa	O
.	O

This	O
paper	O
presents	O
a	O
zero	O
-	O
shot	O
NMT	B-TaskName
task	O
on	O
the	O
Russian	O
⇔	O
Hindi	O
translation	O
,	O
this	O
system	O
was	O
used	O
to	O
participate	O
in	O
the	O
LoResMT	B-DatasetName
2020	O
shared	O
task	O
.	O

In	O
future	O
,	O
we	O
will	O
use	O
IndicNLP	B-MethodName
tokenizer	I-MethodName
(	O
Kunchukuttan	O
,	O
2020	O
)	O
.	O

In	O
this	O
work	O
,	O
we	O
have	O
used	O
the	O
default	O
tokenizer	O
i.e.	O
Moses	B-MethodName
.	O

Task	O
BLEU	B-MetricName
Precision	B-MetricName
Recall	I-MetricName
F	B-MetricName
-	I-MetricName
measure	I-MetricName
RIBES	B-MetricName
.	O

The	O
results	O
are	O
evaluated	O
using	O
automatic	O
evaluation	O
metrics	O
,	O
BLEU	B-MetricName
(	O
Papineni	O
et	O
al	O
.	O
,	O
2002	O
)	O
,	O
precision	B-MetricName
,	O
recall	B-MetricName
,	O
F	B-MetricName
-	I-MetricName
measure	I-MetricName
and	O
RIBES	B-MetricName
(	O
Isozaki	O
et	O
al	O
.	O
,	O
2010	O
)	O
.	O

The	O
LoResMT	B-DatasetName
2020	O
shared	O
task	O
organizer	O
declared	O
the	O
evaluation	O
result	O
5	O
of	O
zero	O
-	O
shot	O
NMT	B-TaskName
on	O
the	O
language	O
pairs	O
namely	O
,	O
Hindi	O
-	O
Bhojpuri	O
,	O
Hindi	O
-	O
Magahi	O
,	O
and	O
Russian	O
-	O
Hindi	O
,	O
and	O
participated	O
by	O
two	O
teams	O
only	O
.	O

Due	O
to	O
limited	O
computational	O
resources	O
,	O
we	O
have	O
used	O
256	B-HyperparameterValue
embedding	B-HyperparameterName
layers	I-HyperparameterName
with	O
batch	B-HyperparameterName
size	I-HyperparameterName
32	B-HyperparameterValue
,	O
tokens	B-HyperparameterName
per	I-HyperparameterName
batch	I-HyperparameterName
500	B-HyperparameterValue
and	O
dropout	B-HyperparameterName
0.1	B-HyperparameterValue
.	O
The	O
obtained	O
pre	O
-	O
trained	O
model	O
from	O
4.1	O
are	O
fine	O
-	O
tuned	O
with	O
pseudo	O
bilingual	O
corpus	O
through	O
self	O
-	O
generated	O
back	O
-	O
translation	O
data	O
following	O
default	O
settings	O
of	O
(	O
Song	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

In	O
the	O
pre	O
-	O
training	O
step	O
,	O
we	O
have	O
followed	O
the	O
default	O
settings	O
of	O
Transformer	B-MethodName
model	I-MethodName
-	I-MethodName
based	I-MethodName
Mass	I-MethodName
(	O
Song	O
et	O
al	O
.	O
,	O
2019	O
)	O
,	O
where	O
6	B-HyperparameterValue
layers	B-HyperparameterName
with	O
8	B-HyperparameterValue
attention	B-HyperparameterName
heads	I-HyperparameterName
are	O
used	O
.	O

During	O
pre	O
-	O
processing	O
of	O
the	O
data	O
,	O
following	O
(	O
Song	O
et	O
al	O
.	O
,	O
2019	O
)	O
and	O
using	O
the	O
code	O
provided	O
by	O
(	O
Lample	O
and	O
Conneau	O
,	O
2019	O
)	O
,	O
we	O
used	O
fastBPE	B-MethodName
4	O
to	O
learn	O
byte	O
pair	O
encoding	O
(	O
BPE	O
)	O
vocabulary	O
with	O
50,000	O
codes	O
.	O

(	O
1	O
)	O
Here	O
,	O
the	O
seq2seq	O
model	O
learns	O
the	O
parameter	O
θ	O
to	O
compute	O
the	O
conditional	O
probability	O
.	O

For	O
the	O
pre	O
-	O
training	O
step	O
,	O
following	O
(	O
Song	O
et	O
al	O
.	O
,	O
2019	O
)	O
we	O
have	O
undertaken	O
the	O
log	O
likelihood	O
objective	O
function	O
(	O
LF	O
)	O
as	O
shown	O
in	O
Equation	O
1	O
.	O

The	O
MASS	B-MethodName
(	O
Song	O
et	O
al	O
.	O
,	O
2019	O
)	O
based	O
model	O
leverages	O
encode	B-MethodName
-	I-MethodName
decoder	O
framework	O
to	O
develop	O
complete	O
sentences	O
from	O
given	O
fractured	O
pieces	O
of	O
sentences	O
as	O
shown	O
in	O
Figure	O
1	O
.	O

Moses	O
is	O
used	O
for	O
tokenization	O
(	O
Koehn	O
and	O
Hoang	O
,	O
2010	O
)	O
.	O

Our	O
system	O
consists	O
of	O
two	O
major	O
steps	O
namely	O
the	O
pre	O
-	O
training	O
and	O
then	O
the	O
fine	O
-	O
tuning	O
step	O
which	O
are	O
discussed	O
in	O
the	O
sub	O
-	O
sections	O
4.1	O
and	O
4.2	O
.	O
For	O
BPE	O
(	O
Sennrich	O
et	O
al	O
.	O
,	O
2016	O
)	O
and	O
vocabulary	O
creation	O
,	O
we	O
have	O
used	O
the	O
cross	O
-	O
language	O
model	O
(	O
XLM	O
)	O
(	O
Lample	O
and	O
Conneau	O
,	O
2019	O
)	O
codebase	O
as	O
given	O
in	O
their	O
repository	O
3	O
.	O

We	O
have	O
adopted	O
MASS	B-MethodName
based	O
unsupervised	B-TaskName
NMT	B-TaskName
(	O
Song	O
et	O
al	O
.	O
,	O
2019	O
)	O
to	O
build	O
our	O
system	O
on	O
a	O
single	O
GPU	O
.	O

Additionally	O
,	O
we	O
have	O
used	O
external	O
monolingual	O
data	O
set	O
of	O
Hindi	O
(	O
9	O
GB	O
)	O
from	O
IITB	B-DatasetName
1	O
(	O
Kunchukuttan	O
et	O
al	O
.	O
,	O
2018;Bojar	O
et	O
al	O
.	O
,	O
2014	O
)	O
and	O
Russian	O
(	O
9	O
GB	O
)	O
from	O
WMT16	B-DatasetName
2	O
.	O

The	O
LoResMT	B-DatasetName
2020	O
shared	O
task	O
organizer	O
(	O
Ojha	O
et	O
al	O
.	O
,	O
2020	O
)	O
provided	O
the	O
Russian	O
-	O
Hindi	O
monolingual	O
dataset	O
of	O
train	O
,	O
valid	O
,	O
and	O
test	O
sets	O
,	O
which	O
is	O
summarized	O
in	O
Table	O
1	O
.	O

Figure	O
1	O
:	O
The	O
encoder	B-MethodName
-	I-MethodName
decoder	I-MethodName
framework	I-MethodName
of	O
the	O
MASS	B-MethodName
model	O
used	O
(	O
as	O
adopted	O
from	O
(	O
Song	O
et	O
al	O
.	O
,	O
2019	O
)	O
)	O
.	O

However	O
,	O
the	O
literature	O
survey	O
finds	O
work	O
on	O
unsupervised	B-TaskName
NMT	I-TaskName
using	O
MASS	B-MethodName
(	O
Song	O
et	O
al	O
.	O
,	O
2019	O
)	O
which	O
outperform	O
previous	O
unsupervised	O
approaches	O
(	O
Lample	O
and	O
Conneau	O
,	O
2019;Lample	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

The	O
reason	O
behind	O
choosing	O
MASS	B-MethodName
-	O
based	O
unsupervised	B-TaskName
NMT	B-TaskName
is	O
that	O
it	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
the	O
unsupervised	O
English	O
-	O
French	O
pair	O
translation	O
.	O

In	O
this	O
paper	O
,	O
we	O
have	O
participated	O
in	O
the	O
LoResMT	B-DatasetName
2020	O
shared	O
task	O
of	O
zero	O
-	O
shot	O
NMT	B-TaskName
approach	O
on	O
Russian	O
-	O
Hindi	O
pair	O
using	O
the	O
only	O
monolingual	O
corpus	O
and	O
the	O
same	O
has	O
been	O
implemented	O
using	O
MASS	O
-	O
based	O
unsupervised	B-TaskName
NMT	I-TaskName
(	O
Song	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

(	O
Johnson	O
et	O
al	O
.	O
,	O
2017	O
)	O
introduced	O
a	O
zero	O
-	O
shot	O
approach	O
to	O
language	B-TaskName
pair	I-TaskName
translation	I-TaskName
without	O
considering	O
the	O
parallel	O
data	O
using	O
multilingual	B-MethodName
-	I-MethodName
based	I-MethodName
NMT	I-MethodName
.	O

For	O
low	O
resource	O
language	O
pair	O
translation	O
,	O
pivot	B-MethodName
-	I-MethodName
based	I-MethodName
NMT	B-TaskName
(	O
Kim	O
et	O
al	O
.	O
,	O
2019	O
)	O
is	O
an	O
effective	O
approach	O
where	O
an	O
intermediate	O
language	O
is	O
considered	O
as	O
a	O
pivot	O
language	O
(	O
source	O
to	O
pivot	O
and	O
pivot	O
to	O
target	O
)	O
.	O

The	O
RNN	B-MethodName
based	O
NMT	B-TaskName
approach	O
is	O
not	O
able	O
to	O
process	O
all	O
the	O
input	O
words	O
parallelly	O
,	O
to	O
solve	O
parallelization	O
transformer	O
-	O
based	O
NMT	B-TaskName
(	O
Vaswani	O
et	O
al	O
.	O
,	O
2017	O
)	O
is	O
proposed	O
by	O
using	O
a	O
self	O
-	O
attention	O
mechanism	O
.	O

The	O
end	O
-	O
to	O
-	O
end	O
recurrent	B-MethodName
neural	I-MethodName
network	I-MethodName
(	O
RNN	B-MethodName
)	O
based	O
NMT	B-TaskName
(	O
Cho	O
et	O
al	O
.	O
,	O
2014b	O
,	O
a	O
)	O
approach	O
attracts	O
attention	O
in	O
MT	B-TaskName
because	O
it	O
deals	O
with	O
many	O
challenges	O
like	O
variable	O
-	O
length	O
phrases	O
using	O
sequence	O
to	O
sequence	O
learning	O
concept	O
,	O
long	O
-	O
term	O
dependency	O
problem	O
adopting	O
long	B-MethodName
short	I-MethodName
term	I-MethodName
memory	I-MethodName
(	O
LSTM	I-MethodName
)	O
(	O
Sutskever	O
et	O
al	O
.	O
,	O
2014	O
)	O
,	O
attention	O
mechanism	O
(	O
Bahdanau	O
et	O
al	O
.	O
,	O
2015;Luong	O
et	O
al	O
.	O
,	O
2015	O
)	O
which	O
pays	O
attention	O
globally	O
and	O
locally	O
to	O
all	O
source	O
words	O
.	O

And	O
for	O
Hindi	O
to	O
Russian	O
translation	I-TaskName
,	O
we	O
have	O
achieved	O
BLEU	B-MetricName
,	O
precision	B-MetricName
,	O
recall	B-MetricName
,	O
F	B-MetricName
-	I-MetricName
measure	I-MetricName
,	O
and	O
RIBES	B-MetricName
score	O
of	O
1.11	B-MetricValue
,	O
4.72	B-MetricValue
,	O
4.41	B-MetricValue
,	O
4.56	B-MetricValue
,	O
and	O
0.026842	B-MetricValue
respectively	O
.	O

The	O
evaluated	O
results	O
are	O
declared	O
at	O
the	O
LoResMT	B-DatasetName
2020	O
shared	O
task	O
,	O
which	O
reports	O
that	O
our	O
system	O
achieves	O
the	O
bilingual	B-TaskName
evaluation	I-DatasetName
understudy	I-TaskName
(	O
BLEU	B-MetricName
)	O
score	O
of	O
0.59	B-MetricValue
,	O
precision	B-MetricName
score	O
of	O
3.43	B-MetricValue
,	O
recall	B-MetricName
score	O
of	O
5.48	B-MetricValue
,	O
F	B-MetricName
-	I-MetricName
measure	I-MetricName
score	O
of	O
4.22	B-MetricValue
,	O
and	O
rank	B-MetricName
-	I-MetricName
based	I-MetricName
intuitive	I-MetricName
bilingual	I-MetricName
evaluation	I-MetricName
score	O
(	O
RIBES	B-MetricName
)	O
of	O
0.180147	B-MetricValue
in	O
Russian	O
to	O
Hindi	O
translation	O
.	O

We	O
have	O
used	O
masked	O
sequence	O
to	O
sequence	O
pre	O
-	O
training	O
for	O
language	O
generation	O
(	O
MASS	B-DatasetName
)	O
with	O
only	O
monolingual	O
corpus	O
following	O
the	O
unsupervised	B-TaskName
NMT	I-TaskName
architecture	O
.	O

Workshop	O
on	O
Technologies	O
for	O
MT	B-TaskName
of	O
Low	O
Resource	O
Languages	O
(	O
LoResMT	O
2020	O
)	O
organized	O
shared	O
tasks	O
of	O
low	B-TaskName
resource	I-TaskName
language	I-TaskName
pair	I-TaskName
translation	I-TaskName
using	O
zero	O
-	O
shot	O
NMT	B-TaskName
.	O

To	O
mitigate	O
this	O
issue	O
,	O
NMT	B-TaskName
attempts	O
to	O
utilize	O
a	O
monolingual	O
corpus	O
to	O
get	O
better	O
at	O
translation	O
for	O
low	O
resource	O
language	O
pairs	O
.	O

Neural	B-TaskName
machine	I-TaskName
translation	I-TaskName
(	O
NMT	B-TaskName
)	O
is	O
a	O
widely	O
accepted	O
approach	O
in	O
the	O
machine	O
translation	O
(	O
MT	B-TaskName
)	O
community	O
,	O
translating	O
from	O
one	O
natural	O
language	O
to	O
another	O
natural	O
language	O
.	O

Zero	O
-	O
Shot	O
Neural	B-TaskName
Machine	I-TaskName
Translation	I-TaskName
:	O
Russian	O
-	O
Hindi	O
@LoResMT	O
2020	O
.	O

Also	O
,	O
thank	O
to	O
LoResMT	O
2020	O
shared	O
task	O
organizers	O
.	O

We	O
would	O
like	O
to	O
thank	O
Center	O
for	O
Natural	O
Language	O
Processing	O
(	O
CNLP	O
)	O
and	O
Department	O
of	O
Computer	O
Science	O
and	O
Engineering	O
at	O
National	O
Institute	O
of	O
Technology	O
,	O
Silchar	O
,	O
India	O
for	O
providing	O
the	O
requisite	O
support	O
and	O
infrastructure	O
to	O
execute	O
this	O
work	O
.	O

Acknowledgement	O
.	O

The	O
obtained	O
scores	O
and	O
closely	O
observed	O
predicted	O
output	O
remarks	O
that	O
our	O
future	O
works	O
require	O
significant	O
improvement	O
to	O
achieve	O
better	O
translation	O
accuracies	B-MetricName
in	O
both	O
directions	O
.	O

Conclusion	O
and	O
Future	O
Work	O
.	O

This	O
tokenizer	O
is	O
specifically	O
designed	O
for	O
Indic	O
languages	O
,	O
in	O
order	O
to	O
improve	O
the	O
overall	O
performance	O
of	O
predictive	O
models	O
in	O
Hindi	O
languages	O
.	O

To	O
achieve	O
better	O
translation	O
accuracy	B-MetricName
,	O
we	O
need	O
to	O
improve	O
both	O
adequacy	O
as	O
well	O
as	O
fluency	O
of	O
predicted	O
translations	O
.	O

Moreover	O
,	O
from	O
the	O
predicted	O
translation	O
as	O
shown	O
in	O
Figure	O
2	O
,	O
it	O
is	O
quite	O
clear	O
that	O
the	O
translation	O
accuracy	B-MetricName
is	O
very	O
poor	O
in	O
terms	O
of	O
adequacy	O
but	O
better	O
in	O
the	O
fluency	B-MetricName
factor	I-MetricName
of	O
translation	O
.	O

However	O
,	O
it	O
is	O
to	O
be	O
noted	O
that	O
with	O
increasing	O
monolingual	O
data	O
,	O
the	O
performance	O
of	O
our	O
systems	O
improves	O
.	O

From	O
Table	O
2	O
,	O
it	O
is	O
observed	O
that	O
our	O
scores	O
are	O
very	O
low	O
.	O

We	O
have	O
submitted	O
two	O
systems	O
result	O
,	O
one	O
only	O
using	O
provided	O
monolingual	O
data	O
(	O
extension	O
-a	O
)	O
and	O
another	O
with	O
external	O
monolingual	O
data	O
addition	O
of	O
provided	O
monolingual	O
data	O
(	O
extension	O
-c	O
)	O
and	O
the	O
same	O
have	O
been	O
reported	O
in	O
Table	O
2	O
.	O

For	O
the	O
Russian	O
-	O
Hindi	O
language	O
pair	O
,	O
only	O
our	O
team	O
participated	O
and	O
our	O
team	O
name	O
is	O
CNLP	O
-	O
NITS	O
.	O

Result	O
and	O
Analysis	O
.	O

Also	O
,	O
for	O
leveraging	O
the	O
model	O
features	O
,	O
we	O
have	O
followed	O
the	O
settings	O
of	O
(	O
Song	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

Experimental	O
Setup	O
.	O

Experiment	O
.	O

Here	O
simply	O
backtranslation	O
is	O
employed	O
to	O
generate	O
pseudo	O
bilin-	O
.	O

Only	O
the	O
monolingual	O
data	O
is	O
used	O
here	O
.	O

Since	O
,	O
the	O
parallel	O
data	O
is	O
not	O
made	O
available	O
by	O
the	O
LoResMT	B-DatasetName
2020	O
organizers	O
for	O
this	O
specific	O
task	O
,	O
we	O
have	O
undertaken	O
the	O
unsupervised	O
approach	O
as	O
followed	O
by	O
(	O
Song	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

Fine	O
Tuning	O
.	O

t	O
denotes	O
the	O
word	O
position	O
.	O

LF	O
(	O
θ	O
;	O
S	O
)	O
=	O
1	O
|S|	O
Σ	O
s∈S	O
log	O
P	O
(	O
s	O
u	O
:	O
v	O
|s	O
\u	O
:	O
v	O
;	O
θ	O
)	O
=	O
1	O
|S|	O
Σ	O
s∈S	O
log	O
v	O
t	O
=	O
u	O
P	O
(	O
s	O
u	O
:	O
v	O
t	O
|s	O
u	O
:	O
v	O
<	O
t	O
,	O
s	O
\u	O
:	O
v	O
;	O
θ	O
)	O
.	O

And	O
in	O
a	O
particular	O
sentence	O
s	O
,	O
the	O
region	O
from	O
u	O
to	O
v	O
is	O
masked	O
,	O
such	O
that	O
the	O
sentence	O
length	O
remains	O
constant	O
.	O

Here	O
,	O
s	O
belongs	O
to	O
the	O
source	O
sentence	O
corpus	O
S.	O

Pre	O
-	O
training	O
.	O

The	O
model	O
details	O
are	O
further	O
described	O
in	O
Section	O
4.1	O
and	O
4.2	O
,	O
where	O
we	O
have	O
shown	O
the	O
pre	O
-	O
training	O
and	O
fine	O
tuning	O
step	O
respectively	O
.	O

System	O
Description	O
.	O

Dataset	O
Description	O
.	O

Attention	O
.	O

(	O
Song	O
et	O
al	O
.	O
,	O
2019	O
)	O
without	O
us-	O
X	O
8	O
X	O
4	O
X	O
7	O
X	O
1	O
X	O
2	O
X	O
3	O
_	O
X	O
6	O
_	O
_	O
_	O
_	O
_	O
Encoder	O
Decoder	O
_	O
_	O
X	O
5	O
.	O

There	O
is	O
a	O
lack	O
of	O
background	O
work	O
on	O
Russian	O
-	O
Hindi	O
translation	O
.	O

Related	O
Work	O
.	O

Generally	O
,	O
language	O
pairs	O
can	O
be	O
considered	O
as	O
low	O
-	O
resource	O
when	O
training	O
data	O
is	O
less	O
than	O
a	O
million	O
(	O
Kocmi	O
,	O
2020	O
)	O
.	O

Despite	O
modifying	O
NMT	B-TaskName
architecture	O
,	O
it	O
needs	O
reasonable	O
parallel	O
training	O
data	O
which	O
is	O
a	O
challenge	O
for	O
low	O
resource	O
language	O
pair	O
translation	O
.	O

Introduction	O
.	O

We	O
have	O
participated	O
in	O
the	O
same	O
shared	O
task	O
with	O
our	O
team	O
name	O
CNLP	O
-	O
NITS	O
for	O
the	O
Russian	O
-	O
Hindi	O
language	O
pair	O
.	O

Here	O
,	O
the	O
parallel	O
corpus	O
is	O
not	O
used	O
and	O
only	O
monolingual	O
corpora	O
is	O
allowed	O
.	O

The	O
availability	O
of	O
a	O
parallel	O
corpus	O
in	O
low	O
resource	O
language	O
pairs	O
is	O
one	O
of	O
the	O
challenging	O
tasks	O
in	O
MT	B-TaskName
.	O

Although	O
,	O
NMT	B-TaskName
shows	O
remarkable	O
performance	O
in	O
both	O
high	O
and	O
low	O
resource	O
languages	O
,	O
it	O
needs	O
sufficient	O
training	O
corpus	O
.	O

6	O
.	O

Finally	O
,	O
we	O
note	O
that	O
the	O
NER	B-TaskName
tagging	O
provided	O
by	O
LitNER	B-MethodName
has	O
been	O
integrated	O
into	O
the	O
GutenTag	O
tool	O
(	O
as	O
of	O
version	O
0.1.4	O
)	O
.	O

Our	O
results	O
show	O
that	O
a	O
simple	O
classifier	O
,	O
trained	O
only	O
with	O
noisy	O
examples	O
derived	O
in	O
an	O
unsupervised	O
fashion	O
,	O
can	O
easily	O
beat	O
a	O
general	O
-	O
purpose	O
supervised	O
system	O
,	O
provided	O
it	O
has	O
access	O
to	O
the	O
full	O
context	O
of	O
the	O
text	O
.	O

In	O
this	O
paper	O
,	O
we	O
have	O
presented	O
LitNER	B-MethodName
,	O
an	O
NER	B-TaskName
system	O
targeted	O
specifically	O
at	O
fiction	O
.	O

Conclusion	O
.	O

When	O
these	O
and	O
non	O
-	O
entities	O
are	O
excluded	O
from	O
OTHER	O
,	O
what	O
remains	O
is	O
eclectic	O
,	O
including	O
names	O
referring	O
to	O
small	O
groups	O
of	O
people	O
(	O
e.g.	O
families	O
)	O
,	O
animals	O
,	O
gods	O
,	O
ships	O
,	O
and	O
titles	O
of	O
other	O
works	O
of	O
literature	O
.	O

The	O
other	O
clusters	O
in	O
Table	O
1	O
reflect	O
word	O
categories	O
which	O
are	O
relatively	O
closed	O
-	O
class	O
and	O
much	O
less	O
central	O
to	O
the	O
fictional	O
narratives	O
as	O
character	O
and	O
setting	O
;	O
we	O
do	O
n't	O
see	O
a	O
compelling	O
case	O
for	O
tagging	O
them	O
.	O

This	O
coarse	O
-	O
grained	O
tag	O
set	O
reflects	O
not	O
only	O
the	O
practical	O
limitations	O
of	O
the	O
method	O
,	O
but	O
also	O
where	O
we	O
believe	O
automatic	O
methods	O
have	O
potential	O
to	O
provide	O
useful	O
information	O
for	O
literary	O
analysis	O
.	O

LitNER	B-MethodName
tags	O
names	O
into	O
only	O
two	O
main	O
classes	O
,	O
PERSON	O
and	O
LOCATION	O
,	O
plus	O
a	O
catch	O
-	O
all	O
OTHER	O
.	O

We	O
also	O
note	O
that	O
relying	O
primarily	O
on	O
contextual	O
classification	O
while	O
eschewing	O
resources	O
such	O
as	O
gazetteers	O
makes	O
much	O
less	O
sense	O
outside	O
the	O
context	O
of	O
fiction	O
;	O
we	O
would	O
expect	O
relatively	O
few	O
fictitious	O
entities	O
in	O
most	O
genres	O
.	O

Except	O
in	O
cases	O
where	O
it	O
is	O
possible	O
to	O
collapse	O
texts	O
into	O
appropriately	O
-	O
sized	O
groups	O
where	O
the	O
use	O
of	O
a	O
particular	O
name	O
is	O
likely	O
to	O
be	O
both	O
common	O
and	O
consistent	O
-	O
an	O
example	O
might	O
be	O
a	O
collection	O
of	O
texts	O
written	O
by	O
a	O
single	O
author	O
,	O
which	O
in	O
social	O
media	O
such	O
as	O
Twitter	O
seems	O
to	O
obey	O
the	O
classic	O
one	O
-	O
sense	O
-	O
per	O
-	O
discourse	O
rule	O
(	O
Gella	O
et	O
al	O
.	O
,	O
2014)-it	O
's	O
not	O
clear	O
that	O
this	O
approach	O
can	O
be	O
applied	O
successfully	O
in	O
cases	O
where	O
texts	O
are	O
relatively	O
short	O
,	O
which	O
is	O
a	O
far	O
more	O
common	O
situation	O
.	O

Also	O
less	O
than	O
promising	O
is	O
the	O
potential	O
for	O
using	O
text	B-TaskName
-	I-TaskName
level	I-TaskName
classification	I-TaskName
in	O
other	O
genres	O
:	O
whereas	O
the	O
average	O
number	O
of	O
token	O
occurrences	O
of	O
distinct	O
name	O
types	O
within	O
a	O
single	O
text	O
in	O
the	O
Project	B-DatasetName
Gutenberg	I-DatasetName
corpus	O
is	O
5.9	O
,	O
this	O
number	O
is	O
just	O
1.6	O
for	O
the	O
much	O
-	O
shorter	O
texts	O
of	O
the	O
Gigaword	B-DatasetName
corpus	O
.	O

For	O
the	O
latter	O
,	O
there	O
were	O
several	O
clusters	O
in	O
the	O
top	O
10	O
(	O
including	O
the	O
first	O
one	O
)	O
which	O
corresponded	O
to	O
LOCATION	O
,	O
while	O
the	O
first	O
(	O
fairly	O
)	O
clean	O
PERSON	O
cluster	O
was	O
the	O
15th	O
largest	O
;	O
in	O
general	O
,	O
individual	O
people	O
,	O
organizations	O
,	O
and	O
other	O
groupings	O
of	O
people	O
(	O
e.g.	O
by	O
country	O
of	O
origin	O
)	O
were	O
not	O
well	O
distinguished	O
by	O
Brown	O
clustering	O
in	O
the	O
Gigaword	B-DatasetName
corpus	O
,	O
at	O
least	O
not	O
with	O
the	O
same	O
low	O
number	O
of	O
clusters	O
that	O
worked	O
well	O
in	O
the	O
Project	B-DatasetName
Gutenberg	I-DatasetName
corpus	O
.	O

With	O
respect	O
to	O
the	O
former	O
,	O
the	O
obvious	O
issue	O
is	O
considerably	O
more	O
complex	O
proper	O
nouns	O
phrases	O
such	O
as	O
governmental	O
organizations	O
and	O
related	O
titles	O
.	O

We	O
have	O
found	O
more	O
subtle	O
genre	O
effects	O
as	O
well	O
:	O
for	O
comparison	O
,	O
we	O
applied	O
the	O
preliminary	O
steps	O
of	O
our	O
approach	O
to	O
another	O
corpus	O
of	O
published	O
texts	O
which	O
is	O
of	O
comparable	O
(	O
token	O
)	O
size	O
to	O
the	O
Project	B-DatasetName
Gutenberg	I-DatasetName
corpus	O
,	O
namely	O
the	O
Gigaword	B-DatasetName
newswire	I-DatasetName
corpus	O
(	O
Graff	O
and	O
Cieri	O
,	O
2003	O
)	O
,	O
and	O
noted	O
degraded	O
performance	O
for	O
both	O
segmentation	O
and	O
Brown	B-MethodName
clustering	I-MethodName
.	O

The	O
initial	O
rule	O
-	O
based	O
segmentation	O
,	O
for	O
instance	O
,	O
depends	O
on	O
reliable	O
capitalization	O
of	O
names	O
,	O
which	O
is	O
often	O
not	O
present	O
in	O
social	O
media	O
,	O
or	O
in	O
most	O
non	O
-	O
European	O
languages	O
.	O

Aspects	O
of	O
the	O
method	O
presented	O
here	O
could	O
theoretically	O
be	O
applied	O
to	O
NER	B-TaskName
in	O
other	O
genres	O
and	O
other	O
languages	O
,	O
but	O
one	O
important	O
point	O
we	O
wish	O
to	O
make	O
is	O
that	O
our	O
approach	O
clearly	O
takes	O
advantage	O
of	O
specific	O
properties	O
of	O
(	O
English	O
)	O
literature	O
.	O

Discussion	O
.	O

This	O
potential	O
0.002	O
improvement	O
is	O
tiny	O
compared	O
to	O
the	O
0.085	O
difference	O
in	O
accuracy	B-MetricName
between	O
the	O
two	O
systems	O
.	O

Only	O
2	O
of	O
the	O
15	O
were	O
segmented	O
correctly	O
by	O
Stanford	O
CoreNLP	O
.	O

For	O
the	O
segmentation	O
errors	O
,	O
we	O
compared	O
our	O
corrected	O
segmentations	O
with	O
the	O
segmentation	O
provided	O
by	O
the	O
CRF	B-MethodName
-	O
based	O
Stanford	O
CoreNLP	O
system	O
,	O
our	O
best	O
competitor	O
.	O

With	O
regards	O
to	O
different	O
options	O
for	O
LitNER	B-MethodName
,	O
we	O
see	O
a	O
major	O
benefit	O
from	O
considering	O
all	O
occurrences	O
of	O
the	O
name	O
in	O
the	O
texts	O
rather	O
than	O
just	O
the	O
one	O
we	O
are	O
testing	O
on	O
(	O
Section	O
3.3	O
)	O
,	O
and	O
a	O
more	O
modest	O
benefit	O
from	O
using	O
the	O
information	O
on	O
parts	O
of	O
phrases	O
taken	O
from	O
the	O
Brown	B-MethodName
clustering	I-MethodName
(	O
Section	O
3.4	O
)	O
.	O

LitNER	B-MethodName
is	O
also	O
clearly	O
better	O
than	O
the	O
Brown	B-MethodName
clusters	I-MethodName
it	O
was	O
trained	O
on	O
,	O
particularly	O
for	O
F	B-MetricName
M	I-MetricName
(	O
+0.120	B-MetricValue
absolute	O
)	O
.	O

Stanford	O
CoreNLP	O
is	O
the	O
only	O
competitive	O
off	O
-	O
the	O
-	O
shelf	O
system	O
-	O
the	O
other	O
two	O
are	O
far	O
too	O
conservative	O
when	O
encountering	O
names	O
they	O
have	O
n't	O
seen	O
before	O
.	O

The	O
results	O
in	O
Table	O
2	O
show	O
that	O
our	O
system	O
easily	O
bests	O
the	O
off	O
-	O
the	O
-	O
shelf	O
systems	O
when	O
it	O
is	O
given	O
the	O
contextual	O
information	O
from	O
the	O
entire	O
text	O
;	O
the	O
difference	O
is	O
more	O
stark	O
for	O
accuracy	B-MetricName
(	O
+0.085	B-MetricValue
absolute	O
)	O
,	O
though	O
consistent	O
for	O
F	B-MetricName
M	I-MetricName
(	O
+0.041	B-MetricValue
absolute	O
)	O
.	O

Results	O
.	O

We	O
evaluate	O
using	O
two	O
standard	O
metrics	O
:	O
accuracy	B-MetricName
(	O
"	O
Acc	B-MetricName
"	O
)	O
,	O
and	O
macroaveraged	B-MetricName
F	I-MetricName
-	I-MetricName
score	I-MetricName
(	O
"	O
F	O
M	O
"	O
)	O
.	O

For	O
our	O
system	O
(	O
LitNER	B-MethodName
)	O
,	O
we	O
test	O
a	O
version	O
where	O
only	O
the	O
immediate	O
sentence	O
context	O
is	O
used	O
(	O
"	O
sentence	O
"	O
)	O
,	O
and	O
versions	O
based	O
on	O
text	O
context	O
(	O
"	O
text	O
"	O
)	O
with	O
or	O
without	O
our	O
phrase	O
improvement	O
(	O
"	O
±phrase	O
"	O
)	O
.	O

Since	O
the	O
exact	O
segmentation	O
guidelines	O
likely	O
varied	O
across	O
these	O
systems	O
-	O
in	O
particular	O
,	O
we	O
found	O
that	O
Stanford	O
CoreNLP	O
often	O
left	O
off	O
the	O
title	O
in	O
names	O
such	O
as	O
Mr.	O
Smithand	O
we	O
did	O
n't	O
want	O
to	O
focus	O
on	O
these	O
issues	O
,	O
we	O
did	O
not	O
require	O
exact	O
matches	O
of	O
our	O
name	O
segmentation	O
;	O
instead	O
,	O
we	O
consider	O
the	O
entire	O
name	O
as	O
PERSON	O
or	O
LOCATION	O
if	O
any	O
of	O
the	O
tokens	O
were	O
tagged	O
as	O
such	O
(	O
names	O
with	O
both	O
tags	O
were	O
considered	O
OTHER	O
)	O
.	O

OpenNLP	B-MethodName
allowed	O
us	O
to	O
classify	O
only	O
PERSON	O
and	O
LOCATION	O
,	O
but	O
for	O
Stanford	O
CoreNLP	O
and	O
LingPipe	O
we	O
used	O
the	O
existing	O
3	O
-	O
entity	O
systems	O
,	O
with	O
the	O
ORGANI	O
-	O
ZATION	O
tag	O
collapsed	O
into	O
OTHER	O
(	O
as	O
it	O
was	O
in	O
our	O
guidelines	O
;	O
instances	O
of	O
ORGANIZATION	O
are	O
rare	O
in	O
literature	O
)	O
.	O

We	O
compare	O
our	O
system	O
to	O
a	O
selection	O
of	O
publicly	O
available	O
,	O
off	O
-	O
the	O
-	O
shelf	O
NER	B-TaskName
systems	O
:	O
OpenNLP	B-MethodName
,	O
4	O
LingPipe	B-MethodName
,	O
5	O
and	O
Stanford	O
CoreNLP	O
(	O
Finkel	O
et	O
al	O
.	O
,	O
2005	O
)	O
,	O
as	O
well	O
as	O
the	O
initial	O
Brown	O
clustering	I-MethodName
.	O

We	O
ran	O
a	O
separate	O
two	O
-	O
annotator	O
agreement	O
study	O
over	O
200	O
examples	O
which	O
yielded	O
a	O
Cohen	B-MetricName
's	I-MetricName
Kappa	I-MetricName
of	O
0.84	B-MetricValue
,	O
suggesting	O
high	O
enough	O
reliability	O
that	O
a	O
single	O
annotator	O
was	O
sufficient	O
.	O

The	O
annotator	O
was	O
presented	O
with	O
the	O
sentence	O
and	O
the	O
pre	O
-	O
segmented	O
name	O
of	O
interest	O
,	O
and	O
asked	O
(	O
via	O
written	O
instructions	O
)	O
to	O
categorize	O
the	O
indicated	O
name	O
into	O
PERSON	O
,	O
LOCATION	O
,	O
OTHER	O
,	O
UNCERTAIN	O
due	O
to	O
ambiguity	O
,	O
or	O
segmentation	O
error	O
.	O

These	O
were	O
tagged	O
by	O
a	O
single	O
annotator	O
,	O
an	O
English	O
native	O
speaker	O
with	O
a	O
PhD	O
in	O
English	O
Literature	O
.	O

To	O
this	O
end	O
,	O
we	O
randomly	O
sampled	O
texts	O
,	O
sentences	O
,	O
and	O
then	O
names	O
within	O
those	O
sentences	O
from	O
our	O
name	O
-	O
segmented	O
Project	I-DatasetName
Gutenberg	I-DatasetName
corpus	O
to	O
produce	O
a	O
set	O
of	O
1000	O
examples	O
.	O

Though	O
there	O
are	O
a	O
few	O
novels	O
which	O
have	O
been	O
tagged	O
for	O
characters	O
(	O
Vala	O
et	O
al	O
.	O
,	O
2015	O
)	O
,	O
we	O
wanted	O
to	O
test	O
our	O
system	O
relative	O
to	O
a	O
much	O
wider	O
range	O
of	O
fiction	O
.	O

Our	O
interest	O
is	O
in	O
a	O
general	O
NER	B-TaskName
system	O
for	O
literature	O
.	O

Evaluation	O
.	O

Once	O
we	O
have	O
calculated	O
p	O
(	O
t|r	O
)	O
for	O
each	O
class	O
,	O
we	O
choose	O
the	O
t	O
with	O
the	O
highest	O
p	O
(	O
t|r	O
)	O
.	O

Note	O
that	O
if	O
r	O
s	O
/	O
∈	O
W	O
s	O
,	O
no	O
modification	O
is	O
made	O
,	O
and	O
for	O
the	O
OTHER	O
type	O
p	O
(	O
t|r	O
)	O
=	O
p(t|r	O
)	O
.	O

As	O
such	O
,	O
the	O
total	O
effect	O
on	O
the	O
score	O
can	O
be	O
negative	O
.	O

To	O
avoid	O
applying	O
too	O
much	O
weight	O
to	O
the	O
homogeneous	O
classes	O
,	O
the	O
second	O
term	O
in	O
the	O
summation	O
subtracts	O
the	O
average	O
number	O
of	O
occurrences	O
in	O
the	O
given	O
position	O
for	O
all	O
words	O
in	O
W	O
s	O
.	O

For	O
our	O
two	O
homogenous	O
entity	O
types	O
(	O
PERSON	O
and	O
LOCATION	O
)	O
,	O
we	O
calculate	O
a	O
new	O
score	O
p	O
:	O
p	O
(	O
t|r	O
)	O
=	O
p(t|r	O
)	O
+	O
s∈S	O
c(r	O
s	O
,	O
t	O
,	O
s	O
)	O
t	O
∈T	O
c(r	O
s	O
,	O
t	O
,	O
s	O
)	O
−	O
w	O
∈Ws	O
c(w	O
,	O
t	O
,	O
s	O
)	O
t	O
∈T	O
c(w	O
,	O
t	O
,	O
s	O
)	O
|W	O
s	O
|	O
(	O
1	O
)	O
The	O
first	O
term	O
in	O
the	O
outermost	O
summation	O
in	O
Equation	O
1	O
is	O
the	O
proportion	O
of	O
occurrences	O
of	O
the	O
given	O
expression	O
in	O
position	O
s	O
which	O
correspond	O
to	O
type	O
t.	O

Let	O
c(w	O
,	O
t	O
,	O
s	O
)	O
be	O
the	O
the	O
number	O
of	O
times	O
a	O
word	O
w	O
∈	O
W	O
s	O
appears	O
in	O
the	O
corpus	O
at	O
position	O
s	O
in	O
phrases	O
which	O
were	O
Brown	O
clustered	O
into	O
the	O
entity	O
type	O
t	O
∈	O
T	O
,	O
and	O
p(t|r	O
)	O
be	O
the	O
original	O
probability	O
of	O
phrase	O
r	O
being	O
type	O
t	O
as	O
determined	O
by	O
the	O
logistic	O
regression	O
classifier	O
.	O

Our	O
final	O
model	O
addresses	O
this	O
failing	O
somewhat	O
by	O
using	O
more	O
information	O
from	O
our	O
Brown	B-MethodName
clustering	I-MethodName
:	O
from	O
each	O
of	O
the	O
initial	O
and	O
final	O
words	O
across	O
all	O
names	O
,	O
we	O
extract	O
a	O
set	O
of	O
words	O
W	O
s	O
that	O
appear	O
at	O
least	O
ten	O
times	O
in	O
position	O
s	O
∈	O
S	O
,	O
S	O
=	O
{	O
initial	O
,	O
f	O
inal	O
}	O
across	O
all	O
phrases	O
.	O

In	O
the	O
case	O
of	O
names	O
which	O
are	O
phrases	O
,	O
this	O
is	O
troubling	O
because	O
there	O
are	O
many	O
generalizations	O
to	O
be	O
made	O
;	O
for	O
instance	O
names	O
ending	O
with	O
City	O
are	O
locations	O
.	O

Relative	O
to	O
(	O
true	O
)	O
supervised	O
models	O
,	O
our	O
bootstrapped	O
model	O
suffers	O
from	O
being	O
able	O
to	O
use	O
only	O
context	O
,	O
and	O
not	O
the	O
identity	O
of	O
the	O
name	O
itself	O
.	O

Improved	O
phrase	B-TaskName
classification	I-TaskName
.	O

3	O
The	O
only	O
non	O
-	O
standard	O
setting	O
that	O
we	O
use	O
is	O
the	O
"	O
balanced	O
"	O
option	O
,	O
which	O
weights	O
classes	O
by	O
the	O
inverse	O
of	O
their	O
count	O
in	O
the	O
training	O
set	O
,	O
countering	O
the	O
preference	O
for	O
the	O
majority	O
class	O
;	O
we	O
do	O
this	O
because	O
our	O
bootstrapped	O
distribution	O
is	O
an	O
unreliable	O
reflection	O
of	O
the	O
true	O
distribution	O
,	O
and	O
also	O
because	O
it	O
makes	O
it	O
a	O
fairer	O
comparison	O
to	O
off	O
-	O
theshelf	O
models	O
with	O
no	O
access	O
to	O
this	O
distribution	O
.	O

For	O
classification	B-TaskName
,	O
we	O
use	O
logistic	O
regression	O
from	O
scikit	O
-	O
learn	O
(	O
Pedregosa	O
et	O
al	O
.	O
,	O
2011	O
)	O
trained	O
with	O
SGD	O
using	O
L2	B-HyperparameterValue
regularization	I-HyperparameterName
(	O
C	B-HyperparameterName
=	O
1	B-HyperparameterValue
)	O
.	O

Note	O
that	O
given	O
our	O
bootstrapping	O
setup	O
,	O
the	O
word	O
type	O
itself	O
can	O
not	O
be	O
used	O
directly	O
as	O
a	O
feature	O
.	O

To	O
be	O
included	O
as	O
features	O
,	O
the	O
n	O
-	O
grams	O
had	O
to	O
occur	O
with	O
≥	O
10	O
different	O
w	O
0	O
target	O
word	O
types	O
.	O

Across	O
multiple	O
tokens	O
of	O
the	O
same	O
type	O
,	O
we	O
count	O
the	O
same	O
context	O
only	O
once	O
,	O
creating	O
a	O
binary	O
feature	O
vector	O
which	O
was	O
normalized	O
by	O
dividing	O
by	O
the	O
count	O
of	O
all	O
non	O
-	O
zero	O
entries	O
once	O
all	O
contexts	O
were	O
collected	O
.	O

For	O
this	O
we	O
used	O
the	O
name	B-DatasetName
-	O
segmented	O
corpus	O
,	O
and	O
when	O
one	O
of	O
the	O
words	O
in	O
the	O
context	O
was	O
also	O
a	O
name	O
,	O
we	O
take	O
the	O
category	O
from	O
the	O
Brown	O
clustering	O
as	O
the	O
word	O
(	O
so	O
w	O
2	O
for	O
London	O
in	O
from	O
London	O
to	O
New	O
York	O
is	O
LOCATION	O
,	O
not	O
New	O
)	O
.	O

Our	O
feature	O
set	O
consists	O
of	O
filtered	O
word	O
features	O
in	O
a	O
2	O
-	O
word	O
window	O
(	O
w	O
−2	O
w	O
−1	O
w	O
0	O
w	O
+1	O
w	O
+2	O
)	O
around	O
the	O
token	O
occurrences	O
w	O
0	O
of	O
a	O
target	O
type	O
in	O
a	O
given	O
text	O
,	O
made	O
up	O
of	O
position	O
-	O
indexed	O
unigrams	O
(	O
w	O
−2	O
,	O
w	O
−1	O
,	O
w	O
+1	O
and	O
w	O
+2	O
)	O
and	O
bigrams	O
(	O
w	O
−2	O
w	O
−1	O
,	O
w	O
+1	O
w	O
+2	O
and	O
w	O
−1	O
w	O
+1	O
)	O
,	O
excluding	O
unigrams	O
when	O
a	O
subsuming	O
bigram	O
feature	O
matched	O
(	O
e.g.	O
if	O
we	O
match	O
trust	O
in	O
,	O
we	O
do	O
not	O
add	O
trust	O
and	O
in	O
)	O
.	O

The	O
noisy	O
training	O
set	O
thus	O
constructed	O
has	O
about	O
1	O
million	O
examples	O
.	O

Mary	O
is	O
a	O
common	O
name	O
,	O
and	O
may	O
be	O
a	O
major	O
character	O
in	O
one	O
text	O
,	O
but	O
a	O
minor	O
one	O
in	O
another	O
;	O
hence	O
,	O
we	O
build	O
a	O
classifier	O
that	O
deals	O
with	O
both	O
context	O
-	O
rich	O
and	O
context	O
-	O
poor	O
situations	O
.	O

Our	O
rationale	O
here	O
is	O
that	O
the	O
challenging	O
part	O
of	O
NER	B-TaskName
in	O
literature	O
is	O
names	O
that	O
appear	O
only	O
in	O
one	O
text	O
;	O
by	O
limiting	O
our	O
context	O
for	O
common	O
words	O
to	O
a	O
single	O
text	O
,	O
we	O
simulate	O
the	O
task	O
for	O
rarer	O
words	O
.	O

That	O
is	O
,	O
to	O
build	O
a	O
training	O
set	O
,	O
we	O
pass	O
through	O
the	O
corpus	O
and	O
each	O
time	O
we	O
come	O
across	O
a	O
common	O
name	O
in	O
a	O
particular	O
document	O
,	O
we	O
build	O
a	O
feature	O
vector	O
corresponding	O
to	O
all	O
the	O
contexts	O
in	O
that	O
document	O
,	O
with	O
the	O
label	O
taken	O
from	O
the	O
clustering	O
.	O

It	O
is	O
trained	O
on	O
the	O
(	O
text	O
-	O
level	O
)	O
"	O
instances	O
"	O
of	O
relatively	O
common	O
names	O
(	O
appearing	O
more	O
than	O
100	O
times	O
in	O
the	O
corpus	O
)	O
from	O
the	O
3	O
NE	O
label	O
types	O
derived	O
based	O
on	O
the	O
Brown	O
clustering	O
.	O

By	O
text	O
-	O
level	O
,	O
we	O
mean	O
that	O
it	O
assumes	O
one	O
-	O
sense	O
-	O
perdocument	O
,	O
classifying	O
a	O
name	O
for	O
an	O
entire	O
document	O
,	O
based	O
on	O
all	O
instances	O
of	O
the	O
name	O
in	O
the	O
document	O
(	O
Gale	O
et	O
al	O
.	O
,	O
1992	O
)	O
.	O

The	O
central	O
element	O
of	O
our	O
NER	B-TaskName
system	O
is	O
a	O
textlevel	O
classifier	O
of	O
names	O
based	O
on	O
context	O
.	O

Text	O
-	O
level	O
context	O
classifier	O
.	O

To	O
avoid	O
confusion	O
,	O
authors	O
will	O
generally	O
preserve	O
one	O
-	O
sense	O
-	O
per	O
-	O
document	O
,	O
but	O
this	O
is	O
not	O
true	O
at	O
the	O
corpus	O
level	O
.	O

Another	O
problem	O
with	O
Brown	B-MethodName
clustering	I-MethodName
is	O
that	O
ignores	O
possible	O
sense	O
distinctions	O
:	O
for	O
instance	O
,	O
Florence	O
is	O
both	O
a	O
city	O
and	O
a	O
person	O
name	O
.	O

Fiction	O
,	O
though	O
,	O
has	O
many	O
rare	O
names	O
and	O
locations	O
,	O
since	O
authors	O
will	O
often	O
invent	O
them	O
.	O

In	O
any	O
case	O
,	O
Brown	B-MethodName
clustering	I-MethodName
works	O
fairly	O
well	O
for	O
common	O
names	O
,	O
but	O
for	O
rarer	O
ones	O
,	O
the	O
clustering	O
is	O
haphazard	O
.	O

The	O
other	O
clusters	O
are	O
messier	O
,	O
but	O
still	O
in-	O
(	O
Vala	O
et	O
al	O
.	O
,	O
2015	O
)	O
.	O

The	O
most	O
common	O
words	O
in	O
the	O
first	O
two	O
clusters	O
are	O
mostly	O
what	O
we	O
would	O
expect	O
,	O
though	O
there	O
is	O
a	O
bit	O
of	O
noise	O
,	O
e.g.	O
Him	O
included	O
as	O
a	O
place	O
.	O

2	O
Note	O
the	O
presence	O
of	O
the	O
multiword	O
name	O
New	O
York	O
in	O
the	O
second	O
cluster	O
,	O
as	O
a	O
result	O
of	O
the	O
segmentation	O
.	O

The	O
top-5	O
clusters	O
by	O
token	O
count	O
of	O
names	O
are	O
given	O
in	O
Table	O
1	O
.	O

Alternatively	O
,	O
we	O
could	O
have	O
set	O
c	B-HyperparameterName
higher	O
and	O
manually	O
grouped	O
the	O
clusters	O
based	O
on	O
the	O
common	O
words	O
in	O
the	O
clusters	O
,	O
adding	O
a	O
thin	O
layer	O
of	O
supervision	O
to	O
the	O
process	O
;	O
with	O
a	O
low	O
c	B-HyperparameterName
,	O
however	O
,	O
this	O
was	O
unnecessary	O
since	O
the	O
composition	O
and	O
ranking	O
of	O
the	O
clusters	O
conformed	O
exactly	O
to	O
our	O
expectations	O
.	O

To	O
automatically	O
extract	O
a	O
seed	O
list	O
of	O
people	O
and	O
locations	O
,	O
we	O
ranked	O
the	O
clusters	O
by	O
the	O
total	O
(	O
token	O
)	O
count	O
of	O
names	O
(	O
as	O
identified	O
by	O
GutenTag	O
)	O
,	O
and	O
took	O
the	O
first	O
cluster	O
to	O
be	O
PER	O
-	O
SON	O
,	O
and	O
the	O
second	O
to	O
be	O
LOCATION	O
;	O
all	O
other	O
clusters	O
are	O
considered	O
OTHER	O
,	O
our	O
third	O
,	O
catchall	O
category	O
.	O

We	O
did	O
not	O
tune	O
this	O
number	O
,	O
except	O
to	O
observe	O
that	O
larger	O
numbers	O
(	O
e.g.	O
100	B-HyperparameterValue
or	O
200	B-HyperparameterValue
)	O
resulted	O
in	O
increasingly	O
fragmented	O
clusters	O
for	O
our	O
entities	O
of	O
interest	O
.	O

The	O
rationale	O
for	O
such	O
a	O
small	O
cluster	B-HyperparameterName
size	I-HyperparameterName
-	O
the	O
default	O
is	O
1000	B-HyperparameterValue
,	O
and	O
NER	B-TaskName
systems	O
which	O
use	O
Brown	B-MethodName
clusters	O
as	O
features	O
do	O
better	O
with	O
even	O
more	O
(	O
Derczynski	O
et	O
al	O
.	O
,	O
2015)-is	O
that	O
we	O
want	O
to	O
have	O
clusters	O
that	O
correspond	O
to	O
major	O
noun	O
categories	O
(	O
e.g.	O
PERSON	O
and	O
LOCATION	O
)	O
,	O
which	O
we	O
consider	O
the	O
next	O
most	O
fundamental	O
division	O
beyond	O
part	O
-	O
of	O
-	O
speech	O
;	O
50	O
was	O
selected	O
because	O
it	O
is	O
roughly	O
comparable	O
to	O
the	O
size	O
of	O
the	O
Penn	O
Treebank	O
tagset	O
(	O
Marcus	O
et	O
al	O
.	O
,	O
1993	O
)	O
.	O

We	O
used	O
default	O
settings	O
except	O
for	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
clusters	I-HyperparameterName
(	O
c	B-HyperparameterName
):	O
50	B-HyperparameterValue
.	O

Note	O
that	O
using	O
information	O
from	O
Brown	O
clusters	O
is	O
a	O
well	O
established	O
technique	O
in	O
NER	B-TaskName
,	O
but	O
more	O
typically	O
as	O
features	O
within	O
a	O
supervised	O
framework	O
(	O
Miller	O
et	O
al	O
.	O
,	O
2004;Liang	O
,	O
2005;Ritter	O
et	O
al	O
.	O
,	O
2011	O
)	O
;	O
we	O
are	O
unaware	O
of	O
any	O
work	O
using	O
them	O
directly	O
as	O
a	O
source	O
of	O
bootstrapped	O
training	O
examples	O
.	O

Briefly	O
,	O
Brown	O
clusters	O
are	O
formed	O
using	O
an	O
agglomerative	O
hierarchical	O
cluster	O
of	O
terms	O
based	O
on	O
their	O
immediate	O
context	O
,	O
placing	O
terms	O
into	O
categories	O
to	O
maximize	O
the	O
probability	O
of	O
consecutive	O
terms	O
over	O
the	O
entire	O
corpus	O
.	O

The	O
next	O
step	O
is	O
to	O
induce	O
Brown	O
clusters	O
(	O
Brown	O
et	O
al	O
.	O
,	O
1992	O
)	O
over	O
the	O
pre	O
-	O
segmented	O
corpus	O
(	O
including	O
potential	O
names	O
)	O
,	O
using	O
the	O
tool	O
of	O
Liang	O
(	O
2005	O
)	O
.	O

Brown	O
clustering	O
.	O

Though	O
not	O
our	O
primary	O
concern	O
,	O
we	O
return	O
to	O
evaluate	O
the	O
quality	O
of	O
the	O
initial	O
segmentation	O
in	O
Section	O
5	O
.	O

For	O
this	O
work	O
,	O
however	O
,	O
we	O
remove	O
those	O
restrictions	O
to	O
maximize	O
recall	O
.	O

To	O
improve	O
precision	B-MetricName
,	O
the	O
name	O
tagger	O
in	O
the	O
version	O
of	O
GutenTag	O
used	O
for	O
this	O
paper	O
(	O
0.1.3	O
)	O
has	O
lower	O
bounds	O
on	O
token	O
count	O
(	O
at	O
least	O
10	O
)	O
and	O
an	O
upper	O
bound	O
on	O
the	O
length	O
of	O
names	O
(	O
no	O
longer	O
than	O
3	O
words	O
)	O
.	O

It	O
largely	O
(	O
but	O
not	O
entirely	O
)	O
overcomes	O
the	O
problem	O
of	O
sentenceinitial	O
capitalization	O
in	O
English	O
by	O
generalizing	O
over	O
an	O
entire	O
text	O
;	O
as	O
long	O
as	O
a	O
capitalized	O
word	O
or	O
phrase	O
appears	O
in	O
a	O
non	O
-	O
sentence	O
initial	O
position	O
at	O
least	O
once	O
in	O
a	O
text	O
,	O
it	O
will	O
be	O
tagged	O
in	O
the	O
sentence	O
-	O
initial	O
position	O
as	O
well	O
.	O

GutenTag	B-MethodName
also	O
provides	O
an	O
initial	O
segmentation	O
of	O
tokens	O
into	O
potential	O
names	O
,	O
using	O
a	O
simple	O
rule	O
-	O
based	O
system	O
which	O
segments	O
contiguous	O
capitalized	O
words	O
,	O
potentially	O
with	O
common	O
intervening	O
function	O
words	O
like	O
of	O
as	O
well	O
as	O
leading	O
the	O
(	O
e.g.	O
the	O
King	O
of	O
Westeros	O
)	O
.	O

The	O
final	O
corpus	O
size	O
is	O
10844	O
texts	O
.	O

We	O
focus	O
here	O
only	O
on	O
fiction	O
texts	O
(	O
i.e.	O
novels	O
and	O
short	O
stories	O
)	O
;	O
other	O
kinds	O
of	O
literature	O
(	O
e.g.	O
plays	O
)	O
are	O
rare	O
in	O
the	O
corpus	O
and	O
have	O
very	O
different	O
properties	O
in	O
terms	O
of	O
the	O
distribution	O
of	O
names	O
.	O

We	O
access	O
the	O
texts	O
via	O
the	O
GutenTag	O
tool	O
(	O
Brooke	O
et	O
al	O
.	O
,	O
2015	O
)	O
,	O
which	O
allows	O
both	O
filtering	O
of	O
texts	O
by	O
genre	O
as	O
well	O
as	O
within	O
-	O
text	O
filtering	O
to	O
remove	O
Project	O
Gutenberg	O
copyright	O
information	O
,	O
front	O
and	O
back	O
matter	O
(	O
e.g.	O
table	O
of	O
contents	O
)	O
,	O
and	O
headers	O
.	O

The	O
corpus	O
we	O
use	O
for	O
building	O
and	O
testing	O
our	O
NER	B-TaskName
system	O
is	O
the	O
2010	O
image	O
of	O
the	O
(	O
US	O
)	O
Project	O
Gutenberg	O
corpus	O
,	O
1	O
a	O
reasonably	O
comprehensive	O
collection	O
of	O
out	O
-	O
of	O
-	O
copyright	O
English	O
literary	O
texts	O
,	O
to	O
our	O
knowledge	O
the	O
largest	O
that	O
is	O
publicly	O
available	O
in	O
a	O
machine	O
-	O
readable	O
,	O
full	O
-	O
text	O
format	O
.	O

Corpus	O
preparation	O
and	O
segmentation	O
.	O

Method	O
.	O

(	O
2015	O
)	O
identify	O
some	O
of	O
the	O
failures	O
of	O
off	O
-	O
the	O
-	O
shelf	O
NER	B-TaskName
with	O
regards	O
to	O
character	O
identification	O
,	O
and	O
attempt	O
to	O
fix	O
them	O
;	O
their	O
efforts	O
are	O
focused	O
,	O
however	O
,	O
on	O
characters	O
that	O
are	O
referred	O
to	O
by	O
description	O
rather	O
than	O
names	O
or	O
aliases	O
.	O

Vala	O
et	O
al	O
.	O

In	O
addition	O
to	O
NER	B-TaskName
,	O
character	B-TaskName
identifica	I-TaskName
-	I-TaskName
tion	I-TaskName
also	O
involves	O
clustering	O
multiple	O
aliases	O
of	O
the	O
same	O
character	O
,	O
and	O
discarding	O
person	O
names	O
that	O
do	O
n't	O
correspond	O
to	O
characters	O
.	O

We	O
also	O
note	O
that	O
although	O
supervised	O
NER	B-TaskName
is	O
the	O
norm	O
,	O
there	O
is	O
a	O
smaller	O
body	O
of	O
work	O
in	O
semi	O
-	O
supervised	O
and	O
unsupervised	O
approaches	O
to	O
NER	B-TaskName
and	O
semantic	B-TaskName
lexicon	I-TaskName
induction	I-TaskName
,	O
for	O
instance	O
pattern	B-MethodName
bootstrapping	O
(	O
Nadeau	O
et	O
al	O
.	O
,	O
2006;Thelen	O
and	O
Riloff	O
,	O
2002;McIntosh	O
et	O
al	O
.	O
,	O
2011	O
)	O
as	O
well	O
as	O
generative	O
approaches	O
(	O
Elsner	O
et	O
al	O
.	O
,	O
2009	O
)	O
.	O

In	O
the	O
context	O
of	O
literature	O
,	O
the	O
most	O
closely	O
related	O
task	O
is	O
character	B-TaskName
identification	I-TaskName
(	O
Vala	O
et	O
al	O
.	O
,	O
2015	O
)	O
,	O
which	O
is	O
itself	O
an	O
intermediate	O
task	O
for	O
character	O
speech	O
identification	O
(	O
He	O
et	O
al	O
.	O
,	O
2013	O
)	O
,	O
analysis	O
of	O
characterization	O
(	O
Bamman	O
et	O
al	O
.	O
,	O
2014	O
)	O
,	O
and	O
analysis	O
of	O
social	O
networks	O
(	O
Elson	O
et	O
al	O
.	O
,	O
2010;Agarwal	O
et	O
al	O
.	O
,	O
2013;Ardanuy	O
and	O
Sporleder	O
,	O
2015	O
)	O
.	O

Relevant	O
to	O
the	O
present	O
work	O
is	O
the	O
fact	O
that	O
,	O
despite	O
there	O
being	O
some	O
work	O
on	O
enforcing	O
tag	O
consistency	O
across	O
multiple	O
instances	O
of	O
the	O
same	O
token	O
(	O
Finkel	O
et	O
al	O
.	O
,	O
2005	O
)	O
and	O
the	O
use	O
of	O
non	O
-	O
local	O
features	O
(	O
Ratinov	O
and	O
Roth	O
,	O
2009	O
)	O
to	O
improve	O
supervised	O
sequential	O
models	O
,	O
the	O
consensus	O
seems	O
to	O
be	O
that	O
this	O
nonlocal	O
information	O
has	O
a	O
relatively	O
modest	O
effect	O
on	O
performance	O
in	O
standard	O
datasets	O
,	O
and	O
as	O
a	O
result	O
off	O
-	O
the	O
-	O
shelf	O
NER	B-TaskName
systems	O
in	O
practice	O
treat	O
each	O
sentence	O
as	O
a	O
separate	O
document	O
,	O
with	O
multiple	O
instances	O
of	O
the	O
same	O
token	O
in	O
different	O
sentences	O
viewed	O
as	O
entirely	O
independent	O
classification	O
problems	O
.	O

The	O
standard	O
approach	O
to	O
NER	B-TaskName
is	O
to	O
treat	O
it	O
as	O
a	O
supervised	O
sequential	O
classification	O
problem	O
,	O
typically	O
using	O
conditional	O
random	O
fields	O
or	O
similar	O
models	O
,	O
based	O
on	O
local	O
context	O
features	O
as	O
well	O
as	O
properties	O
of	O
the	O
token	O
itself	O
.	O

Related	O
work	O
.	O

Notably	O
,	O
we	O
do	O
this	O
without	O
any	O
hand	O
-	O
labelled	O
data	O
whatsoever	O
,	O
bootstrapping	O
a	O
text	O
-	O
level	O
context	O
classifier	O
from	O
a	O
low	O
-	O
dimensional	O
Brown	O
clustering	O
of	O
the	O
Project	B-DatasetName
Gutenberg	I-DatasetName
corpus	O
.	O

In	O
this	O
paper	O
,	O
we	O
show	O
that	O
it	O
is	O
possible	O
to	O
take	O
advantage	O
of	O
the	O
properties	O
of	O
fiction	O
texts	O
,	O
in	O
particular	O
the	O
repetition	O
of	O
names	O
,	O
to	O
build	O
a	O
high	O
-	O
performing	O
3	O
-	O
class	O
NER	B-TaskName
system	O
which	O
distinguishes	O
people	O
and	O
locations	O
from	O
other	O
capitalized	O
words	O
and	O
phrases	O
.	O

There	O
are	O
not	O
,	O
to	O
our	O
knowledge	O
,	O
any	O
NER	B-TaskName
systems	O
that	O
are	O
specifically	O
targeted	O
at	O
literature	O
,	O
and	O
most	O
related	O
work	O
has	O
used	O
Stanford	O
CoreNLP	O
as	O
an	O
off	O
-	O
the	O
-	O
shelf	O
solution	O
(	O
Bamman	O
et	O
al	O
.	O
,	O
2014;Vala	O
et	O
al	O
.	O
,	O
2015	O
)	O
.	O

Named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
NER	B-TaskName
)	O
of	O
person	O
names	O
is	O
generally	O
the	O
first	O
step	O
in	O
identifying	O
characters	O
;	O
locations	O
are	O
also	O
a	O
prevalent	O
NE	O
type	O
,	O
and	O
can	O
be	O
useful	O
when	O
tracking	O
different	O
plot	O
threads	O
(	O
Wallace	O
,	O
2012	O
)	O
,	O
or	O
trends	O
in	O
the	O
settings	O
of	O
fiction	O
.	O

Much	O
of	O
the	O
work	O
on	O
applying	O
NLP	O
to	O
the	O
analysis	O
of	O
literature	O
has	O
focused	O
on	O
literary	O
figures	O
/	O
characters	O
in	O
the	O
text	O
,	O
e.g.	O
in	O
the	O
context	O
of	O
social	O
network	O
analysis	O
(	O
Elson	O
et	O
al	O
.	O
,	O
2010;Agarwal	O
et	O
al	O
.	O
,	O
2013;Ardanuy	O
and	O
Sporleder	O
,	O
2015	O
)	O
or	O
analysis	O
of	O
characterization	O
(	O
Bamman	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

Introduction	O
.	O

Our	O
experiments	O
show	O
it	O
to	O
substantially	O
outperform	O
off	O
-	O
the	O
-	O
shelf	O
supervised	O
NER	B-TaskName
systems	O
.	O

Relative	O
to	O
more	O
traditional	O
approaches	O
,	O
LitNER	B-MethodName
has	O
two	O
important	O
properties	O
:	O
(	O
1	O
)	O
it	O
makes	O
no	O
use	O
of	O
handtagged	O
data	O
or	O
gazetteers	O
,	O
instead	O
it	O
bootstraps	O
a	O
model	O
from	O
term	O
clusters	O
;	O
and	O
(	O
2	O
)	O
it	O
leverages	O
multiple	O
instances	O
of	O
the	O
same	O
name	O
in	O
a	O
text	O
.	O

We	O
present	O
a	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
NER	B-TaskName
)	O
system	O
for	O
tagging	O
fiction	O
:	O
LitNER	B-MethodName
.	O

Bootstrapped	O
Text	O
-	O
level	O
Named	B-TaskName
Entity	I-TaskName
Recognition	I-TaskName
for	O
Literature	O
.	O

To	O
address	O
these	O
issues	O
,	O
in	O
our	O
proposed	O
approach	O
,	O
instead	O
of	O
representing	O
each	O
item	O
with	O
a	O
unique	O
identifier	O
,	O
we	O
choose	O
to	O
represent	O
each	O
item	O
with	O
its	O
title	O
tokens	O
,	O
which	O
are	O
further	O
mapped	O
to	O
a	O
continuous	O
vector	O
representation	O
.	O

their	O
helpful	O
comments	O
.	O

The	O
authors	O
would	O
like	O
to	O
thank	O
Sriganesh	O
Madhvanath	O
,	O
Hua	O
Yang	O
,	O
Xiaoyuan	O
Wu	O
,	O
Alan	O
Lu	O
,	O
Timothy	O
Heath	O
,	O
and	O
Kyunghyun	O
Cho	O
for	O
their	O
support	O
and	O
discussion	O
,	O
as	O
well	O
as	O
anonymous	O
reviewers	O
for	O
.	O

Acknowledgments	O
.	O

A	O
Appendix	O
.	O

We	O
demonstrate	O
the	O
superiority	O
of	O
our	O
model	O
over	O
a	O
traditional	O
neural	O
network	O
model	O
in	O
understanding	O
item	O
titles	O
and	O
learning	O
relationships	O
between	O
items	O
across	O
vast	O
inventory	O
.	O

Instead	O
of	O
directly	O
representing	O
an	O
item	O
with	O
a	O
unique	O
identifier	O
,	O
we	O
use	O
the	O
item	O
's	O
title	O
tokens	O
as	O
content	O
,	O
along	O
with	O
token	O
embeddings	O
,	O
to	O
address	O
the	O
cold	O
start	O
problem	O
.	O

In	O
this	O
paper	O
,	O
we	O
adapt	O
the	O
BERT	B-MethodName
model	O
for	O
the	O
task	O
of	O
item	O
-	O
based	O
recommendations	O
.	O

Summary	O
.	O

From	O
these	O
two	O
examples	O
,	O
we	O
see	O
that	O
the	O
proposed	O
model	O
appears	O
to	O
automatically	O
find	O
relevant	O
selection	O
criteria	O
without	O
manual	O
specification	O
,	O
as	O
well	O
as	O
make	O
decisions	O
between	O
focusing	O
on	O
a	O
specific	O
category	O
and	O
catering	O
to	O
a	O
wide	O
range	O
of	O
inventory	O
by	O
learning	O
from	O
the	O
data	O
.	O

For	O
the	O
second	O
seed	O
item	O
'	O
Microsoft	O
Surface	O
Pro	O
4	O
12.3	O
"	O
Multi	O
-	O
Touch	O
Tablet	O
(	O
Intel	O
i5	O
,	O
128	O
GB	O
)	O
+	O
Keyboard	O
'	O
,	O
the	O
recommended	O
items	O
span	O
a	O
wide	O
range	O
of	O
categories	O
including	O
tablets	O
,	O
digital	O
memberships	O
,	O
electronic	O
accessories	O
,	O
and	O
computer	O
hardware	O
.	O

For	O
the	O
first	O
seed	O
item	O
'	O
Marvel	O
Spiderman	O
T	O
-	O
shirt	O
Small	O
Black	O
Tee	O
Superhero	O
Comic	O
Book	O
Character	O
'	O
,	O
most	O
of	O
the	O
recommended	O
items	O
are	O
T	O
-	O
shirts	O
,	O
paired	O
with	O
clothing	O
accessories	O
and	O
tableware	O
decoration	O
,	O
all	O
having	O
Marvel	O
as	O
the	O
theme	O
.	O

2	O
.	O

In	O
order	O
to	O
visually	O
examine	O
the	O
quality	O
of	O
recommendations	O
,	O
we	O
present	O
the	O
recommended	O
items	O
for	O
two	O
different	O
seed	O
items	O
in	O
Table	O
.	O

It	O
is	O
also	O
clear	O
that	O
adapting	O
the	O
token	O
distribution	O
for	O
the	O
e	O
-	O
commerce	O
context	O
with	O
masked	O
language	O
model	O
within	O
BERT	B-MethodName
is	O
essential	O
for	O
achieving	O
the	O
best	O
performance	O
.	O

From	O
the	O
experiment	O
,	O
the	O
superiority	O
of	O
proposed	O
BERT	B-MethodName
model	O
for	O
item	B-TaskName
-	I-TaskName
based	I-TaskName
collaborative	I-TaskName
filtering	I-TaskName
is	O
clear	O
.	O

When	O
fine	O
tuning	O
for	O
the	O
masked	O
language	O
model	O
task	O
is	O
added	O
,	O
we	O
see	O
the	O
metrics	O
improved	O
further	O
by	O
another	O
111.0	B-MetricValue
%	I-MetricValue
,	O
38.6	B-MetricValue
%	I-MetricValue
,	O
38.3	B-MetricValue
%	I-MetricValue
,	O
and	O
64.0	B-MetricValue
%	I-MetricValue
.	O

When	O
only	O
fine	O
-	O
tuned	O
for	O
the	O
Next	B-TaskName
Purchase	I-TaskName
Prediction	I-TaskName
task	O
,	O
our	O
model	O
exceeds	O
the	O
baseline	O
by	O
310.9	B-MetricValue
%	I-MetricValue
,	O
96.6	B-MetricValue
%	I-MetricValue
,	O
93.9	B-MetricValue
%	I-MetricValue
,	O
and	O
150.3	B-MetricValue
%	I-MetricValue
in	O
precision@1	B-MetricName
,	O
precision@10	B-MetricName
,	O
recall@10	B-MetricName
,	O
and	O
NDCG@10	B-MetricName
respectively	O
.	O

We	O
observe	O
that	O
the	O
proposed	O
BERT	B-MethodName
model	O
greatly	O
outperforms	O
the	O
LSTM	B-MethodName
-	I-MethodName
based	I-MethodName
model	O
.	O

We	O
believe	O
its	O
a	O
practical	O
experiment	O
setting	O
,	O
as	O
for	O
a	O
large	O
-	O
scale	O
e	O
-	O
commerce	O
platform	O
,	O
a	O
massive	O
amount	O
of	O
new	O
items	O
would	O
be	O
created	O
every	O
moment	O
,	O
and	O
ignoring	O
those	O
items	O
from	O
the	O
recommender	O
system	O
would	O
be	O
costly	O
and	O
inefficient	O
.	O

Following	O
the	O
same	O
reason	O
,	O
other	O
approaches	O
relying	O
on	O
unique	O
item	O
identifier	O
(	O
e.g.	O
itemId	O
)	O
could	O
n't	O
be	O
considered	O
either	O
in	O
our	O
experiment	O
.	O

We	O
do	O
not	O
consider	O
the	O
traditional	O
item	B-MethodName
-	I-MethodName
toitem	I-MethodName
collaborative	I-MethodName
filtering	I-MethodName
model	O
(	O
Linden	O
et	O
al	O
.	O
,	O
2003	O
)	O
here	O
since	O
the	O
evaluation	O
is	O
conducted	O
assuming	O
a	O
complete	O
cold	O
-	O
start	O
setting	O
,	O
with	O
all	O
seed	O
items	O
unobserved	O
in	O
the	O
training	O
set	O
,	O
resulting	O
in	O
complete	O
failure	O
of	O
such	O
a	O
model	O
.	O

1	O
.	O

The	O
results	O
of	O
our	O
evaluation	O
are	O
presented	O
in	O
Table	O
.	O

Results	O
.	O

For	O
each	O
positive	O
sample	O
containing	O
a	O
seed	O
item	O
and	O
a	O
ground	O
-	O
truth	O
co	O
-	O
purchased	O
item	O
,	O
we	O
paired	O
the	O
seed	O
item	O
with	O
999	O
random	O
negative	O
samples	O
,	O
and	O
for	O
testing	O
,	O
we	O
use	O
the	O
trained	O
model	O
to	O
rank	O
the	O
total	O
of	O
1000	O
items	O
given	O
each	O
seed	O
item	O
.	O

For	O
testing	O
,	O
in	O
order	O
to	O
mimic	O
the	O
cold	O
-	O
start	O
scenario	O
in	O
the	O
production	O
system	O
wherein	O
traditional	O
item	B-TaskName
-	I-MethodName
item	I-TaskName
collaborative	I-TaskName
filtering	I-TaskName
fails	O
completely	O
,	O
we	O
sampled	O
10,000	O
pairs	O
of	O
co	O
-	O
purchased	O
items	O
with	O
the	O
seed	O
item	O
not	O
present	O
in	O
the	O
training	O
set	O
.	O

Another	O
250,799	O
pairs	O
of	O
items	O
are	O
sampled	O
in	O
the	O
same	O
manner	O
for	O
use	O
as	O
a	O
validation	O
set	O
,	O
for	O
conducting	O
early	O
stopping	O
for	O
training	O
.	O

The	O
rationale	O
would	O
be	O
further	O
explained	O
with	O
the	O
presence	O
of	O
the	O
statistics	O
of	O
our	O
dataset	O
.	O

The	O
sparsity	O
of	O
data	O
forces	O
the	O
model	O
to	O
focus	O
on	O
generalization	O
rather	O
than	O
memorization	O
.	O

99.9999	O
%	O
of	O
entries	O
of	O
the	O
item	O
-	O
item	O
interaction	O
matrix	O
is	O
empty	O
.	O

We	O
collected	O
8,001,577	O
pairs	O
of	O
items	O
,	O
of	O
which	O
33	O
%	O
are	O
co	O
-	O
purchased	O
(	O
BIN	O
event	O
)	O
within	O
the	O
same	O
user	O
session	O
,	O
while	O
the	O
rest	O
are	O
randomly	O
sampled	O
as	O
negative	O
samples	O
.	O

We	O
train	O
our	O
models	O
on	O
an	O
e	O
-	O
commerce	O
website	O
data	O
.	O

Dataset	O
.	O

The	O
baseline	O
model	O
is	O
trained	O
using	O
the	O
same	O
cross	O
-	O
entropy	O
loss	O
shown	O
in	O
Eq	O
.	O
1	O
.	O

The	O
MLP	B-MethodName
layer	O
with	O
logistic	O
function	O
produces	O
the	O
estimated	O
probability	O
score	O
.	O

After	O
going	O
through	O
the	O
embedding	O
layer	O
,	O
the	O
bidirectional	O
LSTM	B-MethodName
reads	O
through	O
the	O
entire	O
sequence	O
and	O
generates	O
a	O
representation	O
at	O
the	O
last	O
timestep	O
.	O

For	O
every	O
pair	O
of	O
items	O
,	O
the	O
two	O
titles	O
are	O
concatenated	O
into	O
a	O
sequence	O
.	O

As	O
the	O
evaluation	O
is	O
conducted	O
on	O
the	O
dataset	O
having	O
a	O
complete	O
cold	O
-	O
start	O
setting	O
,	O
for	O
the	O
sake	O
of	O
comparison	O
,	O
we	O
build	O
a	O
baseline	O
model	O
consisting	O
of	O
a	O
title	O
token	O
embedding	O
layer	O
with	O
768	O
dimensions	O
,	O
a	O
bidirectional	O
LSTM	B-MethodName
layer	O
with	O
64	B-HyperparameterValue
hidden	B-HyperparameterName
units	I-HyperparameterName
,	O
and	O
a	O
2	B-HyperparameterValue
-	O
layer	B-HyperparameterName
MLP	B-MethodName
with	O
128	B-HyperparameterValue
and	O
32	B-HyperparameterValue
hidden	B-HyperparameterName
units	I-HyperparameterName
respectively	O
.	O

Bi	B-MethodName
-	I-MethodName
LSTM	I-MethodName
Model	I-MethodName
(	O
baseline	O
)	O
.	O

(	O
2	O
)	O
The	O
whole	O
model	O
is	O
optimized	O
against	O
the	O
joint	O
loss	O
L	O
lm	O
+	O
L	O
np	O
.	O

Given	O
the	O
set	O
of	O
chosen	O
tokens	O
M	O
,	O
the	O
corresponding	O
loss	O
for	O
masked	O
language	O
model	O
is	O
L	O
lm	O
=	O
−	O
m	O
i	O
∈M	O
log	O
p(m	O
i	O
)	O
.	O

(	O
2018	O
)	O
wherein	O
15	O
%	O
of	O
the	O
tokens	O
in	O
the	O
title	O
are	O
chosen	O
to	O
be	O
replaced	O
by	O
[	O
MASK	O
]	O
,	O
random	O
token	O
,	O
or	O
left	O
unchanged	O
,	O
with	O
a	O
probability	O
of	O
80	O
%	O
,	O
10	O
%	O
and	O
10	O
%	O
respectively	O
.	O

In	O
the	O
masked	B-TaskName
language	I-TaskName
model	I-TaskName
task	O
,	O
we	O
follow	O
the	O
training	O
schema	O
outlined	O
in	O
Devlin	O
et	O
al	O
.	O

As	O
the	O
distribution	O
of	O
item	O
title	O
tokens	O
is	O
different	O
from	O
the	O
natural	O
language	O
corpus	O
used	O
to	O
train	O
BERT	B-MethodName
base	I-MethodName
,	O
we	O
further	O
fine	O
-	O
tune	O
the	O
model	O
for	O
the	O
masked	B-TaskName
language	I-TaskName
model	I-TaskName
(	O
MLM	I-TaskName
)	O
task	O
as	O
well	O
.	O

Masked	O
Language	O
Model	O
.	O

(	O
1	O
)	O
.	O

Given	O
the	O
positive	O
item	O
set	O
I	O
p	O
,	O
and	O
the	O
negative	O
item	O
set	O
I	O
n	O
,	O
the	O
cross	O
-	O
entropy	O
loss	O
for	O
next	O
purchase	O
prediction	O
may	O
be	O
computed	O
as	O
L	O
np	O
=	O
−	O
i	O
j	O
∈Ip	O
log	O
p(i	O
j	O
)	O
−	O
i	O
j	O
∈In	O
log(1	O
−	O
p(i	O
j	O
)	O
)	O
.	O

For	O
a	O
seed	O
item	O
,	O
its	O
positive	O
items	O
are	O
generated	O
by	O
collecting	O
items	O
purchased	O
within	O
the	O
same	O
user	O
session	O
,	O
and	O
the	O
negative	O
ones	O
are	O
randomly	O
sampled	O
.	O

Both	O
item	O
titles	O
are	O
concatenated	O
and	O
truncated	O
to	O
have	O
at	O
most	O
128	O
tokens	O
,	O
including	O
one	O
[	O
CLS	O
]	O
and	O
two	O
[	O
SEP	O
]	O
tokens	O
.	O

We	O
feed	O
seed	O
item	O
as	O
sentence	O
A	O
,	O
and	O
target	O
item	O
as	O
sentence	O
B.	O

We	O
start	O
with	O
a	O
pre	O
-	O
trained	O
BERT	B-MethodName
base	O
model	O
,	O
and	O
fine	O
-	O
tune	O
it	O
for	O
our	O
next	O
purchase	O
prediction	O
task	O
.	O

The	O
goal	O
of	O
this	O
task	O
is	O
to	O
predict	O
the	O
next	O
item	O
a	O
user	O
is	O
going	O
to	O
purchase	O
given	O
the	O
seed	O
item	O
he	O
/	O
she	O
has	O
just	O
bought	O
.	O

Next	O
Purchase	O
Prediction	O
.	O

The	O
encoder	O
of	O
BERT	B-MethodName
base	O
contains	O
12	B-HyperparameterValue
Transformer	B-HyperparameterName
layers	I-HyperparameterName
,	O
with	O
768	B-HyperparameterValue
hidden	B-HyperparameterName
units	I-HyperparameterName
,	O
and	O
12	B-HyperparameterValue
self	B-HyperparameterName
-	I-HyperparameterName
attention	I-HyperparameterName
heads	I-HyperparameterName
.	O

Our	O
model	O
is	O
based	O
on	O
the	O
architecture	O
of	O
BERT	B-MethodName
base	I-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

Model	O
.	O

We	O
adapt	O
the	O
masked	O
language	O
modelling	O
and	O
next	O
sentence	O
prediction	O
tasks	O
from	O
the	O
natural	O
language	O
context	O
to	O
the	O
e	O
-	O
commerce	O
context	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
approach	O
for	O
item	B-TaskName
-	I-TaskName
based	I-TaskName
collaborative	I-TaskName
filtering	I-TaskName
,	O
by	O
leveraging	O
the	O
BERT	B-MethodName
model	O
(	O
Devlin	O
et	O
al	O
.	O
,	O
2018	O
)	O
to	O
understand	O
item	O
titles	O
and	O
model	O
relevance	O
between	O
different	O
items	O
.	O

Due	O
to	O
the	O
recent	O
success	O
of	O
neural	O
networks	O
in	O
multiple	O
AI	O
domains	O
(	O
LeCun	O
et	O
al	O
.	O
,	O
2015	O
)	O
and	O
their	O
superior	O
modeling	O
capacity	O
,	O
a	O
number	O
of	O
research	O
efforts	O
have	O
explored	O
new	O
recommendation	O
algorithms	O
based	O
on	O
Deep	O
Learning	O
(	O
see	O
,	O
e.g.	O
,	O
Barkan	O
and	O
Koenigstein	O
,	O
2016;He	O
et	O
al	O
.	O
,	O
2017;Hidasi	O
et	O
al	O
.	O
,	O
2015;Covington	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

Although	O
less	O
impacted	O
by	O
data	O
sparsity	O
,	O
due	O
to	O
their	O
reliance	O
on	O
content	O
rather	O
than	O
behavior	O
,	O
they	O
can	O
struggle	O
to	O
provide	O
novel	O
recommendations	O
which	O
may	O
activate	O
the	O
user	O
's	O
latent	O
interests	O
,	O
a	O
highly	O
desirable	O
quality	O
for	O
recommender	O
systems	O
(	O
Castells	O
et	O
al	O
.	O
,	O
2011	O
)	O
.	O

Content	O
-	O
based	O
recommendation	O
algorithms	O
calculate	O
similarities	O
in	O
content	O
between	O
candidate	O
items	O
and	O
seed	O
items	O
that	O
the	O
user	O
has	O
provided	O
feedback	O
for	O
(	O
which	O
may	O
be	O
implicit	O
e.g.	O
clicking	O
,	O
or	O
explicit	O
e.g.	O
rating	O
)	O
,	O
and	O
then	O
select	O
the	O
most	O
similar	O
items	O
to	O
recommend	O
.	O

However	O
,	O
for	O
highly	O
active	O
e	O
-	O
commerce	O
platforms	O
with	O
large	O
and	O
constantly	O
changing	O
inventory	O
,	O
both	O
approaches	O
are	O
severely	O
impacted	O
by	O
data	O
sparsity	O
in	O
the	O
user	O
-	O
item	O
interaction	O
matrix	O
.	O

For	O
item	B-TaskName
-	I-TaskName
based	I-TaskName
collaborative	I-TaskName
filtering	I-TaskName
(	O
see	O
,	O
e.g.	O
,	O
Linden	O
et	O
al	O
.	O
,	O
2003	O
)	O
,	O
given	O
a	O
seed	O
item	O
,	O
recommended	O
items	O
are	O
chosen	O
to	O
have	O
most	O
similar	O
user	O
feedback	O
.	O

Introduction	O
.	O

In	O
this	O
work	O
,	O
we	O
present	O
a	O
novel	O
approach	O
for	O
item	B-TaskName
-	I-TaskName
based	I-TaskName
collaborative	I-TaskName
filtering	I-TaskName
,	O
by	O
leveraging	O
BERT	B-MethodName
to	O
understand	O
items	O
,	O
and	O
score	O
relevancy	O
between	O
different	O
items	O
.	O

Item	B-TaskName
-	I-TaskName
based	I-TaskName
Collaborative	I-TaskName
Filtering	I-TaskName
with	O
BERT	B-MethodName
.	O

For	O
example	O
,	O
traditional	O
userbased	B-MethodName
collaborative	I-MethodName
filtering	I-TaskName
recommendation	I-MethodName
algorithms	O
(	O
see	O
,	O
e.g.	O
,	O
Schafer	O
et	O
al	O
.	O
,	O
2007	O
)	O
find	O
the	O
most	O
similar	O
users	O
based	O
on	O
the	O
seed	O
user	O
's	O
rated	O
items	O
,	O
and	O
then	O
recommend	O
new	O
items	O
which	O
other	O
users	O
rated	O
highly	O
.	O

However	O
,	O
these	O
have	O
their	O
own	O
limitations	O
when	O
applied	O
directly	O
to	O
real	O
-	O
world	O
ecommerce	O
platforms	O
.	O

Traditional	O
recommendation	O
algorithms	O
can	O
be	O
divided	O
into	O
two	O
types	O
:	O
collaborative	B-MethodName
filtering	I-MethodName
-	I-MethodName
based	I-MethodName
(	O
Schafer	O
et	O
al	O
.	O
,	O
2007;Linden	O
et	O
al	O
.	O
,	O
2003	O
)	O
and	O
content	B-MethodName
-	I-MethodName
based	I-MethodName
(	O
Lops	O
et	O
al	O
.	O
,	O
2011;Pazzani	O
and	O
Billsus	O
,	O
2007	O
)	O
.	O

Recommender	O
systems	O
are	O
an	O
integral	O
part	O
of	O
ecommerce	O
platforms	O
,	O
helping	O
users	O
pick	O
out	O
items	O
of	O
interest	O
from	O
large	O
inventories	O
at	O
scale	O
.	O

We	O
conducted	O
experiments	O
on	O
a	O
large	O
-	O
scale	O
realworld	O
dataset	O
with	O
full	O
cold	O
-	O
start	O
scenario	O
,	O
and	O
the	O
proposed	O
approach	O
significantly	O
outperforms	O
the	O
popular	O
Bi	B-MethodName
-	I-MethodName
LSTM	I-MethodName
model	O
.	O

Our	O
proposed	O
method	O
could	O
address	O
problems	O
that	O
plague	O
traditional	O
recommender	O
systems	O
such	O
as	O
cold	O
start	O
,	O
and	O
"	O
more	O
of	O
the	O
same	O
"	O
recommended	O
content	O
.	O

In	O
e	O
-	O
commerce	O
,	O
recommender	O
systems	O
have	O
become	O
an	O
indispensable	O
part	O
of	O
helping	O
users	O
explore	O
the	O
available	O
inventory	O
.	O

Next	O
Purchase	O
Prediction	O
can	O
directly	O
be	O
used	O
as	O
the	O
relevance	O
scoring	O
function	O
for	O
our	O
item	B-TaskName
collaborative	I-TaskName
filtering	I-TaskName
task	O
.	O

Since	O
the	O
distribution	O
of	O
item	O
title	O
tokens	O
differs	O
drastically	O
from	O
words	O
in	O
natural	O
language	O
which	O
the	O
original	O
BERT	B-MethodName
model	O
is	O
trained	O
on	O
,	O
retraining	O
the	O
masked	O
language	O
model	O
allows	O
better	O
understanding	O
of	O
item	O
information	O
for	O
our	O
use	O
-	O
case	O
.	O

We	O
re	O
-	O
purpose	O
these	O
tasks	O
for	O
the	O
e	O
-	O
commerce	O
context	O
into	O
Masked	O
Language	O
Model	O
on	O
Item	O
Titles	O
,	O
and	O
Next	O
Purchase	O
Prediction	O
.	O

The	O
training	O
of	O
BERT	B-MethodName
model	O
can	O
be	O
divided	O
into	O
two	O
parts	O
:	O
Masked	O
Language	O
Model	O
and	O
Next	O
Sentence	O
Prediction	O
.	O

Rather	O
than	O
the	O
traditional	O
RNN	B-MethodName
/	O
CNN	B-MethodName
structure	O
,	O
BERT	B-MethodName
adopts	O
transformer	O
encoder	O
as	O
a	O
language	O
model	O
,	O
and	O
use	O
attention	O
mechanism	O
to	O
calculate	O
the	O
relationship	O
between	O
input	O
and	O
output	O
.	O

Our	O
model	O
is	O
based	O
on	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

The	O
goal	O
of	O
item	B-TaskName
-	I-TaskName
based	I-TaskName
collaborative	I-TaskName
filtering	I-TaskName
is	O
to	O
score	O
the	O
relevance	O
between	O
two	O
items	O
,	O
and	O
for	O
a	O
seed	O
item	O
,	O
the	O
top	O
scored	O
items	O
would	O
be	O
recommended	O
as	O
a	O
result	O
.	O

For	O
a	O
newly	O
listed	O
item	O
in	O
the	O
cold	O
-	O
start	O
setting	O
,	O
the	O
model	O
can	O
utilize	O
the	O
similarity	O
of	O
the	O
item	O
title	O
to	O
ones	O
observed	O
before	O
to	O
find	O
relevant	O
recommended	O
items	O
.	O

By	O
doing	O
so	O
,	O
essentially	O
two	O
items	O
with	O
the	O
same	O
title	O
would	O
be	O
treated	O
as	O
the	O
same	O
,	O
and	O
can	O
aggregate	O
user	O
behaviors	O
accordingly	O
.	O

In	O
addition	O
to	O
the	O
challenge	O
of	O
long	O
-	O
tail	O
recommendations	O
,	O
this	O
also	O
requires	O
the	O
recommender	O
system	O
to	O
be	O
continuously	O
retrained	O
and	O
redeployed	O
in	O
order	O
to	O
accommodate	O
newly	O
listed	O
items	O
.	O

As	O
mentioned	O
earlier	O
,	O
for	O
a	O
dynamic	O
e	O
-	O
commerce	O
platform	O
,	O
items	O
enter	O
and	O
leave	O
the	O
market	O
continuously	O
,	O
resulting	O
in	O
an	O
extremely	O
sparse	O
user	O
-	O
item	O
interaction	O
matrix	O
.	O

Item	B-TaskName
-	I-TaskName
based	I-TaskName
Collaborative	I-TaskName
Filtering	I-TaskName
with	O
BERT	B-MethodName
.	O

•	O
We	O
conduct	O
experiments	O
on	O
a	O
large	O
-	O
scale	O
e	B-DatasetName
-	I-DatasetName
commerce	I-DatasetName
dataset	O
,	O
demonstrating	O
the	O
effectiveness	O
of	O
our	O
approach	O
and	O
producing	O
recommendation	O
results	O
with	O
higher	O
quality	O
.	O

•	O
By	O
training	O
model	O
with	O
user	O
behavior	O
data	O
,	O
our	O
model	O
learns	O
user	O
's	O
latent	O
interests	O
more	O
than	O
item	O
similarities	O
,	O
while	O
traditional	O
recommendation	O
algorithms	O
and	O
some	O
pair	O
-	O
wise	O
deep	O
learning	O
algorithms	O
only	O
provide	O
similar	O
items	O
which	O
users	O
may	O
have	O
bought	O
.	O

The	O
contributions	O
of	O
this	O
work	O
are	O
summarized	O
as	O
follows	O
:	O
•	O
Instead	O
of	O
relying	O
on	O
unique	O
item	O
identifier	O
to	O
aggregate	O
history	O
information	O
,	O
we	O
only	O
use	O
item	O
's	O
title	O
as	O
content	O
,	O
along	O
with	O
token	O
embeddings	O
to	O
solve	O
the	O
cold	O
start	O
problem	O
,	O
which	O
is	O
the	O
main	O
shortcoming	O
of	O
traditional	O
recommendation	O
algorithms	O
.	O

To	O
compare	O
our	O
results	O
with	O
AutoPrompt	B-MethodName
on	O
the	O
SICK	B-DatasetName
-	I-DatasetName
E	I-DatasetName
task	O
,	O
we	O
report	O
accuracy	B-MetricName
score	O
of	O
SP	B-MethodName
for	O
the	O
standard	O
test	O
set	O
(	O
with	O
neutral	O
majority	O
)	O
and	O
its	O
balanced	O
variant	O
.	O

The	O
most	O
probable	O
predicted	O
words	O
for	O
the	O
top	O
three	O
examples	O
indicate	O
that	O
the	O
PLM	O
has	O
spotted	O
the	O
correct	O
senses	O
in	O
both	O
contexts	O
.	O

The	O
table	O
presents	O
our	O
generated	O
prompts	O
,	O
top-5	O
most	O
probable	O
words	O
predicted	O
by	O
RoBERTa	B-MethodName
-	O
Large	O
for	O
each	O
prompt	O
and	O
the	O
final	O
prediction	O
of	O
SP	B-MethodName
.	O

The	O
examples	O
are	O
those	O
from	O
WiC	B-DatasetName
dev	O
set	O
which	O
had	O
negative	O
labels	O
.	O

We	O
include	O
some	O
examples	O
of	O
how	O
SP	B-MethodName
works	O
on	O
WiC	B-DatasetName
in	O
Table	O
4	O
for	O
qualitative	O
analysis	O
.	O

Since	O
our	O
cloze	O
-	O
style	O
prompt	O
template	O
is	O
not	O
applicable	O
to	O
GPT2	B-MethodName
,	O
we	O
use	O
a	O
different	O
template	O
for	O
it	O
:	O
sentence	O
+	O
targetword	O
+	O
"	O
means	O
--	O
"	O
.	O

Table	O
3	O
shows	O
full	O
test	O
set	O
results	O
of	O
SP	B-MethodName
for	O
different	O
PLMs	O
and	O
similarity	O
measures	O
to	O
compare	O
the	O
performance	O
of	O
SP	B-MethodName
in	O
different	O
scenarios	O
.	O

A	O
Experiments	O
with	O
other	O
PLMs	O
.	O
This	O
appendix	O
contains	O
more	O
details	O
on	O
WiC	O
experiments	O
.	O

We	O
hope	O
that	O
our	O
positive	O
results	O
inspire	O
other	O
prompting	O
strategies	O
to	O
better	O
exploit	O
the	O
encoded	O
knowledge	O
in	O
PLMs	O
.	O
As	O
future	O
work	O
,	O
one	O
interesting	O
direction	O
could	O
be	O
to	O
perform	O
further	O
analysis	O
on	O
the	O
behaviour	O
of	O
Spearman	B-MetricName
's	I-MetricName
correlation	I-MetricName
compared	O
to	O
cosine	O
similarity	O
anywhere	O
it	O
is	O
applicable	O
as	O
a	O
similarity	O
measure	O
.	O

We	O
also	O
showed	O
that	O
Spearman	B-MetricName
's	I-MetricName
ranking	I-MetricName
correlation	I-MetricName
is	O
a	O
more	O
robust	O
choice	O
of	O
similarity	O
measure	O
compared	O
to	O
cosine	O
similarity	O
in	O
this	O
setting	O
.	O

In	O
this	O
work	O
we	O
showed	O
that	O
similarity	O
based	O
approach	O
to	O
promptbased	B-MethodName
learning	I-MethodName
is	O
capable	O
of	O
achieving	O
comparable	O
results	O
to	O
purely	O
fine	O
-	O
tuning	O
based	O
methods	O
on	O
Word	O
-	O
in	O
-	O
Context	O
task	O
,	O
in	O
which	O
previous	O
few	O
-	O
shot	O
attempts	O
have	O
failed	O
.	O

We	O
proposed	O
an	O
adaptation	O
of	O
prompt	B-MethodName
-	I-MethodName
based	I-MethodName
learning	I-MethodName
which	O
addresses	O
the	O
common	O
failure	O
of	O
existing	O
techniques	O
on	O
the	O
WiC	B-DatasetName
dataset	O
.	O

This	O
further	O
supports	O
the	O
sensitivity	O
of	O
cosine	O
similarity	O
for	O
WiC	O
to	O
the	O
noisy	O
variations	O
along	O
the	O
most	O
dominant	O
dimension	O
compared	O
to	O
the	O
other	O
two	O
tasks	O
.	O

The	O
ratio	B-MetricName
of	O
variance	I-MetricName
is	O
6.5	B-MetricValue
times	O
for	O
WiC	O
compared	O
to	O
SST	B-MethodName
and	O
27.3	B-MetricValue
times	O
compared	O
to	O
SICK	B-MethodName
.	O

It	O
is	O
known	O
that	O
the	O
most	O
dominant	O
dimensions	O
in	O
PLMs	O
often	O
encode	O
irrelevant	O
information	O
,	O
such	O
as	O
word	O
frequency	O
(	O
Gao	O
et	O
al	O
.	O
,	O
2019	O
)	O
,	O
therefore	O
hampering	O
performance	O
for	O
sensitive	O
metrics	O
such	O
as	O
cosine	O
similarity	O
.	O

†	O
*	O
Work	O
done	O
as	O
a	O
Master	O
's	O
student	O
at	O
IUST	O
.	O

We	O
also	O
show	O
that	O
this	O
approach	O
can	O
be	O
effectively	O
extended	O
to	O
other	O
downstream	O
tasks	O
for	O
which	O
a	O
single	O
prompt	O
is	O
sufficient	O
.	O

Trying	O
to	O
fill	O
this	O
gap	O
,	O
we	O
propose	O
a	O
new	O
prompting	O
technique	O
,	O
based	O
on	O
similarity	O
metrics	O
,	O
which	O
boosts	O
few	O
-	O
shot	O
performance	O
to	O
the	O
level	O
of	O
fully	O
supervised	O
methods	O
.	O

This	O
results	O
in	O
a	O
higher	O
spread	O
on	O
the	O
most	O
dominant	O
dimension	O
in	O
the	O
case	O
of	O
WiC.	O

However	O
,	O
in	O
SST	B-MethodName
and	O
SICK	B-DatasetName
the	O
MASK	O
template	O
embedding	O
is	O
more	O
restricted	O
,	O
often	O
representing	O
a	O
closely	O
related	O
word	O
to	O
one	O
of	O
the	O
class	O
centroid	O
embeddings	O
(	O
e.g.	O
,	O
in	O
SST	B-MethodName
the	O
MASK	O
embedding	O
almost	O
always	O
represents	O
a	O
positive	O
or	O
negative	O
adjective	O
)	O
.	O

In	O
WiC	O
,	O
the	O
MASK	O
embeddings	O
can	O
potentially	O
refer	O
to	O
any	O
word	O
,	O
varying	O
from	O
sample	O
to	O
sample	O
.	O

The	O
results	O
approve	O
the	O
assumption	O
:	O
pruned	B-MethodName
cosine	I-MetricName
similarity	I-MetricName
gains	O
around	O
10	B-MetricValue
%	I-MetricValue
absolute	O
performance	O
boost	O
on	O
WiC	B-DatasetName
,	O
filling	O
the	O
gap	O
to	O
Spearman	B-MetricName
correlation	I-MetricName
.	O

This	O
superiority	O
can	O
be	O
explained	O
by	O
the	O
assumption	O
that	O
cosine	O
similarity	O
is	O
more	O
susceptible	O
to	O
variations	O
in	O
the	O
dominant	O
dimensions	O
.	O

Notably	O
,	O
the	O
Spearman	B-MetricName
correlation	I-MetricName
score	O
,	O
which	O
is	O
less	O
commonly	O
used	O
for	O
comparing	O
embeddings	O
,	O
outperforms	O
the	O
cosine	O
similarity	O
on	O
WiC	B-DatasetName
by	O
a	O
large	O
margin	O
while	O
maintaining	O
the	O
same	O
level	O
of	O
performance	O
on	O
other	O
tasks	O
.	O

In	O
fact	O
,	O
one	O
could	O
argue	O
that	O
the	O
auto	B-MethodName
-	I-MethodName
generated	O
prompt	I-MethodName
of	O
Auto	B-MethodName
-	I-MethodName
Prompt	I-MethodName
is	O
sub	O
-	O
optimal	O
for	O
our	O
model	O
,	O
which	O
results	O
in	O
dropped	O
performance	O
on	O
the	O
SICK	B-DatasetName
-	I-DatasetName
E	I-DatasetName
dataset	O
.	O

We	O
note	O
that	O
the	O
goal	O
of	O
this	O
experiment	O
was	O
to	O
showcase	O
that	O
our	O
simple	O
adaptation	O
is	O
also	O
applicable	O
to	O
scenarios	O
other	O
than	O
the	O
setting	O
of	O
WiC.	O

SP	B-MethodName
retains	O
an	O
acceptable	O
level	O
of	O
performance	O
,	O
particularly	O
with	O
the	O
manual	B-MethodName
prompt	I-MethodName
,	O
but	O
lags	O
behind	O
with	O
the	O
auto	B-MethodName
-	I-MethodName
generated	I-MethodName
prompt	I-MethodName
.	O

For	O
SST-2	B-DatasetName
,	O
we	O
observe	O
that	O
SP	B-MethodName
can	O
exploit	O
a	O
manual	O
prompt	O
template	O
significantly	O
better	O
than	O
Auto	B-MethodName
-	I-MethodName
Prompt	I-MethodName
,	O
while	O
being	O
competitive	O
using	O
the	O
best	O
template	O
optimized	O
by	O
AutoPrompt	B-MethodName
(	O
auto	O
-	O
generated	O
)	O
.	O

We	O
compare	O
SP	B-MethodName
with	O
AutoPrompt	B-MethodName
which	O
searches	O
for	O
the	O
best	O
template	O
for	O
each	O
task	O
.	O

The	O
results	O
on	O
SST-2	B-DatasetName
and	O
SICK	B-MethodName
-	I-DatasetName
E	I-DatasetName
are	O
shown	O
in	O
Table	O
2	O
.	O

SICK	B-DatasetName
and	O
SST-2	B-DatasetName
.	O

We	O
report	O
SP	B-MethodName
's	O
performance	O
on	O
WiC	B-DatasetName
for	O
other	O
PLMs	O
in	O
the	O
Appendix	O
which	O
shows	O
our	O
method	O
/	O
observation	O
does	O
not	O
depend	O
on	O
a	O
specific	O
PLM	O
.	O

We	O
also	O
include	O
some	O
detailed	O
examples	O
of	O
how	O
SP	B-MethodName
works	O
for	O
WiC	O
in	O
the	O
Appendix	O
.	O

Therefore	O
,	O
using	O
limited	O
examples	O
in	O
the	O
fewshot	O
setting	O
they	O
are	O
able	O
to	O
reach	O
their	O
maximum	O
fine	O
-	O
tuning	O
potential	O
on	O
WiC.	O

This	O
observation	O
suggests	O
that	O
PLMs	O
already	O
encode	O
a	O
certain	O
amount	O
of	O
task	O
-	O
related	O
knowledge	O
and	O
the	O
supervised	O
fine	O
-	O
tuning	O
mainly	O
updates	O
their	O
task	O
description	O
(	O
i.e.	O
,	O
what	O
the	O
task	O
is	O
,	O
not	O
how	O
to	O
solve	O
it	O
)	O
.	O

The	O
performance	O
of	O
SP	B-MethodName
in	O
the	O
few	O
-	O
shot	O
setting	O
is	O
in	O
the	O
same	O
ballpark	O
as	O
supervised	B-MethodName
fine	I-MethodName
-	I-MethodName
tuning	I-MethodName
(	O
with	O
nearly	O
170	O
times	O
the	O
data	O
,	O
i.e.	O
,	O
2,714	O
instances	O
per	O
class	O
)	O
.	O

Table	O
1	O
summarizes	O
the	O
results	O
on	O
WiC	O
with	O
RoBERTa	O
-	O
Large	O
as	O
SP	O
's	O
PLM	O
.	O

WiC.	O

Given	O
that	O
our	O
experiments	O
are	O
mainly	O
focused	O
on	O
the	O
WiC	B-DatasetName
dataset	O
,	O
we	O
first	O
report	O
our	O
results	O
on	O
this	O
benchmark	O
,	O
and	O
then	O
provide	O
additional	O
results	O
for	O
the	O
other	O
two	O
tasks	O
.	O

As	O
for	O
PLM	O
,	O
we	O
opted	O
for	O
RoBERTA	B-MethodName
-	I-MethodName
large	I-MethodName
to	O
be	O
able	O
to	O
benchmark	O
our	O
results	O
against	O
Auto	B-MethodName
-	I-MethodName
Prompt	I-MethodName
's	I-MethodName
(	O
Shin	O
et	O
al	O
.	O
,	O
2020	O
)	O
For	O
each	O
experiment	O
,	O
we	O
report	O
the	O
average	O
performance	O
along	O
with	O
the	O
standard	O
deviation	O
.	O

In	O
our	O
experiments	O
,	O
we	O
only	O
use	O
the	O
former	O
annotations	O
(	O
SICK	O
-	O
E	O
)	O
to	O
compare	O
our	O
results	O
with	O
AutoPrompt	B-MethodName
,	O
which	O
only	O
reports	O
results	O
for	O
its	O
optimized	O
prompt	O
.	O

SICK	O
.	O

Sentences	O
Involving	O
Compositional	O
.	O

This	O
is	O
the	O
same	O
manual	O
prompt	O
used	O
in	O
AutoPrompt	B-DatasetName
.	O

For	O
this	O
task	O
we	O
used	O
the	O
automatically	O
-	O
generated	O
template	O
of	O
Auto	B-MethodName
-	I-MethodName
Prompt	I-MethodName
,	O
along	O
with	O
the	O
following	O
manual	O
template	O
:	O
T	O
(	O
sent	O
)	O
=	O
sent	O
+	O
"	O
this	O
movie	O
was	O
--	O
.	O
"	O
,	O
where	O
sent	O
is	O
the	O
input	O
sentence	O
and	O
"	O
+	O
"	O
is	O
concatenation	O
operator	O
.	O

We	O
follow	O
the	O
latter	O
(	O
SST-2	B-MethodName
)	O
in	O
our	O
experiments	O
.	O

Systems	O
are	O
evaluated	O
either	O
on	O
a	O
five	O
-	O
way	O
fine	O
-	O
grained	O
or	O
binary	B-TaskName
classification	I-TaskName
task	O
.	O

Stanford	B-DatasetName
Sentiment	I-DatasetName
Treebank	I-DatasetName
(	O
Socher	O
et	O
al	O
.	O
,	O
2013	O
)	O
contains	O
fine	O
-	O
grained	O
sentiment	O
labeled	O
parse	O
trees	O
of	O
sentences	O
from	O
movie	O
reviews	O
.	O

Following	O
AutoPrompt	B-MethodName
,	O
we	O
report	O
results	O
for	O
the	O
following	O
two	O
task	O
:	O
SST	B-TaskName
.	O

For	O
this	O
experiment	O
,	O
we	O
compare	O
against	O
AutoPrompt	B-MethodName
(	O
Shin	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

The	O
goal	O
of	O
this	O
additional	O
experiment	O
is	O
twofold	O
:	O
first	O
,	O
to	O
show	O
the	O
applicability	O
of	O
SP	B-MethodName
to	O
other	O
settings	O
,	O
including	O
tasks	O
with	O
single	O
input	O
sequence	O
;	O
and	O
second	O
,	O
to	O
evaluate	O
if	O
SP	B-MethodName
is	O
effective	O
when	O
using	O
prompt	O
templates	O
from	O
other	O
techniques	O
,	O
including	O
those	O
optimized	O
for	O
specific	O
tasks	O
.	O

In	O
addition	O
to	O
WiC	B-TaskName
,	O
we	O
also	O
carried	O
out	O
experiments	O
on	O
two	O
more	O
tasks	O
.	O

GPT3	B-MethodName
(	O
Brown	O
et	O
al	O
.	O
,	O
2020	O
)	O
is	O
different	O
in	O
that	O
it	O
employs	O
the	O
so	O
-	O
called	O
in	O
-	O
context	O
learning	O
which	O
involves	O
no	O
parameter	O
tuning	O
.	O

P	B-MethodName
-	I-MethodName
tuning	I-MethodName
(	O
Liu	O
et	O
al	O
.	O
,	O
2021	O
)	O
uses	O
the	O
same	O
PLM	O
as	O
PET	B-MethodName
,	O
but	O
optimizes	O
a	O
continuous	O
prompt	O
instead	O
of	O
tuning	O
PLM	O
parameters	O
.	O

PET	B-MethodName
(	O
Schick	O
and	O
Schütze	O
,	O
2021b	O
)	O
prefers	O
ALBERT	B-MethodName
-	I-MethodName
xxlarge	I-MethodName
-	I-MethodName
v2	I-MethodName
(	O
Lan	O
et	O
al	O
.	O
,	O
2019	O
)	O
over	O
RoBERTa	B-MethodName
(	O
with	O
an	O
average	O
gain	O
of	O
8	O
points	O
on	O
a	O
subset	O
of	O
SuperGLUE	B-DatasetName
tasks	O
)	O
and	O
fine	O
-	O
tunes	O
it	O
with	O
manually	O
engineered	O
cloze	O
-	O
style	O
prompts	O
.	O

We	O
compare	O
our	O
results	O
on	O
WiC	B-DatasetName
with	O
three	O
other	O
methods	O
,	O
all	O
of	O
which	O
use	O
32	O
examples	O
for	O
their	O
training	O
.	O

We	O
opted	O
for	O
two	O
similarity	O
metrics	O
:	O
cosine	B-MetricName
similarity	I-MetricName
and	O
Spearman	B-MetricName
's	I-MetricName
rank	I-MetricName
correlation	I-MetricName
.	O

Next	O
the	O
prompts	O
are	O
separately	O
fed	O
to	O
PLM	O
,	O
resulting	O
in	O
a	O
pair	O
of	O
mask	O
embeddings	O
as	O
PLM	O
's	O
response	O
.	O

In	O
the	O
first	O
step	O
of	O
SP	B-MethodName
,	O
we	O
apply	O
this	O
template	O
function	O
to	O
both	O
input	O
sentences	O
which	O
generates	O
a	O
pair	O
of	O
prompts	O
.	O

Therefore	O
,	O
we	O
ask	O
PLM	O
about	O
the	O
triggered	O
meaning	O
of	O
the	O
target	O
word	O
,	O
separately	O
for	O
each	O
context	O
,	O
and	O
leave	O
the	O
comparison	O
to	O
similarity	O
measures	O
.	O

Previous	O
work	O
has	O
fallen	O
short	O
of	O
designing	O
a	O
single	O
prompt	O
template	O
which	O
make	O
the	O
PLM	O
answer	O
about	O
the	O
target	O
word	O
having	O
the	O
same	O
meaning	O
or	O
not	O
(	O
e.g.	O
,	O
with	O
"	O
yes	O
"	O
or	O
"	O
no	O
"	O
)	O
.	O

Given	O
an	O
ambiguous	O
target	O
word	O
in	O
two	O
different	O
contexts	O
,	O
the	O
task	O
in	O
WiC	B-TaskName
is	O
defined	O
as	O
a	O
simple	O
binary	B-TaskName
classification	O
problem	O
to	O
identify	O
if	O
the	O
triggered	O
meaning	O
of	O
the	O
target	O
word	O
differs	O
in	O
the	O
two	O
contexts	O
or	O
not	O
.	O

The	O
surprising	O
failure	O
of	O
existing	O
prompt	B-MethodName
-	I-MethodName
based	I-MethodName
techniques	I-MethodName
on	O
the	O
Word	B-DatasetName
-	I-TaskName
in	I-DatasetName
-	I-TaskName
Context	I-DatasetName
task	O
(	O
Pilehvar	O
and	O
Camacho	O
-	O
Collados	O
,	O
2019	O
,	O
WiC	O
)	O
,	O
motivated	O
us	O
to	O
focus	O
on	O
filling	O
this	O
gap	O
.	O

Similarity	O
Prompting	O
for	O
WiC.	O

This	O
linear	O
model	O
is	O
then	O
used	O
at	O
inference	O
time	O
to	O
evaluate	O
SP	B-MethodName
on	O
test	O
set	O
.	O

To	O
alleviate	O
the	O
problem	O
,	O
we	O
perform	O
a	O
class	O
centroid	O
-	O
based	O
dimension	O
reduction	O
(	O
i.e.	O
by	O
taking	O
the	O
similarity	O
to	O
each	O
centroid	O
as	O
a	O
feature	O
)	O
,	O
and	O
train	O
a	O
simple	O
linear	O
classifier	O
.	O

The	O
third	O
step	O
is	O
where	O
SP	B-MethodName
differs	O
from	O
existing	O
prompt	B-MethodName
-	I-MethodName
based	I-MethodName
approaches	O
.	O

This	O
is	O
done	O
by	O
giving	O
the	O
generated	O
prompts	O
to	O
the	O
PLM	O
as	O
input	O
and	O
obtaining	O
its	O
contextualized	O
embedding	O
at	O
the	O
MASK	O
index	O
.	O

The	O
next	O
step	O
is	O
feature	O
extraction	O
from	O
a	O
PLM	O
.	O

As	O
shown	O
in	O
Figure	O
1	O
,	O
SP	B-MethodName
consists	O
of	O
three	O
main	O
steps	O
:	O
(	O
1	O
)	O
prompt	O
generation	O
,	O
(	O
2	O
)	O
feature	O
extraction	O
,	O
and	O
(	O
3	O
)	O
prediction	O
.	O

In	O
what	O
follows	O
in	O
this	O
section	O
,	O
we	O
describe	O
our	O
similarity	B-MethodName
-	O
based	O
prompting	O
approach	O
which	O
we	O
will	O
refer	O
to	O
as	O
SP	B-MethodName
(	O
Similarity	B-MethodName
Prompting	I-MethodName
)	O
.	O

We	O
propose	O
a	O
similarity	O
-	O
based	O
method	O
that	O
not	O
only	O
better	O
exploits	O
the	O
response	O
,	O
but	O
also	O
allows	O
using	O
multiple	O
prompts	O
which	O
paves	O
the	O
way	O
for	O
comparisonbased	O
tasks	O
,	O
such	O
as	O
WiC.	B-TaskName

Assuming	O
that	O
PLMs	O
know	O
how	O
to	O
solve	O
some	O
tasks	O
(	O
to	O
some	O
extent	O
)	O
,	O
prompt	B-MethodName
-	I-MethodName
based	I-MethodName
learning	I-MethodName
focuses	O
on	O
the	O
former	O
,	O
i.e.	O
,	O
teaching	O
the	O
model	O
what	O
the	O
task	O
is	O
,	O
without	O
needing	O
to	O
resort	O
to	O
large	O
amounts	O
of	O
data	O
or	O
additional	O
parameters	O
.	O

Fine	O
-	O
tuning	O
on	O
a	O
specific	O
task	O
can	O
potentially	O
update	O
PLMs	O
on	O
what	O
the	O
task	O
is	O
and	O
how	O
to	O
solve	O
it	O
.	O

The	O
experimental	O
results	O
on	O
the	O
WiC	B-DatasetName
dataset	O
shows	O
that	O
,	O
with	O
only	O
16	O
instances	O
per	O
class	O
,	O
our	O
proposed	O
prompt	B-MethodName
-	I-MethodName
based	I-MethodName
technique	O
can	O
achieve	O
comparable	O
results	O
to	O
the	O
fine	O
-	O
tuned	O
models	O
(	O
with	O
access	O
to	O
full	O
training	O
data	O
of	O
2700	O
+	O
instances	O
per	O
class	O
)	O
.	O

Hence	O
,	O
instead	O
of	O
relying	O
on	O
a	O
single	O
response	O
,	O
we	O
make	O
use	O
of	O
the	O
similarity	O
of	O
PLM	O
's	O
response	O
to	O
the	O
combination	O
of	O
a	O
pair	O
of	O
prompts	O
.	O

Given	O
the	O
comparison	O
-	O
based	O
nature	O
of	O
WiC	B-MethodName
,	O
we	O
hypothesize	O
that	O
conventional	O
prompting	O
methods	O
fall	O
short	O
since	O
they	O
only	O
utilize	O
a	O
single	O
prompt	O
response	O
.	O

However	O
,	O
none	O
of	O
these	O
have	O
shown	O
success	O
on	O
the	O
WiC	B-TaskName
task	O
.	O

Two	O
issues	O
could	O
be	O
responsible	O
for	O
the	O
latter	O
case	O
:	O
(	O
1	O
)	O
improper	O
prompt	O
,	O
or	O
(	O
2	O
)	O
inefficient	O
utilization	O
of	O
PLM	O
's	O
response	O
.	O

The	O
natural	O
question	O
that	O
arises	O
here	O
is	O
if	O
the	O
failure	O
of	O
few	O
-	O
shot	O
techniques	O
on	O
WiC	B-TaskName
is	O
due	O
to	O
lack	O
of	O
relevant	O
encoded	O
knowledge	O
in	O
PLMs	O
or	O
the	O
inefficiency	O
of	O
the	O
employed	O
prompt	O
-	O
based	O
methods	O
.	O

‡	O
While	O
a	O
simple	O
fine	O
-	O
tuned	O
BERT	B-MethodName
-	I-MethodName
base	I-MethodName
model	O
achieves	O
around	O
69	B-MetricValue
%	I-MetricValue
accuracy	B-MetricName
on	O
this	O
task	O
(	O
Wang	O
et	O
al	O
.	O
,	O
2019	O
)	O
,	O
GPT-3	B-MethodName
,	O
with	O
more	O
than	O
100	O
times	O
the	O
number	O
of	O
parameters	O
,	O
performs	O
no	O
better	O
than	O
a	O
random	B-MethodName
baseline	I-MethodName
by	O
employing	O
a	O
promptbased	B-MethodName
approach	O
(	O
Brown	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

However	O
,	O
surprisingly	O
,	O
the	O
Word	O
-	O
in	O
-	O
Context	O
task	O
(	O
Pilehvar	O
and	O
Camacho	O
-	O
Collados	O
,	O
2019	O
)	O
-one	O
of	O
the	O
tasks	O
in	O
the	O
SuperGLUE	B-DatasetName
benchmark	O
(	O
Wang	O
et	O
al	O
.	O
,	O
2019)-is	O
one	O
exception	O
on	O
which	O
these	O
methods	O
fail	O
to	O
stay	O
on	O
par	O
with	O
their	O
fine	O
-	O
tuned	O
counterparts	O
.	O

From	O
the	O
practical	O
point	O
of	O
view	O
,	O
prompt	B-MethodName
-	I-MethodName
based	I-MethodName
learning	I-MethodName
is	O
particularly	O
well	O
-	O
suited	O
for	O
massive	O
models	O
,	O
such	O
as	O
GPT-3	B-MethodName
,	O
since	O
it	O
does	O
not	O
involve	O
parameter	O
tuning	O
.	O

This	O
paradigm	O
has	O
proven	O
its	O
effectiveness	O
in	O
the	O
few	O
-	O
shot	O
setting	O
,	O
even	O
for	O
relatively	O
smaller	O
models	O
,	O
such	O
as	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O
2019	O
)	O
and	O
RoBERTA	B-MethodName
(	O
Liu	O
et	O
al	O
.	O
,	O
2019	O
)	O
,	O
when	O
combined	O
with	O
ensembling	O
and	O
fine	O
-	O
tuning	O
(	O
Schick	O
and	O
Schütze	O
,	O
2021a	O
)	O
.	O

The	O
current	O
dominant	O
few	O
-	O
shot	O
approach	O
is	O
the	O
so	O
-	O
called	O
promptbased	B-MethodName
learning	I-MethodName
which	O
involves	O
a	O
simple	O
reformulation	O
of	O
the	O
target	O
task	O
as	O
a	O
cloze	O
-	O
style	O
(	O
Taylor	O
,	O
1953	O
)	O
fill	O
-	O
in	O
-	O
the	O
-	O
blank	O
objective	O
.	O

Recently	O
,	O
there	O
has	O
been	O
a	O
resurgence	O
of	O
interest	O
in	O
few	O
-	O
shot	O
learning	O
,	O
especially	O
after	O
the	O
introduction	O
of	O
GPT-3	B-MethodName
(	O
Brown	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

Our	O
simple	O
adaptation	O
shows	O
that	O
the	O
failure	O
of	O
existing	O
prompt	B-MethodName
-	O
based	I-MethodName
techniques	O
in	O
semantic	O
distinction	O
is	O
due	O
to	O
their	O
improper	O
configuration	O
,	O
rather	O
than	O
lack	O
of	O
relevant	O
knowledge	O
in	O
the	O
representations	O
.	O

Specifically	O
,	O
none	O
of	O
the	O
existing	O
few	O
-	O
shot	O
approaches	O
(	O
including	O
the	O
incontext	O
learning	O
of	O
GPT-3	B-MethodName
)	O
can	O
attain	O
a	O
performance	O
that	O
is	O
meaningfully	O
different	O
from	O
the	O
random	B-MethodName
baseline	I-MethodName
.	O

Exploiting	O
Language	O
Model	O
Prompts	O
Using	O
Similarity	O
Measures	O
:	O
A	O
Case	O
Study	O
on	O
the	O
Word	O
-	O
in	O
-	O
Context	O
Task	O
.	O

However	O
,	O
despite	O
proving	O
competitive	O
on	O
most	O
tasks	O
in	O
the	O
GLUE	B-DatasetName
and	O
SuperGLUE	B-DatasetName
benchmarks	O
,	O
existing	O
prompt	B-MethodName
-	I-MethodName
based	I-MethodName
techniques	O
fail	O
on	O
the	O
semantic	O
distinction	O
task	O
of	O
the	O
Word	B-DatasetName
-	I-DatasetName
in	I-DatasetName
-	I-DatasetName
Context	I-DatasetName
(	O
WiC	B-DatasetName
)	O
dataset	O
.	O

As	O
a	O
recent	O
development	O
in	O
few	O
-	O
shot	O
learning	O
,	O
prompt	B-MethodName
-	I-MethodName
based	I-MethodName
techniques	I-MethodName
have	O
demonstrated	O
promising	O
potential	O
in	O
a	O
variety	O
of	O
natural	O
language	O
processing	O
tasks	O
.	O

Not	O
matched	O
(	O
threats	O
,	O
ridicule	O
,	O
mockery	O
,	O
attacks	O
,	O
threat	O
)	O
(	O
backlash	O
,	O
ridicule	O
,	O
mockery	O
,	O
condemnation	O
,	O
criticism	O
)	O
.	O

Matched	O
.	O

The	O
politician	O
received	O
a	O
lot	O
of	O
public	O
criticism	O
or	O
--for	O
his	O
controversial	O
stance	O
on	O
the	O
issue	O
.	O

Not	O
matched	O
(	O
something	O
,	O
leak	O
,	O
obstruction	O
,	O
defect	O
,	O
overflow	O
)	O
(	O
debris	O
,	O
obstruction	O
,	O
water	O
,	O
leak	O
,	O
crack	O
)	O
The	O
senator	O
received	O
severe	O
criticism	O
or	O
--from	O
his	O
opponent	O
.	O

Matched	O
.	O

We	O
had	O
to	O
call	O
a	O
plumber	O
to	O
clear	O
out	O
the	O
blockage	O
or	O
--in	O
the	O
drainpipe	O
.	O

Not	O
matched	O
(	O
anger	O
,	O
disgust	O
,	O
irritation	O
,	O
contempt	O
,	O
frustration	O
)	O
(	O
anger	O
,	O
rage	O
,	O
frustration	O
,	O
aggression	O
,	O
disgust	O
)	O
There	O
was	O
a	O
blockage	O
or	O
--in	O
the	O
sewer	O
,	O
so	O
we	O
called	O
out	O
the	O
plumber	O
.	O

Matched	O
.	O

He	O
could	O
no	O
longer	O
contain	O
his	O
hostility	O
or	O
--	O
.	O

Not	O
matched	O
(	O
river	O
,	O
bay	O
,	O
sea	O
,	O
ocean	O
,	O
channel	O
)	O
(	O
voices	O
,	O
footsteps	O
,	O
whispers	O
,	O
conversations	O
,	O
cries	O
)	O
He	O
could	O
not	O
conceal	O
his	O
hostility	O
or	O
--	O
.	O

Not	O
matched	O
.	O

He	O
strained	O
to	O
hear	O
the	O
faint	O
sounds	O
or	O
--	O
.	O

Not	O
matched	O
Not	O
matched	O
(	O
trunk	O
,	O
roof	O
,	O
chassis	O
,	O
frame	O
,	O
grill	O
)	O
(	O
agency	O
,	O
institution	O
,	O
government	O
,	O
commission	O
,	O
equivalent	O
)	O
The	O
main	O
body	O
of	O
the	O
sound	O
or	O
-ran	O
parallel	O
to	O
the	O
coast	O
.	O

Administrative	O
body	O
or	O
--	O
.	O

Not	O
matched	O
(	O
use	O
,	O
extraction	O
,	O
taking	O
,	O
pumping	O
,	O
consumption	O
)	O
(	O
paintings	O
,	O
sculptures	O
,	O
something	O
,	O
more	O
,	O
looked	O
)	O
The	O
body	O
or	O
--of	O
the	O
car	O
was	O
badly	O
rusted	O
.	O

Not	O
matched	O
.	O

He	O
did	O
complicated	O
pen	O
-	O
and	O
-	O
ink	O
drawings	O
or	O
--like	O
medieval	O
miniatures	O
.	O

The	O
drawing	O
or	O
--of	O
water	O
from	O
the	O
well	O
.	O

Prompt2	O
(	O
Top-5	O
words	O
)	O
Prediction	O
Ground	O
Truth	O
.	O

Prompt1	O
(	O
Top-5	O
words	O
)	O
.	O

For	O
the	O
bottom	O
three	O
where	O
the	O
model	O
fails	O
,	O
we	O
can	O
observe	O
that	O
the	O
target	O
words	O
have	O
very	O
similar	O
or	O
close	O
senses	O
,	O
making	O
them	O
really	O
hard	O
to	O
distinguish	O
.	O

The	O
top	O
three	O
examples	O
are	O
correctly	O
predicted	O
as	O
negative	O
with	O
high	O
confidence	O
(	O
high	O
similarity	O
score	O
)	O
,	O
while	O
the	O
bottom	O
three	O
are	O
predicted	O
positive	O
again	O
with	O
high	O
confidence	O
.	O

We	O
did	O
not	O
include	O
the	O
positive	O
examples	O
,	O
since	O
the	O
observation	O
that	O
the	O
same	O
words	O
with	O
the	O
same	O
senses	O
are	O
treated	O
similarly	O
,	O
might	O
not	O
provide	O
a	O
useful	O
insight	O
.	O

B	O
Qualitative	O
Analysis	O
.	O

The	O
results	O
in	O
.	O

Conclusion	O
.	O

Figure	O
2	O
illustrates	O
the	O
distribution	O
of	O
values	O
for	O
the	O
most	O
dominant	O
dimension	O
.	O

To	O
verify	O
our	O
hypothesis	O
,	O
we	O
ran	O
an	O
experiment	O
using	O
1200	O
sample	O
MASK	O
embeddings	O
for	O
each	O
of	O
our	O
three	O
tasks	O
.	O

The	O
difference	O
in	O
the	O
gain	O
across	O
tasks	O
can	O
be	O
explained	O
by	O
the	O
difference	O
in	O
their	O
underlying	O
nature	O
.	O

However	O
,	O
the	O
gain	O
in	O
the	O
other	O
two	O
tasks	O
is	O
negligible	O
.	O

To	O
evaluate	O
this	O
hypothesis	O
,	O
we	O
performed	O
an	O
experiment	O
in	O
which	O
the	O
most	O
dominant	O
dimension	O
was	O
set	O
to	O
zero	O
for	O
all	O
the	O
embeddings	O
(	O
the	O
dominant	O
dimension	O
is	O
identical	O
across	O
all	O
vectors	O
)	O
.	O

Similarity	O
Measures	O
Comparison	O
.	O

This	O
suggests	O
that	O
it	O
is	O
possible	O
to	O
gain	O
significant	O
improvement	O
by	O
simply	O
exploiting	O
a	O
non	O
-	O
optimized	O
manual	O
prompt	O
template	O
.	O

Results	O
.	O

To	O
train	O
our	O
models	O
,	O
we	O
only	O
used	O
16	O
examples	O
per	O
class	O
.	O

Setup	O
.	O

Answer	O
:	O
--	O
,	O
"	O
+	O
hyp	O
,	O
where	O
pre	O
is	O
the	O
premise	O
and	O
hyp	O
is	O
the	O
hypothesis	O
of	O
an	O
input	O
example	O
.	O

Thus	O
we	O
define	O
our	O
own	O
manual	O
template	O
function	O
as	O
:	O
T	O
(	O
pre	O
,	O
hyp	O
)	O
=	O
pre	O
+	O
"	O
?	O

Knowledge	O
(	O
Marelli	O
et	O
al	O
.	O
,	O
2014	O
)	O
is	O
a	O
collection	O
of	O
sentence	O
pairs	O
annotated	O
with	O
their	O
entailment	O
relationship	O
as	O
well	O
as	O
a	O
quantified	O
measurement	O
of	O
their	O
semantic	O
similarity	O
.	O

The	O
approach	O
makes	O
use	O
of	O
full	O
training	O
set	O
to	O
optimize	O
discrete	O
prompts	O
for	O
each	O
specific	O
target	O
task	O
.	O

Tasks	O
.	O

Comparison	O
Systems	O
.	O

Experiments	O
.	O

The	O
latter	O
is	O
a	O
rank	O
-	O
based	O
comparison	O
measure	O
which	O
is	O
insensitive	O
to	O
the	O
absolute	O
values	O
of	O
individual	O
dimensions	O
(	O
rather	O
checks	O
for	O
their	O
relative	O
rankings	O
)	O
.	O

Similarity	O
Measures	O
.	O

We	O
then	O
train	O
the	O
same	O
linear	O
model	O
as	O
before	O
on	O
the	O
similarity	O
scores	O
of	O
the	O
training	O
set	O
examples	O
to	O
find	O
the	O
best	O
discriminating	O
threshold	O
.	O

Finally	O
,	O
our	O
classification	O
step	O
reduces	O
to	O
that	O
of	O
directly	O
comparing	O
our	O
pair	O
of	O
embedding	O
vectors	O
using	O
a	O
similarity	O
function	O
,	O
to	O
produce	O
a	O
single	O
similarity	O
score	O
for	O
each	O
instance	O
.	O

Having	O
an	O
input	O
sentence	O
and	O
the	O
target	O
word	O
index	O
,	O
we	O
insert	O
"	O
or	O
--	O
"	O
after	O
the	O
target	O
word	O
,	O
where	O
"	O
--	O
"	O
indicates	O
the	O
MASK	O
token	O
.	O

However	O
,	O
this	O
assumes	O
the	O
variance	O
of	O
different	O
classes	O
to	O
be	O
equal	O
in	O
the	O
embedding	O
space	O
.	O

To	O
classify	O
a	O
new	O
sample	O
at	O
inference	O
time	O
,	O
a	O
simple	O
approach	O
would	O
be	O
to	O
employ	O
a	O
nearest	O
centroid	O
classifier	O
.	O

Here	O
,	O
we	O
first	O
obtain	O
class	O
-	O
specific	O
centroids	O
by	O
taking	O
the	O
average	O
of	O
the	O
MASK	O
embeddings	O
of	O
our	O
few	O
training	O
examples	O
.	O

this	O
movie	O
was	O
--	O
.	O
"	O
.	O

For	O
instance	O
,	O
in	O
sentiment	B-TaskName
analysis	I-TaskName
,	O
for	O
the	O
movie	O
review	O
"	O
Just	O
give	O
it	O
a	O
chance	O
.	O
"	O
,	O
a	O
valid	O
template	O
function	O
would	O
generate	O
as	O
output	O
prompt	O
:	O
"	O
Just	O
give	O
it	O
a	O
chance	O
.	O

Given	O
a	O
task	O
-	O
specific	O
input	O
consisting	O
of	O
one	O
or	O
more	O
text	O
sequences	O
,	O
we	O
first	O
use	O
a	O
template	O
function	O
to	O
generate	O
a	O
prompt	O
-	O
a	O
sequence	O
of	O
tokens	O
containing	O
one	O
[	O
MASK	O
]	O
token	O
-	O
per	O
input	O
sequence	O
.	O

Existing	O
methods	O
often	O
pick	O
a	O
set	O
of	O
one	O
or	O
few	O
word	O
predictions	O
as	O
a	O
representative	O
for	O
each	O
class	O
,	O
utilizing	O
the	O
language	O
model	O
's	O
response	O
in	O
a	O
sub	O
-	O
optimal	O
manner	O
.	O

For	O
instance	O
,	O
to	O
ask	O
about	O
the	O
sentiment	O
of	O
a	O
movie	O
review	O
,	O
one	O
can	O
augment	O
the	O
review	O
with	O
a	O
cloze	O
question	O
like	O
"	O
this	O
movie	O
was	O
--	O
.	O
"	O
.	O

The	O
common	O
approach	O
in	O
prompt	B-MethodName
-	I-MethodName
based	I-MethodName
learning	I-MethodName
is	O
to	O
reformulate	O
the	O
task	O
as	O
a	O
cloze	O
-	O
style	O
question	O
.	O

Methodology	O
.	O

Moreover	O
,	O
we	O
show	O
that	O
with	O
few	O
adjustments	O
,	O
this	O
simple	O
approach	O
can	O
be	O
effectively	O
used	O
for	O
other	O
downstream	O
tasks	O
.	O

In	O
this	O
work	O
we	O
investigate	O
the	O
latter	O
issue	O
by	O
introducing	O
a	O
new	O
configuration	O
for	O
prompting	O
.	O

To	O
address	O
the	O
first	O
issue	O
,	O
there	O
have	O
been	O
proposals	O
to	O
automatically	O
find	O
a	O
suitable	O
prompt	O
template	O
using	O
a	O
search	O
in	O
the	O
discrete	O
token	O
space	O
(	O
Shin	O
et	O
al	O
.	O
,	O
2020	O
)	O
or	O
in	O
the	O
continuous	O
embedding	O
space	O
(	O
Liu	O
et	O
al	O
.	O
,	O
2021	O
)	O
.	O

The	O
same	O
pattern	O
of	O
failure	O
is	O
also	O
observed	O
in	O
the	O
more	O
recent	O
prompt	O
based	O
attempts	O
(	O
Liu	O
et	O
al	O
.	O
,	O
2021;Schick	O
and	O
Schütze	O
,	O
2021a	O
)	O
.	O

Prompt	B-MethodName
-	I-MethodName
based	I-MethodName
techniques	I-MethodName
have	O
shown	O
impressive	O
performance	O
in	O
the	O
few	O
-	O
shot	O
setting	O
,	O
especially	O
when	O
compared	O
to	O
standard	O
fine	O
-	O
tuning	O
on	O
datasets	O
of	O
hundreds	O
of	O
data	O
points	O
(	O
Le	O
Scao	O
and	O
Rush	O
,	O
2021	O
)	O
.	O

The	O
core	O
idea	O
is	O
to	O
extract	O
knowledge	O
by	O
asking	O
the	O
right	O
question	O
from	O
the	O
pre	O
-	O
trained	O
language	O
model	O
(	O
PLM	O
)	O
using	O
a	O
task	O
-	O
specific	O
prompting	O
template	O
which	O
directs	O
the	O
PLM	O
to	O
generate	O
a	O
textual	O
output	O
corresponding	O
to	O
a	O
target	O
class	O
.	O

Introduction	O
.	O

We	O
have	O
also	O
experimented	O
with	O
different	O
forms	O
of	O
training	O
data	O
generated	O
from	O
the	O
BabelDr	B-DatasetName
SCFG	O
.	O

Figure	O
1	O
provides	O
an	O
overview	O
of	O
the	O
ellipsis	B-TaskName
translation	I-TaskName
task	O
as	O
it	O
would	O
be	O
performed	O
in	O
BabelDr	B-MethodName
.	O
Starting	O
with	O
a	O
source	O
sentence	O
,	O
we	O
perform	O
ellipsis	O
detection	O
using	O
a	O
binary	O
classifier	O
(	O
support	O
-	O
vector	O
machine	O
)	O
trained	O
on	O
handcrafted	O
features	O
.	O

For	O
this	O
study	O
,	O
we	O
have	O
therefore	O
used	O
a	O
test	O
suite	O
based	O
on	O
the	O
Ba	B-DatasetName
-	O
belDr	O
coverage	O
and	O
described	O
in	O
(	O
Rayner	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

Since	O
the	O
currently	O
deployed	O
version	O
of	O
babelDr	B-MethodName
only	O
handles	O
ellipsis	O
in	O
a	O
limited	O
manner	O
,	O
doctors	O
were	O
instructed	O
to	O
use	O
only	O
complete	O
sentences	O
.	O

Ellipsis	O
in	O
BabelDr	B-MethodName
.	O
In	O
the	O
BabelDr	B-DatasetName
context	O
,	O
instead	O
of	O
producing	O
a	O
literal	O
translation	O
of	O
the	O
ellipsis	O
,	O
we	O
aim	O
at	O
mapping	O
elliptical	O
utterances	O
to	O
the	O
closest	O
non	O
-	O
elliptical	O
core	O
sentence	O
,	O
for	O
which	O
translations	O
are	O
available	O
in	O
the	O
system	O
.	O

To	O
handle	O
sentences	O
that	O
are	O
out	O
of	O
grammar	O
coverage	O
,	O
BabelDr	B-MethodName
also	O
includes	O
a	O
large	O
vocabulary	O
recogniser	O
.	O

BabelDr	B-MethodName
is	O
a	O
speech	O
-	O
enabled	O
fixed	O
-	O
phrase	O
translator	O
designed	O
to	O
allow	O
French	O
speaking	O
doctors	O
to	O
carry	O
out	O
diagnostic	O
interviews	O
with	O
patients	O
with	O
whom	O
they	O
do	O
n't	O
have	O
any	O
common	O
language	O
in	O
emergency	O
settings	O
where	O
no	O
interpreters	O
are	O
available	O
.	O

2	O
The	O
context	O
:	O
BabelDr	B-MethodName
.	O
The	O
BabelDr	B-MethodName
system	O
.	O

The	O
aim	O
of	O
this	O
paper	O
is	O
to	O
compare	O
different	O
approaches	O
to	O
translate	O
ellipsis	O
in	O
the	O
context	O
of	O
Ba	B-MethodName
-	I-MethodName
belDr	I-MethodName
.	O
Section	O
2	O
describes	O
the	O
BabelDr	B-MethodName
system	O
.	O

In	O
this	O
paper	O
,	O
we	O
focus	O
on	O
automatic	O
translation	O
of	O
ellipsis	O
in	O
medical	O
dialogues	O
,	O
in	O
the	O
particular	O
context	O
of	O
BabelDr	B-MethodName
,	O
a	O
speech	B-TaskName
to	O
speech	O
translation	O
system	O
for	O
the	O
medical	O
domain	O
(	O
Spechbach	O
et	O
al	O
.	O
,	O
2019	O
)	O
2	O
.	O

In	O
this	O
work	O
,	O
we	O
evaluate	O
four	O
different	O
approaches	O
to	O
translate	O
ellipsis	O
in	O
medical	O
dialogues	O
in	O
the	O
context	O
of	O
the	O
speech	B-TaskName
to	I-TaskName
speech	I-TaskName
translation	O
system	O
BabelDr	B-MethodName
.	O
We	O
also	O
investigate	O
the	O
impact	O
of	O
training	O
data	O
,	O
using	O
an	O
undersampling	O
method	O
and	O
data	O
with	O
elliptical	O
utterances	O
in	O
context	O
.	O

A	O
further	O
aspect	O
worth	O
investigating	O
is	O
exploring	O
novel	O
architectures	O
to	O
add	O
the	O
context	O
in	O
different	O
ways	O
:	O
train	O
a	O
context	O
aware	O
decoder	O
to	O
correct	O
translations	O
(	O
Voita	O
et	O
al	O
.	O
,	O
2019	O
,	O
for	O
neural	O
machine	O
translation	O
,	O
)	O
or	O
train	O
a	O
dual	B-MethodName
-	I-MethodName
source	I-MethodName
BERT	B-MethodName
(	O
Correia	O
and	O
Martins	O
,	O
2019	O
)	O
adding	O
context	O
on	O
the	O
tuning	O
step	O
for	O
sequence	B-TaskName
classification	I-TaskName
.	O

Each	O
source	O
variation	O
has	O
been	O
annotated	O
with	O
a	O
single	O
correct	O
core	O
sentence	O
,	O
but	O
this	O
does	O
not	O
reflect	O
the	O
real	O
use	O
case	O
:	O
the	O
purpose	O
of	O
BabelDr	B-MethodName
is	O
to	O
allow	O
doctors	O
to	O
collect	O
information	O
from	O
the	O
patient	O
,	O
not	O
to	O
translate	O
their	O
exact	O
utterance	O
.	O

Of	O
all	O
the	O
tested	O
systems	O
,	O
the	O
hybrid	O
approach	O
,	O
combining	O
neural	O
machine	I-TaskName
translation	I-TaskName
and	O
classification	O
models	O
is	O
the	O
most	O
successful	O
both	O
in	O
terms	O
of	O
our	O
task	O
specific	O
metric	O
(	O
SER	B-TaskName
)	O
and	O
in	O
terms	O
of	O
precision	B-MetricName
/	O
recall	B-MetricName
/	O
F1	B-MetricName
.	O

In	O
this	O
study	O
we	O
have	O
applied	O
different	O
approaches	O
to	O
an	O
ellipsis	B-TaskName
translation	I-TaskName
task	O
,	O
in	O
the	O
context	O
of	O
a	O
medical	O
speech	O
translator	O
.	O

With	O
these	O
results	O
,	O
we	O
confirmed	O
that	O
in	O
this	O
context	O
,	O
training	O
models	O
with	O
ellipsis	O
improves	O
performance	O
in	O
terms	O
of	O
SER	B-TaskName
,	O
precision	B-MetricName
,	O
recall	B-MetricName
and	O
F1	B-MetricName
.	O

11	O
%	O
of	O
the	O
sentences	O
were	O
classified	O
correctly	O
by	O
CamemBERT	B-MethodName
and	O
badly	O
by	O
LSTM	B-MethodName
,	O
and	O
4	O
%	O
the	O
other	O
way	O
around	O
.	O

with	O
the	O
additional	O
ellipsis	O
corpus	O
outperforms	O
the	O
one	O
trained	O
with	O
only	O
the	O
sampled	O
data	O
by	O
0.29	B-MetricValue
,	O
0.34	B-MetricValue
,	O
0.34	B-MetricValue
and	O
0.34	B-MetricValue
for	O
each	O
metric	O
respectively	O
.	O

For	O
those	O
sentences	O
that	O
the	O
hybrid	O
classifies	O
/	O
translates	O
adequately	O
,	O
52	O
%	O
are	O
well	O
translated	O
/	O
classified	O
by	O
both	O
models	O
,	O
20	O
%	O
by	O
LSTM	B-MethodName
only	O
and	O
the	O
rest	O
by	O
CamemBERT	B-MethodName
only	O
.	O

This	O
hybrid	O
achieved	O
0.23	B-MetricValue
and	O
0.50	B-MetricValue
on	O
elliptical	O
sentences	O
for	O
SER	B-TaskName
and	O
F1	B-MetricName
,	O
outperforming	O
the	O
best	O
model	O
by	O
0.21	B-MetricValue
and	O
0.11	B-MetricValue
for	O
those	O
metrics	O
respectively	O
.	O

Based	O
on	O
the	O
observation	O
that	O
sentences	O
that	O
were	O
not	O
well	O
classified	O
by	O
CamemBERT	B-MethodName
were	O
classified	O
correctly	O
by	O
LSTM	B-MethodName
,	O
we	O
decided	O
to	O
combine	O
LSTM	B-MethodName
and	O
camemBERT	B-MethodName
to	O
build	O
a	O
hybrid	O
system	O
.	O

However	O
,	O
LSTM	B-MethodName
outperforms	O
tf	B-MethodName
-	I-MethodName
idf	I-MethodName
for	O
all	O
sentences	O
,	O
showing	O
that	O
LSTM	B-MethodName
is	O
better	O
suited	O
for	O
non	O
-	O
elliptical	O
sentences	O
.	O

For	O
elliptical	O
sentences	O
only	O
,	O
tf	O
-	O
idf	O
is	O
the	O
second	O
best	O
approach	O
with	O
0.53	B-MetricValue
,	O
0.34	B-MetricValue
,	O
0.32	B-MetricValue
,	O
0.32	B-MetricValue
for	O
SER	B-TaskName
,	O
precision	B-MetricName
,	O
recall	B-MetricName
and	O
F1	B-MetricName
.	O

Classification	B-TaskName
,	O
with	O
CamemBERT	O
,	O
achieves	O
the	O
best	O
scores	O
across	O
all	O
approaches	O
for	O
both	O
ellip	O
-	O
tical	O
and	O
all	O
sentences	O
.	O

Table	O
5	O
presents	O
the	O
SER	B-MetricName
,	O
precision	B-MetricName
,	O
recall	B-MetricName
and	O
F1	B-MetricName
for	O
elliptical	O
and	O
all	O
sentences	O
.	O

section	O
4	O
)	O
,	O
except	O
for	O
machine	B-TaskName
translation	I-TaskName
where	O
we	O
already	O
chose	O
LSTM	B-MethodName
(	O
cf	O
.	O

Accordingly	O
,	O
we	O
carried	O
out	O
the	O
subsequent	O
experiments	O
using	O
the	O
LSTM	B-MethodName
model	O
for	O
the	O
machine	B-TaskName
translation	I-TaskName
approach	O
.	O

Because	O
of	O
training	O
data	O
size	O
and	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
,	O
training	O
time	O
was	O
considerably	O
lower	O
for	O
the	O
LSTM	B-MethodName
architecture	O
with	O
sampled	O
data	O
.	O

Regarding	O
the	O
machine	B-TaskName
translation	I-TaskName
approaches	O
,	O
while	O
results	O
suggest	O
that	O
both	O
architectures	O
are	O
suitable	O
for	O
the	O
task	O
,	O
we	O
observe	O
that	O
LSTMsampled	B-MethodName
and	O
LSTM	B-MethodName
slightly	O
outperform	O
Transformer	B-MethodName
and	O
Transformer	B-MethodName
-	I-MethodName
sampled	I-MethodName
on	O
SER	B-MetricName
,	O
precision	B-MetricName
,	O
recall	B-MetricName
and	O
F1	B-MetricName
.	O

We	O
observe	O
that	O
the	O
proposed	O
under	O
-	O
sampling	O
method	O
(	O
fastText	B-MethodName
-	I-MethodName
sampled	I-MethodName
,	O
LSTM	B-MethodName
-	I-MethodName
sampled	I-MethodName
and	O
Transformer	B-MethodName
-	I-MethodName
sampled	I-MethodName
)	O
produces	O
better	O
results	O
in	O
this	O
particular	O
context	O
indicating	O
that	O
a	O
more	O
balanced	O
data	O
set	O
improves	O
performance	O
in	O
terms	O
of	O
SER	B-TaskName
,	O
precision	B-MetricName
,	O
recall	B-MetricName
and	O
F1	B-MetricName
.	O

We	O
then	O
compared	O
performance	O
by	O
calculating	O
SER	B-MetricName
,	O
precision	B-MetricName
,	O
recall	B-MetricName
and	O
F1	B-MetricName
.	O

To	O
evaluate	O
the	O
under	O
-	O
sampling	O
method	O
,	O
we	O
ran	O
the	O
experiment	O
with	O
two	O
approaches	O
,	O
machine	B-TaskName
translation	I-TaskName
(	O
LSTM	B-MethodName
,	O
Transformer	B-MethodName
)	O
and	O
classification	B-TaskName
(	O
fast	B-MethodName
-	I-MethodName
Text	I-MethodName
)	O
,	O
trained	O
with	O
two	O
different	O
data	O
sets	O
:	O
undersampled	O
data	O
(	O
hereafter	O
sampled	O
)	O
and	O
all	O
data	O
.	O

Finally	O
,	O
including	O
only	O
the	O
best	O
model	O
for	O
each	O
approach	O
in	O
terms	O
of	O
F1	B-MetricName
,	O
we	O
evaluate	O
the	O
impact	O
of	O
training	O
on	O
Ellipsis	B-DatasetName
data	O
(	O
subsection	O
6.3	O
)	O
.	O

To	O
select	O
the	O
best	O
result	O
in	O
this	O
list	O
,	O
we	O
used	O
the	O
log	B-MetricName
probability	I-MetricName
of	O
the	O
generated	O
core	O
sentence	O
from	O
the	O
neural	B-TaskName
machine	I-TaskName
translation	I-TaskName
:	O
if	O
it	O
was	O
below	O
a	O
threshold	O
(	O
<	O
−0.25	O
)	O
,	O
we	O
kept	O
the	O
core	O
sentence	O
generated	O
by	O
the	O
classifier	O
,	O
else	O
we	O
kept	O
the	O
NMT	B-TaskName
result	O
.	O

We	O
used	O
OpenNMT	B-DatasetName
(	O
Klein	O
et	O
al	O
.	O
,	O
2017	O
)	O
to	O
train	O
the	O
models	O
.	O

The	O
model	O
was	O
trained	O
with	O
a	O
dropout	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.3	B-HyperparameterValue
and	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
64	B-HyperparameterValue
examples	B-HyperparameterName
.	O

Encoder	O
and	O
decoder	O
were	O
each	O
composed	O
of	O
two	O
LSTM	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
with	O
an	O
attention	O
mechanism	O
on	O
the	O
decoder	O
side	O
(	O
Bahdanau	O
et	O
al	O
.	O
,	O
2014;Luong	O
et	O
al	O
.	O
,	O
2015	O
)	O
.	O

We	O
have	O
trained	O
two	O
different	O
NMT	B-TaskName
models	O
:	O
LSTM	B-MethodName
We	O
trained	O
a	O
neural	B-TaskName
machine	I-TaskName
translation	I-TaskName
model	O
with	O
an	O
embedding	B-HyperparameterName
size	I-HyperparameterName
of	O
512	B-HyperparameterValue
in	O
the	O
encoder	O
and	O
decoder	O
.	O

Machine	B-TaskName
Translation	I-TaskName
.	O

fastText	O
The	O
second	O
approach	O
uses	O
a	O
sequence	O
classification	O
baseline	O
based	O
on	O
bag	O
of	O
tricks	O
(	O
Joulin	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

To	O
do	O
so	O
,	O
we	O
set	O
-	O
up	O
10	B-HyperparameterValue
epochs	B-HyperparameterName
using	O
the	O
Transformer	O
framework	O
for	O
python	O
(	O
Wolf	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

We	O
used	O
the	O
CamemBERT	B-MethodName
pre	O
-	O
trained	O
model	O
(	O
Martin	O
et	O
al	O
.	O
,	O
2019	O
)	O
and	O
added	O
a	O
classification	O
layer	O
on	O
top	O
of	O
the	O
model	O
to	O
fine	O
-	O
tune	O
it	O
with	O
our	O
data	O
(	O
Sun	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

We	O
trained	O
two	O
different	O
neural	O
classifiers	O
:	O
CamemBERT	B-MethodName
This	O
classifier	O
uses	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
for	O
French	O
based	O
on	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
.	O
,	O
2019	O
)	O
,	O
which	O
is	O
used	O
for	O
many	O
NLP	O
tasks	O
.	O

Sequence	B-TaskName
Classification	I-TaskName
.	O

To	O
encode	O
each	O
source	O
sentence	O
,	O
we	O
used	O
an	O
already	O
trained	O
Universal	B-MethodName
Sentence	I-MethodName
Encoder	I-MethodName
4	O
(	O
hereafter	O
uencoder	B-MethodName
)	O
.	O

Universal	O
Sentence	O
Encoder	O
The	O
second	O
approach	O
uses	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
for	O
multilingual	O
encoding	O
(	O
Chidambaram	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

Common	O
pre	O
-	O
processing	O
methods	O
for	O
tf	O
-	O
idf	O
are	O
lemmatizing	O
and	O
removing	O
stop	O
words	O
;	O
however	O
,	O
since	O
accurate	O
preservation	O
of	O
meaning	O
is	O
imperative	O
in	O
a	O
medical	O
dialog	O
context	O
,	O
e.g.	O
in	O
terms	O
of	O
verb	O
tenses	O
,	O
in	O
our	O
experiments	O
words	O
were	O
left	O
as	O
word	O
forms	O
.	O

We	O
employed	O
two	O
approaches	O
to	O
embed	O
each	O
sentence	O
:	O
tf	O
-	O
idf	O
The	O
first	O
approach	O
uses	O
a	O
customised	O
tf	O
-	O
idf	O
(	O
Salton	O
and	O
Buckley	O
,	O
1988	O
)	O
,	O
where	O
tf	O
-	O
idf	O
was	O
applied	O
to	O
subword	O
occurrences	O
(	O
two	O
to	O
four	O
characters	O
)	O
in	O
variations	O
for	O
a	O
given	O
core	O
sentence	O
.	O

This	O
method	O
achieves	O
98	B-MetricValue
%	I-MetricValue
of	O
accuracy	B-MetricName
on	O
ellipsis	O
detection	O
.	O

We	O
could	O
have	O
applied	O
the	O
standard	O
BLEU	B-MetricName
score	O
for	O
the	O
evaluation	O
of	O
the	O
MT	B-TaskName
approaches	O
,	O
but	O
since	O
it	O
is	O
not	O
applicable	O
to	O
the	O
other	O
approaches	O
,	O
it	O
is	O
not	O
appropriate	O
for	O
our	O
comparison	O
.	O

Since	O
the	O
target	O
is	O
a	O
finite	O
set	O
of	O
sentences	O
,	O
we	O
also	O
measured	O
system	O
performance	O
on	O
the	O
test	O
data	O
using	O
three	O
standard	O
metrics	O
for	O
classification	O
:	O
recall	B-MetricName
,	O
precision	B-MetricName
and	O
F1	B-MetricName
.	O

We	O
therefore	O
measured	O
the	O
sentence	B-MetricName
error	I-MetricName
rate	I-MetricName
(	O
SER	B-MetricName
)	O
,	O
defined	O
as	O
the	O
percentage	O
of	O
utterances	O
for	O
which	O
the	O
resulting	O
core	O
sentence	O
is	O
not	O
identical	O
to	O
the	O
annotated	O
correct	O
core	O
sentence	O
.	O

The	O
aim	O
of	O
this	O
study	O
is	O
to	O
evaluate	O
the	O
performance	O
of	O
four	O
different	O
approaches	O
for	O
the	O
ellipsis	B-TaskName
translation	I-TaskName
task	I-TaskName
:	O
indexing	B-TaskName
,	O
classification	B-TaskName
,	O
neural	B-MethodName
machine	I-MethodName
translation	I-MethodName
and	O
hybrid	B-MethodName
.	O

Previous	O
studies	O
have	O
focused	O
on	O
automatic	O
ellipsis	B-TaskName
detection	O
and	O
resolution	O
,	O
but	O
only	O
few	O
specifically	O
address	O
the	O
problem	O
of	O
automatic	O
translation	O
of	O
ellipsis	O
.	O

Ellipsis	O
Translation	O
for	O
a	O
Medical	O
Speech	O
to	O
Speech	O
Translation	O
System	O
In	O
diagnostic	O
interviews	O
,	O
elliptical	O
utterances	O
allow	O
doctors	O
to	O
question	O
patients	O
in	O
a	O
more	O
efficient	O
and	O
economical	O
way	O
.	O

However	O
,	O
only	O
few	O
studies	O
specifically	O
address	O
this	O
problem	O
in	O
machine	B-TaskName
translation	I-TaskName
(	O
MT	B-TaskName
)	O
,	O
despite	O
the	O
recent	O
interest	O
for	O
context	O
modelling	O
in	O
neural	O
machine	O
translation	O
(	O
see	O
for	O
example	O
,	O
Bawden	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

In	O
NLP	O
,	O
different	O
studies	O
have	O
focused	O
on	O
automatic	O
ellipsis	B-TaskName
detection	I-TaskName
and	O
resolution	O
either	O
with	O
rules	O
(	O
patterns	O
or	O
grammars	O
)	O
(	O
for	O
example	O
,	O
the	O
pioneer	O
work	O
from	O
Hardt	O
,	O
1992	O
)	O
or	O
classification	O
techniques	O
(	O
for	O
example	O
,	O
Hardt	O
and	O
Rambow	O
,	O
2001;Bos	O
and	O
Spenader	O
,	O
2011;Liu	O
et	O
al	O
.	O
,	O
2016;Kenyon	O
-	O
Dean	O
et	O
al	O
.	O
,	O
2016;McShane	O
and	O
Babkin	O
,	O
2016;Rønning	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

Like	O
anaphora	O
,	O
ellipsis	O
require	O
context	O
to	O
be	O
understood	O
,	O
but	O
contrary	O
to	O
anaphora	O
,	O
there	O
is	O
no	O
indicator	O
that	O
there	O
is	O
a	O
missing	O
part	O
in	O
the	O
sentence	O
1	O
.	O

This	O
project	O
is	O
funded	O
by	O
the	O
"	O
Fondation	O
Privée	O
des	O
Hôpitaux	O
Universitaires	O
de	O
Genève	O
"	O
.	O

Acknowledgements	O
.	O

Finally	O
,	O
future	O
work	O
will	O
also	O
include	O
the	O
replication	O
of	O
these	O
experiments	O
with	O
data	O
from	O
real	O
diagnostic	O
interviews	O
and	O
with	O
data	O
from	O
other	O
diagnostic	O
domains	O
.	O

In	O
future	O
work	O
,	O
a	O
more	O
task	O
-	O
oriented	O
annotation	O
approach	O
would	O
be	O
interesting	O
.	O

Often	O
,	O
even	O
if	O
the	O
core	O
sentence	O
is	O
not	O
an	O
exact	O
match	O
(	O
e.g.	O
"	O
"	O
in	O
the	O
lower	O
part	O
"	O
vs	O
"	O
in	O
the	O
lower	O
part	O
of	O
the	O
abdomen	O
"	O
)	O
,	O
in	O
context	O
it	O
still	O
allows	O
the	O
doctor	O
to	O
obtain	O
the	O
required	O
information	O
.	O

One	O
limitation	O
of	O
this	O
study	O
is	O
the	O
annotation	O
of	O
the	O
test	O
data	O
.	O

We	O
also	O
observe	O
that	O
the	O
inclusion	O
of	O
ellipsis	O
training	O
data	O
further	O
improves	O
results	O
.	O

Results	O
show	O
that	O
under	O
-	O
sampling	O
the	O
training	O
data	O
improves	O
results	O
for	O
all	O
tested	O
approaches	O
.	O

Conclusion	O
.	O

We	O
also	O
observe	O
many	O
cases	O
where	O
the	O
core	O
sentence	O
was	O
very	O
close	O
to	O
the	O
correct	O
one	O
,	O
but	O
more	O
or	O
less	O
generic	O
.	O

Some	O
of	O
the	O
classification	O
errors	O
were	O
due	O
to	O
ambiguous	O
cases	O
where	O
more	O
than	O
one	O
core	O
sentence	O
would	O
be	O
appropriate	O
for	O
a	O
given	O
elliptical	O
utterance	O
.	O

Closer	O
investigation	O
of	O
the	O
15	O
%	O
of	O
elliptical	O
sentences	O
which	O
were	O
badly	O
classified	O
revealed	O
several	O
cases	O
.	O

We	O
observed	O
that	O
85	O
%	O
of	O
the	O
elliptical	O
sentences	O
were	O
well	O
classified	O
by	O
both	O
models	O
.	O

Table	O
5	O
)	O
.	O

With	O
the	O
additional	O
ellipsis	O
training	O
data	O
,	O
Hybrid	B-MethodName
also	O
outperforms	O
the	O
other	O
approaches	O
(	O
88	O
%	O
of	O
elliptical	O
utterances	O
are	O
translated	O
correctly	O
)	O
,	O
yet	O
the	O
difference	O
is	O
not	O
as	O
large	O
as	O
with	O
plain	O
training	O
data	O
only	O
(	O
cf	O
.	O

To	O
determine	O
if	O
the	O
inclusion	O
of	O
ellipsis	O
data	O
in	O
the	O
training	O
data	O
affects	O
performance	O
,	O
we	O
selected	O
the	O
three	O
best	O
models	O
based	O
on	O
the	O
results	O
described	O
in	O
the	O
previous	O
section	O
and	O
trained	O
them	O
with	O
the	O
ellipsis	O
corpus	O
described	O
in	O
section	O
5.3	O
in	O
addition	O
to	O
the	O
sampled	O
training	O
data	O
.	O

Ellipsis	B-DatasetName
Training	O
Data	O
.	O

subsection	O
6.1	O
)	O
.	O

In	O
order	O
to	O
select	O
the	O
best	O
approach	O
and	O
model	O
to	O
handle	O
ellipsis	O
in	O
this	O
context	O
,	O
we	O
measured	O
the	O
performance	O
of	O
two	O
different	O
models	O
for	O
each	O
approach	O
(	O
cf	O
.	O

Approaches	O
.	O

Under	O
-	O
sampling	O
.	O

We	O
then	O
give	O
results	O
for	O
different	O
models	O
trained	O
with	O
under	O
-	O
sampled	O
data	O
(	O
subsection	O
6.2	O
)	O
.	O

In	O
this	O
section	O
we	O
first	O
describe	O
the	O
evaluation	O
of	O
the	O
under	O
-	O
sampling	O
method	O
(	O
subsection	O
6.1	O
)	O
.	O

Results	O
.	O

The	O
same	O
concatenation	O
was	O
performed	O
on	O
the	O
test	O
data	O
.	O

To	O
train	O
the	O
models	O
,	O
we	O
transformed	O
the	O
elliptical	O
source	O
variations	O
by	O
concatenating	O
them	O
with	O
the	O
context	O
source	O
variation	O
.	O

Each	O
of	O
these	O
elliptical	O
variation	O
-	O
core	O
pairs	O
follows	O
a	O
matching	O
complete	O
variation	O
-	O
core	O
pair	O
which	O
serves	O
as	O
context	O
,	O
as	O
shown	O
in	O
Table	O
3	O
.	O

To	O
produce	O
elliptical	O
utterances	O
,	O
we	O
have	O
kept	O
only	O
the	O
value	O
of	O
the	O
variable	O
as	O
source	O
variation	O
,	O
associated	O
with	O
a	O
corresponding	O
complete	O
core	O
sentence	O
.	O

]	O
recently	O
?	O
"	O
)	O
.	O

These	O
variables	O
are	O
placeholders	O
that	O
are	O
replaced	O
by	O
different	O
values	O
at	O
system	O
-	O
compile	O
time	O
,	O
e.g.	O
"	O
avez	O
-	O
vous	O
pris	O
[	O
des	O
anti	O
-	O
douleurs|des	O
medicaments	O
contre	O
l'acidité|	O
...	O
]	O
récemment	O
?	O
"	O
(	O
"	O
Did	O
you	O
take	O
[	O
painkillers|antacids|	O
.	O

To	O
generate	O
training	O
data	O
for	O
ellipsis	O
in	O
context	O
,	O
we	O
exploit	O
grammar	O
rules	O
that	O
contain	O
variables	O
.	O

Ellipsis	O
Corpus	O
.	O

For	O
example	O
,	O
"	O
avez	O
-	O
vous	O
mal	O
au	O
ventre	O
en	O
position	O
de	O
chien	O
de	O
fusil	O
?	O
"	O
(	O
do	O
you	O
have	O
abdominal	O
pain	O
in	O
a	O
fetal	O
position	O
?	O
)	O
still	O
had	O
731	O
variations	O
whereas	O
"	O
combien	O
de	O
kilos	O
avezvous	O
pris	O
?	O
"	O
(	O
how	O
much	O
weight	O
did	O
you	O
gain	O
?	O
)	O
had	O
only	O
1	O
.	O

Even	O
though	O
we	O
managed	O
to	O
reduce	O
most	O
of	O
the	O
categories	O
,	O
minority	O
classes	O
were	O
still	O
under	O
-	O
represented	O
compared	O
to	O
the	O
majority	O
classes	O
.	O

Furthermore	O
,	O
75	O
%	O
of	O
the	O
core	O
sentences	O
were	O
mapped	O
to	O
less	O
than	O
32	O
variations	O
.	O

After	O
under	O
-	O
sampling	O
,	O
the	O
resulting	O
corpus	O
contained	O
159'902	O
variations	O
and	O
87	O
ambiguous	O
samples	O
.	O

Build	O
a	O
new	O
list	O
of	O
variations	O
by	O
iteratively	O
extracting	O
variations	O
from	O
a	O
list	O
in	O
randomised	O
order	O
until	O
all	O
bigrams	O
are	O
covered	O
.	O

2	O
.	O

For	O
each	O
core	O
sentence	O
,	O
extract	O
all	O
bigrams	O
present	O
in	O
the	O
associated	O
variations	O
.	O

To	O
reduce	O
the	O
number	O
of	O
variations	O
by	O
core	O
sentence	O
while	O
keeping	O
data	O
as	O
representative	O
as	O
possible	O
,	O
we	O
propose	O
a	O
new	O
algorithm	O
for	O
undersampling	O
based	O
on	O
bigrams	O
consisting	O
in	O
the	O
following	O
steps	O
:	O
1	O
.	O

We	O
applied	O
under	O
-	O
sampling	O
,	O
which	O
is	O
suggested	O
as	O
the	O
best	O
alternative	O
when	O
the	O
training	O
sample	O
size	O
is	O
too	O
large	O
(	O
Mazurowski	O
et	O
al	O
.	O
,	O
2008;Haixiang	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

Therefore	O
,	O
we	O
used	O
resampling	O
techniques	O
to	O
rebalance	O
the	O
sample	O
space	O
in	O
order	O
to	O
alleviate	O
the	O
effect	O
of	O
the	O
skewed	O
class	O
distribution	O
on	O
the	O
learning	O
process	O
.	O

In	O
this	O
context	O
,	O
where	O
all	O
core	O
sentences	O
are	O
relevant	O
for	O
the	O
task	O
,	O
the	O
exclusion	O
or	O
misclassification	O
/	O
translation	O
of	O
minority	O
categories	O
(	O
in	O
our	O
case	O
,	O
core	O
sentences	O
)	O
on	O
the	O
dataset	O
could	O
lead	O
to	O
a	O
heavy	O
cost	O
(	O
Haixiang	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

As	O
mentioned	O
in	O
the	O
previous	O
section	O
,	O
our	O
main	O
corpus	O
is	O
highly	O
imbalanced	O
.	O

Sampled	O
Data	O
.	O

Since	O
we	O
are	O
interested	O
in	O
evaluating	O
the	O
complete	O
set	O
of	O
core	O
sentences	O
,	O
we	O
have	O
maintained	O
the	O
same	O
distribution	O
when	O
splitting	O
the	O
data	O
into	O
development	O
and	O
training	O
.	O

For	O
example	O
,	O
the	O
core	O
sentence	O
"	O
avez	O
-	O
vous	O
pris	O
des	O
médicaments	O
contre	O
la	O
douleur	O
?	O
"	O
(	O
have	O
you	O
taken	O
any	O
painkillers	O
?	O
)	O
is	O
mapped	O
to	O
3'496'503	O
source	O
variations	O
(	O
14	O
%	O
of	O
the	O
entire	O
dataset	O
)	O
whereas	O
"	O
avez	O
-	O
vous	O
de	O
l'oxygène	O
à	O
la	O
maison	O
?	O
"	O
(	O
do	O
you	O
have	O
oxygen	O
at	O
home	O
?	O
)	O
is	O
only	O
mapped	O
to	O
5	O
source	O
variations	O
.	O

These	O
core	O
sentences	O
are	O
not	O
represented	O
equally	O
in	O
the	O
corpus	O
:	O
50	O
%	O
of	O
the	O
4'132	O
core	O
sentences	O
occur	O
less	O
than	O
52	O
times	O
in	O
the	O
data	O
.	O

The	O
variations	O
are	O
mapped	O
to	O
the	O
4'132	O
different	O
core	O
sentences	O
available	O
for	O
the	O
abdominal	O
domain	O
.	O

Table	O
2	O
shows	O
two	O
examples	O
of	O
such	O
sentences	O
.	O

Most	O
of	O
these	O
ambiguous	O
sentences	O
are	O
elliptical	O
.	O

The	O
main	O
data	O
set	O
includes	O
23	O
M	O
variations	O
,	O
of	O
which	O
321'698	O
are	O
ambiguous	O
(	O
i.e.	O
sentences	O
that	O
can	O
be	O
mapped	O
to	O
more	O
than	O
one	O
core	O
sentence	O
)	O
.	O

All	O
Data	O
.	O

Table	O
1	O
summarises	O
the	O
number	O
of	O
sentences	O
,	O
words	O
and	O
vocabulary	O
for	O
each	O
set	O
.	O

All	O
data	O
were	O
generated	O
6	O
https://opennmt.net/OpenNMT-tf/model.html	O
from	O
a	O
recent	O
version	O
of	O
the	O
BabelDr	B-MethodName
SCFG	O
for	O
the	O
abdominal	O
diagnostic	O
domain	O
and	O
consist	O
of	O
variation	O
-	O
core	O
pairs	O
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
the	O
training	O
data	O
sets	O
used	O
for	O
this	O
study	O
.	O

Training	O
Data	O
.	O

The	O
threshold	O
was	O
set	O
based	O
on	O
the	O
observation	O
that	O
93	O
%	O
of	O
the	O
sentences	O
above	O
that	O
threshold	O
were	O
mistranslated	O
.	O

The	O
hybrid	O
approach	O
combines	O
the	O
best	O
neural	O
machine	O
translation	O
model	O
with	O
the	O
best	O
classification	O
model	O
to	O
build	O
an	O
N	O
-	O
best	O
list	O
of	O
sentences	O
,	O
in	O
this	O
experiment	O
a	O
2	O
-	O
best	O
list	O
which	O
includes	O
the	O
core	O
sentence	O
generated	O
by	O
machine	O
translation	O
and	O
one	O
sentence	O
from	O
the	O
classification	O
results	O
.	O

Hybrid	O
.	O

For	O
both	O
architectures	O
,	O
early	O
stopping	O
was	O
used	O
to	O
reduce	O
the	O
number	O
of	O
training	O
steps	O
by	O
monitoring	O
the	O
performance	O
on	O
the	O
development	O
set	O
.	O

Transformer	B-MethodName
The	O
second	O
model	O
relies	O
on	O
a	O
transformer	O
based	O
architecture	O
for	O
machine	O
translation	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O
2017	O
)	O
with	O
default	O
parameters	O
and	O
size	O
6	O
.	O

This	O
system	O
is	O
described	O
in	O
detail	O
in	O
(	O
Mutal	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

With	O
these	O
approaches	O
,	O
the	O
task	O
is	O
to	O
translate	O
the	O
source	O
utterance	O
into	O
a	O
core	O
sentence	O
.	O

The	O
other	O
hyper	O
parameters	O
were	O
set	O
by	O
default	O
5	O
.	O

We	O
used	O
fastText	O
on	O
bigrams	O
with	O
100	B-HyperparameterValue
epochs	B-HyperparameterName
and	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0,2	B-HyperparameterValue
.	O

In	O
this	O
approach	O
,	O
the	O
task	O
is	O
to	O
classify	O
each	O
variation	O
into	O
a	O
core	O
sentence	O
using	O
a	O
distance	O
based	O
classification	O
method	O
(	O
Xing	O
et	O
al	O
.	O
,	O
2010	O
)	O
.	O

We	O
then	O
used	O
the	O
approximate	O
nearest	O
neighbor	O
search	O
(	O
Andoni	O
and	O
Indyk	O
,	O
2006	O
)	O
to	O
extract	O
the	O
closest	O
variation	O
sentence	O
with	O
cosine	O
similarity	O
,	O
and	O
return	O
the	O
corresponding	O
core	O
sentence	O
.	O

To	O
do	O
so	O
,	O
each	O
sentence	O
was	O
represented	O
by	O
a	O
vector	O
and	O
a	O
similarity	O
metric	O
was	O
used	O
to	O
compare	O
them	O
.	O

In	O
this	O
approach	O
,	O
the	O
task	O
is	O
to	O
find	O
the	O
source	O
variations	O
that	O
are	O
the	O
closest	O
matches	O
for	O
a	O
new	O
utterance	O
.	O

Indexing	O
.	O

Each	O
approach	O
has	O
its	O
own	O
built	O
-	O
in	O
tokenization	O
method	O
to	O
reach	O
optimal	O
results	O
,	O
except	O
for	O
machine	B-TaskName
translation	I-TaskName
where	O
we	O
applied	O
BPE	O
.	O

The	O
source	O
sentences	O
were	O
preprocessed	O
using	O
the	O
same	O
method	O
for	O
all	O
the	O
models	O
:	O
they	O
have	O
been	O
lower	O
cased	O
and	O
tokenized	O
.	O

The	O
same	O
training	O
data	O
(	O
described	O
in	O
Section	O
5	O
)	O
was	O
used	O
for	O
all	O
approaches	O
.	O

In	O
the	O
following	O
sections	O
,	O
we	O
describe	O
the	O
four	O
approaches	O
applied	O
after	O
concatenation	O
.	O

This	O
concatenated	O
sentence	O
is	O
then	O
processed	O
like	O
other	O
utterances	O
.	O

If	O
the	O
utterance	O
is	O
identified	O
as	O
an	O
ellipsis	O
,	O
it	O
is	O
concatenated	O
with	O
the	O
previous	O
utterance	O
from	O
the	O
dialog	O
(	O
Tiedemann	O
and	O
Scherrer	O
,	O
2017	O
)	O
.	O

Therefore	O
,	O
the	O
sentence	O
length	O
,	O
the	O
first	O
word	O
of	O
the	O
sentence	O
and	O
its	O
part	O
-	O
of	O
-	O
speech	O
are	O
used	O
as	O
features	O
to	O
train	O
the	O
classifier	O
.	O

In	O
this	O
context	O
,	O
elliptical	O
sentences	O
can	O
easily	O
be	O
detected	O
by	O
sentence	O
length	O
and	O
syntactic	O
structure	O
.	O

As	O
mentioned	O
earlier	O
,	O
our	O
objective	O
is	O
to	O
use	O
the	O
context	O
(	O
previous	O
utterance	O
)	O
to	O
map	O
elliptical	O
utterances	O
to	O
the	O
closest	O
core	O
sentence	O
.	O

Approaches	O
.	O

The	O
metrics	O
were	O
calculated	O
using	O
a	O
module	O
in	O
Sklearn	O
3	O
.	O

The	O
macroaverage	O
better	O
reflects	O
the	O
statistics	O
of	O
the	O
smaller	O
classes	O
and	O
therefore	O
is	O
more	O
appropriate	O
when	O
all	O
classes	O
are	O
equally	O
important	O
(	O
Jurafsky	O
and	O
Martin	O
,	O
2014	O
)	O
.	O

As	O
the	O
test	O
data	O
is	O
not	O
perfectly	O
balanced	O
,	O
we	O
computed	O
the	O
performance	O
for	O
each	O
class	O
,	O
and	O
then	O
averaged	O
over	O
the	O
number	O
of	O
classes	O
,	O
i.e.	O
by	O
macro	O
-	O
averaging	O
.	O

Section	O
2	O
)	O
,	O
a	O
correct	O
core	O
sentence	O
is	O
equivalent	O
to	O
a	O
correct	O
translation	O
.	O

Since	O
the	O
system	O
relies	O
on	O
human	O
pre	O
-	O
translation	O
(	O
cf	O
.	O

We	O
want	O
to	O
compare	O
the	O
different	O
approaches	O
at	O
the	O
task	O
level	O
,	O
namely	O
how	O
many	O
elliptical	O
utterances	O
will	O
result	O
in	O
a	O
correct	O
translation	O
for	O
the	O
patient	O
.	O

Evaluation	O
.	O

Since	O
the	O
focus	O
of	O
this	O
study	O
is	O
not	O
on	O
speech	B-TaskName
recognition	I-TaskName
performance	O
,	O
but	O
on	O
the	O
subsequent	O
processing	O
,	O
we	O
performed	O
our	O
experiments	O
with	O
the	O
transcriptions	O
as	O
input	O
rather	O
than	O
the	O
speech	O
recogniser	O
output	O
,	O
thereby	O
assuming	O
recognition	O
is	O
perfect	O
.	O

The	O
average	O
utterance	O
length	O
was	O
8.96	B-MetricValue
words	O
for	O
the	O
plain	O
utterances	O
and	O
3.14	O
words	O
for	O
the	O
elliptical	O
utterances	O
(	O
Rayner	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

This	O
process	O
finally	O
produced	O
838	O
recorded	O
pairs	O
,	O
with	O
the	O
corresponding	O
core	O
sentences	O
.	O

If	O
the	O
second	O
sentence	O
of	O
the	O
pair	O
was	O
not	O
elliptical	O
because	O
subjects	O
did	O
not	O
follow	O
instructions	O
,	O
they	O
were	O
removed	O
from	O
the	O
test	O
suite	O
.	O

Each	O
utterance	O
was	O
then	O
transcribed	O
and	O
matched	O
to	O
the	O
most	O
plausible	O
core	O
sentence	O
by	O
two	O
judges	O
and	O
when	O
necessary	O
disagreement	O
between	O
judges	O
resolved	O
.	O

This	O
produced	O
a	O
total	O
of	O
1'676	O
recorded	O
utterances	O
.	O

Data	O
were	O
collected	O
using	O
a	O
web	O
tool	O
which	O
prompted	O
the	O
subjects	O
and	O
recorded	O
their	O
responses	O
.	O

Five	O
native	O
francophone	O
subjects	O
were	O
then	O
asked	O
to	O
speak	O
the	O
pairs	O
(	O
context	O
and	O
elliptical	O
utterance	O
)	O
in	O
a	O
natural	O
way	O
,	O
freely	O
varying	O
the	O
wording	O
,	O
but	O
with	O
the	O
instruction	O
to	O
respect	O
the	O
distinction	O
between	O
elliptical	O
and	O
plain	O
utterances	O
.	O

This	O
was	O
created	O
by	O
extracting	O
the	O
list	O
of	O
available	O
core	O
sentences	O
for	O
the	O
abdominal	O
domain	O
and	O
transforming	O
complete	O
sentences	O
into	O
elliptical	O
sentences	O
where	O
possible	O
,	O
for	O
example	O
:	O
avez	O
-	O
vous	O
mal	O
au	O
ventre	O
avez	O
-	O
vous	O
mal	O
dans	O
le	O
bas	O
-	O
ventre	O
--	O
>	O
dans	O
le	O
bas	O
-	O
ventre	O
avez	O
-	O
vous	O
mal	O
dans	O
le	O
haut	O
du	O
ventre	O
--	O
>	O
le	O
haut	O
du	O
ventre	O
Each	O
elliptical	O
utterance	O
was	O
associated	O
with	O
a	O
corresponding	O
complete	O
utterance	O
to	O
serve	O
as	O
context	O
.	O

Consequently	O
,	O
real	O
usage	O
data	O
contains	O
very	O
few	O
elliptical	O
utterances	O
.	O

Test	O
data	O
.	O

3	O
)	O
Does	O
inclusion	O
of	O
ellipsis	O
-	O
specific	O
training	O
data	O
improve	O
performance	O
?	O

2	O
)	O
How	O
does	O
the	O
distribution	O
of	O
class	O
instances	O
affect	O
the	O
performance	O
of	O
the	O
proposed	O
models	O
?	O

The	O
research	O
questions	O
guiding	O
our	O
experiments	O
are	O
listed	O
as	O
follows	O
1	O
)	O
What	O
is	O
the	O
best	O
approach	O
to	O
handle	O
ellipsis	O
in	O
this	O
context	O
?	O

Objective	O
and	O
research	O
question	O
.	O

Methodology	O
.	O

The	O
proposed	O
ellipsis	O
processing	O
workflow	O
is	O
illustrated	O
in	O
Figure	O
1	O
and	O
will	O
be	O
discussed	O
in	O
further	O
detail	O
in	O
Section	O
4	O
.	O

To	O
resolve	O
the	O
ellipsis	O
,	O
we	O
use	O
context	O
information	O
,	O
which	O
in	O
a	O
diagnostic	O
interview	O
is	O
the	O
previous	O
translated	O
utterance	O
.	O

This	O
presents	O
the	O
advantage	O
of	O
removing	O
all	O
ambiguity	O
related	O
to	O
ellipsis	O
and	O
their	O
translation	O
.	O

The	O
latter	O
is	O
one	O
of	O
the	O
approaches	O
evaluated	O
in	O
the	O
present	O
study	O
,	O
where	O
it	O
has	O
been	O
extended	O
to	O
handle	O
elliptical	O
utterances	O
.	O

Results	O
from	O
this	O
recogniser	O
must	O
then	O
be	O
mapped	O
to	O
the	O
closest	O
core	O
sentences	O
,	O
a	O
task	O
to	O
which	O
several	O
approaches	O
have	O
been	O
applied	O
,	O
including	O
tf	B-MethodName
-	I-MethodName
idf	I-MethodName
indexing	I-MethodName
and	O
dynamic	O
programming	O
(	O
DP	O
,	O
Rayner	O
et	O
al	O
.	O
,	O
2017	O
)	O
and	O
,	O
more	O
recently	O
,	O
a	O
NMT	B-TaskName
approach	O
(	O
Mutal	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

This	O
grammar	O
based	O
speech	O
recognition	I-TaskName
produces	O
high	O
quality	O
results	O
for	O
in	O
coverage	O
items	O
.	O

A	O
Synchronous	B-MethodName
Context	I-MethodName
Free	I-MethodName
Grammar	I-MethodName
(	O
SCFG	O
,	O
Aho	O
and	O
Ullman	O
,	O
1969	O
)	O
which	O
describes	O
source	O
language	O
variation	O
patterns	O
and	O
their	O
mapping	O
to	O
core	O
sentences	O
is	O
used	O
to	O
compile	O
a	O
language	O
model	O
used	O
by	O
Nuance	O
for	O
speech	B-TaskName
recognition	I-TaskName
.	O

The	O
scarcity	O
of	O
training	O
data	O
available	O
for	O
this	O
domain	O
,	O
a	O
consequence	O
of	O
data	O
confidentiality	O
issues	O
and	O
of	O
the	O
minority	O
languages	O
involved	O
(	O
e.g.	O
,	O
Tigrinya	O
,	O
Farsi	O
,	O
Albanian	O
)	O
,	O
has	O
at	O
first	O
led	O
to	O
the	O
development	O
of	O
a	O
grammar	O
-	O
based	O
approach	O
.	O

This	O
ensures	O
the	O
reliability	O
of	O
speech	B-TaskName
recognition	I-TaskName
and	O
of	O
translation	O
,	O
essential	O
for	O
safe	O
use	O
in	O
the	O
medical	O
domain	O
.	O

Doctors	O
can	O
freely	O
speak	O
their	O
questions	O
,	O
the	O
system	O
maps	O
the	O
recognised	O
utterance	O
(	O
hereafter	O
:	O
variation	O
)	O
to	O
the	O
closest	O
pre	O
-	O
translated	O
sentence	O
(	O
hereafter	O
:	O
core	O
sentence	O
)	O
,	O
and	O
,	O
after	O
approval	O
by	O
the	O
doctor	O
,	O
the	O
core	O
sentence	O
is	O
translated	O
for	O
the	O
patient	O
.	O

It	O
combines	O
speech	O
recognition	O
with	O
manually	O
pre	O
-	O
translated	O
sentences	O
,	O
grouped	O
by	O
diagnostic	O
domains	O
.	O

Section	O
6	O
presents	O
the	O
results	O
and	O
Section	O
7	O
concludes	O
.	O

Section	O
4	O
presents	O
the	O
approaches	O
and	O
models	O
,	O
followed	O
by	O
Section	O
5	O
which	O
describes	O
the	O
different	O
sets	O
of	O
training	O
data	O
.	O

Section	O
3	O
outlines	O
the	O
methodology	O
,	O
including	O
the	O
objective	O
and	O
research	O
questions	O
,	O
the	O
test	O
data	O
and	O
the	O
evaluation	O
metrics	O
.	O

-	O
>	O
MT	O
:	O
*	O
chuuteido	O
?	O

Source	O
:	O
moderate	O
?	O

Source	O
:	O
is	O
the	O
pain	O
severe	O
-	O
>	O
MT	O
:	O
hageshii	O
itami	O
desu	O
ka	O
?	O

-	O
>	O
MT	O
:	O
*	O
en	O
la	O
cabeza	O
?	O

Source	O
:	O
in	O
your	O
head	O
?	O

-	O
>	O
MT	O
:	O
le	O
duele	O
el	O
estómago	O
?	O

-	O
>	O
MT	O
:	O
*	O
soudain	O
Source	O
:	O
do	O
you	O
have	O
pain	O
in	O
your	O
stomach	O
?	O

->MT	O
:	O
la	O
douleur	O
est	O
-	O
elle	O
intense	O
Source	O
:	O
sudden	O
?	O

Source	O
:	O
is	O
the	O
pain	O
intense	O
?	O

The	O
following	O
examples	O
illustrate	O
elliptical	O
utterances	O
where	O
literal	O
translation	O
is	O
problematic	O
,	O
as	O
it	O
produces	O
agreement	O
errors	O
,	O
wrong	O
prepositions	O
or	O
other	O
syntactical	O
or	O
grammatical	O
issues	O
that	O
can	O
make	O
the	O
elliptical	O
utterance	O
difficult	O
to	O
understand	O
.	O

For	O
example	O
in	O
Japanese	O
,	O
adjectival	O
ellipsis	O
are	O
very	O
informal	O
and	O
should	O
be	O
translated	O
by	O
complete	O
sentences	O
(	O
Bouillon	O
et	O
al	O
.	O
,	O
2007	O
)	O
.	O

Literal	O
translation	O
of	O
these	O
elliptical	O
utterances	O
is	O
rarely	O
possible	O
without	O
affecting	O
communication	O
,	O
in	O
particular	O
with	O
structurally	O
different	O
languages	O
which	O
do	O
not	O
share	O
the	O
same	O
type	O
of	O
ellipsis	O
.	O

Moderate	O
?	O
)	O
(	O
Tanguy	O
et	O
al	O
.	O
,	O
2011	O
)	O
.	O

Is	O
the	O
pain	O
severe	O
?	O

In	O
the	O
back	O
?	O

In	O
the	O
medical	O
dialogues	O
we	O
are	O
interested	O
in	O
,	O
ellipsis	O
allows	O
doctors	O
to	O
question	O
patients	O
in	O
a	O
more	O
efficient	O
way	O
(	O
Where	O
is	O
your	O
pain	O
?	O

Elliptical	O
utterances	O
are	O
very	O
common	O
in	O
dialogues	O
,	O
since	O
they	O
ensure	O
the	O
principle	O
of	O
economy	O
and	O
provide	O
a	O
way	O
to	O
avoid	O
duplication	O
(	O
Hamza	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

Very	O
recently	O
,	O
some	O
qualitative	O
studies	O
showed	O
the	O
negative	O
impact	O
of	O
ellipsis	O
on	O
generalist	O
neural	O
systems	O
(	O
DeepL	O
,	O
Google	O
Translate	O
,	O
etc	O
.	O
)	O
from	O
a	O
translation	O
point	O
of	O
view	O
in	O
the	O
English	O
-	O
French	O
pair	O
(	O
for	O
example	O
,	O
Hamza	O
,	O
2019	O
)	O
.	O

More	O
specifically	O
,	O
the	O
implicit	O
semantic	O
context	O
is	O
recovered	O
from	O
elements	O
of	O
linguistic	O
and	O
extralinguistic	O
context	O
"	O
(	O
Ginzburg	O
and	O
Miller	O
,	O
2018	O
)	O
.	O

The	O
syntax	O
thus	O
appears	O
to	O
be	O
incomplete	O
.	O

The	O
characterising	O
feature	O
of	O
ellipsis	O
is	O
that	O
"	O
elements	O
of	O
semantic	O
content	O
are	O
obtained	O
in	O
the	O
absence	O
of	O
any	O
corresponding	O
form	O
.	O

Ellipsis	O
is	O
one	O
of	O
the	O
least	O
studied	O
discursive	O
phenomena	O
in	O
automatic	O
translation	O
.	O

Introduction	O
.	O

Results	O
show	O
that	O
the	O
best	O
model	O
is	O
able	O
to	O
translate	O
88	B-MetricValue
%	O
of	O
elliptical	O
utterances	O
correctly	O
.	O

However	O
,	O
literal	O
translation	O
of	O
such	O
incomplete	O
utterances	O
is	O
rarely	O
possible	O
without	O
affecting	O
communication	O
.	O

