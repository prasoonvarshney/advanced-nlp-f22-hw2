[{"id":331,"annotations":[{"id":228,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:04:48.589868Z","updated_at":"2022-10-18T21:46:15.760976Z","lead_time":9.908999999999999,"prediction":{},"result_count":0,"task":331,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The advantage of this procedure is that the Transformer encoder does not know which words it will be asked to predict or which have been replaced by random words , so it is forced to keep a distributional contextual representation of every input token ."},"meta":{},"created_at":"2022-10-17T21:25:49.773360Z","updated_at":"2022-10-18T21:46:15.777140Z","inner_id":329,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":321,"annotations":[{"id":329,"completed_by":1,"result":[{"value":{"start":130,"end":134,"text":"BERT","labels":["MethodName"]},"id":"VwmCNSxhhZ","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":140,"end":144,"text":"80 %","labels":["HyperparameterValue"]},"id":"HCAgyRiUBA","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":147,"end":152,"text":"10 % ","labels":["HyperparameterValue"]},"id":"JWG32Ff83b","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":154,"end":159,"text":"10 % ","labels":["HyperparameterValue"]},"id":"Ng5lGxIyzF","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:20:17.390158Z","updated_at":"2022-10-18T21:45:25.435692Z","lead_time":46.989000000000004,"prediction":{},"result_count":0,"task":321,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The numbers in the left part of the table represent the probabilities of the specific strategies used during MLM pre - training ( BERT uses 80 % , 10 % , 10 % ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.772980Z","updated_at":"2022-10-18T21:45:25.452319Z","inner_id":319,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":309,"annotations":[{"id":317,"completed_by":1,"result":[{"value":{"start":4,"end":8,"text":"GLUE","labels":["DatasetName"]},"id":"jyvbx362T7","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":139,"end":143,"text":"GLUE","labels":["DatasetName"]},"id":"8oJbKeHEZj","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":173,"end":177,"text":"65.1","labels":["MetricValue"]},"id":"KeZcy7zHfS","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":187,"end":195,"text":"accuracy","labels":["MetricName"]},"id":"65owWaSgUU","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:19:05.307289Z","updated_at":"2022-10-18T21:44:22.086300Z","lead_time":23.765,"prediction":{},"result_count":0,"task":309,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The GLUE webpage notes that there are issues with the construction of this dataset , 15 and every trained system that 's been submitted to GLUE has performed worse than the 65.1 baseline accuracy of predicting the majority class ."},"meta":{},"created_at":"2022-10-17T21:25:49.772535Z","updated_at":"2022-10-18T21:44:22.101139Z","inner_id":307,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":310,"annotations":[{"id":334,"completed_by":1,"result":[{"value":{"start":44,"end":54,"text":"OpenAI GPT","labels":["MethodName"]},"id":"EPHFSmVcXP","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T21:43:57.340697Z","updated_at":"2022-10-18T21:43:57.340722Z","lead_time":6.325,"prediction":{},"result_count":0,"task":310,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"We therefore exclude this set to be fair to OpenAI GPT ."},"meta":{},"created_at":"2022-10-17T21:25:49.772572Z","updated_at":"2022-10-18T21:43:57.360903Z","inner_id":308,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":302,"annotations":[{"id":310,"completed_by":1,"result":[{"value":{"start":0,"end":4,"text":"CoLA","labels":["DatasetName"]},"id":"0SPCy8meUA","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":9,"end":43,"text":"Corpus of Linguistic Acceptability","labels":["DatasetName"]},"id":"76Dg8wfgI-","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":49,"end":88,"text":"binary single - sentence classification","labels":["TaskName"]},"id":"PQXSHqhuSo","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:17:48.052890Z","updated_at":"2022-10-18T21:43:42.851731Z","lead_time":34.257999999999996,"prediction":{},"result_count":0,"task":302,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"CoLA The Corpus of Linguistic Acceptability is a binary single - sentence classification task , where the goal is to predict whether an English sentence is linguistically \" acceptable \" or not ( Warstadt et al . , 2018 ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.772275Z","updated_at":"2022-10-18T21:43:42.868642Z","inner_id":300,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":295,"annotations":[{"id":303,"completed_by":1,"result":[{"value":{"start":0,"end":4,"text":"BERT","labels":["MethodName"]},"id":"bOZYCZMk4z","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:16:54.447355Z","updated_at":"2022-10-18T21:42:24.292129Z","lead_time":7.196,"prediction":{},"result_count":0,"task":295,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"BERT Tok 1 Tok 2 Tok N ..."},"meta":{},"created_at":"2022-10-17T21:25:49.772013Z","updated_at":"2022-10-18T21:42:24.308055Z","inner_id":293,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":293,"annotations":[{"id":301,"completed_by":1,"result":[{"value":{"start":25,"end":29,"text":"BERT","labels":["MethodName"]},"id":"pLIMrm8aZh","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:16:47.864544Z","updated_at":"2022-10-18T21:42:15.571546Z","lead_time":9.254999999999999,"prediction":{},"result_count":0,"task":293,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Tok M Question Paragraph BERT E [ CLS ] E 1 E 2 E N C T 1 T 2 T N Single Sentence ..."},"meta":{},"created_at":"2022-10-17T21:25:49.771917Z","updated_at":"2022-10-18T21:42:15.590314Z","inner_id":291,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":286,"annotations":[{"id":295,"completed_by":1,"result":[{"value":{"start":30,"end":51,"text":"binary classification","labels":["TaskName"]},"id":"XilxLfSx8U","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":0,"end":3,"text":"QQP","labels":["DatasetName"]},"id":"lmAWiOTuqi","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":4,"end":24,"text":"Quora Question Pairs","labels":["DatasetName"]},"id":"luqDJlyj7w","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:15:46.099462Z","updated_at":"2022-10-18T21:41:54.784084Z","lead_time":40.342000000000006,"prediction":{},"result_count":0,"task":286,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"QQP Quora Question Pairs is a binary classification task where the goal is to determine if two questions asked on Quora are semantically equivalent ( Chen et al . , 2018 ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.769638Z","updated_at":"2022-10-18T21:41:54.801554Z","inner_id":284,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":284,"annotations":[{"id":293,"completed_by":1,"result":[{"value":{"start":11,"end":15,"text":"MNLI","labels":["DatasetName"]},"id":"kxmYTRHYS3","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":16,"end":56,"text":"Multi - Genre Natural Language Inference","labels":["DatasetName"]},"id":"1CEcH3moHm","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":91,"end":116,"text":"entailment classification","labels":["TaskName"]},"id":"wkq9dAUVdj","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:15:21.517987Z","updated_at":"2022-10-18T21:40:40.179818Z","lead_time":35.394999999999996,"prediction":{},"result_count":0,"task":284,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"( 2018a ): MNLI Multi - Genre Natural Language Inference is a large - scale , crowdsourced entailment classification task ( Williams et al . , 2018 ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.769564Z","updated_at":"2022-10-18T21:40:40.197352Z","inner_id":282,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":277,"annotations":[{"id":286,"completed_by":1,"result":[{"value":{"start":2,"end":5,"text":"GPT","labels":["MethodName"]},"id":"Pr72chy6VL","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":26,"end":31,"text":"steps","labels":["HyperparameterName"]},"id":"iEeF7DIcZN","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":39,"end":49,"text":"batch size","labels":["HyperparameterName"]},"id":"gea1R7rzPC","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":93,"end":98,"text":"steps","labels":["HyperparameterName"]},"id":"Ngk5JHjAjN","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":106,"end":116,"text":"batch size","labels":["HyperparameterName"]},"id":"onPSM7kGo4","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":68,"end":72,"text":"BERT","labels":["MethodName"]},"id":"CC9qYj7E3o","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":53,"end":65,"text":"32,000 words","labels":["HyperparameterValue"]},"id":"qax1oF7JA8","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":22,"end":25,"text":"1 M","labels":["HyperparameterValue"]},"id":"vmGz7OhZNC","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":89,"end":92,"text":"1 M","labels":["HyperparameterValue"]},"id":"R7zspfhvDP","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":120,"end":133,"text":"128,000 words","labels":["HyperparameterValue"]},"id":"9SOIhmks3U","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:14:08.370602Z","updated_at":"2022-10-18T21:40:09.333526Z","lead_time":79.436,"prediction":{},"result_count":0,"task":277,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"• GPT was trained for 1 M steps with a batch size of 32,000 words ; BERT was trained for 1 M steps with a batch size of 128,000 words ."},"meta":{},"created_at":"2022-10-17T21:25:49.769303Z","updated_at":"2022-10-18T21:40:09.350568Z","inner_id":275,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":118,"annotations":[{"id":117,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:25:00.161303Z","updated_at":"2022-10-18T21:38:55.602456Z","lead_time":12.518,"prediction":{},"result_count":0,"task":118,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"6 he likes play # # ing [ SEP ] my dog is cute [ SEP ] Input E [ CLS ] E he E likes E play E # # ing E [ SEP ] E my E dog E is E cute E [ SEP ] Token Embeddings E A E B E B E B E B E B E A E A E A E A E A Segment Embeddings E 0 E 6 E 7 E 8 E 9 E 10 E 1 E 2 E 3 E 4 E 5 Position Embeddings The NSP task is closely related to representationlearning objectives used in Jernite et al ."},"meta":{},"created_at":"2022-10-17T21:25:49.756264Z","updated_at":"2022-10-18T21:38:55.618467Z","inner_id":116,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":116,"annotations":[{"id":115,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:24:41.467428Z","updated_at":"2022-10-18T21:38:31.034573Z","lead_time":19.852,"prediction":{},"result_count":0,"task":116,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"As we show in Figure 1 , C is used for next sentence prediction ( NSP ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.756189Z","updated_at":"2022-10-18T21:38:31.053454Z","inner_id":114,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":113,"annotations":[{"id":112,"completed_by":1,"result":[{"value":{"start":142,"end":160,"text":"Question Answering","labels":["TaskName"]},"id":"hJjvolQSWL","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":163,"end":165,"text":"QA","labels":["TaskName"]},"id":"oP01pHcTvu","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":172,"end":198,"text":"Natural Language Inference","labels":["TaskName"]},"id":"GdyF6jLgDV","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":201,"end":204,"text":"NLI","labels":["TaskName"]},"id":"oEVpX5Svmj","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:24:12.261421Z","updated_at":"2022-10-18T21:38:11.958579Z","lead_time":44.68,"prediction":{},"result_count":0,"task":113,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"We compare variations of this procedure in Appendix C.2 . Task # 2 : Next Sentence Prediction ( NSP ) Many important downstream tasks such as Question Answering ( QA ) and Natural Language Inference ( NLI ) are based on understanding the relationship between two sentences , which is not directly captured by language modeling ."},"meta":{},"created_at":"2022-10-17T21:25:49.756077Z","updated_at":"2022-10-18T21:38:11.975496Z","inner_id":111,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":104,"annotations":[{"id":102,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:21:09.717577Z","updated_at":"2022-10-18T21:37:15.976756Z","lead_time":12.849,"prediction":{},"result_count":0,"task":104,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"We refer to this procedure as a \" masked LM \" ( MLM ) , although it is often referred to as a Cloze task in the literature ( Taylor , 1953 ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.755730Z","updated_at":"2022-10-18T21:37:15.995189Z","inner_id":102,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":100,"annotations":[{"id":333,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T21:37:06.540878Z","updated_at":"2022-10-18T21:37:06.540901Z","lead_time":1.08,"prediction":{},"result_count":0,"task":100,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Task # 1 : Masked LM Intuitively , it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left - to - right model or the shallow concatenation of a left - toright and a right - to - left model ."},"meta":{},"created_at":"2022-10-17T21:25:49.755562Z","updated_at":"2022-10-18T21:37:06.558867Z","inner_id":98,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":76,"annotations":[{"id":75,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:14:33.043057Z","updated_at":"2022-10-18T21:34:33.678862Z","lead_time":18.021,"prediction":{},"result_count":0,"task":76,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"1 Because the use of Transformers has become common and our implementation is almost identical to the original , we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al ."},"meta":{},"created_at":"2022-10-17T21:25:49.753990Z","updated_at":"2022-10-18T21:34:33.698538Z","inner_id":74,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":74,"annotations":[{"id":73,"completed_by":1,"result":[{"value":{"start":19,"end":23,"text":"BERT","labels":["MethodName"]},"id":"9nRxI4PvUG","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:14:05.325900Z","updated_at":"2022-10-18T21:34:19.417692Z","lead_time":85.709,"prediction":{},"result_count":0,"task":74,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Model Architecture BERT 's model architecture is a multi - layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al ."},"meta":{},"created_at":"2022-10-17T21:25:49.753866Z","updated_at":"2022-10-18T21:34:19.436440Z","inner_id":72,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":51,"annotations":[{"id":50,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:06:18.373632Z","updated_at":"2022-10-18T21:31:55.528763Z","lead_time":79.401,"prediction":{},"result_count":0,"task":51,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"More recently , sentence or document encoders which produce contextual token representations have been pre - trained from unlabeled text and fine - tuned for a supervised downstream task ( Dai and Le , 2015;Howard and Ruder , 2018;Radford et al . , 2018 ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.749317Z","updated_at":"2022-10-18T21:31:55.545714Z","inner_id":49,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":48,"annotations":[{"id":47,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:05:35.277426Z","updated_at":"2022-10-18T21:31:01.981386Z","lead_time":14.081,"prediction":{},"result_count":0,"task":48,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"( 2018 ) shows that the cloze task can be used to improve the robustness of text generation models ."},"meta":{},"created_at":"2022-10-17T21:25:49.749207Z","updated_at":"2022-10-18T21:31:01.998657Z","inner_id":46,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":40,"annotations":[{"id":39,"completed_by":1,"result":[{"value":{"start":285,"end":306,"text":"denoising autoencoder","labels":["MethodName"]},"id":"Fvuk08trhV","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:54:35.748609Z","updated_at":"2022-10-18T21:30:40.462294Z","lead_time":39.986000000000004,"prediction":{},"result_count":0,"task":40,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"To train sentence representations , prior work has used objectives to rank candidate next sentences ( Jernite et al . , 2017;Logeswaran and Lee , 2018 ) , left - to - right generation of next sentence words given a representation of the previous sentence ( Kiros et al . , 2015 ) , or denoising autoencoder derived objectives ( Hill et al . , 2016 ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.748915Z","updated_at":"2022-10-18T21:30:40.478788Z","inner_id":38,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":32,"annotations":[{"id":31,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:53:51.535817Z","updated_at":"2022-10-18T21:30:10.778975Z","lead_time":13.46,"prediction":{},"result_count":0,"task":32,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The code and pre - trained models are available at https:\/\/github.com\/ google - research \/ bert ."},"meta":{},"created_at":"2022-10-17T21:25:49.748575Z","updated_at":"2022-10-18T21:30:10.802566Z","inner_id":30,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":24,"annotations":[{"id":332,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T21:29:44.722193Z","updated_at":"2022-10-18T21:29:44.722220Z","lead_time":0.464,"prediction":{},"result_count":0,"task":24,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"In addition to the masked language model , we also use a \" next sentence prediction \" task that jointly pretrains text - pair representations ."},"meta":{},"created_at":"2022-10-17T21:25:49.748280Z","updated_at":"2022-10-18T21:29:44.739478Z","inner_id":22,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":25,"annotations":[{"id":24,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:52:39.853307Z","updated_at":"2022-10-18T21:29:43.181061Z","lead_time":4.065,"prediction":{},"result_count":0,"task":25,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The contributions of our paper are as follows : • We demonstrate the importance of bidirectional pre - training for language representations ."},"meta":{},"created_at":"2022-10-17T21:25:49.748318Z","updated_at":"2022-10-18T21:29:43.199304Z","inner_id":23,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":8,"annotations":[{"id":8,"completed_by":1,"result":[{"value":{"start":114,"end":118,"text":"GLUE","labels":["MetricName"]},"id":"twa73ny_uo","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":128,"end":134,"text":"80.5 %","labels":["MetricValue"]},"id":"zEB09RnQms","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":174,"end":182,"text":"MultiNLI","labels":["DatasetName"]},"id":"HRLIR2Ir-A","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":235,"end":245,"text":"SQuAD v1.1","labels":["DatasetName"]},"id":"dKxexOICiM","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":246,"end":264,"text":"question answering","labels":["TaskName"]},"id":"r1MNaIVSN-","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":270,"end":272,"text":"F1","labels":["MetricName"]},"id":"8F3CbSk99A","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":276,"end":280,"text":"93.2","labels":["MetricValue"]},"id":"aDFP-c-4Qw","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":183,"end":191,"text":"accuracy","labels":["MetricName"]},"id":"ZlT1ow__B6","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":195,"end":201,"text":"86.7 %","labels":["MetricValue"]},"id":"qSFLOmRNjo","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":336,"end":338,"text":"F1","labels":["MetricName"]},"id":"CXpzKi6ZaH","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":342,"end":346,"text":"83.1","labels":["MetricValue"]},"id":"w4Z4jEqPHg","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":320,"end":330,"text":"SQuAD v2.0","labels":["DatasetName"]},"id":"UT1CgntPWk","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":204,"end":209,"text":"4.6 %","labels":["MetricValue"]},"id":"4nDw--Z2jQ","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":349,"end":352,"text":"5.1","labels":["MetricValue"]},"id":"GWTrSQBYro","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":283,"end":286,"text":"1.5","labels":["MetricValue"]},"id":"4oXe0tBa53","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":137,"end":142,"text":"7.7 %","labels":["MetricValue"]},"id":"44VU37Z-uY","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:44:19.751499Z","updated_at":"2022-10-18T21:28:31.654674Z","lead_time":72.342,"prediction":{},"result_count":0,"task":8,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQuAD v1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQuAD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.747673Z","updated_at":"2022-10-18T21:28:31.672179Z","inner_id":6,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":3,"annotations":[{"id":3,"completed_by":1,"result":[{"value":{"start":0,"end":4,"text":"BERT","labels":["MethodName"]},"id":"OUx9LJw6f8","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:43:50.628606Z","updated_at":"2022-10-18T21:27:36.476579Z","lead_time":17.747,"prediction":{},"result_count":0,"task":3,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"BERT : Pre - training of Deep Bidirectional Transformers for Language Understanding ."},"meta":{},"created_at":"2022-10-17T21:25:49.747436Z","updated_at":"2022-10-18T21:27:36.492462Z","inner_id":1,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":21,"annotations":[{"id":20,"completed_by":1,"result":[{"value":{"start":0,"end":4,"text":"BERT","labels":["MethodName"]},"id":"Ezq7rfN6ZX","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:50:39.183239Z","updated_at":"2022-10-18T21:27:18.492908Z","lead_time":491.145,"prediction":{},"result_count":0,"task":21,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"BERT alleviates the previously mentioned unidirectionality constraint by using a \" masked language model \" ( MLM ) pre - training objective , inspired by the Cloze task ( Taylor , 1953 ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.748165Z","updated_at":"2022-10-18T21:27:18.510043Z","inner_id":19,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":323,"annotations":[{"id":331,"completed_by":1,"result":[{"value":{"start":71,"end":75,"text":"BERT","labels":["MethodName"]},"id":"cUleuXeB_V","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:20:25.088278Z","updated_at":"2022-10-18T20:20:25.088301Z","lead_time":3.295,"prediction":{},"result_count":0,"task":323,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"For the feature - based approach , we concatenate the last 4 layers of BERT as the features , which was shown to be the best approach in Section 5.3 . From the table it can be seen that fine - tuning is surprisingly robust to different masking strategies ."},"meta":{},"created_at":"2022-10-17T21:25:49.773056Z","updated_at":"2022-10-18T20:20:25.107201Z","inner_id":321,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":322,"annotations":[{"id":330,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:20:20.622387Z","updated_at":"2022-10-18T20:20:20.622410Z","lead_time":1.046,"prediction":{},"result_count":0,"task":322,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The right part of the paper represents the Dev set results ."},"meta":{},"created_at":"2022-10-17T21:25:49.773018Z","updated_at":"2022-10-18T20:20:20.639476Z","inner_id":320,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":320,"annotations":[{"id":328,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:20:14.661227Z","updated_at":"2022-10-18T20:20:14.661252Z","lead_time":2.579,"prediction":{},"result_count":0,"task":320,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"In the table , MASK means that we replace the target token with the [ MASK ] symbol for MLM ; SAME means that we keep the target token as is ; RND means that we replace the target token with another random token ."},"meta":{},"created_at":"2022-10-17T21:25:49.772943Z","updated_at":"2022-10-18T20:20:14.677460Z","inner_id":318,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":319,"annotations":[{"id":327,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:20:11.292084Z","updated_at":"2022-10-18T20:20:11.292126Z","lead_time":0.304,"prediction":{},"result_count":0,"task":319,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The results are presented in Table 8 ."},"meta":{},"created_at":"2022-10-17T21:25:49.772905Z","updated_at":"2022-10-18T20:20:11.308976Z","inner_id":317,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":318,"annotations":[{"id":326,"completed_by":1,"result":[{"value":{"start":4,"end":7,"text":"NER","labels":["TaskName"]},"id":"f0sXjyIzOt","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:20:09.906465Z","updated_at":"2022-10-18T20:20:09.906488Z","lead_time":7.28,"prediction":{},"result_count":0,"task":318,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"For NER , we report both fine - tuning and feature - based approaches , as we expect the mismatch will be amplified for the feature - based approach as the model will not have the chance to adjust the representations ."},"meta":{},"created_at":"2022-10-17T21:25:49.772869Z","updated_at":"2022-10-18T20:20:09.922537Z","inner_id":316,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":317,"annotations":[{"id":325,"completed_by":1,"result":[{"value":{"start":35,"end":39,"text":"MNLI","labels":["DatasetName"]},"id":"c0c7tr63Uy","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":44,"end":47,"text":"NER","labels":["TaskName"]},"id":"Tvr1tBnFEj","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:20:00.498002Z","updated_at":"2022-10-18T20:20:00.498026Z","lead_time":7.08,"prediction":{},"result_count":0,"task":317,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"We report the Dev results for both MNLI and NER ."},"meta":{},"created_at":"2022-10-17T21:25:49.772832Z","updated_at":"2022-10-18T20:20:00.521018Z","inner_id":315,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":316,"annotations":[{"id":323,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:19:35.838516Z","updated_at":"2022-10-18T20:19:35.838538Z","lead_time":2.719,"prediction":{},"result_count":0,"task":316,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Note that the purpose of the masking strategies is to reduce the mismatch between pre - training and fine - tuning , as the [ MASK ] symbol never appears during the fine - tuning stage ."},"meta":{},"created_at":"2022-10-17T21:25:49.772795Z","updated_at":"2022-10-18T20:19:35.858910Z","inner_id":314,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":315,"annotations":[{"id":322,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:19:32.024410Z","updated_at":"2022-10-18T20:19:32.024433Z","lead_time":1.452,"prediction":{},"result_count":0,"task":315,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The following is an ablation study to evaluate the effect of different masking strategies ."},"meta":{},"created_at":"2022-10-17T21:25:49.772758Z","updated_at":"2022-10-18T20:19:32.043766Z","inner_id":313,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":314,"annotations":[{"id":321,"completed_by":1,"result":[{"value":{"start":33,"end":37,"text":"BERT","labels":["MethodName"]},"id":"uH2faOyRxL","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:19:29.327017Z","updated_at":"2022-10-18T20:19:29.327044Z","lead_time":10.005,"prediction":{},"result_count":0,"task":314,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"In Section 3.1 , we mention that BERT uses a mixed strategy for masking the target tokens when pre - training with the masked language model ( MLM ) objective ."},"meta":{},"created_at":"2022-10-17T21:25:49.772721Z","updated_at":"2022-10-18T20:19:29.344503Z","inner_id":312,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":313,"annotations":[{"id":320,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:19:17.847199Z","updated_at":"2022-10-18T20:19:17.847300Z","lead_time":0.602,"prediction":{},"result_count":0,"task":313,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"C.2 Ablation for Different Masking Procedures ."},"meta":{},"created_at":"2022-10-17T21:25:49.772683Z","updated_at":"2022-10-18T20:19:17.865199Z","inner_id":311,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":312,"annotations":[{"id":319,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:19:16.323183Z","updated_at":"2022-10-18T20:19:16.323205Z","lead_time":0.554,"prediction":{},"result_count":0,"task":312,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"C Additional Ablation Studies ."},"meta":{},"created_at":"2022-10-17T21:25:49.772646Z","updated_at":"2022-10-18T20:19:16.341748Z","inner_id":310,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":311,"annotations":[{"id":318,"completed_by":1,"result":[{"value":{"start":8,"end":12,"text":"GLUE","labels":["DatasetName"]},"id":"BfIhzPWDCB","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:19:14.597685Z","updated_at":"2022-10-18T20:19:14.597708Z","lead_time":3.746,"prediction":{},"result_count":0,"task":311,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"For our GLUE submission , we always predicted the majority class ."},"meta":{},"created_at":"2022-10-17T21:25:49.772609Z","updated_at":"2022-10-18T20:19:14.615171Z","inner_id":309,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":308,"annotations":[{"id":316,"completed_by":1,"result":[{"value":{"start":3,"end":7,"text":"WNLI","labels":["DatasetName"]},"id":"CAro-8C3nA","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":8,"end":20,"text":"Winograd NLI","labels":["DatasetName"]},"id":"7rH_Z0MUlI","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":32,"end":58,"text":"natural language inference","labels":["TaskName"]},"id":"8tWrcXm2w2","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:18:51.123842Z","updated_at":"2022-10-18T20:18:51.123863Z","lead_time":13.661,"prediction":{},"result_count":0,"task":308,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"14 WNLI Winograd NLI is a small natural language inference dataset ( Levesque et al . , 2011 ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.772498Z","updated_at":"2022-10-18T20:18:51.140319Z","inner_id":306,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":307,"annotations":[{"id":315,"completed_by":1,"result":[{"value":{"start":0,"end":3,"text":"RTE","labels":["DatasetName"]},"id":"jNW_vg1Q51","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":4,"end":34,"text":"Recognizing Textual Entailment","labels":["DatasetName"]},"id":"wqDr0OhPff","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":74,"end":78,"text":"MNLI","labels":["DatasetName"]},"id":"i-29-aeM0K","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":40,"end":57,"text":"binary entailment","labels":["TaskName"]},"id":"94Okx22vvV","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:18:35.881676Z","updated_at":"2022-10-18T20:18:35.881703Z","lead_time":20.958,"prediction":{},"result_count":0,"task":307,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"RTE Recognizing Textual Entailment is a binary entailment task similar to MNLI , but with much less training data ( Bentivogli et al . , 2009 ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.772462Z","updated_at":"2022-10-18T20:18:35.899821Z","inner_id":305,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":306,"annotations":[{"id":314,"completed_by":1,"result":[{"value":{"start":0,"end":4,"text":"MRPC","labels":["DatasetName"]},"id":"Cz91qmRYd2","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":5,"end":41,"text":"Microsoft Research Paraphrase Corpus","labels":["DatasetName"]},"id":"58gPSMnCk2","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:18:13.195060Z","updated_at":"2022-10-18T20:18:13.195101Z","lead_time":9.432,"prediction":{},"result_count":0,"task":306,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"MRPC Microsoft Research Paraphrase Corpus consists of sentence pairs automatically extracted from online news sources , with human annotations for whether the sentences in the pair are semantically equivalent ( Dolan and Brockett , 2005 ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.772425Z","updated_at":"2022-10-18T20:18:13.213310Z","inner_id":304,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":305,"annotations":[{"id":313,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:17:59.319882Z","updated_at":"2022-10-18T20:17:59.319906Z","lead_time":1.805,"prediction":{},"result_count":0,"task":305,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"They were annotated with a score from 1 to 5 denoting how similar the two sentences are in terms of semantic meaning ."},"meta":{},"created_at":"2022-10-17T21:25:49.772387Z","updated_at":"2022-10-18T20:17:59.337416Z","inner_id":303,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":304,"annotations":[{"id":312,"completed_by":1,"result":[{"value":{"start":4,"end":41,"text":"Semantic Textual Similarity Benchmark","labels":["DatasetName"]},"id":"tMWvdSlmj9","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:17:56.495686Z","updated_at":"2022-10-18T20:17:56.495707Z","lead_time":6.037,"prediction":{},"result_count":0,"task":304,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The Semantic Textual Similarity Benchmark is a collection of sentence pairs drawn from news headlines and other sources ( Cer et al . , 2017 ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.772350Z","updated_at":"2022-10-18T20:17:56.513495Z","inner_id":302,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":303,"annotations":[{"id":311,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:17:49.627813Z","updated_at":"2022-10-18T20:17:49.627834Z","lead_time":0.128,"prediction":{},"result_count":0,"task":303,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"STS - B."},"meta":{},"created_at":"2022-10-17T21:25:49.772311Z","updated_at":"2022-10-18T20:17:49.647587Z","inner_id":301,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":301,"annotations":[{"id":309,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:17:17.881556Z","updated_at":"2022-10-18T20:17:17.881581Z","lead_time":0.235,"prediction":{},"result_count":0,"task":301,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"with human annotations of their sentiment ( Socher et al . , 2013 ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.772236Z","updated_at":"2022-10-18T20:17:17.901608Z","inner_id":299,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":300,"annotations":[{"id":308,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:17:16.279257Z","updated_at":"2022-10-18T20:17:16.279281Z","lead_time":0.26,"prediction":{},"result_count":0,"task":300,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"E M ' C T 1 T [ SEP ] ..."},"meta":{},"created_at":"2022-10-17T21:25:49.772200Z","updated_at":"2022-10-18T20:17:16.296016Z","inner_id":298,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":299,"annotations":[{"id":307,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:17:14.925524Z","updated_at":"2022-10-18T20:17:14.925551Z","lead_time":1.491,"prediction":{},"result_count":0,"task":299,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"E N E 1 ' ..."},"meta":{},"created_at":"2022-10-17T21:25:49.772163Z","updated_at":"2022-10-18T20:17:14.943808Z","inner_id":297,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":298,"annotations":[{"id":306,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:17:03.846212Z","updated_at":"2022-10-18T20:17:03.846235Z","lead_time":0.144,"prediction":{},"result_count":0,"task":298,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"E [ CLS ] E 1 E [ SEP ] Class Label ..."},"meta":{},"created_at":"2022-10-17T21:25:49.772126Z","updated_at":"2022-10-18T20:17:03.863768Z","inner_id":296,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":297,"annotations":[{"id":305,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:17:02.577266Z","updated_at":"2022-10-18T20:17:02.577288Z","lead_time":2.026,"prediction":{},"result_count":0,"task":297,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"..."},"meta":{},"created_at":"2022-10-17T21:25:49.772088Z","updated_at":"2022-10-18T20:17:02.593638Z","inner_id":295,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":296,"annotations":[{"id":304,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:16:59.416637Z","updated_at":"2022-10-18T20:16:59.416661Z","lead_time":0.189,"prediction":{},"result_count":0,"task":296,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"[ CLS ] E [ CLS ] E 1 E 2 E N C T 1 T 2 T N Single Sentence B - PER O O ..."},"meta":{},"created_at":"2022-10-17T21:25:49.772051Z","updated_at":"2022-10-18T20:16:59.438925Z","inner_id":294,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":294,"annotations":[{"id":302,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:16:53.483368Z","updated_at":"2022-10-18T20:16:53.483389Z","lead_time":1.914,"prediction":{},"result_count":0,"task":294,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"..."},"meta":{},"created_at":"2022-10-17T21:25:49.771975Z","updated_at":"2022-10-18T20:16:53.499754Z","inner_id":292,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":291,"annotations":[{"id":300,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:16:43.987565Z","updated_at":"2022-10-18T20:16:43.987588Z","lead_time":0.123,"prediction":{},"result_count":0,"task":291,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"T M ' [ CLS ] Tok 1 [ SEP ] ..."},"meta":{},"created_at":"2022-10-17T21:25:49.769821Z","updated_at":"2022-10-18T20:16:44.008161Z","inner_id":289,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":290,"annotations":[{"id":299,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:16:43.005626Z","updated_at":"2022-10-18T20:16:43.005652Z","lead_time":0.121,"prediction":{},"result_count":0,"task":290,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"T N T 1 ' ..."},"meta":{},"created_at":"2022-10-17T21:25:49.769785Z","updated_at":"2022-10-18T20:16:43.041824Z","inner_id":288,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":289,"annotations":[{"id":298,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:16:41.914972Z","updated_at":"2022-10-18T20:16:41.914996Z","lead_time":0.166,"prediction":{},"result_count":0,"task":289,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"E M ' C T 1 T [ SEP ] ..."},"meta":{},"created_at":"2022-10-17T21:25:49.769748Z","updated_at":"2022-10-18T20:16:41.933799Z","inner_id":287,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":288,"annotations":[{"id":297,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:16:40.848302Z","updated_at":"2022-10-18T20:16:40.848320Z","lead_time":0.484,"prediction":{},"result_count":0,"task":288,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"E N E 1 ' ..."},"meta":{},"created_at":"2022-10-17T21:25:49.769711Z","updated_at":"2022-10-18T20:16:40.865209Z","inner_id":286,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":287,"annotations":[{"id":296,"completed_by":1,"result":[{"value":{"start":0,"end":4,"text":"QNLI","labels":["DatasetName"]},"id":"v4LE4xEtPW","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":5,"end":40,"text":"Question Natural Language Inference","labels":["DatasetName"]},"id":"xNMrKEZB8G","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":61,"end":96,"text":"Stanford Question Answering Dataset","labels":["DatasetName"]},"id":"50b8nMA2mR","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":156,"end":177,"text":"binary classification","labels":["TaskName"]},"id":"9ePOBJKXj_","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:16:39.463148Z","updated_at":"2022-10-18T20:16:39.463172Z","lead_time":35.131,"prediction":{},"result_count":0,"task":287,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"QNLI Question Natural Language Inference is a version of the Stanford Question Answering Dataset ( Rajpurkar et al . , 2016 ) which has been converted to a binary classification task ( Wang et al . , 2018a ..."},"meta":{},"created_at":"2022-10-17T21:25:49.769674Z","updated_at":"2022-10-18T20:16:39.486452Z","inner_id":285,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":285,"annotations":[{"id":294,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:15:24.349941Z","updated_at":"2022-10-18T20:15:37.134818Z","lead_time":2.7510000000000003,"prediction":{},"result_count":0,"task":285,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Given a pair of sentences , the goal is to predict whether the second sentence is an entailment , contradiction , or neutral with respect to the first one ."},"meta":{},"created_at":"2022-10-17T21:25:49.769601Z","updated_at":"2022-10-18T20:15:37.152209Z","inner_id":283,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":283,"annotations":[{"id":292,"completed_by":1,"result":[{"value":{"start":4,"end":8,"text":"GLUE","labels":["DatasetName"]},"id":"_bl3TosK6q","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:15:09.478552Z","updated_at":"2022-10-18T20:15:09.478576Z","lead_time":9.625,"prediction":{},"result_count":0,"task":283,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The GLUE benchmark includes the following datasets , the descriptions of which were originally summarized in Wang et al ."},"meta":{},"created_at":"2022-10-17T21:25:49.769527Z","updated_at":"2022-10-18T20:15:09.494609Z","inner_id":281,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":282,"annotations":[{"id":291,"completed_by":1,"result":[{"value":{"start":55,"end":59,"text":"BERT","labels":["MethodName"]},"id":"ATTWXuxnOX","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:14:58.794884Z","updated_at":"2022-10-18T20:14:58.794909Z","lead_time":5.142,"prediction":{},"result_count":0,"task":282,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Our task - specific models are formed by incorporating BERT with one additional output layer , so a minimal number of parameters need to be learned from scratch ."},"meta":{},"created_at":"2022-10-17T21:25:49.769489Z","updated_at":"2022-10-18T20:14:58.812774Z","inner_id":280,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":281,"annotations":[{"id":290,"completed_by":1,"result":[{"value":{"start":34,"end":38,"text":"BERT","labels":["MethodName"]},"id":"Hg4_xaahf2","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:14:50.081170Z","updated_at":"2022-10-18T20:14:50.081191Z","lead_time":3.798,"prediction":{},"result_count":0,"task":281,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The illustration of fine - tuning BERT on different tasks can be seen in Figure 4 ."},"meta":{},"created_at":"2022-10-17T21:25:49.769452Z","updated_at":"2022-10-18T20:14:50.097895Z","inner_id":279,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":280,"annotations":[{"id":289,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:14:45.323989Z","updated_at":"2022-10-18T20:14:45.324013Z","lead_time":0.477,"prediction":{},"result_count":0,"task":280,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"A.5 Illustrations of Fine - tuning on Different Tasks ."},"meta":{},"created_at":"2022-10-17T21:25:49.769415Z","updated_at":"2022-10-18T20:14:45.340783Z","inner_id":278,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":279,"annotations":[{"id":288,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:14:43.737494Z","updated_at":"2022-10-18T20:14:43.737523Z","lead_time":1.593,"prediction":{},"result_count":0,"task":279,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"To isolate the effect of these differences , we perform ablation experiments in Section 5.1 which demonstrate that the majority of the improvements are in fact coming from the two pre - training tasks and the bidirectionality they enable ."},"meta":{},"created_at":"2022-10-17T21:25:49.769378Z","updated_at":"2022-10-18T20:14:43.754447Z","inner_id":277,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":278,"annotations":[{"id":287,"completed_by":1,"result":[{"value":{"start":2,"end":5,"text":"GPT","labels":["MethodName"]},"id":"G77DuPhp1V","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":20,"end":33,"text":"learning rate","labels":["HyperparameterName"]},"id":"4shyb4tf_s","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":37,"end":41,"text":"5e-5","labels":["HyperparameterValue"]},"id":"ly69hmFKVh","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":78,"end":82,"text":"BERT","labels":["MethodName"]},"id":"GVcVs5Eo3q","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:14:40.062816Z","updated_at":"2022-10-18T20:14:40.062841Z","lead_time":29.824,"prediction":{},"result_count":0,"task":278,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"• GPT used the same learning rate of 5e-5 for all fine - tuning experiments ; BERT chooses a task - specific fine - tuning learning rate which performs the best on the development set ."},"meta":{},"created_at":"2022-10-17T21:25:49.769340Z","updated_at":"2022-10-18T20:14:40.081972Z","inner_id":276,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":276,"annotations":[{"id":285,"completed_by":1,"result":[{"value":{"start":2,"end":5,"text":"GPT","labels":["MethodName"]},"id":"EsiepWyv9C","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":127,"end":131,"text":"BERT","labels":["MethodName"]},"id":"6TMvHH3X1m","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:13:25.590521Z","updated_at":"2022-10-18T20:13:25.590545Z","lead_time":8.651,"prediction":{},"result_count":0,"task":276,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"• GPT uses a sentence separator ( [ SEP ] ) and classifier token ( [ CLS ] ) which are only introduced at fine - tuning time ; BERT learns [ SEP ] , [ CLS ] and sentence A \/ B embeddings during pre - training ."},"meta":{},"created_at":"2022-10-17T21:25:49.769265Z","updated_at":"2022-10-18T20:13:25.608432Z","inner_id":274,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":275,"annotations":[{"id":284,"completed_by":1,"result":[{"value":{"start":243,"end":247,"text":"BERT","labels":["MethodName"]},"id":"pGW6RSZkI-","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":252,"end":255,"text":"GPT","labels":["MethodName"]},"id":"LHOvNQ_Dbc","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":273,"end":276,"text":"GPT","labels":["MethodName"]},"id":"kfH3U8f8RV","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":295,"end":306,"text":"BooksCorpus","labels":["DatasetName"]},"id":"uC7iZIpfd3","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":348,"end":359,"text":"BooksCorpus","labels":["DatasetName"]},"id":"_RYooBrlkm","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":325,"end":329,"text":"BERT","labels":["MethodName"]},"id":"X6Qu3nVF_r","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":380,"end":389,"text":"Wikipedia","labels":["DatasetName"]},"id":"KfywZffSEi","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:13:15.916604Z","updated_at":"2022-10-18T20:13:15.916625Z","lead_time":35.926,"prediction":{},"result_count":0,"task":275,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The core argument of this work is that the bi - directionality and the two pretraining tasks presented in Section 3.1 account for the majority of the empirical improvements , but we do note that there are several other differences between how BERT and GPT were trained : • GPT is trained on the BooksCorpus ( 800 M words ) ; BERT is trained on the BooksCorpus ( 800 M words ) and Wikipedia ( 2,500 M words ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.769227Z","updated_at":"2022-10-18T20:13:15.933541Z","inner_id":273,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":274,"annotations":[{"id":283,"completed_by":1,"result":[{"value":{"start":42,"end":46,"text":"BERT","labels":["MethodName"]},"id":"8IHU2XYe5A","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":94,"end":97,"text":"GPT","labels":["MethodName"]},"id":"TLcUogQmPL","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:12:34.275619Z","updated_at":"2022-10-18T20:12:38.594714Z","lead_time":8.7,"prediction":{},"result_count":0,"task":274,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"In fact , many of the design decisions in BERT were intentionally made to make it as close to GPT as possible so that the two methods could be minimally compared ."},"meta":{},"created_at":"2022-10-17T21:25:49.769189Z","updated_at":"2022-10-18T20:12:38.610524Z","inner_id":272,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":273,"annotations":[{"id":282,"completed_by":1,"result":[{"value":{"start":54,"end":58,"text":"BERT","labels":["MethodName"]},"id":"UhvUZQFsrh","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":62,"end":72,"text":"OpenAI GPT","labels":["MethodName"]},"id":"9A2YHgZ0hI","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:12:17.800387Z","updated_at":"2022-10-18T20:12:25.806170Z","lead_time":15.297,"prediction":{},"result_count":0,"task":273,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The most comparable existing pre - training method to BERT is OpenAI GPT , which trains a left - to - right Transformer LM on a large text corpus ."},"meta":{},"created_at":"2022-10-17T21:25:49.769151Z","updated_at":"2022-10-18T20:12:25.822387Z","inner_id":271,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":272,"annotations":[{"id":281,"completed_by":1,"result":[{"value":{"start":56,"end":60,"text":"BERT","labels":["MethodName"]},"id":"TcRWCmaD59","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":65,"end":75,"text":"OpenAI GPT","labels":["MethodName"]},"id":"khUm-HwgHZ","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":110,"end":114,"text":"ELMo","labels":["MethodName"]},"id":"uedwHznYyn","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:12:01.521128Z","updated_at":"2022-10-18T20:12:09.115538Z","lead_time":14.026,"prediction":{},"result_count":0,"task":272,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Note that in addition to the architecture differences , BERT and OpenAI GPT are finetuning approaches , while ELMo is a feature - based approach ."},"meta":{},"created_at":"2022-10-17T21:25:49.769114Z","updated_at":"2022-10-18T20:12:09.131121Z","inner_id":270,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":271,"annotations":[{"id":280,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:11:53.811037Z","updated_at":"2022-10-18T20:11:53.811061Z","lead_time":2.314,"prediction":{},"result_count":0,"task":271,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The comparisons between the model architectures are shown visually in Figure 3 ."},"meta":{},"created_at":"2022-10-17T21:25:49.769077Z","updated_at":"2022-10-18T20:11:53.831490Z","inner_id":269,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":270,"annotations":[{"id":279,"completed_by":1,"result":[{"value":{"start":18,"end":22,"text":"BERT","labels":["MethodName"]},"id":"xTzmfJ6iGf","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":25,"end":29,"text":"ELMo","labels":["MethodName"]},"id":"wy5jcPbcGr","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":36,"end":46,"text":"OpenAI GPT","labels":["MethodName"]},"id":"zaQzyEuwQj","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":138,"end":142,"text":"ELMo","labels":["MethodName"]},"id":"MTl8KvOvek","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":145,"end":155,"text":"OpenAI GPT","labels":["MethodName"]},"id":"8V5aq2qOfb","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":160,"end":164,"text":"BERT","labels":["MethodName"]},"id":"tAprl0_OtY","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:11:49.564460Z","updated_at":"2022-10-18T20:11:49.564481Z","lead_time":29.452,"prediction":{},"result_count":0,"task":270,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"A.4 Comparison of BERT , ELMo , and OpenAI GPT Here we studies the differences in recent popular representation learning models including ELMo , OpenAI GPT and BERT ."},"meta":{},"created_at":"2022-10-17T21:25:49.769041Z","updated_at":"2022-10-18T20:11:49.585291Z","inner_id":268,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":269,"annotations":[{"id":278,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:11:18.924969Z","updated_at":"2022-10-18T20:11:18.924994Z","lead_time":1.03,"prediction":{},"result_count":0,"task":269,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Fine - tuning is typically very fast , so it is reasonable to simply run an exhaustive search over the above parameters and choose the model that performs best on the development set ."},"meta":{},"created_at":"2022-10-17T21:25:49.769004Z","updated_at":"2022-10-18T20:11:18.941485Z","inner_id":267,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":268,"annotations":[{"id":277,"completed_by":1,"result":[{"value":{"start":4,"end":23,"text":"dropout probability","labels":["HyperparameterName"]},"id":"5GInqBrQ_s","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":43,"end":46,"text":"0.1","labels":["HyperparameterValue"]},"id":"0EIQtPYxdW","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":191,"end":201,"text":"Batch size","labels":["HyperparameterName"]},"id":"0Lvi3_7J0u","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":204,"end":206,"text":"16","labels":["HyperparameterValue"]},"id":"EsbWpine3O","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":209,"end":211,"text":"32","labels":["HyperparameterValue"]},"id":"uqUXPxmWdM","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:11:16.852649Z","updated_at":"2022-10-18T20:11:16.852670Z","lead_time":25.003,"prediction":{},"result_count":0,"task":268,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The dropout probability was always kept at 0.1 . The optimal hyperparameter values are task - specific , but we found the following range of possible values to work well across all tasks : • Batch size : 16 , 32 We also observed that large data sets ( e.g. , 100k+ labeled training examples ) were far less sensitive to hyperparameter choice than small data sets ."},"meta":{},"created_at":"2022-10-17T21:25:49.768966Z","updated_at":"2022-10-18T20:11:16.871123Z","inner_id":266,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":267,"annotations":[{"id":276,"completed_by":1,"result":[{"value":{"start":109,"end":119,"text":"batch size","labels":["HyperparameterName"]},"id":"B7jeWJEtOe","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":122,"end":135,"text":"learning rate","labels":["HyperparameterName"]},"id":"PKOf4FPMQx","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":142,"end":167,"text":"number of training epochs","labels":["HyperparameterName"]},"id":"SP-cebFmSB","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:10:39.853372Z","updated_at":"2022-10-18T20:10:50.778960Z","lead_time":11.461,"prediction":{},"result_count":0,"task":267,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"For fine - tuning , most model hyperparameters are the same as in pre - training , with the exception of the batch size , learning rate , and number of training epochs ."},"meta":{},"created_at":"2022-10-17T21:25:49.768927Z","updated_at":"2022-10-18T20:10:50.796498Z","inner_id":265,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":266,"annotations":[{"id":275,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:10:38.233254Z","updated_at":"2022-10-18T20:10:38.233281Z","lead_time":0.375,"prediction":{},"result_count":0,"task":266,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"A.3 Fine - tuning Procedure ."},"meta":{},"created_at":"2022-10-17T21:25:49.768890Z","updated_at":"2022-10-18T20:10:38.251458Z","inner_id":264,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":265,"annotations":[{"id":274,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:10:36.605474Z","updated_at":"2022-10-18T20:10:36.605501Z","lead_time":6.546,"prediction":{},"result_count":0,"task":265,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Then , we train the rest 10 % of the steps of sequence of 512 to learn the positional embeddings ."},"meta":{},"created_at":"2022-10-17T21:25:49.768852Z","updated_at":"2022-10-18T20:10:36.621898Z","inner_id":263,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":263,"annotations":[{"id":273,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:10:20.920752Z","updated_at":"2022-10-18T20:10:20.920783Z","lead_time":0.455,"prediction":{},"result_count":0,"task":263,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Longer sequences are disproportionately expensive because attention is quadratic to the sequence length ."},"meta":{},"created_at":"2022-10-17T21:25:49.768777Z","updated_at":"2022-10-18T20:10:20.939102Z","inner_id":261,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":262,"annotations":[{"id":272,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:10:19.335536Z","updated_at":"2022-10-18T20:10:19.335562Z","lead_time":2.413,"prediction":{},"result_count":0,"task":262,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Each pretraining took 4 days to complete ."},"meta":{},"created_at":"2022-10-17T21:25:49.768740Z","updated_at":"2022-10-18T20:10:19.352470Z","inner_id":260,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":261,"annotations":[{"id":271,"completed_by":1,"result":[{"value":{"start":15,"end":25,"text":"BERT LARGE","labels":["MethodName"]},"id":"Wido6rY2nD","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:10:15.179052Z","updated_at":"2022-10-18T20:10:15.179075Z","lead_time":3.184,"prediction":{},"result_count":0,"task":261,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"13 Training of BERT LARGE was performed on 16 Cloud TPUs ( 64 TPU chips total ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.768703Z","updated_at":"2022-10-18T20:10:15.206246Z","inner_id":259,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":260,"annotations":[{"id":270,"completed_by":1,"result":[{"value":{"start":12,"end":21,"text":"BERT BASE","labels":["MethodName"]},"id":"uzNqUbRW-F","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:10:10.989254Z","updated_at":"2022-10-18T20:10:10.989279Z","lead_time":3.704,"prediction":{},"result_count":0,"task":260,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Training of BERT BASE was performed on 4 Cloud TPUs in Pod configuration ( 16 TPU chips total ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.768665Z","updated_at":"2022-10-18T20:10:11.012788Z","inner_id":258,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":259,"annotations":[{"id":269,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:10:06.411051Z","updated_at":"2022-10-18T20:10:06.411083Z","lead_time":0.206,"prediction":{},"result_count":0,"task":259,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"E 1 E 2 E N ..."},"meta":{},"created_at":"2022-10-17T21:25:49.768629Z","updated_at":"2022-10-18T20:10:06.429656Z","inner_id":257,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":258,"annotations":[{"id":268,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:10:04.979821Z","updated_at":"2022-10-18T20:10:04.979845Z","lead_time":0.392,"prediction":{},"result_count":0,"task":258,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"T 1 T 2 T N ..."},"meta":{},"created_at":"2022-10-17T21:25:49.768592Z","updated_at":"2022-10-18T20:10:04.998687Z","inner_id":256,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":257,"annotations":[{"id":267,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:10:01.810275Z","updated_at":"2022-10-18T20:10:01.810301Z","lead_time":0.117,"prediction":{},"result_count":0,"task":257,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"E 1 E 2 E N ..."},"meta":{},"created_at":"2022-10-17T21:25:49.768555Z","updated_at":"2022-10-18T20:10:01.832422Z","inner_id":255,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":256,"annotations":[{"id":266,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:10:00.974155Z","updated_at":"2022-10-18T20:10:00.974182Z","lead_time":0.2,"prediction":{},"result_count":0,"task":256,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"T 1 T 2 T N ..."},"meta":{},"created_at":"2022-10-17T21:25:49.768519Z","updated_at":"2022-10-18T20:10:00.993565Z","inner_id":254,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":255,"annotations":[{"id":265,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:09:59.988965Z","updated_at":"2022-10-18T20:09:59.989037Z","lead_time":0.55,"prediction":{},"result_count":0,"task":255,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"E N ..."},"meta":{},"created_at":"2022-10-17T21:25:49.768482Z","updated_at":"2022-10-18T20:10:00.009760Z","inner_id":253,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":254,"annotations":[{"id":264,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:09:58.656720Z","updated_at":"2022-10-18T20:09:58.656748Z","lead_time":0.848,"prediction":{},"result_count":0,"task":254,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood ."},"meta":{},"created_at":"2022-10-17T21:25:49.768445Z","updated_at":"2022-10-18T20:09:58.675500Z","inner_id":252,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":253,"annotations":[{"id":263,"completed_by":1,"result":[{"value":{"start":99,"end":109,"text":"OpenAI GPT","labels":["MethodName"]},"id":"8VeZ2XGg18","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:09:56.761033Z","updated_at":"2022-10-18T20:09:56.761065Z","lead_time":6.946,"prediction":{},"result_count":0,"task":253,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"We use a gelu activation ( Hendrycks and Gimpel , 2016 ) rather than the standard relu , following OpenAI GPT ."},"meta":{},"created_at":"2022-10-17T21:25:49.768408Z","updated_at":"2022-10-18T20:09:56.778217Z","inner_id":251,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":252,"annotations":[{"id":262,"completed_by":1,"result":[{"value":{"start":9,"end":28,"text":"dropout probability","labels":["HyperparameterName"]},"id":"IdSskwXbjT","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":32,"end":35,"text":"0.1","labels":["HyperparameterValue"]},"id":"A9AZmlKJGE","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:09:48.817968Z","updated_at":"2022-10-18T20:09:48.817992Z","lead_time":9.616,"prediction":{},"result_count":0,"task":252,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"We use a dropout probability of 0.1 on all layers ."},"meta":{},"created_at":"2022-10-17T21:25:49.768371Z","updated_at":"2022-10-18T20:09:48.835743Z","inner_id":250,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":251,"annotations":[{"id":261,"completed_by":1,"result":[{"value":{"start":17,"end":30,"text":"learning rate","labels":["HyperparameterName"]},"id":"QZ5wblkvDN","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":34,"end":38,"text":"1e-4","labels":["HyperparameterValue"]},"id":"3rQZDpcR6V","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":41,"end":44,"text":"β 1","labels":["HyperparameterName"]},"id":"CdIonnfzua","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":47,"end":50,"text":"0.9","labels":["HyperparameterValue"]},"id":"h6wGZgrAi4","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":53,"end":56,"text":"β 2","labels":["HyperparameterName"]},"id":"C-CReVuzyn","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":59,"end":64,"text":"0.999","labels":["HyperparameterValue"]},"id":"Bby7sf2nEm","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":67,"end":82,"text":"L2 weight decay","labels":["HyperparameterName"]},"id":"SgiEAFf_DU","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":86,"end":90,"text":"0.01","labels":["HyperparameterValue"]},"id":"10JZ4pB-L2","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:09:37.869717Z","updated_at":"2022-10-18T20:09:37.869741Z","lead_time":41.088,"prediction":{},"result_count":0,"task":251,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"We use Adam with learning rate of 1e-4 , β 1 = 0.9 , β 2 = 0.999 , L2 weight decay of 0.01 , learning rate warmup over the first 10,000 steps , and linear decay of the learning rate ."},"meta":{},"created_at":"2022-10-17T21:25:49.768334Z","updated_at":"2022-10-18T20:09:37.888098Z","inner_id":249,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":250,"annotations":[{"id":260,"completed_by":1,"result":[{"value":{"start":14,"end":24,"text":"batch size","labels":["HyperparameterName"]},"id":"GUwRKvDeoK","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":28,"end":31,"text":"256","labels":["HyperparameterValue"]},"id":"-Avl5Fn_cB","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":112,"end":117,"text":"steps","labels":["HyperparameterName"]},"id":"dTOdTX4BuQ","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":102,"end":111,"text":"1,000,000","labels":["HyperparameterValue"]},"id":"qk5CyG7Fif","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:08:55.632798Z","updated_at":"2022-10-18T20:08:55.632823Z","lead_time":25.333,"prediction":{},"result_count":0,"task":250,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"We train with batch size of 256 sequences ( 256 sequences * 512 tokens = 128,000 tokens \/ batch ) for 1,000,000 steps , which is approximately 40 epochs over the 3.3 billion word corpus ."},"meta":{},"created_at":"2022-10-17T21:25:49.768297Z","updated_at":"2022-10-18T20:08:55.651061Z","inner_id":248,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":249,"annotations":[{"id":259,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:08:21.178930Z","updated_at":"2022-10-18T20:08:21.178956Z","lead_time":1.22,"prediction":{},"result_count":0,"task":249,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The LM masking is applied after WordPiece tokenization with a uniform masking rate of 15 % , and no special consideration given to partial word pieces ."},"meta":{},"created_at":"2022-10-17T21:25:49.768260Z","updated_at":"2022-10-18T20:08:21.196316Z","inner_id":247,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":248,"annotations":[{"id":258,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:08:19.338664Z","updated_at":"2022-10-18T20:08:19.338692Z","lead_time":3.776,"prediction":{},"result_count":0,"task":248,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"They are sampled such that the combined length is ≤ 512 tokens ."},"meta":{},"created_at":"2022-10-17T21:25:49.768223Z","updated_at":"2022-10-18T20:08:19.357486Z","inner_id":246,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":247,"annotations":[{"id":257,"completed_by":1,"result":[{"value":{"start":132,"end":157,"text":" next sentence prediction","labels":["TaskName"]},"id":"0koaxqnXar","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:08:06.494862Z","updated_at":"2022-10-18T20:08:14.549273Z","lead_time":9.229,"prediction":{},"result_count":0,"task":247,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"50 % of the time B is the actual next sentence that follows A and 50 % of the time it is a random sentence , which is done for the \" next sentence prediction \" task ."},"meta":{},"created_at":"2022-10-17T21:25:49.768186Z","updated_at":"2022-10-18T20:08:14.567404Z","inner_id":245,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":246,"annotations":[{"id":256,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:08:02.069438Z","updated_at":"2022-10-18T20:08:02.069467Z","lead_time":0.536,"prediction":{},"result_count":0,"task":246,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The first sentence receives the A embedding and the second receives the B embedding ."},"meta":{},"created_at":"2022-10-17T21:25:49.768148Z","updated_at":"2022-10-18T20:08:02.086265Z","inner_id":244,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":245,"annotations":[{"id":255,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:08:00.661119Z","updated_at":"2022-10-18T20:08:00.661147Z","lead_time":1.787,"prediction":{},"result_count":0,"task":245,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"To generate each training input sequence , we sample two spans of text from the corpus , which we refer to as \" sentences \" even though they are typically much longer than single sentences ( but can be shorter also ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.768111Z","updated_at":"2022-10-18T20:08:00.677731Z","inner_id":243,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":244,"annotations":[{"id":254,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:07:57.530216Z","updated_at":"2022-10-18T20:07:57.530237Z","lead_time":1.821,"prediction":{},"result_count":0,"task":244,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Our major contribution is further generalizing these findings to deep bidirectional architectures , allowing the same pre - trained model to successfully tackle a broad set of NLP tasks ."},"meta":{},"created_at":"2022-10-17T21:25:49.768073Z","updated_at":"2022-10-18T20:07:57.546080Z","inner_id":242,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":243,"annotations":[{"id":253,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:07:53.344256Z","updated_at":"2022-10-18T20:07:53.344280Z","lead_time":3.088,"prediction":{},"result_count":0,"task":243,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"In particular , these results enable even low - resource tasks to benefit from deep unidirectional architectures ."},"meta":{},"created_at":"2022-10-17T21:25:49.768035Z","updated_at":"2022-10-18T20:07:53.360611Z","inner_id":241,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":242,"annotations":[{"id":252,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:07:49.077716Z","updated_at":"2022-10-18T20:07:49.077743Z","lead_time":1.196,"prediction":{},"result_count":0,"task":242,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Recent empirical improvements due to transfer learning with language models have demonstrated that rich , unsupervised pre - training is an integral part of many language understanding systems ."},"meta":{},"created_at":"2022-10-17T21:25:49.767998Z","updated_at":"2022-10-18T20:07:49.094868Z","inner_id":240,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":241,"annotations":[{"id":248,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:07:14.218387Z","updated_at":"2022-10-18T20:07:46.864930Z","lead_time":0.511,"prediction":{},"result_count":0,"task":241,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Conclusion ."},"meta":{},"created_at":"2022-10-17T21:25:49.767961Z","updated_at":"2022-10-18T20:07:46.882874Z","inner_id":239,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":240,"annotations":[{"id":251,"completed_by":1,"result":[{"value":{"start":23,"end":27,"text":"BERT","labels":["MethodName"]},"id":"yU2D8xEBTh","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:07:45.698336Z","updated_at":"2022-10-18T20:07:45.698362Z","lead_time":4.013,"prediction":{},"result_count":0,"task":240,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"This demonstrates that BERT is effective for both finetuning and feature - based approaches ."},"meta":{},"created_at":"2022-10-17T21:25:49.767923Z","updated_at":"2022-10-18T20:07:45.719131Z","inner_id":238,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":239,"annotations":[{"id":250,"completed_by":1,"result":[{"value":{"start":119,"end":130,"text":"Transformer","labels":["MethodName"]},"id":"p5C5-Di03J","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":151,"end":153,"text":"F1","labels":["MetricName"]},"id":"msr-ku2j_B","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":147,"end":150,"text":"0.3","labels":["MetricValue"]},"id":"RzlkLr_h3M","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:07:40.180829Z","updated_at":"2022-10-18T20:07:40.180851Z","lead_time":20.17,"prediction":{},"result_count":0,"task":239,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The best performing method concatenates the token representations from the top four hidden layers of the pre - trained Transformer , which is only 0.3 F1 behind fine - tuning the entire model ."},"meta":{},"created_at":"2022-10-17T21:25:49.767885Z","updated_at":"2022-10-18T20:07:40.199170Z","inner_id":237,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":238,"annotations":[{"id":249,"completed_by":1,"result":[{"value":{"start":0,"end":10,"text":"BERT LARGE","labels":["MethodName"]},"id":"JWpD2Q7ce6","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:07:19.060194Z","updated_at":"2022-10-18T20:07:19.060216Z","lead_time":3.556,"prediction":{},"result_count":0,"task":238,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"BERT LARGE performs competitively with state - of - the - art methods ."},"meta":{},"created_at":"2022-10-17T21:25:49.767848Z","updated_at":"2022-10-18T20:07:19.077565Z","inner_id":236,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":237,"annotations":[{"id":247,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:07:10.688956Z","updated_at":"2022-10-18T20:07:10.688976Z","lead_time":0.734,"prediction":{},"result_count":0,"task":237,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Results are presented in Table 7 ."},"meta":{},"created_at":"2022-10-17T21:25:49.767811Z","updated_at":"2022-10-18T20:07:10.721187Z","inner_id":235,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":236,"annotations":[{"id":246,"completed_by":1,"result":[{"value":{"start":102,"end":108,"text":"BiLSTM","labels":["MethodName"]},"id":"dRX19bZhmN","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:07:07.995026Z","updated_at":"2022-10-18T20:07:07.995047Z","lead_time":10.263,"prediction":{},"result_count":0,"task":236,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"These contextual embeddings are used as input to a randomly initialized two - layer 768 - dimensional BiLSTM before the classification layer ."},"meta":{},"created_at":"2022-10-17T21:25:49.767773Z","updated_at":"2022-10-18T20:07:08.012860Z","inner_id":234,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":235,"annotations":[{"id":245,"completed_by":1,"result":[{"value":{"start":171,"end":175,"text":"BERT","labels":["MethodName"]},"id":"URkjZOkJvl","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:06:56.728643Z","updated_at":"2022-10-18T20:06:56.728670Z","lead_time":2.612,"prediction":{},"result_count":0,"task":235,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"To ablate the fine - tuning approach , we apply the feature - based approach by extracting the activations from one or more layers without fine - tuning any parameters of BERT ."},"meta":{},"created_at":"2022-10-17T21:25:49.767718Z","updated_at":"2022-10-18T20:06:56.745945Z","inner_id":233,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":234,"annotations":[{"id":243,"completed_by":1,"result":[{"value":{"start":105,"end":108,"text":"NER","labels":["TaskName"]},"id":"pwf71WyB7G","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:06:36.058396Z","updated_at":"2022-10-18T20:06:39.663409Z","lead_time":3.9800000000000004,"prediction":{},"result_count":0,"task":234,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"We use the representation of the first sub - token as the input to the token - level classifier over the NER label set ."},"meta":{},"created_at":"2022-10-17T21:25:49.765717Z","updated_at":"2022-10-18T20:06:39.679557Z","inner_id":232,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":233,"annotations":[{"id":242,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:06:34.667665Z","updated_at":"2022-10-18T20:06:34.667692Z","lead_time":1.788,"prediction":{},"result_count":0,"task":233,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Following standard practice , we formulate this as a tagging task but do not use a CRF layer in the output ."},"meta":{},"created_at":"2022-10-17T21:25:49.765680Z","updated_at":"2022-10-18T20:06:34.683823Z","inner_id":231,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":232,"annotations":[{"id":241,"completed_by":1,"result":[{"value":{"start":16,"end":20,"text":"BERT","labels":["MethodName"]},"id":"rCuU1rSNVn","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:06:30.062721Z","updated_at":"2022-10-18T20:06:30.062753Z","lead_time":2.822,"prediction":{},"result_count":0,"task":232,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"In the input to BERT , we use a case - preserving WordPiece model , and we include the maximal document context provided by the data ."},"meta":{},"created_at":"2022-10-17T21:25:49.765641Z","updated_at":"2022-10-18T20:06:30.080059Z","inner_id":230,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":231,"annotations":[{"id":240,"completed_by":1,"result":[{"value":{"start":60,"end":64,"text":"BERT","labels":["MethodName"]},"id":"jl6L3ewpDq","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":83,"end":107,"text":"Named Entity Recognition","labels":["TaskName"]},"id":"n-pQD2oc6h","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":110,"end":113,"text":"NER","labels":["TaskName"]},"id":"aUtdMzJokB","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:06:11.482192Z","updated_at":"2022-10-18T20:06:24.386333Z","lead_time":17.988,"prediction":{},"result_count":0,"task":231,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"In this section , we compare the two approaches by applying BERT to the CoNLL-2003 Named Entity Recognition ( NER ) task ( Tjong Kim Sang and De Meulder , 2003 ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.765604Z","updated_at":"2022-10-18T20:06:24.401657Z","inner_id":229,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":230,"annotations":[{"id":239,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:06:05.102419Z","updated_at":"2022-10-18T20:06:05.102444Z","lead_time":2.714,"prediction":{},"result_count":0,"task":230,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Second , there are major computational benefits to pre - compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation ."},"meta":{},"created_at":"2022-10-17T21:25:49.765567Z","updated_at":"2022-10-18T20:06:05.119579Z","inner_id":228,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":229,"annotations":[{"id":238,"completed_by":1,"result":[{"value":{"start":53,"end":64,"text":"Transformer","labels":["MethodName"]},"id":"sb1Jc2_Jk-","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:06:01.259226Z","updated_at":"2022-10-18T20:06:01.259252Z","lead_time":6.255,"prediction":{},"result_count":0,"task":229,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"First , not all tasks can be easily represented by a Transformer encoder architecture , and therefore require a task - specific model architecture to be added ."},"meta":{},"created_at":"2022-10-17T21:25:49.765529Z","updated_at":"2022-10-18T20:06:01.277963Z","inner_id":227,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":228,"annotations":[{"id":237,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:05:53.448257Z","updated_at":"2022-10-18T20:05:53.448278Z","lead_time":2.297,"prediction":{},"result_count":0,"task":228,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"However , the feature - based approach , where fixed features are extracted from the pretrained model , has certain advantages ."},"meta":{},"created_at":"2022-10-17T21:25:49.765492Z","updated_at":"2022-10-18T20:05:53.465472Z","inner_id":226,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":227,"annotations":[{"id":236,"completed_by":1,"result":[{"value":{"start":11,"end":15,"text":"BERT","labels":["MethodName"]},"id":"mwpeqziHMs","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:05:48.674337Z","updated_at":"2022-10-18T20:05:48.674361Z","lead_time":5.993,"prediction":{},"result_count":0,"task":227,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"All of the BERT results presented so far have used the fine - tuning approach , where a simple classification layer is added to the pre - trained model , and all parameters are jointly fine - tuned on a downstream task ."},"meta":{},"created_at":"2022-10-17T21:25:49.765454Z","updated_at":"2022-10-18T20:05:48.690588Z","inner_id":225,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":324,"annotations":[{"id":235,"completed_by":1,"result":[{"value":{"start":112,"end":115,"text":"NER","labels":["TaskName"]},"id":"SUehkv4-0I","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:05:29.681738Z","updated_at":"2022-10-18T20:05:29.681763Z","lead_time":5.747,"prediction":{},"result_count":0,"task":324,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"However , as expected , using only the MASK strategy was problematic when applying the featurebased approach to NER ."},"meta":{},"created_at":"2022-10-17T21:25:49.773093Z","updated_at":"2022-10-18T20:05:29.704835Z","inner_id":322,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":325,"annotations":[{"id":234,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:05:23.050369Z","updated_at":"2022-10-18T20:05:23.050453Z","lead_time":0.848,"prediction":{},"result_count":0,"task":325,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Interestingly , using only the RND strategy performs much worse than our strategy as well ."},"meta":{},"created_at":"2022-10-17T21:25:49.773130Z","updated_at":"2022-10-18T20:05:23.067401Z","inner_id":323,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":326,"annotations":[{"id":233,"completed_by":1,"result":[{"value":{"start":15,"end":19,"text":"BERT","labels":["MethodName"]},"id":"Zl482qavHL","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":45,"end":71,"text":"Bidirectional Transformers","labels":["MethodName"]},"id":"czgwJnL7RA","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":188,"end":192,"text":"BERT","labels":["MethodName"]},"id":"TgCjGaJxML","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:05:09.294111Z","updated_at":"2022-10-18T20:05:21.358800Z","lead_time":15.164000000000001,"prediction":{},"result_count":0,"task":326,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Appendix for \" BERT : Pre - training of Deep Bidirectional Transformers for Language Understanding \" We organize the appendix into three sections : • Additional implementation details for BERT are presented in Appendix A ; ."},"meta":{},"created_at":"2022-10-17T21:25:49.773169Z","updated_at":"2022-10-18T20:05:21.374458Z","inner_id":324,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":327,"annotations":[{"id":232,"completed_by":1,"result":[{"value":{"start":25,"end":29,"text":"BERT","labels":["MethodName"]},"id":"IWkc8WHyzP","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:05:04.082797Z","updated_at":"2022-10-18T20:05:04.082821Z","lead_time":3.338,"prediction":{},"result_count":0,"task":327,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"A Additional Details for BERT ."},"meta":{},"created_at":"2022-10-17T21:25:49.773206Z","updated_at":"2022-10-18T20:05:04.101242Z","inner_id":325,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":328,"annotations":[{"id":231,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:04:59.362612Z","updated_at":"2022-10-18T20:04:59.362642Z","lead_time":2.272,"prediction":{},"result_count":0,"task":328,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"A.1 Illustration of the Pre - training Tasks We provide examples of the pre - training tasks in the following ."},"meta":{},"created_at":"2022-10-17T21:25:49.773244Z","updated_at":"2022-10-18T20:04:59.379303Z","inner_id":326,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":329,"annotations":[{"id":230,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:04:55.213681Z","updated_at":"2022-10-18T20:04:55.213705Z","lead_time":2.742,"prediction":{},"result_count":0,"task":329,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Masked LM and the Masking Procedure Assuming the unlabeled sentence is my dog is hairy , and during the random masking procedure we chose the 4 - th token ( which corresponding to hairy ) , our masking procedure can be further illustrated by • 10 % of the time : Replace the word with a random word , e.g. , my dog is hairy → my dog is apple • 10 % of the time : Keep the word unchanged , e.g. , my dog is hairy → my dog is hairy ."},"meta":{},"created_at":"2022-10-17T21:25:49.773284Z","updated_at":"2022-10-18T20:04:55.230118Z","inner_id":327,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":330,"annotations":[{"id":229,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:04:51.420183Z","updated_at":"2022-10-18T20:04:51.420204Z","lead_time":1.072,"prediction":{},"result_count":0,"task":330,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The purpose of this is to bias the representation towards the actual observed word ."},"meta":{},"created_at":"2022-10-17T21:25:49.773322Z","updated_at":"2022-10-18T20:04:51.435598Z","inner_id":328,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":334,"annotations":[{"id":227,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:04:42.503841Z","updated_at":"2022-10-18T20:04:42.503866Z","lead_time":2.042,"prediction":{},"result_count":0,"task":334,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Compared to standard langauge model training , the masked LM only make predictions on 15 % of tokens in each batch , which suggests that more pre - training steps may be required for the model ."},"meta":{},"created_at":"2022-10-17T21:25:49.773471Z","updated_at":"2022-10-18T20:04:42.521726Z","inner_id":332,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":332,"annotations":[{"id":226,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:04:38.914840Z","updated_at":"2022-10-18T20:04:38.914864Z","lead_time":2.491,"prediction":{},"result_count":0,"task":332,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Additionally , because random replacement only occurs for 1.5 % of all tokens ( i.e. , 10 % of 15 % ) , this does not seem to harm the model 's language understanding capability ."},"meta":{},"created_at":"2022-10-17T21:25:49.773397Z","updated_at":"2022-10-18T20:04:38.931648Z","inner_id":330,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":333,"annotations":[{"id":225,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:04:35.350260Z","updated_at":"2022-10-18T20:04:35.350291Z","lead_time":1.153,"prediction":{},"result_count":0,"task":333,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"In Section C.2 , we evaluate the impact this procedure ."},"meta":{},"created_at":"2022-10-17T21:25:49.773434Z","updated_at":"2022-10-18T20:04:35.369291Z","inner_id":331,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":226,"annotations":[{"id":224,"completed_by":1,"result":[{"value":{"start":30,"end":34,"text":"BERT","labels":["MethodName"]},"id":"UoZ9Sz427W","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:04:19.156977Z","updated_at":"2022-10-18T20:04:19.157004Z","lead_time":5.24,"prediction":{},"result_count":0,"task":226,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Feature - based Approach with BERT ."},"meta":{},"created_at":"2022-10-17T21:25:49.765417Z","updated_at":"2022-10-18T20:04:19.175297Z","inner_id":224,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":225,"annotations":[{"id":223,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:04:12.181815Z","updated_at":"2022-10-18T20:04:12.181839Z","lead_time":3.431,"prediction":{},"result_count":0,"task":225,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Both of these prior works used a featurebased approach -we hypothesize that when the model is fine - tuned directly on the downstream tasks and uses only a very small number of randomly initialized additional parameters , the taskspecific models can benefit from the larger , more expressive pre - trained representations even when downstream task data is very small ."},"meta":{},"created_at":"2022-10-17T21:25:49.765379Z","updated_at":"2022-10-18T20:04:12.198291Z","inner_id":223,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":224,"annotations":[{"id":222,"completed_by":1,"result":[{"value":{"start":46,"end":67,"text":"hidden dimension size","labels":["HyperparameterName"]},"id":"dXesY3P396","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":73,"end":76,"text":"200","labels":["HyperparameterValue"]},"id":"lxYK8lXXnz","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":80,"end":83,"text":"600","labels":["HyperparameterValue"]},"id":"0S-8HuOkkB","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":119,"end":124,"text":"1,000","labels":["HyperparameterValue"]},"id":"yWTjOM_MXO","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:03:42.688218Z","updated_at":"2022-10-18T20:04:07.625716Z","lead_time":26.767,"prediction":{},"result_count":0,"task":224,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"( 2016 ) mentioned in passing that increasing hidden dimension size from 200 to 600 helped , but increasing further to 1,000 did not bring further improvements ."},"meta":{},"created_at":"2022-10-17T21:25:49.765340Z","updated_at":"2022-10-18T20:04:07.640937Z","inner_id":222,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":223,"annotations":[{"id":221,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:03:39.267396Z","updated_at":"2022-10-18T20:03:39.267427Z","lead_time":4.492,"prediction":{},"result_count":0,"task":223,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"( 2018b ) presented mixed results on the downstream task impact of increasing the pre - trained bi - LM size from two to four layers and Melamud et al ."},"meta":{},"created_at":"2022-10-17T21:25:49.765303Z","updated_at":"2022-10-18T20:03:39.284122Z","inner_id":221,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":222,"annotations":[{"id":220,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:03:33.961146Z","updated_at":"2022-10-18T20:03:33.961169Z","lead_time":0.254,"prediction":{},"result_count":0,"task":222,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Peters et al ."},"meta":{},"created_at":"2022-10-17T21:25:49.765266Z","updated_at":"2022-10-18T20:03:33.977452Z","inner_id":220,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":221,"annotations":[{"id":219,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:03:32.718911Z","updated_at":"2022-10-18T20:03:32.718934Z","lead_time":6.572,"prediction":{},"result_count":0,"task":221,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"However , we believe that this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks , provided that the model has been sufficiently pre - trained ."},"meta":{},"created_at":"2022-10-17T21:25:49.765229Z","updated_at":"2022-10-18T20:03:32.740057Z","inner_id":219,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":220,"annotations":[{"id":218,"completed_by":1,"result":[{"value":{"start":121,"end":140,"text":"machine translation","labels":["TaskName"]},"id":"qomgN4LDx5","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:03:24.201384Z","updated_at":"2022-10-18T20:03:24.201407Z","lead_time":11.747,"prediction":{},"result_count":0,"task":220,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"It has long been known that increasing the model size will lead to continual improvements on large - scale tasks such as machine translation and language modeling , which is demonstrated by the LM perplexity of held - out training data shown in Table 6 ."},"meta":{},"created_at":"2022-10-17T21:25:49.765191Z","updated_at":"2022-10-18T20:03:24.218338Z","inner_id":218,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":219,"annotations":[{"id":217,"completed_by":1,"result":[{"value":{"start":14,"end":23,"text":"BERT BASE","labels":["MethodName"]},"id":"Awcy1vFdp4","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":54,"end":64,"text":"BERT LARGE","labels":["MethodName"]},"id":"H3dK68GoLI","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:03:11.065768Z","updated_at":"2022-10-18T20:03:11.065791Z","lead_time":12.989,"prediction":{},"result_count":0,"task":219,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"By contrast , BERT BASE contains 110 M parameters and BERT LARGE contains 340 M parameters ."},"meta":{},"created_at":"2022-10-17T21:25:49.765153Z","updated_at":"2022-10-18T20:03:11.082701Z","inner_id":217,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":218,"annotations":[{"id":216,"completed_by":1,"result":[{"value":{"start":14,"end":15,"text":"L","labels":["HyperparameterName"]},"id":"6vtWqvQHad","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":20,"end":21,"text":"H","labels":["HyperparameterName"]},"id":"-Rl_02Z8-x","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":29,"end":30,"text":"A","labels":["HyperparameterName"]},"id":"F-sKy-yGmt","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":141,"end":142,"text":"L","labels":["HyperparameterName"]},"id":"_jDqbvrchG","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":148,"end":149,"text":"H","labels":["HyperparameterName"]},"id":"RhFLIFq4NR","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":156,"end":157,"text":"A","labels":["HyperparameterName"]},"id":"QrBfgvPlXn","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":16,"end":17,"text":"6","labels":["HyperparameterValue"]},"id":"sWZNsW1mt_","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":22,"end":26,"text":"1024","labels":["HyperparameterValue"]},"id":"IeL_HTJHxd","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":31,"end":33,"text":"16","labels":["HyperparameterValue"]},"id":"XbsRFuMhzO","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":143,"end":145,"text":"64","labels":["HyperparameterValue"]},"id":"FXU_Zhpa0e","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":150,"end":153,"text":"512","labels":["HyperparameterValue"]},"id":"4hIVjWUQsq","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":158,"end":159,"text":"2","labels":["HyperparameterValue"]},"id":"GlaaiJtFiI","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:02:56.378764Z","updated_at":"2022-10-18T20:02:56.378859Z","lead_time":47.252,"prediction":{},"result_count":0,"task":218,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"( 2017 ) is ( L=6 , H=1024 , A=16 ) with 100 M parameters for the encoder , and the largest Transformer we have found in the literature is ( L=64 , H=512 , A=2 ) with 235 M parameters ( Al - Rfou et al . , 2018 ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.765116Z","updated_at":"2022-10-18T20:02:56.394734Z","inner_id":216,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":217,"annotations":[{"id":215,"completed_by":1,"result":[{"value":{"start":26,"end":37,"text":"Transformer","labels":["MethodName"]},"id":"4b6r9sylFh","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:02:04.363142Z","updated_at":"2022-10-18T20:02:08.026865Z","lead_time":10.029,"prediction":{},"result_count":0,"task":217,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"For example , the largest Transformer explored in Vaswani et al ."},"meta":{},"created_at":"2022-10-17T21:25:49.765079Z","updated_at":"2022-10-18T20:02:08.043287Z","inner_id":215,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":216,"annotations":[{"id":214,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:01:56.648162Z","updated_at":"2022-10-18T20:01:56.648188Z","lead_time":1.095,"prediction":{},"result_count":0,"task":216,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"It is also perhaps surprising that we are able to achieve such significant improvements on top of models which are already quite large relative to the existing literature ."},"meta":{},"created_at":"2022-10-17T21:25:49.765042Z","updated_at":"2022-10-18T20:01:56.665227Z","inner_id":214,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":215,"annotations":[{"id":213,"completed_by":1,"result":[{"value":{"start":104,"end":108,"text":"MRPC","labels":["DatasetName"]},"id":"IIbgGtO5ZK","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:01:53.523652Z","updated_at":"2022-10-18T20:01:53.523676Z","lead_time":5.697,"prediction":{},"result_count":0,"task":215,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"We can see that larger models lead to a strict accuracy improvement across all four datasets , even for MRPC which only has 3,600 labeled training examples , and is substantially different from the pre - training tasks ."},"meta":{},"created_at":"2022-10-17T21:25:49.765004Z","updated_at":"2022-10-18T20:01:53.540389Z","inner_id":213,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":214,"annotations":[{"id":212,"completed_by":1,"result":[{"value":{"start":46,"end":54,"text":"accuracy","labels":["MetricName"]},"id":"2vpPng8MYx","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:01:46.909021Z","updated_at":"2022-10-18T20:01:46.909041Z","lead_time":12.398,"prediction":{},"result_count":0,"task":214,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"In this table , we report the average Dev Set accuracy from 5 random restarts of fine - tuning ."},"meta":{},"created_at":"2022-10-17T21:25:49.764966Z","updated_at":"2022-10-18T20:01:46.925398Z","inner_id":212,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":213,"annotations":[{"id":211,"completed_by":1,"result":[{"value":{"start":20,"end":24,"text":"GLUE","labels":["DatasetName"]},"id":"1P1ZWOOB2r","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:01:31.718577Z","updated_at":"2022-10-18T20:01:31.718602Z","lead_time":5.188,"prediction":{},"result_count":0,"task":213,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Results on selected GLUE tasks are shown in Table 6 ."},"meta":{},"created_at":"2022-10-17T21:25:49.764928Z","updated_at":"2022-10-18T20:01:31.736556Z","inner_id":211,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":211,"annotations":[{"id":210,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:01:15.269275Z","updated_at":"2022-10-18T20:01:15.269297Z","lead_time":5.693,"prediction":{},"result_count":0,"task":211,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"In this section , we explore the effect of model size on fine - tuning task accuracy ."},"meta":{},"created_at":"2022-10-17T21:25:49.764852Z","updated_at":"2022-10-18T20:01:15.285887Z","inner_id":209,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":210,"annotations":[{"id":209,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:01:08.675327Z","updated_at":"2022-10-18T20:01:08.675361Z","lead_time":0.787,"prediction":{},"result_count":0,"task":210,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Effect of Model Size ."},"meta":{},"created_at":"2022-10-17T21:25:49.764814Z","updated_at":"2022-10-18T20:01:08.692979Z","inner_id":208,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":208,"annotations":[{"id":208,"completed_by":1,"result":[{"value":{"start":150,"end":154,"text":"ELMo","labels":["MethodName"]},"id":"KXIZNjLrT7","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:01:05.376484Z","updated_at":"2022-10-18T20:01:05.376508Z","lead_time":4.697,"prediction":{},"result_count":0,"task":208,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"We recognize that it would also be possible to train separate LTR and RTL models and represent each token as the concatenation of the two models , as ELMo does ."},"meta":{},"created_at":"2022-10-17T21:25:49.764738Z","updated_at":"2022-10-18T20:01:05.393889Z","inner_id":206,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":209,"annotations":[{"id":207,"completed_by":1,"result":[{"value":{"start":122,"end":124,"text":"QA","labels":["TaskName"]},"id":"6o7Z43LjiI","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:00:59.368363Z","updated_at":"2022-10-18T20:00:59.368384Z","lead_time":10.42,"prediction":{},"result_count":0,"task":209,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"However : ( a ) this is twice as expensive as a single bidirectional model ; ( b ) this is non - intuitive for tasks like QA , since the RTL model would not be able to condition the answer on the question ; ( c ) this it is strictly less powerful than a deep bidirectional model , since it can use both left and right context at every layer ."},"meta":{},"created_at":"2022-10-17T21:25:49.764776Z","updated_at":"2022-10-18T20:00:59.385478Z","inner_id":207,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":207,"annotations":[{"id":206,"completed_by":1,"result":[{"value":{"start":4,"end":10,"text":"BiLSTM","labels":["MethodName"]},"id":"XDHXd2--dk","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":36,"end":40,"text":"GLUE","labels":["DatasetName"]},"id":"OLv5N6a4V-","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:00:22.510709Z","updated_at":"2022-10-18T20:00:22.510728Z","lead_time":8.655,"prediction":{},"result_count":0,"task":207,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The BiLSTM hurts performance on the GLUE tasks ."},"meta":{},"created_at":"2022-10-17T21:25:49.764700Z","updated_at":"2022-10-18T20:00:22.528344Z","inner_id":205,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":206,"annotations":[{"id":205,"completed_by":1,"result":[{"value":{"start":43,"end":48,"text":"SQuAD","labels":["DatasetName"]},"id":"zS6GELVdTC","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:00:12.660127Z","updated_at":"2022-10-18T20:00:12.660150Z","lead_time":5.026,"prediction":{},"result_count":0,"task":206,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"This does significantly improve results on SQuAD , but the results are still far worse than those of the pretrained bidirectional models ."},"meta":{},"created_at":"2022-10-17T21:25:49.764663Z","updated_at":"2022-10-18T20:00:12.676417Z","inner_id":204,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":205,"annotations":[{"id":204,"completed_by":1,"result":[{"value":{"start":104,"end":110,"text":"BiLSTM","labels":["MethodName"]},"id":"1bapi-TPQG","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T20:00:01.427450Z","updated_at":"2022-10-18T20:00:06.074387Z","lead_time":10.487,"prediction":{},"result_count":0,"task":205,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"In order to make a good faith attempt at strengthening the LTR system , we added a randomly initialized BiLSTM on top ."},"meta":{},"created_at":"2022-10-17T21:25:49.764625Z","updated_at":"2022-10-18T20:00:06.091774Z","inner_id":203,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":204,"annotations":[{"id":203,"completed_by":1,"result":[{"value":{"start":4,"end":9,"text":"SQuAD","labels":["DatasetName"]},"id":"Dub0k_qmG2","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:59:54.170227Z","updated_at":"2022-10-18T19:59:54.170256Z","lead_time":6.324,"prediction":{},"result_count":0,"task":204,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"For SQuAD it is intuitively clear that a LTR model will perform poorly at token predictions , since the token - level hidden states have no rightside context ."},"meta":{},"created_at":"2022-10-17T21:25:49.764586Z","updated_at":"2022-10-18T19:59:54.187751Z","inner_id":202,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":203,"annotations":[{"id":202,"completed_by":1,"result":[{"value":{"start":83,"end":87,"text":"MRPC","labels":["DatasetName"]},"id":"bAkoRJb-YX","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":92,"end":97,"text":"SQuAD","labels":["DatasetName"]},"id":"ugUNBS-WAa","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:59:46.470069Z","updated_at":"2022-10-18T19:59:46.470093Z","lead_time":17.134,"prediction":{},"result_count":0,"task":203,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The LTR model performs worse than the MLM model on all tasks , with large drops on MRPC and SQuAD ."},"meta":{},"created_at":"2022-10-17T21:25:49.764548Z","updated_at":"2022-10-18T19:59:46.487385Z","inner_id":201,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":202,"annotations":[{"id":201,"completed_by":1,"result":[{"value":{"start":35,"end":38,"text":"NSP","labels":["TaskName"]},"id":"C43NpH2aTH","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":74,"end":78,"text":"QNLI","labels":["DatasetName"]},"id":"U4EPgdFyjn","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":81,"end":85,"text":"MNLI","labels":["DatasetName"]},"id":"bTsGKXFLzF","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":92,"end":101,"text":"SQuAD 1.1","labels":["DatasetName"]},"id":"7oG3dX00KS","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":194,"end":197,"text":"NSP","labels":["TaskName"]},"id":"B9qn7dnUDH","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":214,"end":217,"text":"NSP","labels":["TaskName"]},"id":"wAp47F0_TJ","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:59:27.654335Z","updated_at":"2022-10-18T19:59:27.654358Z","lead_time":25.539,"prediction":{},"result_count":0,"task":202,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"In Table 5 , we show that removing NSP hurts performance significantly on QNLI , MNLI , and SQuAD 1.1 . Next , we evaluate the impact of training bidirectional representations by comparing \" No NSP \" to \" LTR & No NSP \" ."},"meta":{},"created_at":"2022-10-17T21:25:49.764510Z","updated_at":"2022-10-18T19:59:27.673456Z","inner_id":200,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":201,"annotations":[{"id":200,"completed_by":1,"result":[{"value":{"start":43,"end":46,"text":"NSP","labels":["TaskName"]},"id":"e_0Eo9mmm4","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:59:00.877772Z","updated_at":"2022-10-18T19:59:00.877797Z","lead_time":6.426,"prediction":{},"result_count":0,"task":201,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"We first examine the impact brought by the NSP task ."},"meta":{},"created_at":"2022-10-17T21:25:49.764471Z","updated_at":"2022-10-18T19:59:00.896060Z","inner_id":199,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":200,"annotations":[{"id":199,"completed_by":1,"result":[{"value":{"start":31,"end":41,"text":"OpenAI GPT","labels":["MethodName"]},"id":"s_1t1fpo-W","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:58:52.608774Z","updated_at":"2022-10-18T19:58:52.608801Z","lead_time":3.413,"prediction":{},"result_count":0,"task":200,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"This is directly comparable to OpenAI GPT , but using our larger training dataset , our input representation , and our fine - tuning scheme ."},"meta":{},"created_at":"2022-10-17T21:25:49.764433Z","updated_at":"2022-10-18T19:58:52.625482Z","inner_id":198,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":199,"annotations":[{"id":198,"completed_by":1,"result":[{"value":{"start":56,"end":59,"text":"NSP","labels":["TaskName"]},"id":"-x46oCaVrY","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:58:47.972274Z","updated_at":"2022-10-18T19:58:47.972303Z","lead_time":6.689,"prediction":{},"result_count":0,"task":199,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Additionally , this model was pre - trained without the NSP task ."},"meta":{},"created_at":"2022-10-17T21:25:49.764395Z","updated_at":"2022-10-18T19:58:47.989886Z","inner_id":197,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":198,"annotations":[{"id":197,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:58:40.327889Z","updated_at":"2022-10-18T19:58:40.327911Z","lead_time":3.137,"prediction":{},"result_count":0,"task":198,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The left - only constraint was also applied at fine - tuning , because removing it introduced a pre - train \/ fine - tune mismatch that degraded downstream performance ."},"meta":{},"created_at":"2022-10-17T21:25:49.764357Z","updated_at":"2022-10-18T19:58:40.344138Z","inner_id":196,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":197,"annotations":[{"id":196,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:58:35.989708Z","updated_at":"2022-10-18T19:58:35.989734Z","lead_time":3.96,"prediction":{},"result_count":0,"task":197,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"A left - context - only model which is trained using a standard Left - to - Right ( LTR ) LM , rather than an MLM ."},"meta":{},"created_at":"2022-10-17T21:25:49.764318Z","updated_at":"2022-10-18T19:58:36.008625Z","inner_id":195,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":196,"annotations":[{"id":195,"completed_by":1,"result":[{"value":{"start":9,"end":12,"text":"NSP","labels":["TaskName"]},"id":"BV8ESqHCcD","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:58:16.214945Z","updated_at":"2022-10-18T19:58:29.391928Z","lead_time":13.552,"prediction":{},"result_count":0,"task":196,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"LTR & No NSP : ."},"meta":{},"created_at":"2022-10-17T21:25:49.764279Z","updated_at":"2022-10-18T19:58:29.409983Z","inner_id":194,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":195,"annotations":[{"id":194,"completed_by":1,"result":[{"value":{"start":62,"end":66,"text":"BERT","labels":["MethodName"]},"id":"DAQfxN4WoL","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":196,"end":205,"text":"BERT BASE","labels":["MethodName"]},"id":"Mui7hMx2xv","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":306,"end":330,"text":"next sentence prediction","labels":["TaskName"]},"id":"XNVN_HscWz","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":335,"end":338,"text":"NSP","labels":["TaskName"]},"id":"_WbWdQtPcZ","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:57:56.419473Z","updated_at":"2022-10-18T19:58:10.890244Z","lead_time":18.785,"prediction":{},"result_count":0,"task":195,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"We demonstrate the importance of the deep bidirectionality of BERT by evaluating two pretraining objectives using exactly the same pretraining data , fine - tuning scheme , and hyperparameters as BERT BASE : No NSP : A bidirectional model which is trained using the \" masked LM \" ( MLM ) but without the \" next sentence prediction \" ( NSP ) task ."},"meta":{},"created_at":"2022-10-17T21:25:49.764201Z","updated_at":"2022-10-18T19:58:10.906161Z","inner_id":193,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":194,"annotations":[{"id":193,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:57:50.913982Z","updated_at":"2022-10-18T19:57:50.914009Z","lead_time":0.186,"prediction":{},"result_count":0,"task":194,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Effect of Pre - training Tasks ."},"meta":{},"created_at":"2022-10-17T21:25:49.764059Z","updated_at":"2022-10-18T19:57:50.931666Z","inner_id":192,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":193,"annotations":[{"id":192,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:57:49.911249Z","updated_at":"2022-10-18T19:57:49.911277Z","lead_time":0.254,"prediction":{},"result_count":0,"task":193,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Additional ablation studies can be found in Appendix C."},"meta":{},"created_at":"2022-10-17T21:25:49.764017Z","updated_at":"2022-10-18T19:57:49.928241Z","inner_id":191,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":192,"annotations":[{"id":191,"completed_by":1,"result":[{"value":{"start":77,"end":81,"text":"BERT","labels":["MethodName"]},"id":"U5AhqxO4bp","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:57:48.237155Z","updated_at":"2022-10-18T19:57:48.237178Z","lead_time":9.936,"prediction":{},"result_count":0,"task":192,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"In this section , we perform ablation experiments over a number of facets of BERT in order to better understand their relative importance ."},"meta":{},"created_at":"2022-10-17T21:25:49.763979Z","updated_at":"2022-10-18T19:57:48.254475Z","inner_id":190,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":191,"annotations":[{"id":190,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:57:37.440700Z","updated_at":"2022-10-18T19:57:37.440726Z","lead_time":0.426,"prediction":{},"result_count":0,"task":191,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Ablation Studies ."},"meta":{},"created_at":"2022-10-17T21:25:49.763943Z","updated_at":"2022-10-18T19:57:37.457415Z","inner_id":189,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":190,"annotations":[{"id":189,"completed_by":1,"result":[{"value":{"start":0,"end":10,"text":"BERT LARGE","labels":["MethodName"]},"id":"QAwHUhMdEI","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":51,"end":55,"text":"ELMo","labels":["MethodName"]},"id":"vvuaaHTabf","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":46,"end":50,"text":"ESIM","labels":["MethodName"]},"id":"rrZX6osRhr","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":78,"end":88,"text":"OpenAI GPT","labels":["MethodName"]},"id":"C7DLFb8DCJ","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":66,"end":73,"text":"+27.1 %","labels":["MetricValue"]},"id":"DzoXsTo2AK","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":92,"end":97,"text":"8.3 %","labels":["MetricValue"]},"id":"ZrvH9vgHfl","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:57:21.334492Z","updated_at":"2022-10-18T19:57:34.824803Z","lead_time":32.147999999999996,"prediction":{},"result_count":0,"task":190,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"BERT LARGE outperforms the authors ' baseline ESIM+ELMo system by +27.1 % and OpenAI GPT by 8.3 % ."},"meta":{},"created_at":"2022-10-17T21:25:49.763905Z","updated_at":"2022-10-18T19:57:34.839931Z","inner_id":188,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":189,"annotations":[{"id":188,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:57:01.524539Z","updated_at":"2022-10-18T19:57:01.524560Z","lead_time":0.347,"prediction":{},"result_count":0,"task":189,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Results are presented in Table 4 ."},"meta":{},"created_at":"2022-10-17T21:25:49.763868Z","updated_at":"2022-10-18T19:57:01.539712Z","inner_id":187,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":188,"annotations":[{"id":187,"completed_by":1,"result":[{"value":{"start":45,"end":58,"text":"learning rate","labels":["HyperparameterName"]},"id":"lnWXhM2zDI","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":73,"end":83,"text":"batch size","labels":["HyperparameterName"]},"id":"AtcpTdQA96","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":62,"end":66,"text":"2e-5","labels":["HyperparameterValue"]},"id":"JDkLc-iUAN","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":87,"end":89,"text":"16","labels":["HyperparameterValue"]},"id":"fuHVCfQopU","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:57:00.188393Z","updated_at":"2022-10-18T19:57:00.188417Z","lead_time":18.481,"prediction":{},"result_count":0,"task":188,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"We fine - tune the model for 3 epochs with a learning rate of 2e-5 and a batch size of 16 ."},"meta":{},"created_at":"2022-10-17T21:25:49.763831Z","updated_at":"2022-10-18T19:57:00.206396Z","inner_id":186,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":187,"annotations":[{"id":186,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:56:39.816146Z","updated_at":"2022-10-18T19:56:39.816174Z","lead_time":3.152,"prediction":{},"result_count":0,"task":187,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The only task - specific parameters introduced is a vector whose dot product with the [ CLS ] token representation C denotes a score for each choice which is normalized with a softmax layer ."},"meta":{},"created_at":"2022-10-17T21:25:49.763793Z","updated_at":"2022-10-18T19:56:39.834788Z","inner_id":185,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":186,"annotations":[{"id":185,"completed_by":1,"result":[{"value":{"start":26,"end":30,"text":"SWAG","labels":["DatasetName"]},"id":"90do4ieQYD","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:56:35.465633Z","updated_at":"2022-10-18T19:56:35.465677Z","lead_time":10.346,"prediction":{},"result_count":0,"task":186,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"When fine - tuning on the SWAG dataset , we construct four input sequences , each containing the concatenation of the given sentence ( sentence A ) and a possible continuation ( sentence B ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.763756Z","updated_at":"2022-10-18T19:56:35.484905Z","inner_id":184,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":185,"annotations":[{"id":184,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:56:23.871078Z","updated_at":"2022-10-18T19:56:23.871108Z","lead_time":1.686,"prediction":{},"result_count":0,"task":185,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Given a sentence , the task is to choose the most plausible continuation among four choices ."},"meta":{},"created_at":"2022-10-17T21:25:49.763718Z","updated_at":"2022-10-18T19:56:23.887663Z","inner_id":183,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":184,"annotations":[{"id":183,"completed_by":1,"result":[{"value":{"start":4,"end":43,"text":"Situations With Adversarial Generations","labels":["DatasetName"]},"id":"8Vvnsd1O8F","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":46,"end":50,"text":"SWAG","labels":["DatasetName"]},"id":"vuhDthDSNw","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:56:21.089612Z","updated_at":"2022-10-18T19:56:21.089638Z","lead_time":12.787,"prediction":{},"result_count":0,"task":184,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The Situations With Adversarial Generations ( SWAG ) dataset contains 113k sentence - pair completion examples that evaluate grounded commonsense inference ( Zellers et al . , 2018 ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.763680Z","updated_at":"2022-10-18T19:56:21.106883Z","inner_id":182,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":183,"annotations":[{"id":182,"completed_by":1,"result":[{"value":{"start":0,"end":4,"text":"SWAG","labels":["DatasetName"]},"id":"BQdZdjCUKB","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:56:05.349664Z","updated_at":"2022-10-18T19:56:05.349694Z","lead_time":37.762,"prediction":{},"result_count":0,"task":183,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"SWAG ."},"meta":{},"created_at":"2022-10-17T21:25:49.763639Z","updated_at":"2022-10-18T19:56:05.367943Z","inner_id":181,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":182,"annotations":[{"id":181,"completed_by":1,"result":[{"value":{"start":18,"end":20,"text":"F1","labels":["MetricName"]},"id":"C4DwCDq6OE","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":13,"end":17,"text":"+5.1","labels":["MetricValue"]},"id":"amh1UT7EOq","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:55:19.902634Z","updated_at":"2022-10-18T19:55:25.051795Z","lead_time":14.73,"prediction":{},"result_count":0,"task":182,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"We observe a +5.1 F1 improvement over the previous best system ."},"meta":{},"created_at":"2022-10-17T21:25:49.763600Z","updated_at":"2022-10-18T19:55:25.067042Z","inner_id":180,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":181,"annotations":[{"id":180,"completed_by":1,"result":[{"value":{"start":167,"end":171,"text":"BERT","labels":["MethodName"]},"id":"5dBxcjhpmL","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:55:07.679637Z","updated_at":"2022-10-18T19:55:07.679661Z","lead_time":5.195,"prediction":{},"result_count":0,"task":181,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The results compared to prior leaderboard entries and top published work ( Sun et al . , 2018;Wang et al . , 2018b ) are shown in Table 3 , excluding systems that use BERT as one of their components ."},"meta":{},"created_at":"2022-10-17T21:25:49.763561Z","updated_at":"2022-10-18T19:55:07.696977Z","inner_id":179,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":180,"annotations":[{"id":179,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:55:01.631889Z","updated_at":"2022-10-18T19:55:01.631916Z","lead_time":0.203,"prediction":{},"result_count":0,"task":180,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"System ."},"meta":{},"created_at":"2022-10-17T21:25:49.763523Z","updated_at":"2022-10-18T19:55:01.647903Z","inner_id":178,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":179,"annotations":[{"id":178,"completed_by":1,"result":[{"value":{"start":36,"end":49,"text":"learning rate","labels":["HyperparameterName"]},"id":"-D8jlz3oGp","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":53,"end":57,"text":"5e-5","labels":["HyperparameterValue"]},"id":"-vaZPDx8Mw","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":78,"end":80,"text":"48","labels":["HyperparameterValue"]},"id":"hX5EZZkRBJ","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":64,"end":74,"text":"batch size","labels":["HyperparameterName"]},"id":"Nt04YphLC4","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:55:00.120772Z","updated_at":"2022-10-18T19:55:00.120797Z","lead_time":28.157,"prediction":{},"result_count":0,"task":179,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"We fine - tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48 ."},"meta":{},"created_at":"2022-10-17T21:25:49.763484Z","updated_at":"2022-10-18T19:55:00.142329Z","inner_id":177,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":178,"annotations":[{"id":177,"completed_by":1,"result":[{"value":{"start":15,"end":23,"text":"TriviaQA","labels":["DatasetName"]},"id":"4jWw85OXr_","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:54:30.803846Z","updated_at":"2022-10-18T19:54:30.803872Z","lead_time":5.94,"prediction":{},"result_count":0,"task":178,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"We did not use TriviaQA data for this model ."},"meta":{},"created_at":"2022-10-17T21:25:49.763445Z","updated_at":"2022-10-18T19:54:30.822246Z","inner_id":176,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":177,"annotations":[{"id":176,"completed_by":1,"result":[{"value":{"start":120,"end":122,"text":"F1","labels":["MetricName"]},"id":"EC54VM0Nfr","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:54:17.144265Z","updated_at":"2022-10-18T19:54:21.614512Z","lead_time":5.636,"prediction":{},"result_count":0,"task":177,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"We predict a non - null answer when ŝ i , j > s null + τ , where the threshold τ is selected on the dev set to maximize F1 ."},"meta":{},"created_at":"2022-10-17T21:25:49.763384Z","updated_at":"2022-10-18T19:54:21.630203Z","inner_id":175,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":176,"annotations":[{"id":175,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:54:14.946140Z","updated_at":"2022-10-18T19:54:14.946164Z","lead_time":0.437,"prediction":{},"result_count":0,"task":176,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"ŝ i , j = max j≥i S•T i + E•T j ."},"meta":{},"created_at":"2022-10-17T21:25:49.760814Z","updated_at":"2022-10-18T19:54:14.963478Z","inner_id":174,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":175,"annotations":[{"id":174,"completed_by":1,"result":[{"value":{"start":181,"end":196,"text":"TriviaQA - Wiki","labels":["DatasetName"]},"id":"9FfbyRX9OD","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":131,"end":139,"text":"TriviaQA","labels":["DatasetName"]},"id":"-ca_bJVKDF","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:53:57.241659Z","updated_at":"2022-10-18T19:54:07.054857Z","lead_time":16.637999999999998,"prediction":{},"result_count":0,"task":175,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"For prediction , we compare the score of the no - answer span : s null = S•C + E•C to the score of the best non - null span 12 The TriviaQA data we used consists of paragraphs from TriviaQA - Wiki formed of the first 400 tokens in documents , that contain at least one of the provided possible answers ."},"meta":{},"created_at":"2022-10-17T21:25:49.760776Z","updated_at":"2022-10-18T19:54:07.073001Z","inner_id":173,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":174,"annotations":[{"id":173,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:53:48.243809Z","updated_at":"2022-10-18T19:53:48.243833Z","lead_time":1.48,"prediction":{},"result_count":0,"task":174,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The probability space for the start and end answer span positions is extended to include the position of the [ CLS ] token ."},"meta":{},"created_at":"2022-10-17T21:25:49.760737Z","updated_at":"2022-10-18T19:53:48.261761Z","inner_id":172,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":173,"annotations":[{"id":172,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:53:45.562634Z","updated_at":"2022-10-18T19:53:45.562660Z","lead_time":2.106,"prediction":{},"result_count":0,"task":173,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"We treat questions that do not have an answer as having an answer span with start and end at the [ CLS ] token ."},"meta":{},"created_at":"2022-10-17T21:25:49.760699Z","updated_at":"2022-10-18T19:53:45.580690Z","inner_id":171,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":172,"annotations":[{"id":171,"completed_by":1,"result":[{"value":{"start":50,"end":54,"text":"BERT","labels":["MethodName"]},"id":"Iw6uuuEofK","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":39,"end":49,"text":"SQuAD v1.1","labels":["DatasetName"]},"id":"VGVJux0eo_","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:53:41.046489Z","updated_at":"2022-10-18T19:53:41.046514Z","lead_time":13.58,"prediction":{},"result_count":0,"task":172,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"We use a simple approach to extend the SQuAD v1.1 BERT model for this task ."},"meta":{},"created_at":"2022-10-17T21:25:49.760662Z","updated_at":"2022-10-18T19:53:41.064099Z","inner_id":170,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":171,"annotations":[{"id":170,"completed_by":1,"result":[{"value":{"start":7,"end":17,"text":"SQuAD v2.0","labels":["DatasetName"]},"id":"rElwjoF-RH","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":22,"end":31,"text":"SQuAD 2.0","labels":["DatasetName"]},"id":"10GknJ9NEs","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":49,"end":58,"text":"SQuAD 1.1","labels":["DatasetName"]},"id":"I6S1ajm7Pp","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:53:24.820601Z","updated_at":"2022-10-18T19:53:24.820621Z","lead_time":32.835,"prediction":{},"result_count":0,"task":171,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"12 4.3 SQuAD v2.0 The SQuAD 2.0 task extends the SQuAD 1.1 problem definition by allowing for the possibility that no short answer exists in the provided paragraph , making the problem more realistic ."},"meta":{},"created_at":"2022-10-17T21:25:49.760624Z","updated_at":"2022-10-18T19:53:24.838345Z","inner_id":169,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":170,"annotations":[{"id":169,"completed_by":1,"result":[{"value":{"start":37,"end":39,"text":"F1","labels":["MetricName"]},"id":"C5NkHF1zUS","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":27,"end":30,"text":"0.1","labels":["MetricValue"]},"id":"EmoSq5vYxS","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":33,"end":36,"text":"0.4","labels":["MetricValue"]},"id":"htLfM2jHZc","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:52:50.077620Z","updated_at":"2022-10-18T19:52:50.077664Z","lead_time":18.085,"prediction":{},"result_count":0,"task":170,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"tuning data , we only lose 0.1 - 0.4 F1 , still outperforming all existing systems by a wide margin ."},"meta":{},"created_at":"2022-10-17T21:25:49.760586Z","updated_at":"2022-10-18T19:52:50.095740Z","inner_id":168,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":169,"annotations":[{"id":168,"completed_by":1,"result":[{"value":{"start":21,"end":25,"text":"BERT","labels":["MethodName"]},"id":"jIZRnfrthB","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":80,"end":82,"text":"F1","labels":["MetricName"]},"id":"vgWTD8Phj0","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:52:30.579454Z","updated_at":"2022-10-18T19:52:30.579485Z","lead_time":9.751,"prediction":{},"result_count":0,"task":169,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"In fact , our single BERT model outperforms the top ensemble system in terms of F1 score ."},"meta":{},"created_at":"2022-10-17T21:25:49.760548Z","updated_at":"2022-10-18T19:52:30.598584Z","inner_id":167,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":168,"annotations":[{"id":167,"completed_by":1,"result":[{"value":{"start":74,"end":76,"text":"F1","labels":["MetricName"]},"id":"xJ-e4oj-3u","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":100,"end":102,"text":"F1","labels":["MetricName"]},"id":"xToFyYFF9j","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":69,"end":73,"text":"+1.5","labels":["MetricValue"]},"id":"-kEGpE-nYT","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":95,"end":99,"text":"+1.3","labels":["MetricValue"]},"id":"Gklakcg_IM","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:52:19.476815Z","updated_at":"2022-10-18T19:52:19.476851Z","lead_time":20.256,"prediction":{},"result_count":0,"task":168,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Our best performing system outperforms the top leaderboard system by +1.5 F1 in ensembling and +1.3 F1 as a single system ."},"meta":{},"created_at":"2022-10-17T21:25:49.760502Z","updated_at":"2022-10-18T19:52:19.493664Z","inner_id":166,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":167,"annotations":[{"id":166,"completed_by":1,"result":[{"value":{"start":82,"end":90,"text":"TriviaQA","labels":["DatasetName"]},"id":"v71m2grSQO","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":139,"end":144,"text":"SQuAD","labels":["DatasetName"]},"id":"8-FAsTkVdp","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:51:57.910144Z","updated_at":"2022-10-18T19:51:57.910166Z","lead_time":13.067,"prediction":{},"result_count":0,"task":167,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"We therefore use modest data augmentation in our system by first fine - tuning on TriviaQA ( Joshi et al . , 2017 ) befor fine - tuning on SQuAD ."},"meta":{},"created_at":"2022-10-17T21:25:49.760331Z","updated_at":"2022-10-18T19:51:57.926886Z","inner_id":165,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":166,"annotations":[{"id":165,"completed_by":1,"result":[{"value":{"start":25,"end":30,"text":"SQuAD","labels":["DatasetName"]},"id":"Tly2umpDy9","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:51:42.959499Z","updated_at":"2022-10-18T19:51:42.959521Z","lead_time":5.896,"prediction":{},"result_count":0,"task":166,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The top results from the SQuAD leaderboard do not have up - to - date public system descriptions available , 11 and are allowed to use any public data when training their systems ."},"meta":{},"created_at":"2022-10-17T21:25:49.760294Z","updated_at":"2022-10-18T19:51:42.977885Z","inner_id":164,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":165,"annotations":[{"id":164,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:51:34.424058Z","updated_at":"2022-10-18T19:51:34.424085Z","lead_time":2.856,"prediction":{},"result_count":0,"task":165,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Table 2 shows top leaderboard entries as well as results from top published systems ( Seo et al . , 2017;Clark and Gardner , 2018;Peters et al . , 2018a;Hu et al . , 2018 ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.760256Z","updated_at":"2022-10-18T19:51:34.442845Z","inner_id":163,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":164,"annotations":[{"id":163,"completed_by":1,"result":[{"value":{"start":35,"end":48,"text":"learning rate","labels":["HyperparameterName"]},"id":"CGG-XEteLo","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":52,"end":56,"text":"5e-5","labels":["HyperparameterValue"]},"id":"Ftd3CGEbCP","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":63,"end":73,"text":"batch size","labels":["HyperparameterName"]},"id":"XU-XLVBLDc","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":77,"end":79,"text":"32","labels":["HyperparameterValue"]},"id":"suwk1bDX7N","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:51:12.808868Z","updated_at":"2022-10-18T19:51:28.911547Z","lead_time":16.337,"prediction":{},"result_count":0,"task":164,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"We fine - tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32 ."},"meta":{},"created_at":"2022-10-17T21:25:49.760216Z","updated_at":"2022-10-18T19:51:28.927416Z","inner_id":162,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":163,"annotations":[{"id":162,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:51:11.272092Z","updated_at":"2022-10-18T19:51:11.272121Z","lead_time":1.449,"prediction":{},"result_count":0,"task":163,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The training objective is the sum of the log - likelihoods of the correct start and end positions ."},"meta":{},"created_at":"2022-10-17T21:25:49.760172Z","updated_at":"2022-10-18T19:51:11.288041Z","inner_id":161,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":162,"annotations":[{"id":161,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:51:07.918650Z","updated_at":"2022-10-18T19:51:07.918677Z","lead_time":1.078,"prediction":{},"result_count":0,"task":162,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The score of a candidate span from position i to position j is defined as S•T i + E•T j , and the maximum scoring span where j ≥ i is used as a prediction ."},"meta":{},"created_at":"2022-10-17T21:25:49.760063Z","updated_at":"2022-10-18T19:51:07.935110Z","inner_id":160,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":161,"annotations":[{"id":160,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:51:05.598675Z","updated_at":"2022-10-18T19:51:05.598697Z","lead_time":1.733,"prediction":{},"result_count":0,"task":161,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The analogous formula is used for the end of the answer span ."},"meta":{},"created_at":"2022-10-17T21:25:49.760025Z","updated_at":"2022-10-18T19:51:05.615404Z","inner_id":159,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":160,"annotations":[{"id":159,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:50:42.286892Z","updated_at":"2022-10-18T19:50:42.286917Z","lead_time":4.962,"prediction":{},"result_count":0,"task":160,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The probability of word i being the start of the answer span is computed as a dot product between T i and S followed by a softmax over all of the words in the paragraph : P i = e S•T i j e S•T j ."},"meta":{},"created_at":"2022-10-17T21:25:49.759988Z","updated_at":"2022-10-18T19:50:42.303004Z","inner_id":158,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":159,"annotations":[{"id":158,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:50:36.001440Z","updated_at":"2022-10-18T19:50:36.001463Z","lead_time":4.416,"prediction":{},"result_count":0,"task":159,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"We only introduce a start vector S ∈ R H and an end vector E ∈ R H during fine - tuning ."},"meta":{},"created_at":"2022-10-17T21:25:49.759950Z","updated_at":"2022-10-18T19:50:36.017902Z","inner_id":157,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":158,"annotations":[{"id":157,"completed_by":1,"result":[{"value":{"start":30,"end":48,"text":"question answering","labels":["TaskName"]},"id":"heFutpKDN5","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:50:29.298252Z","updated_at":"2022-10-18T19:50:29.298278Z","lead_time":10.614,"prediction":{},"result_count":0,"task":158,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"As shown in Figure 1 , in the question answering task , we represent the input question and passage as a single packed sequence , with the question using the A embedding and the passage using the B embedding ."},"meta":{},"created_at":"2022-10-17T21:25:49.759913Z","updated_at":"2022-10-18T19:50:29.315149Z","inner_id":156,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":157,"annotations":[{"id":156,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:50:17.239049Z","updated_at":"2022-10-18T19:50:17.239072Z","lead_time":4.984,"prediction":{},"result_count":0,"task":157,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"10 https:\/\/gluebenchmark.com\/leaderboard Wikipedia containing the answer , the task is to predict the answer text span in the passage ."},"meta":{},"created_at":"2022-10-17T21:25:49.759875Z","updated_at":"2022-10-18T19:50:17.255186Z","inner_id":155,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":156,"annotations":[{"id":155,"completed_by":1,"result":[{"value":{"start":42,"end":46,"text":"GLUE","labels":["DatasetName"]},"id":"Kr7uwFb5uG","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":130,"end":134,"text":"GLUE","labels":["DatasetName"]},"id":"8kTH_SabsX","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":176,"end":184,"text":"BERTBASE","labels":["MethodName"]},"id":"eX6u4seCvy","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":189,"end":198,"text":"BERTLARGE","labels":["MethodName"]},"id":"CZgamE3LqN","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:50:09.990262Z","updated_at":"2022-10-18T19:50:09.990286Z","lead_time":22.77,"prediction":{},"result_count":0,"task":156,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Given a question and a passage from 9 The GLUE data set distribution does not include the Test labels , and we only made a single GLUE evaluation server submission for each of BERTBASE and BERTLARGE ."},"meta":{},"created_at":"2022-10-17T21:25:49.759839Z","updated_at":"2022-10-18T19:50:10.008659Z","inner_id":154,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":155,"annotations":[{"id":154,"completed_by":1,"result":[{"value":{"start":70,"end":80,"text":"SQuAD v1.1","labels":["DatasetName"]},"id":"GlsAuY8qWv","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":87,"end":122,"text":"Stanford Question Answering Dataset","labels":["DatasetName"]},"id":"wYdRQtNLsR","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":125,"end":135,"text":"SQuAD v1.1","labels":["DatasetName"]},"id":"_wIVCttrxd","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:49:45.714404Z","updated_at":"2022-10-18T19:49:45.714433Z","lead_time":23.09,"prediction":{},"result_count":0,"task":155,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The effect of model size is explored more thoroughly in Section 5.2 . SQuAD v1.1 . The Stanford Question Answering Dataset ( SQuAD v1.1 ) is a collection of 100k crowdsourced question \/ answer pairs ( Rajpurkar et al . , 2016 ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.759802Z","updated_at":"2022-10-18T19:49:45.731715Z","inner_id":153,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":154,"annotations":[{"id":153,"completed_by":1,"result":[{"value":{"start":13,"end":23,"text":"BERT LARGE","labels":["MethodName"]},"id":"trTTdsaqZN","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":50,"end":59,"text":"BERT BASE","labels":["MethodName"]},"id":"XHUaJzaDyr","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:49:20.502566Z","updated_at":"2022-10-18T19:49:20.502589Z","lead_time":16.361,"prediction":{},"result_count":0,"task":154,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"We find that BERT LARGE significantly outperforms BERT BASE across all tasks , especially those with very little training data ."},"meta":{},"created_at":"2022-10-17T21:25:49.759766Z","updated_at":"2022-10-18T19:49:20.524855Z","inner_id":152,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":152,"annotations":[{"id":152,"completed_by":1,"result":[{"value":{"start":41,"end":45,"text":"GLUE","labels":["DatasetName"]},"id":"Tncbx9CCgk","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":53,"end":57,"text":"MNLI","labels":["DatasetName"]},"id":"nxiZq1KVXz","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":60,"end":64,"text":"BERT","labels":["MethodName"]},"id":"h9pQvkRrpc","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":75,"end":80,"text":"4.6 %","labels":["MetricValue"]},"id":"CQQNwCfD6v","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":90,"end":98,"text":"accuracy","labels":["MetricName"]},"id":"9lIdaQR1fk","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:49:02.859685Z","updated_at":"2022-10-18T19:49:02.859708Z","lead_time":11.637,"prediction":{},"result_count":0,"task":152,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"For the largest and most widely reported GLUE task , MNLI , BERT obtains a 4.6 % absolute accuracy improvement ."},"meta":{},"created_at":"2022-10-17T21:25:49.759691Z","updated_at":"2022-10-18T19:49:02.880271Z","inner_id":150,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":153,"annotations":[{"id":151,"completed_by":1,"result":[{"value":{"start":16,"end":20,"text":"GLUE","labels":["DatasetName"]},"id":"UxsXVfnaGw","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":38,"end":48,"text":"BERT LARGE","labels":["MethodName"]},"id":"6U6YjAaWv6","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":87,"end":97,"text":"OpenAI GPT","labels":["MethodName"]},"id":"nZjNvgp_Yd","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":59,"end":64,"text":"score","labels":["MetricName"]},"id":"N2wso4pC-w","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":68,"end":72,"text":"80.5","labels":["MetricValue"]},"id":"p6LkXge5GJ","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":114,"end":118,"text":"72.8","labels":["MetricValue"]},"id":"-3yp2BtUQ2","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-18T19:48:50.005170Z","updated_at":"2022-10-18T19:48:50.005196Z","lead_time":99.119,"prediction":{},"result_count":0,"task":153,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"On the official GLUE leaderboard 10 , BERT LARGE obtains a score of 80.5 , compared to OpenAI GPT , which obtains 72.8 as of the date of writing ."},"meta":{},"created_at":"2022-10-17T21:25:49.759728Z","updated_at":"2022-10-18T19:48:50.025257Z","inner_id":151,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":151,"annotations":[{"id":150,"completed_by":1,"result":[{"value":{"start":10,"end":19,"text":"BERT BASE","labels":["MethodName"]},"id":"YiqM026bJP","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":24,"end":34,"text":"OpenAI GPT","labels":["MethodName"]},"id":"xxBSCK4v8O","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:43:18.998285Z","updated_at":"2022-10-17T22:43:18.998314Z","lead_time":10.95,"prediction":{},"result_count":0,"task":151,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Note that BERT BASE and OpenAI GPT are nearly identical in terms of model architecture apart from the attention masking ."},"meta":{},"created_at":"2022-10-17T21:25:49.759645Z","updated_at":"2022-10-17T22:43:19.017149Z","inner_id":149,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":150,"annotations":[{"id":149,"completed_by":1,"result":[{"value":{"start":5,"end":14,"text":"BERT BASE","labels":["MethodName"]},"id":"jjtj5u1jWu","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":19,"end":29,"text":"BERT LARGE","labels":["MethodName"]},"id":"hoQvW-grc6","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":137,"end":145,"text":"accuracy","labels":["MethodName"]},"id":"xOgsV4Wn0q","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":102,"end":107,"text":"4.5 %","labels":["MetricValue"]},"id":"N5LstSWpjl","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":112,"end":117,"text":"7.0 %","labels":["MetricValue"]},"id":"9Y_Pf5Bxlc","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:43:04.983051Z","updated_at":"2022-10-17T22:43:04.983074Z","lead_time":26.522,"prediction":{},"result_count":0,"task":150,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Both BERT BASE and BERT LARGE outperform all systems on all tasks by a substantial margin , obtaining 4.5 % and 7.0 % respective average accuracy improvement over the prior state of the art ."},"meta":{},"created_at":"2022-10-17T21:25:49.759557Z","updated_at":"2022-10-17T22:43:05.004524Z","inner_id":148,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":141,"annotations":[{"id":140,"completed_by":1,"result":[{"value":{"start":4,"end":45,"text":"General Language Understanding Evaluation","labels":["DatasetName"]},"id":"mXJzKP68ky","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":48,"end":52,"text":"GLUE","labels":["DatasetName"]},"id":"lYlF9LAelH","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":117,"end":147,"text":"natural language understanding","labels":["TaskName"]},"id":"HwAyLrKR4J","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:29:34.871752Z","updated_at":"2022-10-17T22:42:34.417632Z","lead_time":618.824,"prediction":{},"result_count":0,"task":141,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The General Language Understanding Evaluation ( GLUE ) benchmark ( Wang et al . , 2018a ) is a collection of diverse natural language understanding tasks ."},"meta":{},"created_at":"2022-10-17T21:25:49.759221Z","updated_at":"2022-10-17T22:42:34.435155Z","inner_id":139,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":149,"annotations":[{"id":148,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:31:08.868537Z","updated_at":"2022-10-17T22:31:08.868559Z","lead_time":0.519,"prediction":{},"result_count":0,"task":149,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"9 Results are presented in Table 1 ."},"meta":{},"created_at":"2022-10-17T21:25:49.759520Z","updated_at":"2022-10-17T22:31:08.885350Z","inner_id":147,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":148,"annotations":[{"id":147,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:31:05.729676Z","updated_at":"2022-10-17T22:31:05.729698Z","lead_time":1.001,"prediction":{},"result_count":0,"task":148,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"With random restarts , we use the same pre - trained checkpoint but perform different fine - tuning data shuffling and classifier layer initialization ."},"meta":{},"created_at":"2022-10-17T21:25:49.759482Z","updated_at":"2022-10-17T22:31:05.748409Z","inner_id":146,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":147,"annotations":[{"id":146,"completed_by":1,"result":[{"value":{"start":19,"end":29,"text":"BERT LARGE","labels":["MethodName"]},"id":"LyKbMdHTa9","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:31:03.605599Z","updated_at":"2022-10-17T22:31:03.605622Z","lead_time":8.645,"prediction":{},"result_count":0,"task":147,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Additionally , for BERT LARGE we found that finetuning was sometimes unstable on small datasets , so we ran several random restarts and selected the best model on the Dev set ."},"meta":{},"created_at":"2022-10-17T21:25:49.759444Z","updated_at":"2022-10-17T22:31:03.624449Z","inner_id":145,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":146,"annotations":[{"id":145,"completed_by":1,"result":[{"value":{"start":51,"end":64,"text":"learning rate","labels":["HyperparameterName"]},"id":"44cQQl0-BM","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":73,"end":77,"text":"5e-5","labels":["HyperparameterValue"]},"id":"VJI0GmExEJ","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":80,"end":84,"text":"4e-5","labels":["HyperparameterValue"]},"id":"d47x_47ZNF","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":87,"end":91,"text":"3e-5","labels":["HyperparameterValue"]},"id":"QgFgBFpU4q","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":98,"end":102,"text":"2e-5","labels":["HyperparameterValue"]},"id":"YKpMNOyEMW","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:30:53.429541Z","updated_at":"2022-10-17T22:30:53.429565Z","lead_time":21.914,"prediction":{},"result_count":0,"task":146,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"For each task , we selected the best fine - tuning learning rate ( among 5e-5 , 4e-5 , 3e-5 , and 2e-5 ) on the Dev set ."},"meta":{},"created_at":"2022-10-17T21:25:49.759407Z","updated_at":"2022-10-17T22:30:53.450435Z","inner_id":144,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":145,"annotations":[{"id":144,"completed_by":1,"result":[{"value":{"start":9,"end":19,"text":"batch size","labels":["HyperparameterName"]},"id":"peR_KpsPUG","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":23,"end":25,"text":"32","labels":["HyperparameterValue"]},"id":"bVl_4A8E6F","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":77,"end":81,"text":"GLUE","labels":["DatasetName"]},"id":"MWBmVEQeeG","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:30:30.236070Z","updated_at":"2022-10-17T22:30:30.236093Z","lead_time":18.409,"prediction":{},"result_count":0,"task":145,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"We use a batch size of 32 and fine - tune for 3 epochs over the data for all GLUE tasks ."},"meta":{},"created_at":"2022-10-17T21:25:49.759370Z","updated_at":"2022-10-17T22:30:30.256924Z","inner_id":143,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":144,"annotations":[{"id":143,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:30:10.765051Z","updated_at":"2022-10-17T22:30:10.765078Z","lead_time":0.834,"prediction":{},"result_count":0,"task":144,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"We compute a standard classification loss with C and W , i.e. , log(softmax(CW T ) ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.759333Z","updated_at":"2022-10-17T22:30:10.784564Z","inner_id":142,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":143,"annotations":[{"id":142,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:29:59.942382Z","updated_at":"2022-10-17T22:29:59.942405Z","lead_time":1.115,"prediction":{},"result_count":0,"task":143,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The only new parameters introduced during fine - tuning are classification layer weights W ∈ R K×H , where K is the number of labels ."},"meta":{},"created_at":"2022-10-17T21:25:49.759296Z","updated_at":"2022-10-17T22:29:59.960050Z","inner_id":141,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":142,"annotations":[{"id":141,"completed_by":1,"result":[{"value":{"start":25,"end":29,"text":"GLUE","labels":["DatasetName"]},"id":"TPoSveVh7H","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":88,"end":92,"text":"GLUE","labels":["DatasetName"]},"id":"EKpvpSaw24","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:29:57.761186Z","updated_at":"2022-10-17T22:29:57.761212Z","lead_time":21.874,"prediction":{},"result_count":0,"task":142,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Detailed descriptions of GLUE datasets are included in Appendix B.1 . To fine - tune on GLUE , we represent the input sequence ( for single sentence or sentence pairs ) as described in Section 3 , and use the final hidden vector C ∈ R H corresponding to the first input token ( [ CLS ] ) as the aggregate representation ."},"meta":{},"created_at":"2022-10-17T21:25:49.759258Z","updated_at":"2022-10-17T22:29:57.781279Z","inner_id":140,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":140,"annotations":[{"id":139,"completed_by":1,"result":[{"value":{"start":0,"end":4,"text":"GLUE","labels":["DatasetName"]},"id":"4yo7Xpw0sF","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:29:10.102189Z","updated_at":"2022-10-17T22:29:10.102212Z","lead_time":19.023,"prediction":{},"result_count":0,"task":140,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"GLUE ."},"meta":{},"created_at":"2022-10-17T21:25:49.759183Z","updated_at":"2022-10-17T22:29:10.122229Z","inner_id":138,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":139,"annotations":[{"id":138,"completed_by":1,"result":[{"value":{"start":29,"end":33,"text":"BERT","labels":["MethodName"]},"id":"yFUk8iPCd-","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:28:49.952405Z","updated_at":"2022-10-17T22:28:49.952429Z","lead_time":5.088,"prediction":{},"result_count":0,"task":139,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"In this section , we present BERT fine - tuning results on 11 NLP tasks ."},"meta":{},"created_at":"2022-10-17T21:25:49.759146Z","updated_at":"2022-10-17T22:28:49.971799Z","inner_id":137,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":138,"annotations":[{"id":137,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:28:42.891285Z","updated_at":"2022-10-17T22:28:42.891313Z","lead_time":1.121,"prediction":{},"result_count":0,"task":138,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"More details can be found in Appendix A.5 . Experiments ."},"meta":{},"created_at":"2022-10-17T21:25:49.759109Z","updated_at":"2022-10-17T22:28:42.922058Z","inner_id":136,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":137,"annotations":[{"id":136,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:28:40.594742Z","updated_at":"2022-10-17T22:28:40.594767Z","lead_time":1.189,"prediction":{},"result_count":0,"task":137,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"7 We describe the task - specific details in the corresponding subsections of Section 4 ."},"meta":{},"created_at":"2022-10-17T21:25:49.759073Z","updated_at":"2022-10-17T22:28:40.614133Z","inner_id":135,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":136,"annotations":[{"id":135,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:28:34.299952Z","updated_at":"2022-10-17T22:28:34.299974Z","lead_time":6.767,"prediction":{},"result_count":0,"task":136,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU , or a few hours on a GPU , starting from the exact same pre - trained model ."},"meta":{},"created_at":"2022-10-17T21:25:49.759037Z","updated_at":"2022-10-17T22:28:34.319432Z","inner_id":134,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":135,"annotations":[{"id":134,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:28:26.444070Z","updated_at":"2022-10-17T22:28:26.444094Z","lead_time":1.247,"prediction":{},"result_count":0,"task":135,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Compared to pre - training , fine - tuning is relatively inexpensive ."},"meta":{},"created_at":"2022-10-17T21:25:49.758999Z","updated_at":"2022-10-17T22:28:26.462041Z","inner_id":133,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":134,"annotations":[{"id":133,"completed_by":1,"result":[{"value":{"start":122,"end":140,"text":"question answering","labels":["TaskName"]},"id":"8FvGeRftxG","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":231,"end":241,"text":"entailment","labels":["TaskName"]},"id":"nEBF8jNIas","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":245,"end":263,"text":"sentiment analysis","labels":["TaskName"]},"id":"a7xydWcMZs","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":102,"end":118,"text":"sequence tagging","labels":["TaskName"]},"id":"fFVGkS_6p2","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:28:17.972134Z","updated_at":"2022-10-17T22:28:23.890138Z","lead_time":26.752,"prediction":{},"result_count":0,"task":134,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"At the output , the token representations are fed into an output layer for tokenlevel tasks , such as sequence tagging or question answering , and the [ CLS ] representation is fed into an output layer for classification , such as entailment or sentiment analysis ."},"meta":{},"created_at":"2022-10-17T21:25:49.758962Z","updated_at":"2022-10-17T22:28:23.907443Z","inner_id":132,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":132,"annotations":[{"id":132,"completed_by":1,"result":[{"value":{"start":75,"end":79,"text":"BERT","labels":["MethodName"]},"id":"j-XZArDlMl","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:27:54.944228Z","updated_at":"2022-10-17T22:27:54.944253Z","lead_time":7.255,"prediction":{},"result_count":0,"task":132,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"For each task , we simply plug in the taskspecific inputs and outputs into BERT and finetune all the parameters end - to - end ."},"meta":{},"created_at":"2022-10-17T21:25:49.758887Z","updated_at":"2022-10-17T22:27:54.964310Z","inner_id":130,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":133,"annotations":[{"id":131,"completed_by":1,"result":[{"value":{"start":153,"end":163,"text":"entailment","labels":["TaskName"]},"id":"bDi9OFF3YY","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":200,"end":218,"text":"question answering","labels":["TaskName"]},"id":"ozyNqB9K0r","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":259,"end":278,"text":"text classification","labels":["TaskName"]},"id":"W5yWvq9Z2N","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":282,"end":298,"text":"sequence tagging","labels":["TaskName"]},"id":"Mxs7vIKvSD","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:27:33.796350Z","updated_at":"2022-10-17T22:27:46.208664Z","lead_time":45.197,"prediction":{},"result_count":0,"task":133,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"At the input , sentence A and sentence B from pre - training are analogous to ( 1 ) sentence pairs in paraphrasing , ( 2 ) hypothesis - premise pairs in entailment , ( 3 ) question - passage pairs in question answering , and ( 4 ) a degenerate text-∅ pair in text classification or sequence tagging ."},"meta":{},"created_at":"2022-10-17T21:25:49.758925Z","updated_at":"2022-10-17T22:27:46.229226Z","inner_id":131,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":131,"annotations":[{"id":130,"completed_by":1,"result":[{"value":{"start":0,"end":4,"text":"BERT","labels":["MethodName"]},"id":"EU_SPFZD3k","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:26:50.120117Z","updated_at":"2022-10-17T22:26:50.120142Z","lead_time":4.003,"prediction":{},"result_count":0,"task":131,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"BERT instead uses the self - attention mechanism to unify these two stages , as encoding a concatenated text pair with self - attention effectively includes bidirectional cross attention between two sentences ."},"meta":{},"created_at":"2022-10-17T21:25:49.758850Z","updated_at":"2022-10-17T22:26:50.139479Z","inner_id":129,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":130,"annotations":[{"id":129,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:26:45.333723Z","updated_at":"2022-10-17T22:26:45.333746Z","lead_time":0.544,"prediction":{},"result_count":0,"task":130,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"( 2017 ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.758813Z","updated_at":"2022-10-17T22:26:45.352466Z","inner_id":128,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":129,"annotations":[{"id":128,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:26:38.957167Z","updated_at":"2022-10-17T22:26:38.957194Z","lead_time":0.212,"prediction":{},"result_count":0,"task":129,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"( 2016 ) ; Seo et al ."},"meta":{},"created_at":"2022-10-17T21:25:49.758777Z","updated_at":"2022-10-17T22:26:38.975122Z","inner_id":127,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":128,"annotations":[{"id":127,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:26:37.865075Z","updated_at":"2022-10-17T22:26:37.865101Z","lead_time":1.855,"prediction":{},"result_count":0,"task":128,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"For applications involving text pairs , a common pattern is to independently encode text pairs before applying bidirectional cross attention , such as Parikh et al ."},"meta":{},"created_at":"2022-10-17T21:25:49.758740Z","updated_at":"2022-10-17T22:26:37.889265Z","inner_id":126,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":127,"annotations":[{"id":126,"completed_by":1,"result":[{"value":{"start":74,"end":85,"text":"Transformer","labels":["MethodName"]},"id":"slQywU29Pj","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":93,"end":97,"text":"BERT","labels":["MethodName"]},"id":"dg5qvG65tE","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:26:35.057241Z","updated_at":"2022-10-17T22:26:35.057264Z","lead_time":15.292,"prediction":{},"result_count":0,"task":127,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Fine - tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream taskswhether they involve single text or text pairs - by swapping out the appropriate inputs and outputs ."},"meta":{},"created_at":"2022-10-17T21:25:49.758703Z","updated_at":"2022-10-17T22:26:35.081766Z","inner_id":125,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":126,"annotations":[{"id":125,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:26:17.945534Z","updated_at":"2022-10-17T22:26:17.945561Z","lead_time":0.805,"prediction":{},"result_count":0,"task":126,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Fine - tuning BERT ."},"meta":{},"created_at":"2022-10-17T21:25:49.758665Z","updated_at":"2022-10-17T22:26:17.964599Z","inner_id":124,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":125,"annotations":[{"id":124,"completed_by":1,"result":[{"value":{"start":107,"end":129,"text":"Billion Word Benchmark","labels":["DatasetName"]},"id":"-4z9Y7gYBz","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:26:04.445505Z","updated_at":"2022-10-17T22:26:04.445529Z","lead_time":7.814,"prediction":{},"result_count":0,"task":125,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"It is critical to use a document - level corpus rather than a shuffled sentence - level corpus such as the Billion Word Benchmark ( Chelba et al . , 2013 ) in order to extract long contiguous sequences ."},"meta":{},"created_at":"2022-10-17T21:25:49.758628Z","updated_at":"2022-10-17T22:26:04.466098Z","inner_id":123,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":124,"annotations":[{"id":123,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:25:55.655965Z","updated_at":"2022-10-17T22:25:55.655987Z","lead_time":2.995,"prediction":{},"result_count":0,"task":124,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"For Wikipedia we extract only the text passages and ignore lists , tables , and headers ."},"meta":{},"created_at":"2022-10-17T21:25:49.758587Z","updated_at":"2022-10-17T22:25:55.674315Z","inner_id":122,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":123,"annotations":[{"id":122,"completed_by":1,"result":[{"value":{"start":41,"end":52,"text":"BooksCorpus","labels":["DatasetName"]},"id":"ntNmASonY7","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":96,"end":113,"text":"English Wikipedia","labels":["DatasetName"]},"id":"uCUJxxxKo7","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:25:26.743947Z","updated_at":"2022-10-17T22:25:49.965445Z","lead_time":26.935000000000002,"prediction":{},"result_count":0,"task":123,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"For the pre - training corpus we use the BooksCorpus ( 800 M words ) ( Zhu et al . , 2015 ) and English Wikipedia ( 2,500 M words ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.758550Z","updated_at":"2022-10-17T22:25:49.988667Z","inner_id":121,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":122,"annotations":[{"id":121,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:25:18.545496Z","updated_at":"2022-10-17T22:25:18.545524Z","lead_time":2.182,"prediction":{},"result_count":0,"task":122,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The pre - training procedure largely follows the existing literature on language model pre - training ."},"meta":{},"created_at":"2022-10-17T21:25:49.758514Z","updated_at":"2022-10-17T22:25:18.562961Z","inner_id":120,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":121,"annotations":[{"id":120,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:25:15.262082Z","updated_at":"2022-10-17T22:25:15.262109Z","lead_time":0.409,"prediction":{},"result_count":0,"task":121,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Pre - training data ."},"meta":{},"created_at":"2022-10-17T21:25:49.758478Z","updated_at":"2022-10-17T22:25:15.280190Z","inner_id":119,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":120,"annotations":[{"id":119,"completed_by":1,"result":[{"value":{"start":98,"end":102,"text":"BERT","labels":["MethodName"]},"id":"vM2eAsd9TG","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:25:13.856857Z","updated_at":"2022-10-17T22:25:13.856881Z","lead_time":8.843,"prediction":{},"result_count":0,"task":120,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"However , in prior work , only sentence embeddings are transferred to down - stream tasks , where BERT transfers all parameters to initialize end - task model parameters ."},"meta":{},"created_at":"2022-10-17T21:25:49.758442Z","updated_at":"2022-10-17T22:25:13.876295Z","inner_id":118,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":119,"annotations":[{"id":118,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:25:04.524943Z","updated_at":"2022-10-17T22:25:04.524966Z","lead_time":1.196,"prediction":{},"result_count":0,"task":119,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"( 2017 ) and Logeswaran and Lee ( 2018 ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.758392Z","updated_at":"2022-10-17T22:25:04.543544Z","inner_id":117,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":117,"annotations":[{"id":116,"completed_by":1,"result":[{"value":{"start":122,"end":124,"text":"QA","labels":["TaskName"]},"id":"W00WFbmaEX","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":129,"end":132,"text":"NLI","labels":["TaskName"]},"id":"hcsb92_F_o","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:24:51.234009Z","updated_at":"2022-10-17T22:24:51.234032Z","lead_time":8.436,"prediction":{},"result_count":0,"task":117,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"5 Despite its simplicity , we demonstrate in Section 5.1 that pre - training towards this task is very beneficial to both QA and NLI ."},"meta":{},"created_at":"2022-10-17T21:25:49.756226Z","updated_at":"2022-10-17T22:24:51.253174Z","inner_id":115,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":115,"annotations":[{"id":114,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:24:24.415930Z","updated_at":"2022-10-17T22:24:24.415957Z","lead_time":4.022,"prediction":{},"result_count":0,"task":115,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Specifically , when choosing the sentences A and B for each pretraining example , 50 % of the time B is the actual next sentence that follows A ( labeled as IsNext ) , and 50 % of the time it is a random sentence from the corpus ( labeled as NotNext ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.756151Z","updated_at":"2022-10-17T22:24:24.435205Z","inner_id":113,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":114,"annotations":[{"id":113,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:24:19.363329Z","updated_at":"2022-10-17T22:24:19.363359Z","lead_time":5.947,"prediction":{},"result_count":0,"task":114,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"In order to train a model that understands sentence relationships , we pre - train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus ."},"meta":{},"created_at":"2022-10-17T21:25:49.756114Z","updated_at":"2022-10-17T22:24:19.380894Z","inner_id":112,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":112,"annotations":[{"id":111,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:23:30.382011Z","updated_at":"2022-10-17T22:23:30.382038Z","lead_time":8.353,"prediction":{},"result_count":0,"task":112,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Then , T i will be used to predict the original token with cross entropy loss ."},"meta":{},"created_at":"2022-10-17T21:25:49.756039Z","updated_at":"2022-10-17T22:23:30.400610Z","inner_id":110,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":106,"annotations":[{"id":110,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:23:18.462904Z","updated_at":"2022-10-17T22:23:18.462941Z","lead_time":1.238,"prediction":{},"result_count":0,"task":106,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"In all of our experiments , we mask 15 % of all WordPiece tokens in each sequence at random ."},"meta":{},"created_at":"2022-10-17T21:25:49.755810Z","updated_at":"2022-10-17T22:23:18.483972Z","inner_id":104,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":111,"annotations":[{"id":109,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:22:57.289692Z","updated_at":"2022-10-17T22:22:57.289718Z","lead_time":5.481,"prediction":{},"result_count":0,"task":111,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"If the i - th token is chosen , we replace the i - th token with ( 1 ) the [ MASK ] token 80 % of the time ( 2 ) a random token 10 % of the time ( 3 ) the unchanged i - th token 10 % of the time ."},"meta":{},"created_at":"2022-10-17T21:25:49.756003Z","updated_at":"2022-10-17T22:22:57.307472Z","inner_id":109,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":110,"annotations":[{"id":108,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:22:50.601867Z","updated_at":"2022-10-17T22:22:50.601895Z","lead_time":27.431,"prediction":{},"result_count":0,"task":110,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The training data generator chooses 15 % of the token positions at random for prediction ."},"meta":{},"created_at":"2022-10-17T21:25:49.755967Z","updated_at":"2022-10-17T22:22:50.623557Z","inner_id":108,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":109,"annotations":[{"id":107,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:22:21.499249Z","updated_at":"2022-10-17T22:22:21.499277Z","lead_time":2.02,"prediction":{},"result_count":0,"task":109,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"To mitigate this , we do not always replace \" masked \" words with the actual [ MASK ] token ."},"meta":{},"created_at":"2022-10-17T21:25:49.755929Z","updated_at":"2022-10-17T22:22:21.520537Z","inner_id":107,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":108,"annotations":[{"id":106,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:22:16.457509Z","updated_at":"2022-10-17T22:22:16.457539Z","lead_time":2.351,"prediction":{},"result_count":0,"task":108,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Although this allows us to obtain a bidirectional pre - trained model , a downside is that we are creating a mismatch between pre - training and fine - tuning , since the [ MASK ] token does not appear during fine - tuning ."},"meta":{},"created_at":"2022-10-17T21:25:49.755888Z","updated_at":"2022-10-17T22:22:16.475371Z","inner_id":106,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":107,"annotations":[{"id":105,"completed_by":1,"result":[{"value":{"start":15,"end":40,"text":"denoising auto - encoders","labels":["MethodName"]},"id":"2AmnMGkmrP","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:21:39.886821Z","updated_at":"2022-10-17T22:22:12.995982Z","lead_time":40.738,"prediction":{},"result_count":0,"task":107,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"In contrast to denoising auto - encoders ( Vincent et al . , 2008 ) , we only predict the masked words rather than reconstructing the entire input ."},"meta":{},"created_at":"2022-10-17T21:25:49.755849Z","updated_at":"2022-10-17T22:22:13.014767Z","inner_id":105,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":105,"annotations":[{"id":103,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:21:12.417751Z","updated_at":"2022-10-17T22:21:12.417774Z","lead_time":1.607,"prediction":{},"result_count":0,"task":105,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"In this case , the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary , as in a standard LM ."},"meta":{},"created_at":"2022-10-17T21:25:49.755772Z","updated_at":"2022-10-17T22:21:12.436943Z","inner_id":103,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":103,"annotations":[{"id":101,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:20:59.712139Z","updated_at":"2022-10-17T22:20:59.712166Z","lead_time":4.083,"prediction":{},"result_count":0,"task":103,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"In order to train a deep bidirectional representation , we simply mask some percentage of the input tokens at random , and then predict those masked tokens ."},"meta":{},"created_at":"2022-10-17T21:25:49.755679Z","updated_at":"2022-10-17T22:20:59.731120Z","inner_id":101,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":102,"annotations":[{"id":100,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:20:54.657477Z","updated_at":"2022-10-17T22:20:54.657503Z","lead_time":14.995,"prediction":{},"result_count":0,"task":102,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"former is often referred to as a \" Transformer encoder \" while the left - context - only version is referred to as a \" Transformer decoder \" since it can be used for text generation ."},"meta":{},"created_at":"2022-10-17T21:25:49.755641Z","updated_at":"2022-10-17T22:20:54.674502Z","inner_id":100,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":101,"annotations":[{"id":99,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:20:17.075485Z","updated_at":"2022-10-17T22:20:17.075511Z","lead_time":11.299,"prediction":{},"result_count":0,"task":101,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Unfortunately , standard conditional language models can only be trained left - to - right or right - to - left , since bidirectional conditioning would allow each word to indirectly \" see itself \" , and the model could trivially predict the target word in a multi - layered context ."},"meta":{},"created_at":"2022-10-17T21:25:49.755601Z","updated_at":"2022-10-17T22:20:17.092553Z","inner_id":99,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":99,"annotations":[{"id":98,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:19:45.414458Z","updated_at":"2022-10-17T22:19:45.414485Z","lead_time":1.317,"prediction":{},"result_count":0,"task":99,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"This step is presented in the left part of Figure 1 ."},"meta":{},"created_at":"2022-10-17T21:25:49.755524Z","updated_at":"2022-10-17T22:19:45.444466Z","inner_id":97,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":98,"annotations":[{"id":97,"completed_by":1,"result":[{"value":{"start":25,"end":29,"text":"BERT","labels":["MethodName"]},"id":"xxte9fre8j","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:19:42.510278Z","updated_at":"2022-10-17T22:19:42.510301Z","lead_time":3.101,"prediction":{},"result_count":0,"task":98,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Instead , we pre - train BERT using two unsupervised tasks , described in this section ."},"meta":{},"created_at":"2022-10-17T21:25:49.755486Z","updated_at":"2022-10-17T22:19:42.533818Z","inner_id":96,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":97,"annotations":[{"id":96,"completed_by":1,"result":[{"value":{"start":107,"end":111,"text":"BERT","labels":["MethodName"]},"id":"mkMzV5iOnw","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:19:32.413379Z","updated_at":"2022-10-17T22:19:38.469147Z","lead_time":6.938000000000001,"prediction":{},"result_count":0,"task":97,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"( 2018 ) , we do not use traditional left - to - right or right - to - left language models to pre - train BERT ."},"meta":{},"created_at":"2022-10-17T21:25:49.755448Z","updated_at":"2022-10-17T22:19:38.489731Z","inner_id":95,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":96,"annotations":[{"id":95,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:19:29.995436Z","updated_at":"2022-10-17T22:19:29.995466Z","lead_time":0.398,"prediction":{},"result_count":0,"task":96,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"( 2018a ) and Radford et al ."},"meta":{},"created_at":"2022-10-17T21:25:49.755410Z","updated_at":"2022-10-17T22:19:30.014689Z","inner_id":94,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":95,"annotations":[{"id":94,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:19:28.440022Z","updated_at":"2022-10-17T22:19:28.440045Z","lead_time":0.888,"prediction":{},"result_count":0,"task":95,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Unlike Peters et al ."},"meta":{},"created_at":"2022-10-17T21:25:49.755372Z","updated_at":"2022-10-17T22:19:28.470333Z","inner_id":93,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":94,"annotations":[{"id":93,"completed_by":1,"result":[{"value":{"start":15,"end":19,"text":"BERT","labels":["MethodName"]},"id":"vg196QJBmN","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:19:26.456267Z","updated_at":"2022-10-17T22:19:26.456331Z","lead_time":4.638,"prediction":{},"result_count":0,"task":94,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Pre - training BERT ."},"meta":{},"created_at":"2022-10-17T21:25:49.755334Z","updated_at":"2022-10-17T22:19:26.475336Z","inner_id":92,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":93,"annotations":[{"id":92,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:19:20.576742Z","updated_at":"2022-10-17T22:19:20.576769Z","lead_time":0.636,"prediction":{},"result_count":0,"task":93,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"A visualization of this construction can be seen in Figure 2 ."},"meta":{},"created_at":"2022-10-17T21:25:49.755297Z","updated_at":"2022-10-17T22:19:20.594536Z","inner_id":91,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":92,"annotations":[{"id":91,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:19:18.344326Z","updated_at":"2022-10-17T22:19:18.344351Z","lead_time":1.72,"prediction":{},"result_count":0,"task":92,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"For a given token , its input representation is constructed by summing the corresponding token , segment , and position embeddings ."},"meta":{},"created_at":"2022-10-17T21:25:49.755260Z","updated_at":"2022-10-17T22:19:18.364136Z","inner_id":90,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":91,"annotations":[{"id":90,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:19:15.681688Z","updated_at":"2022-10-17T22:19:15.681715Z","lead_time":2.121,"prediction":{},"result_count":0,"task":91,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"As shown in Figure 1 , we denote input embedding as E , the final hidden vector of the special [ CLS ] token as C ∈ R H , and the final hidden vector for the i th input token as T i ∈ R H ."},"meta":{},"created_at":"2022-10-17T21:25:49.755222Z","updated_at":"2022-10-17T22:19:15.700629Z","inner_id":89,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":90,"annotations":[{"id":89,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:19:12.303915Z","updated_at":"2022-10-17T22:19:12.303935Z","lead_time":0.776,"prediction":{},"result_count":0,"task":90,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Second , we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B."},"meta":{},"created_at":"2022-10-17T21:25:49.755183Z","updated_at":"2022-10-17T22:19:12.322000Z","inner_id":88,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":89,"annotations":[{"id":88,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:19:10.571141Z","updated_at":"2022-10-17T22:19:10.571170Z","lead_time":0.522,"prediction":{},"result_count":0,"task":89,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"First , we separate them with a special token ( [ SEP ] ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.755143Z","updated_at":"2022-10-17T22:19:10.588719Z","inner_id":87,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":88,"annotations":[{"id":87,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:19:08.907920Z","updated_at":"2022-10-17T22:19:08.907946Z","lead_time":2.866,"prediction":{},"result_count":0,"task":88,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"We differentiate the sentences in two ways ."},"meta":{},"created_at":"2022-10-17T21:25:49.755098Z","updated_at":"2022-10-17T22:19:08.928119Z","inner_id":86,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":87,"annotations":[{"id":86,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:19:04.972386Z","updated_at":"2022-10-17T22:19:04.972410Z","lead_time":1.223,"prediction":{},"result_count":0,"task":87,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Sentence pairs are packed together into a single sequence ."},"meta":{},"created_at":"2022-10-17T21:25:49.754952Z","updated_at":"2022-10-17T22:19:04.988308Z","inner_id":85,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":86,"annotations":[{"id":85,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:19:01.682499Z","updated_at":"2022-10-17T22:19:01.682525Z","lead_time":2.701,"prediction":{},"result_count":0,"task":86,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks ."},"meta":{},"created_at":"2022-10-17T21:25:49.754903Z","updated_at":"2022-10-17T22:19:01.699204Z","inner_id":84,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":85,"annotations":[{"id":84,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:18:57.747128Z","updated_at":"2022-10-17T22:18:57.747154Z","lead_time":2.348,"prediction":{},"result_count":0,"task":85,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The first token of every sequence is always a special classification token ( [ CLS ] ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.754864Z","updated_at":"2022-10-17T22:18:57.763372Z","inner_id":83,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":84,"annotations":[{"id":83,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:18:53.586802Z","updated_at":"2022-10-17T22:18:53.586828Z","lead_time":4.788,"prediction":{},"result_count":0,"task":84,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"We use WordPiece embeddings ( Wu et al . , 2016 ) with a 30,000 token vocabulary ."},"meta":{},"created_at":"2022-10-17T21:25:49.754817Z","updated_at":"2022-10-17T22:18:53.609495Z","inner_id":82,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":83,"annotations":[{"id":82,"completed_by":1,"result":[{"value":{"start":53,"end":57,"text":"BERT","labels":["MethodName"]},"id":"Yq7QV88Ffu","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:18:47.823623Z","updated_at":"2022-10-17T22:18:47.823668Z","lead_time":9.857,"prediction":{},"result_count":0,"task":83,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"A \" sequence \" refers to the input token sequence to BERT , which may be a single sentence or two sentences packed together ."},"meta":{},"created_at":"2022-10-17T21:25:49.754634Z","updated_at":"2022-10-17T22:18:47.851207Z","inner_id":81,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":82,"annotations":[{"id":81,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:18:36.665371Z","updated_at":"2022-10-17T22:18:36.665400Z","lead_time":3.981,"prediction":{},"result_count":0,"task":82,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Throughout this work , a \" sentence \" can be an arbitrary span of contiguous text , rather than an actual linguistic sentence ."},"meta":{},"created_at":"2022-10-17T21:25:49.754221Z","updated_at":"2022-10-17T22:18:36.684443Z","inner_id":80,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":81,"annotations":[{"id":80,"completed_by":1,"result":[{"value":{"start":41,"end":45,"text":"BERT","labels":["MethodName"]},"id":"nsM_BSTUQO","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:18:30.718105Z","updated_at":"2022-10-17T22:18:30.718126Z","lead_time":8.819,"prediction":{},"result_count":0,"task":81,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"4 Input \/ Output Representations To make BERT handle a variety of down - stream tasks , our input representation is able to unambiguously represent both a single sentence and a pair of sentences ( e.g. , Question , Answer ) in one token sequence ."},"meta":{},"created_at":"2022-10-17T21:25:49.754183Z","updated_at":"2022-10-17T22:18:30.735931Z","inner_id":79,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":80,"annotations":[{"id":79,"completed_by":1,"result":[{"value":{"start":27,"end":31,"text":"BERT","labels":["MethodName"]},"id":"WFAv2lll2K","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":92,"end":95,"text":"GPT","labels":["MethodName"]},"id":"6kWwEwGhBM","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:18:20.969566Z","updated_at":"2022-10-17T22:18:20.969590Z","lead_time":18.322,"prediction":{},"result_count":0,"task":80,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Critically , however , the BERT Transformer uses bidirectional self - attention , while the GPT Transformer uses constrained self - attention where every token can only attend to context to its left ."},"meta":{},"created_at":"2022-10-17T21:25:49.754144Z","updated_at":"2022-10-17T22:18:20.989148Z","inner_id":78,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":79,"annotations":[{"id":78,"completed_by":1,"result":[{"value":{"start":0,"end":9,"text":"BERT BASE","labels":["MethodName"]},"id":"ExV0flDQhs","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":52,"end":62,"text":"OpenAI GPT","labels":["MethodName"]},"id":"DW8kY-Bm1E","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:18:01.588340Z","updated_at":"2022-10-17T22:18:01.588362Z","lead_time":14.776,"prediction":{},"result_count":0,"task":79,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"BERT BASE was chosen to have the same model size as OpenAI GPT for comparison purposes ."},"meta":{},"created_at":"2022-10-17T21:25:49.754106Z","updated_at":"2022-10-17T22:18:01.609076Z","inner_id":77,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":78,"annotations":[{"id":77,"completed_by":1,"result":[{"value":{"start":51,"end":60,"text":"BERT BASE","labels":["MethodName"]},"id":"rdR7irXiy-","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":63,"end":64,"text":"L","labels":["HyperparameterName"]},"id":"dtS2Sd_eJV","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":65,"end":67,"text":"12","labels":["HyperparameterValue"]},"id":"RAcDVWV0Eg","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":70,"end":71,"text":"H","labels":["HyperparameterName"]},"id":"f1CF9Bh-fX","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":72,"end":75,"text":"768","labels":["HyperparameterValue"]},"id":"TlY90UAv-C","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":105,"end":110,"text":"110 M","labels":["HyperparameterValue"]},"id":"bDwcdo-35N","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":132,"end":134,"text":"24","labels":["HyperparameterValue"]},"id":"DinvL2hMt1","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":139,"end":143,"text":"1024","labels":["HyperparameterValue"]},"id":"uv51zXP9ct","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":137,"end":138,"text":"H","labels":["HyperparameterName"]},"id":"eZm0jZN9JW","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":130,"end":131,"text":"L","labels":["HyperparameterName"]},"id":"HC7T47dk1E","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":117,"end":127,"text":"BERT LARGE","labels":["MethodName"]},"id":"npaqXoxm2r","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":80,"end":82,"text":"12","labels":["HyperparameterValue"]},"id":"bclL9hH8YT","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":146,"end":147,"text":"A","labels":["HyperparameterName"]},"id":"F1tlXx9pWh","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":148,"end":150,"text":"16","labels":["HyperparameterValue"]},"id":"KFqRAlR07Q","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":153,"end":169,"text":"Total Parameters","labels":["HyperparameterName"]},"id":"FXX_PFSMjo","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":170,"end":175,"text":"340 M","labels":["HyperparameterValue"]},"id":"rOG6DwhqrA","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":85,"end":104,"text":"Total Param - eters","labels":["HyperparameterName"]},"id":"gyelg5RyTm","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:17:44.752311Z","updated_at":"2022-10-17T22:17:44.752333Z","lead_time":96.46,"prediction":{},"result_count":0,"task":78,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"3 We primarily report results on two model sizes : BERT BASE ( L=12 , H=768 , A=12 , Total Param - eters=110 M ) and BERT LARGE ( L=24 , H=1024 , A=16 , Total Parameters=340 M ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.754069Z","updated_at":"2022-10-17T22:17:44.769771Z","inner_id":76,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":77,"annotations":[{"id":76,"completed_by":1,"result":[{"value":{"start":108,"end":124,"text":"number of layers","labels":["HyperparameterName"]},"id":"qt1xYPThBm","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":166,"end":177,"text":"hidden size","labels":["HyperparameterName"]},"id":"SK42FTApY7","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":193,"end":225,"text":"number of self - attention heads","labels":["HyperparameterName"]},"id":"o8e8UxIprm","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":158,"end":159,"text":"L","labels":["HyperparameterName"]},"id":"zcbNmOCsU9","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":181,"end":182,"text":"H","labels":["HyperparameterName"]},"id":"miXzOrFQLt","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":229,"end":230,"text":"A","labels":["HyperparameterName"]},"id":"yqYQ6y4i1W","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:14:59.036728Z","updated_at":"2022-10-17T22:16:03.819166Z","lead_time":85.221,"prediction":{},"result_count":0,"task":77,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"( 2017 ) as well as excellent guides such as \" The Annotated Transformer . \" 2 In this work , we denote the number of layers ( i.e. , Transformer blocks ) as L , the hidden size as H , and the number of self - attention heads as A."},"meta":{},"created_at":"2022-10-17T21:25:49.754030Z","updated_at":"2022-10-17T22:16:03.839496Z","inner_id":75,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":75,"annotations":[{"id":74,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:14:09.733988Z","updated_at":"2022-10-17T22:14:25.013452Z","lead_time":11.478,"prediction":{},"result_count":0,"task":75,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"( 2017 ) and released in the tensor2tensor library ."},"meta":{},"created_at":"2022-10-17T21:25:49.753904Z","updated_at":"2022-10-17T22:14:25.031062Z","inner_id":73,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":73,"annotations":[{"id":72,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:12:58.234784Z","updated_at":"2022-10-17T22:12:58.234813Z","lead_time":2.181,"prediction":{},"result_count":0,"task":73,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"There is mini - mal difference between the pre - trained architecture and the final downstream architecture ."},"meta":{},"created_at":"2022-10-17T21:25:49.753829Z","updated_at":"2022-10-17T22:12:58.253508Z","inner_id":71,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":71,"annotations":[{"id":71,"completed_by":1,"result":[{"value":{"start":4,"end":24,"text":"question - answering","labels":["TaskName"]},"id":"Pg7QwhSTqy","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:12:54.400234Z","updated_at":"2022-10-17T22:12:54.400260Z","lead_time":6.417,"prediction":{},"result_count":0,"task":71,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The question - answering example in Figure 1 will serve as a running example for this section ."},"meta":{},"created_at":"2022-10-17T21:25:49.753754Z","updated_at":"2022-10-17T22:12:54.423963Z","inner_id":69,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":72,"annotations":[{"id":70,"completed_by":1,"result":[{"value":{"start":25,"end":29,"text":"BERT","labels":["MethodName"]},"id":"hyMD4_B80F","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:12:47.307117Z","updated_at":"2022-10-17T22:12:47.307143Z","lead_time":6.596,"prediction":{},"result_count":0,"task":72,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"A distinctive feature of BERT is its unified architecture across different tasks ."},"meta":{},"created_at":"2022-10-17T21:25:49.753793Z","updated_at":"2022-10-17T22:12:47.326277Z","inner_id":70,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":70,"annotations":[{"id":69,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:12:21.487011Z","updated_at":"2022-10-17T22:12:21.487035Z","lead_time":1.836,"prediction":{},"result_count":0,"task":70,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Each downstream task has separate fine - tuned models , even though they are initialized with the same pre - trained parameters ."},"meta":{},"created_at":"2022-10-17T21:25:49.753712Z","updated_at":"2022-10-17T22:12:21.505847Z","inner_id":68,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":69,"annotations":[{"id":68,"completed_by":1,"result":[{"value":{"start":21,"end":25,"text":"BERT","labels":["MethodName"]},"id":"ICN1F5bkXq","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:12:16.892859Z","updated_at":"2022-10-17T22:12:16.893045Z","lead_time":7.358,"prediction":{},"result_count":0,"task":69,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"For finetuning , the BERT model is first initialized with the pre - trained parameters , and all of the parameters are fine - tuned using labeled data from the downstream tasks ."},"meta":{},"created_at":"2022-10-17T21:25:49.753642Z","updated_at":"2022-10-17T22:12:16.919996Z","inner_id":67,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":68,"annotations":[{"id":67,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:12:08.399455Z","updated_at":"2022-10-17T22:12:08.399482Z","lead_time":1.691,"prediction":{},"result_count":0,"task":68,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"During pre - training , the model is trained on unlabeled data over different pre - training tasks ."},"meta":{},"created_at":"2022-10-17T21:25:49.753603Z","updated_at":"2022-10-17T22:12:08.417077Z","inner_id":66,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":67,"annotations":[{"id":66,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:12:01.521984Z","updated_at":"2022-10-17T22:12:01.522005Z","lead_time":1.572,"prediction":{},"result_count":0,"task":67,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"There are two steps in our framework : pre - training and fine - tuning ."},"meta":{},"created_at":"2022-10-17T21:25:49.753562Z","updated_at":"2022-10-17T22:12:01.540664Z","inner_id":65,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":66,"annotations":[{"id":65,"completed_by":1,"result":[{"value":{"start":13,"end":17,"text":"BERT","labels":["MethodName"]},"id":"Pvxy9wVwWF","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:11:58.360135Z","updated_at":"2022-10-17T22:11:58.360162Z","lead_time":6.913,"prediction":{},"result_count":0,"task":66,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"We introduce BERT and its detailed implementation in this section ."},"meta":{},"created_at":"2022-10-17T21:25:49.753520Z","updated_at":"2022-10-17T22:11:58.380633Z","inner_id":64,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":64,"annotations":[{"id":64,"completed_by":1,"result":[{"value":{"start":187,"end":198,"text":"Ima - geNet","labels":["DatasetName"]},"id":"uxd_0MUguZ","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:11:08.977938Z","updated_at":"2022-10-17T22:11:48.804461Z","lead_time":13.168000000000001,"prediction":{},"result_count":0,"task":64,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Computer vision research has also demonstrated the importance of transfer learning from large pre - trained models , where an effective recipe is to fine - tune models pre - trained with Ima - geNet ( Deng et al . , 2009;Yosinski et al . , 2014 ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.753390Z","updated_at":"2022-10-17T22:11:48.820425Z","inner_id":62,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":65,"annotations":[{"id":63,"completed_by":1,"result":[{"value":{"start":0,"end":4,"text":"BERT","labels":["MethodName"]},"id":"OPsbPjBDLu","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:11:06.471744Z","updated_at":"2022-10-17T22:11:06.471769Z","lead_time":6.233,"prediction":{},"result_count":0,"task":65,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"BERT ."},"meta":{},"created_at":"2022-10-17T21:25:49.753429Z","updated_at":"2022-10-17T22:11:06.491650Z","inner_id":63,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":63,"annotations":[{"id":62,"completed_by":1,"result":[{"value":{"start":104,"end":130,"text":"natural language inference","labels":["TaskName"]},"id":"QEq4uQdq4M","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":162,"end":181,"text":"machine translation","labels":["TaskName"]},"id":"YuxMKJFvIE","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:10:50.157526Z","updated_at":"2022-10-17T22:10:50.157551Z","lead_time":15.193,"prediction":{},"result_count":0,"task":63,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"There has also been work showing effective transfer from supervised tasks with large datasets , such as natural language inference ( Conneau et al . , 2017 ) and machine translation ( McCann et al . , 2017 ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.753346Z","updated_at":"2022-10-17T22:10:50.184327Z","inner_id":61,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":62,"annotations":[{"id":61,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:10:34.021167Z","updated_at":"2022-10-17T22:10:34.021196Z","lead_time":6.075,"prediction":{},"result_count":0,"task":62,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Transfer Learning from Supervised Data ."},"meta":{},"created_at":"2022-10-17T21:25:49.753304Z","updated_at":"2022-10-17T22:10:34.046906Z","inner_id":60,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":61,"annotations":[{"id":60,"completed_by":1,"result":[{"value":{"start":8,"end":22,"text":"auto - encoder","labels":["MethodName"]},"id":"chEDz63ub6","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:10:24.734221Z","updated_at":"2022-10-17T22:10:24.734248Z","lead_time":11.766,"prediction":{},"result_count":0,"task":61,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"ing and auto - encoder objectives have been used for pre - training such models ( Howard and Ruder , 2018;Radford et al . , 2018;Dai and Le , 2015 ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.753227Z","updated_at":"2022-10-17T22:10:24.751052Z","inner_id":59,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":60,"annotations":[{"id":59,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:10:11.305608Z","updated_at":"2022-10-17T22:10:11.305638Z","lead_time":0.126,"prediction":{},"result_count":0,"task":60,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"E M ' C T 1 T [ SEP ] ..."},"meta":{},"created_at":"2022-10-17T21:25:49.749645Z","updated_at":"2022-10-17T22:10:11.327284Z","inner_id":58,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":59,"annotations":[{"id":58,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:10:10.360592Z","updated_at":"2022-10-17T22:10:10.360617Z","lead_time":0.235,"prediction":{},"result_count":0,"task":59,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"E N E 1 ' ..."},"meta":{},"created_at":"2022-10-17T21:25:49.749609Z","updated_at":"2022-10-17T22:10:10.379739Z","inner_id":57,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":58,"annotations":[{"id":57,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:10:08.472852Z","updated_at":"2022-10-17T22:10:08.472877Z","lead_time":0.473,"prediction":{},"result_count":0,"task":58,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"..."},"meta":{},"created_at":"2022-10-17T21:25:49.749573Z","updated_at":"2022-10-17T22:10:08.490087Z","inner_id":56,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":57,"annotations":[{"id":56,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:10:05.317713Z","updated_at":"2022-10-17T22:10:05.317742Z","lead_time":0.194,"prediction":{},"result_count":0,"task":57,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"..."},"meta":{},"created_at":"2022-10-17T21:25:49.749537Z","updated_at":"2022-10-17T22:10:05.337176Z","inner_id":55,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":56,"annotations":[{"id":55,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:10:03.930905Z","updated_at":"2022-10-17T22:10:03.930929Z","lead_time":0.318,"prediction":{},"result_count":0,"task":56,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"E M ' C T 1 T [ SEP ] ..."},"meta":{},"created_at":"2022-10-17T21:25:49.749501Z","updated_at":"2022-10-17T22:10:03.948655Z","inner_id":54,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":55,"annotations":[{"id":54,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:10:01.718118Z","updated_at":"2022-10-17T22:10:01.718138Z","lead_time":3.241,"prediction":{},"result_count":0,"task":55,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"E N E 1 ' ..."},"meta":{},"created_at":"2022-10-17T21:25:49.749464Z","updated_at":"2022-10-17T22:10:01.738405Z","inner_id":53,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":54,"annotations":[{"id":53,"completed_by":1,"result":[{"value":{"start":35,"end":39,"text":"BERT","labels":["MethodName"]},"id":"RA8Nvm03sy","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":40,"end":44,"text":"BERT","labels":["MethodName"]},"id":"Oaz-Vl9w-D","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:09:52.470029Z","updated_at":"2022-10-17T22:09:52.470058Z","lead_time":9.755,"prediction":{},"result_count":0,"task":54,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Left - to - right language model - BERT BERT E [ CLS ] E 1 E [ SEP ] ..."},"meta":{},"created_at":"2022-10-17T21:25:49.749426Z","updated_at":"2022-10-17T22:09:52.494171Z","inner_id":52,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":52,"annotations":[{"id":52,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:09:12.682846Z","updated_at":"2022-10-17T22:09:12.682873Z","lead_time":1.601,"prediction":{},"result_count":0,"task":52,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The advantage of these approaches is that few parameters need to be learned from scratch ."},"meta":{},"created_at":"2022-10-17T21:25:49.749352Z","updated_at":"2022-10-17T22:09:12.700517Z","inner_id":50,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":53,"annotations":[{"id":51,"completed_by":1,"result":[{"value":{"start":40,"end":50,"text":"OpenAI GPT","labels":["MethodName"]},"id":"c6LFOPvC9L","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":166,"end":170,"text":"GLUE","labels":["DatasetName"]},"id":"vUddDtsPvm","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:08:28.708417Z","updated_at":"2022-10-17T22:08:28.708440Z","lead_time":126.817,"prediction":{},"result_count":0,"task":53,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"At least partly due to this advantage , OpenAI GPT ( Radford et al . , 2018 ) achieved previously state - of - the - art results on many sentencelevel tasks from the GLUE benchmark ( Wang et al . , 2018a ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.749389Z","updated_at":"2022-10-17T22:08:28.727938Z","inner_id":51,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":50,"annotations":[{"id":49,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:05:43.241802Z","updated_at":"2022-10-17T22:05:43.241828Z","lead_time":2.746,"prediction":{},"result_count":0,"task":50,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"As with the feature - based approaches , the first works in this direction only pre - trained word embedding parameters from unlabeled text ( Collobert and Weston , 2008 ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.749280Z","updated_at":"2022-10-17T22:05:43.262614Z","inner_id":48,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":49,"annotations":[{"id":48,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:05:38.191361Z","updated_at":"2022-10-17T22:05:38.191387Z","lead_time":1.677,"prediction":{},"result_count":0,"task":49,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Unsupervised Fine - tuning Approaches ."},"meta":{},"created_at":"2022-10-17T21:25:49.749243Z","updated_at":"2022-10-17T22:05:38.209000Z","inner_id":47,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":47,"annotations":[{"id":46,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:05:23.502587Z","updated_at":"2022-10-17T22:05:23.502612Z","lead_time":7.935,"prediction":{},"result_count":0,"task":47,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Fedus et al ."},"meta":{},"created_at":"2022-10-17T21:25:49.749170Z","updated_at":"2022-10-17T22:05:23.520304Z","inner_id":45,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":46,"annotations":[{"id":45,"completed_by":1,"result":[{"value":{"start":152,"end":156,"text":"ELMo","labels":["MethodName"]},"id":"7yWUGryzNw","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":133,"end":138,"text":"LSTMs","labels":["MethodName"]},"id":"kOKTnTr1f3","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T22:05:13.950211Z","updated_at":"2022-10-17T22:05:13.950240Z","lead_time":6.3,"prediction":{},"result_count":0,"task":46,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"( 2016 ) proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs . Similar to ELMo , their model is feature - based and not deeply bidirectional ."},"meta":{},"created_at":"2022-10-17T21:25:49.749133Z","updated_at":"2022-10-17T22:05:13.969851Z","inner_id":44,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":44,"annotations":[{"id":43,"completed_by":1,"result":[{"value":{"start":90,"end":94,"text":"ELMo","labels":["MethodName"]},"id":"8Gz36gcLHO","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":195,"end":213,"text":"question answering","labels":["TaskName"]},"id":"7Oc98TLqEM","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":296,"end":320,"text":"named entity recognition","labels":["TaskName"]},"id":"VpFFG2EmBh","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":245,"end":263,"text":"sentiment analysis","labels":["TaskName"]},"id":"6rL6OJHtX_","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:55:19.388098Z","updated_at":"2022-10-17T22:04:20.271062Z","lead_time":36.985,"prediction":{},"result_count":0,"task":44,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"When integrating contextual word embeddings with existing task - specific architectures , ELMo advances the state of the art for several major NLP benchmarks ( Peters et al . , 2018a ) including question answering ( Rajpurkar et al . , 2016 ) , sentiment analysis ( Socher et al . , 2013 ) , and named entity recognition ( Tjong Kim Sang and De Meulder , 2003 ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.749061Z","updated_at":"2022-10-17T22:04:20.290226Z","inner_id":42,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":27,"annotations":[{"id":26,"completed_by":1,"result":[{"value":{"start":74,"end":78,"text":"BERT","labels":["MethodName"]},"id":"jMEYOpa2VF","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:53:09.152492Z","updated_at":"2022-10-17T22:03:21.054739Z","lead_time":12.187999999999999,"prediction":{},"result_count":0,"task":27,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"( 2018 ) , which uses unidirectional language models for pre - training , BERT uses masked language models to enable pretrained deep bidirectional representations ."},"meta":{},"created_at":"2022-10-17T21:25:49.748391Z","updated_at":"2022-10-17T22:03:21.071937Z","inner_id":25,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":18,"annotations":[{"id":19,"completed_by":1,"result":[{"value":{"start":17,"end":27,"text":"OpenAI GPT","labels":["MethodName"]},"id":"rBYsFVpT60","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":169,"end":180,"text":"Transformer","labels":["MethodName"]},"id":"G2zOJ_5vfY","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:50:12.062554Z","updated_at":"2022-10-17T22:01:50.226435Z","lead_time":10.642,"prediction":{},"result_count":0,"task":18,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"For example , in OpenAI GPT , the authors use a left - toright architecture , where every token can only attend to previous tokens in the self - attention layers of the Transformer ( Vaswani et al . , 2017 ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.748047Z","updated_at":"2022-10-17T22:01:50.243492Z","inner_id":16,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":4,"annotations":[{"id":4,"completed_by":1,"result":[{"value":{"start":56,"end":60,"text":"BERT","labels":["MethodName"]},"id":"pCECvsvbya","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":80,"end":135,"text":"Bidirectional Encoder Representations from Transformers","labels":["MethodName"]},"id":"8sE848KNFg","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:43:53.049976Z","updated_at":"2022-10-17T22:00:54.315900Z","lead_time":12.717,"prediction":{},"result_count":0,"task":4,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"We introduce a new language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers ."},"meta":{},"created_at":"2022-10-17T21:25:49.747516Z","updated_at":"2022-10-17T22:00:54.334164Z","inner_id":2,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":45,"annotations":[{"id":44,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:55:25.997735Z","updated_at":"2022-10-17T21:55:25.997763Z","lead_time":1.298,"prediction":{},"result_count":0,"task":45,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Melamud et al ."},"meta":{},"created_at":"2022-10-17T21:25:49.749097Z","updated_at":"2022-10-17T21:55:26.022409Z","inner_id":43,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":43,"annotations":[{"id":42,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:54:47.602510Z","updated_at":"2022-10-17T21:54:47.602534Z","lead_time":0.566,"prediction":{},"result_count":0,"task":43,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The contextual representation of each token is the concatenation of the left - to - right and right - to - left representations ."},"meta":{},"created_at":"2022-10-17T21:25:49.749023Z","updated_at":"2022-10-17T21:54:47.621299Z","inner_id":41,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":42,"annotations":[{"id":41,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:54:45.483882Z","updated_at":"2022-10-17T21:54:45.483906Z","lead_time":0.997,"prediction":{},"result_count":0,"task":42,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"They extract context - sensitive features from a left - to - right and a right - to - left language model ."},"meta":{},"created_at":"2022-10-17T21:25:49.748987Z","updated_at":"2022-10-17T21:54:45.504465Z","inner_id":40,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":41,"annotations":[{"id":40,"completed_by":1,"result":[{"value":{"start":0,"end":4,"text":"ELMo","labels":["MethodName"]},"id":"Qxbt97FCES","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:54:43.308723Z","updated_at":"2022-10-17T21:54:43.308745Z","lead_time":6.49,"prediction":{},"result_count":0,"task":41,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"ELMo and its predecessor ( Peters et al . , 2017(Peters et al . , , 2018a ) ) generalize traditional word embedding research along a different dimension ."},"meta":{},"created_at":"2022-10-17T21:25:49.748951Z","updated_at":"2022-10-17T21:54:43.325637Z","inner_id":39,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":39,"annotations":[{"id":38,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:54:17.155237Z","updated_at":"2022-10-17T21:54:17.155263Z","lead_time":1.978,"prediction":{},"result_count":0,"task":39,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"These approaches have been generalized to coarser granularities , such as sentence embeddings ( Kiros et al . , 2015;Logeswaran and Lee , 2018 ) or paragraph embeddings ( Le and Mikolov , 2014 ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.748877Z","updated_at":"2022-10-17T21:54:17.173981Z","inner_id":37,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":38,"annotations":[{"id":37,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:54:13.334296Z","updated_at":"2022-10-17T21:54:13.334327Z","lead_time":3.685,"prediction":{},"result_count":0,"task":38,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"To pretrain word embedding vectors , left - to - right language modeling objectives have been used ( Mnih and Hinton , 2009 ) , as well as objectives to discriminate correct from incorrect words in left and right context ( Mikolov et al . , 2013 ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.748841Z","updated_at":"2022-10-17T21:54:13.355766Z","inner_id":36,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":37,"annotations":[{"id":36,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:54:08.623921Z","updated_at":"2022-10-17T21:54:08.623954Z","lead_time":2.965,"prediction":{},"result_count":0,"task":37,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Pre - trained word embeddings are an integral part of modern NLP systems , offering significant improvements over embeddings learned from scratch ( Turian et al . , 2010 ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.748804Z","updated_at":"2022-10-17T21:54:08.646212Z","inner_id":35,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":36,"annotations":[{"id":35,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:54:03.310866Z","updated_at":"2022-10-17T21:54:03.310892Z","lead_time":2.762,"prediction":{},"result_count":0,"task":36,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Learning widely applicable representations of words has been an active area of research for decades , including non - neural ( Brown et al . , 1992;Ando and Zhang , 2005;Blitzer et al . , 2006 ) and neural ( Mikolov et al . , 2013;Pennington et al . , 2014 ) methods ."},"meta":{},"created_at":"2022-10-17T21:25:49.748767Z","updated_at":"2022-10-17T21:54:03.330176Z","inner_id":34,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":35,"annotations":[{"id":34,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:53:59.104694Z","updated_at":"2022-10-17T21:53:59.104715Z","lead_time":1.332,"prediction":{},"result_count":0,"task":35,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Unsupervised Feature - based Approaches ."},"meta":{},"created_at":"2022-10-17T21:25:49.748729Z","updated_at":"2022-10-17T21:53:59.122841Z","inner_id":33,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":34,"annotations":[{"id":33,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:53:56.785231Z","updated_at":"2022-10-17T21:53:56.785253Z","lead_time":2.331,"prediction":{},"result_count":0,"task":34,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"There is a long history of pre - training general language representations , and we briefly review the most widely - used approaches in this section ."},"meta":{},"created_at":"2022-10-17T21:25:49.748691Z","updated_at":"2022-10-17T21:53:56.805673Z","inner_id":32,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":33,"annotations":[{"id":32,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:53:52.941189Z","updated_at":"2022-10-17T21:53:52.941215Z","lead_time":0.185,"prediction":{},"result_count":0,"task":33,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Related Work ."},"meta":{},"created_at":"2022-10-17T21:25:49.748639Z","updated_at":"2022-10-17T21:53:52.959261Z","inner_id":31,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":31,"annotations":[{"id":30,"completed_by":1,"result":[{"value":{"start":2,"end":6,"text":"BERT","labels":["MethodName"]},"id":"ttA1LFW32u","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:53:41.115241Z","updated_at":"2022-10-17T21:53:41.115265Z","lead_time":7.997,"prediction":{},"result_count":0,"task":31,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"• BERT advances the state of the art for eleven NLP tasks ."},"meta":{},"created_at":"2022-10-17T21:25:49.748537Z","updated_at":"2022-10-17T21:53:41.134808Z","inner_id":29,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":30,"annotations":[{"id":29,"completed_by":1,"result":[{"value":{"start":0,"end":4,"text":"BERT","labels":["MethodName"]},"id":"wfwmKh0Uyd","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:53:28.158621Z","updated_at":"2022-10-17T21:53:28.158648Z","lead_time":7.366,"prediction":{},"result_count":0,"task":30,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"BERT is the first finetuning based representation model that achieves state - of - the - art performance on a large suite of sentence - level and token - level tasks , outperforming many task - specific architectures ."},"meta":{},"created_at":"2022-10-17T21:25:49.748501Z","updated_at":"2022-10-17T21:53:28.176737Z","inner_id":28,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":29,"annotations":[{"id":28,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:53:19.352123Z","updated_at":"2022-10-17T21:53:19.352148Z","lead_time":3.676,"prediction":{},"result_count":0,"task":29,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"( 2018a ) , which uses a shallow concatenation of independently trained left - to - right and right - to - left LMs . • We show that pre - trained representations reduce the need for many heavily - engineered taskspecific architectures ."},"meta":{},"created_at":"2022-10-17T21:25:49.748464Z","updated_at":"2022-10-17T21:53:19.369760Z","inner_id":27,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":28,"annotations":[{"id":27,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:53:14.113959Z","updated_at":"2022-10-17T21:53:14.113986Z","lead_time":1.949,"prediction":{},"result_count":0,"task":28,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"This is also in contrast to Peters et al ."},"meta":{},"created_at":"2022-10-17T21:25:49.748427Z","updated_at":"2022-10-17T21:53:14.134501Z","inner_id":26,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":26,"annotations":[{"id":25,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:52:41.854741Z","updated_at":"2022-10-17T21:52:41.854764Z","lead_time":0.721,"prediction":{},"result_count":0,"task":26,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Unlike Radford et al ."},"meta":{},"created_at":"2022-10-17T21:25:49.748354Z","updated_at":"2022-10-17T21:52:41.871662Z","inner_id":24,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":23,"annotations":[{"id":23,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:50:51.886316Z","updated_at":"2022-10-17T21:50:51.886342Z","lead_time":2.136,"prediction":{},"result_count":0,"task":23,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Unlike left - toright language model pre - training , the MLM objective enables the representation to fuse the left and the right context , which allows us to pretrain a deep bidirectional Transformer ."},"meta":{},"created_at":"2022-10-17T21:25:49.748242Z","updated_at":"2022-10-17T21:50:51.903992Z","inner_id":21,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":22,"annotations":[{"id":22,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:50:48.694926Z","updated_at":"2022-10-17T21:50:48.694954Z","lead_time":7.052,"prediction":{},"result_count":0,"task":22,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The masked language model randomly masks some of the tokens from the input , and the objective is to predict the original vocabulary i d of the masked word based only on its context ."},"meta":{},"created_at":"2022-10-17T21:25:49.748203Z","updated_at":"2022-10-17T21:50:48.712914Z","inner_id":20,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":20,"annotations":[{"id":21,"completed_by":1,"result":[{"value":{"start":75,"end":79,"text":"BERT","labels":["MethodName"]},"id":"ykwBP94qo2","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":82,"end":137,"text":"Bidirectional Encoder Representations from Transformers","labels":["MethodName"]},"id":"x6d2HM7vsN","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:50:40.187178Z","updated_at":"2022-10-17T21:50:40.187205Z","lead_time":0.181,"prediction":{},"result_count":0,"task":20,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"In this paper , we improve the fine - tuning based approaches by proposing BERT : Bidirectional Encoder Representations from Transformers ."},"meta":{},"created_at":"2022-10-17T21:25:49.748120Z","updated_at":"2022-10-17T21:50:40.209170Z","inner_id":18,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":19,"annotations":[{"id":18,"completed_by":1,"result":[{"value":{"start":164,"end":182,"text":"question answering","labels":["TaskName"]},"id":"9i5amMqivp","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:50:09.058402Z","updated_at":"2022-10-17T21:50:09.058426Z","lead_time":14.442,"prediction":{},"result_count":0,"task":19,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Such restrictions are sub - optimal for sentence - level tasks , and could be very harmful when applying finetuning based approaches to token - level tasks such as question answering , where it is crucial to incorporate context from both directions ."},"meta":{},"created_at":"2022-10-17T21:25:49.748083Z","updated_at":"2022-10-17T21:50:09.078892Z","inner_id":17,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":17,"annotations":[{"id":17,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:49:40.054543Z","updated_at":"2022-10-17T21:49:40.054575Z","lead_time":4.571,"prediction":{},"result_count":0,"task":17,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The major limitation is that standard language models are unidirectional , and this limits the choice of architectures that can be used during pre - training ."},"meta":{},"created_at":"2022-10-17T21:25:49.748010Z","updated_at":"2022-10-17T21:49:40.071622Z","inner_id":15,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":16,"annotations":[{"id":16,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:49:34.092641Z","updated_at":"2022-10-17T21:49:34.092667Z","lead_time":1.952,"prediction":{},"result_count":0,"task":16,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"We argue that current techniques restrict the power of the pre - trained representations , especially for the fine - tuning approaches ."},"meta":{},"created_at":"2022-10-17T21:25:49.747973Z","updated_at":"2022-10-17T21:49:34.110383Z","inner_id":14,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":15,"annotations":[{"id":15,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:49:31.012303Z","updated_at":"2022-10-17T21:49:31.012328Z","lead_time":8.743,"prediction":{},"result_count":0,"task":15,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The two approaches share the same objective function during pre - training , where they use unidirectional language models to learn general language representations ."},"meta":{},"created_at":"2022-10-17T21:25:49.747936Z","updated_at":"2022-10-17T21:49:31.030471Z","inner_id":13,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":14,"annotations":[{"id":14,"completed_by":1,"result":[{"value":{"start":80,"end":90,"text":"OpenAI GPT","labels":["MethodName"]},"id":"gCSt0TIupC","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":41,"end":77,"text":"Generative Pre - trained Transformer","labels":["MethodName"]},"id":"P-pziKWwn1","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:49:20.065292Z","updated_at":"2022-10-17T21:49:20.065321Z","lead_time":0.927,"prediction":{},"result_count":0,"task":14,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The fine - tuning approach , such as the Generative Pre - trained Transformer ( OpenAI GPT ) ( Radford et al . , 2018 ) , introduces minimal task - specific parameters , and is trained on the downstream tasks by simply fine - tuning all pretrained parameters ."},"meta":{},"created_at":"2022-10-17T21:25:49.747900Z","updated_at":"2022-10-17T21:49:20.086079Z","inner_id":12,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":13,"annotations":[{"id":13,"completed_by":1,"result":[{"value":{"start":39,"end":43,"text":"ELMo","labels":["MethodName"]},"id":"utHdFZjXK_","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:46:10.878794Z","updated_at":"2022-10-17T21:46:10.878822Z","lead_time":43.058,"prediction":{},"result_count":0,"task":13,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"The feature - based approach , such as ELMo ( Peters et al . , 2018a ) , uses task - specific architectures that include the pre - trained representations as additional features ."},"meta":{},"created_at":"2022-10-17T21:25:49.747863Z","updated_at":"2022-10-17T21:46:10.901084Z","inner_id":11,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":12,"annotations":[{"id":12,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:45:26.885337Z","updated_at":"2022-10-17T21:45:26.885366Z","lead_time":10.756,"prediction":{},"result_count":0,"task":12,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"There are two existing strategies for applying pre - trained language representations to downstream tasks : feature - based and fine - tuning ."},"meta":{},"created_at":"2022-10-17T21:25:49.747825Z","updated_at":"2022-10-17T21:45:26.905702Z","inner_id":10,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":11,"annotations":[{"id":11,"completed_by":1,"result":[{"value":{"start":45,"end":71,"text":"natural language inference","labels":["TaskName"]},"id":"kvdAjxEBfJ","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":300,"end":324,"text":"named entity recognition","labels":["TaskName"]},"id":"jOayAksKwE","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":329,"end":347,"text":"question answering","labels":["TaskName"]},"id":"qTjuiMjbKZ","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:45:14.998672Z","updated_at":"2022-10-17T21:45:14.998701Z","lead_time":31.7,"prediction":{},"result_count":0,"task":11,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"These include sentence - level tasks such as natural language inference ( Bowman et al . , 2015;Williams et al . , 2018 ) and paraphrasing ( Dolan and Brockett , 2005 ) , which aim to predict the relationships between sentences by analyzing them holistically , as well as token - level tasks such as named entity recognition and question answering , where models are required to produce fine - grained output at the token level ( Tjong Kim Sang and De Meulder , 2003;Rajpurkar et al . , 2016 ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.747787Z","updated_at":"2022-10-17T21:45:15.024093Z","inner_id":9,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":10,"annotations":[{"id":10,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:44:29.738376Z","updated_at":"2022-10-17T21:44:34.973725Z","lead_time":5.649,"prediction":{},"result_count":0,"task":10,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Language model pre - training has been shown to be effective for improving many natural language processing tasks ( Dai and Le , 2015 ; Peters et al . , 2018a;Radford et al . , 2018;Howard and Ruder , 2018 ) ."},"meta":{},"created_at":"2022-10-17T21:25:49.747747Z","updated_at":"2022-10-17T21:44:35.000907Z","inner_id":8,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":9,"annotations":[{"id":9,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:44:24.136946Z","updated_at":"2022-10-17T21:44:24.136971Z","lead_time":1.22,"prediction":{},"result_count":0,"task":9,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Introduction ."},"meta":{},"created_at":"2022-10-17T21:25:49.747710Z","updated_at":"2022-10-17T21:44:24.154393Z","inner_id":7,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":7,"annotations":[{"id":7,"completed_by":1,"result":[{"value":{"start":0,"end":4,"text":"BERT","labels":["MethodName"]},"id":"a5kAsZGWxY","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:44:17.480654Z","updated_at":"2022-10-17T21:44:17.480679Z","lead_time":1.844,"prediction":{},"result_count":0,"task":7,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"BERT is conceptually simple and empirically powerful ."},"meta":{},"created_at":"2022-10-17T21:25:49.747634Z","updated_at":"2022-10-17T21:44:17.501567Z","inner_id":5,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":6,"annotations":[{"id":6,"completed_by":1,"result":[{"value":{"start":32,"end":36,"text":"BERT","labels":["MethodName"]},"id":"tb_tA0wIPE","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":174,"end":192,"text":"question answering","labels":["TaskName"]},"id":"HgqD2c0HmC","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":197,"end":215,"text":"language inference","labels":["TaskName"]},"id":"zEHLwfXFcK","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:44:11.435477Z","updated_at":"2022-10-17T21:44:11.435501Z","lead_time":4.637,"prediction":{},"result_count":0,"task":6,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"As a result , the pre - trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models for a wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications ."},"meta":{},"created_at":"2022-10-17T21:25:49.747597Z","updated_at":"2022-10-17T21:44:11.456702Z","inner_id":4,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]},{"id":5,"annotations":[{"id":5,"completed_by":1,"result":[{"value":{"start":97,"end":101,"text":"BERT","labels":["MethodName"]},"id":"yiDFe0hulm","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2022-10-17T21:43:55.131908Z","updated_at":"2022-10-17T21:43:55.131934Z","lead_time":1.022,"prediction":{},"result_count":0,"task":5,"parent_prediction":null,"parent_annotation":null}],"file_upload":"5effecea-N19-1423.pdf.txt","drafts":[],"predictions":[],"data":{"text":"Unlike recent language representation models ( Peters et al . , 2018a;Radford et al . , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers ."},"meta":{},"created_at":"2022-10-17T21:25:49.747557Z","updated_at":"2022-10-17T21:43:55.155362Z","inner_id":3,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":2,"updated_by":1,"comment_authors":[]}]