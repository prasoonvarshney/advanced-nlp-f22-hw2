[
  {
    "text": "The advantage of this procedure is that the Transformer encoder does not know which words it will be asked to predict or which have been replaced by random words , so it is forced to keep a distributional contextual representation of every input token .",
    "id": 331,
    "annotator": 1,
    "annotation_id": 228,
    "created_at": "2022-10-18T20:04:48.589868Z",
    "updated_at": "2022-10-18T21:46:15.760976Z",
    "lead_time": 9.908999999999999
  },
  {
    "text": "The numbers in the left part of the table represent the probabilities of the specific strategies used during MLM pre - training ( BERT uses 80 % , 10 % , 10 % ) .",
    "id": 321,
    "label": [
      {
        "start": 130,
        "end": 134,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 140,
        "end": 144,
        "text": "80 %",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 147,
        "end": 152,
        "text": "10 % ",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 154,
        "end": 159,
        "text": "10 % ",
        "labels": [
          "HyperparameterValue"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 329,
    "created_at": "2022-10-18T20:20:17.390158Z",
    "updated_at": "2022-10-18T21:45:25.435692Z",
    "lead_time": 46.989000000000004
  },
  {
    "text": "The GLUE webpage notes that there are issues with the construction of this dataset , 15 and every trained system that 's been submitted to GLUE has performed worse than the 65.1 baseline accuracy of predicting the majority class .",
    "id": 309,
    "label": [
      {
        "start": 4,
        "end": 8,
        "text": "GLUE",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 139,
        "end": 143,
        "text": "GLUE",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 173,
        "end": 177,
        "text": "65.1",
        "labels": [
          "MetricValue"
        ]
      },
      {
        "start": 187,
        "end": 195,
        "text": "accuracy",
        "labels": [
          "MetricName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 317,
    "created_at": "2022-10-18T20:19:05.307289Z",
    "updated_at": "2022-10-18T21:44:22.086300Z",
    "lead_time": 23.765
  },
  {
    "text": "We therefore exclude this set to be fair to OpenAI GPT .",
    "id": 310,
    "label": [
      {
        "start": 44,
        "end": 54,
        "text": "OpenAI GPT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 334,
    "created_at": "2022-10-18T21:43:57.340697Z",
    "updated_at": "2022-10-18T21:43:57.340722Z",
    "lead_time": 6.325
  },
  {
    "text": "CoLA The Corpus of Linguistic Acceptability is a binary single - sentence classification task , where the goal is to predict whether an English sentence is linguistically \" acceptable \" or not ( Warstadt et al . , 2018 ) .",
    "id": 302,
    "label": [
      {
        "start": 0,
        "end": 4,
        "text": "CoLA",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 9,
        "end": 43,
        "text": "Corpus of Linguistic Acceptability",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 49,
        "end": 88,
        "text": "binary single - sentence classification",
        "labels": [
          "TaskName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 310,
    "created_at": "2022-10-18T20:17:48.052890Z",
    "updated_at": "2022-10-18T21:43:42.851731Z",
    "lead_time": 34.257999999999996
  },
  {
    "text": "BERT Tok 1 Tok 2 Tok N ...",
    "id": 295,
    "label": [
      {
        "start": 0,
        "end": 4,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 303,
    "created_at": "2022-10-18T20:16:54.447355Z",
    "updated_at": "2022-10-18T21:42:24.292129Z",
    "lead_time": 7.196
  },
  {
    "text": "Tok M Question Paragraph BERT E [ CLS ] E 1 E 2 E N C T 1 T 2 T N Single Sentence ...",
    "id": 293,
    "label": [
      {
        "start": 25,
        "end": 29,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 301,
    "created_at": "2022-10-18T20:16:47.864544Z",
    "updated_at": "2022-10-18T21:42:15.571546Z",
    "lead_time": 9.254999999999999
  },
  {
    "text": "QQP Quora Question Pairs is a binary classification task where the goal is to determine if two questions asked on Quora are semantically equivalent ( Chen et al . , 2018 ) .",
    "id": 286,
    "label": [
      {
        "start": 30,
        "end": 51,
        "text": "binary classification",
        "labels": [
          "TaskName"
        ]
      },
      {
        "start": 0,
        "end": 3,
        "text": "QQP",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 4,
        "end": 24,
        "text": "Quora Question Pairs",
        "labels": [
          "DatasetName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 295,
    "created_at": "2022-10-18T20:15:46.099462Z",
    "updated_at": "2022-10-18T21:41:54.784084Z",
    "lead_time": 40.342000000000006
  },
  {
    "text": "( 2018a ): MNLI Multi - Genre Natural Language Inference is a large - scale , crowdsourced entailment classification task ( Williams et al . , 2018 ) .",
    "id": 284,
    "label": [
      {
        "start": 11,
        "end": 15,
        "text": "MNLI",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 16,
        "end": 56,
        "text": "Multi - Genre Natural Language Inference",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 91,
        "end": 116,
        "text": "entailment classification",
        "labels": [
          "TaskName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 293,
    "created_at": "2022-10-18T20:15:21.517987Z",
    "updated_at": "2022-10-18T21:40:40.179818Z",
    "lead_time": 35.394999999999996
  },
  {
    "text": "• GPT was trained for 1 M steps with a batch size of 32,000 words ; BERT was trained for 1 M steps with a batch size of 128,000 words .",
    "id": 277,
    "label": [
      {
        "start": 2,
        "end": 5,
        "text": "GPT",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 26,
        "end": 31,
        "text": "steps",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 39,
        "end": 49,
        "text": "batch size",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 93,
        "end": 98,
        "text": "steps",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 106,
        "end": 116,
        "text": "batch size",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 68,
        "end": 72,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 53,
        "end": 65,
        "text": "32,000 words",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 22,
        "end": 25,
        "text": "1 M",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 89,
        "end": 92,
        "text": "1 M",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 120,
        "end": 133,
        "text": "128,000 words",
        "labels": [
          "HyperparameterValue"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 286,
    "created_at": "2022-10-18T20:14:08.370602Z",
    "updated_at": "2022-10-18T21:40:09.333526Z",
    "lead_time": 79.436
  },
  {
    "text": "6 he likes play # # ing [ SEP ] my dog is cute [ SEP ] Input E [ CLS ] E he E likes E play E # # ing E [ SEP ] E my E dog E is E cute E [ SEP ] Token Embeddings E A E B E B E B E B E B E A E A E A E A E A Segment Embeddings E 0 E 6 E 7 E 8 E 9 E 10 E 1 E 2 E 3 E 4 E 5 Position Embeddings The NSP task is closely related to representationlearning objectives used in Jernite et al .",
    "id": 118,
    "annotator": 1,
    "annotation_id": 117,
    "created_at": "2022-10-17T22:25:00.161303Z",
    "updated_at": "2022-10-18T21:38:55.602456Z",
    "lead_time": 12.518
  },
  {
    "text": "As we show in Figure 1 , C is used for next sentence prediction ( NSP ) .",
    "id": 116,
    "annotator": 1,
    "annotation_id": 115,
    "created_at": "2022-10-17T22:24:41.467428Z",
    "updated_at": "2022-10-18T21:38:31.034573Z",
    "lead_time": 19.852
  },
  {
    "text": "We compare variations of this procedure in Appendix C.2 . Task # 2 : Next Sentence Prediction ( NSP ) Many important downstream tasks such as Question Answering ( QA ) and Natural Language Inference ( NLI ) are based on understanding the relationship between two sentences , which is not directly captured by language modeling .",
    "id": 113,
    "label": [
      {
        "start": 142,
        "end": 160,
        "text": "Question Answering",
        "labels": [
          "TaskName"
        ]
      },
      {
        "start": 163,
        "end": 165,
        "text": "QA",
        "labels": [
          "TaskName"
        ]
      },
      {
        "start": 172,
        "end": 198,
        "text": "Natural Language Inference",
        "labels": [
          "TaskName"
        ]
      },
      {
        "start": 201,
        "end": 204,
        "text": "NLI",
        "labels": [
          "TaskName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 112,
    "created_at": "2022-10-17T22:24:12.261421Z",
    "updated_at": "2022-10-18T21:38:11.958579Z",
    "lead_time": 44.68
  },
  {
    "text": "We refer to this procedure as a \" masked LM \" ( MLM ) , although it is often referred to as a Cloze task in the literature ( Taylor , 1953 ) .",
    "id": 104,
    "annotator": 1,
    "annotation_id": 102,
    "created_at": "2022-10-17T22:21:09.717577Z",
    "updated_at": "2022-10-18T21:37:15.976756Z",
    "lead_time": 12.849
  },
  {
    "text": "Task # 1 : Masked LM Intuitively , it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left - to - right model or the shallow concatenation of a left - toright and a right - to - left model .",
    "id": 100,
    "annotator": 1,
    "annotation_id": 333,
    "created_at": "2022-10-18T21:37:06.540878Z",
    "updated_at": "2022-10-18T21:37:06.540901Z",
    "lead_time": 1.08
  },
  {
    "text": "1 Because the use of Transformers has become common and our implementation is almost identical to the original , we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al .",
    "id": 76,
    "annotator": 1,
    "annotation_id": 75,
    "created_at": "2022-10-17T22:14:33.043057Z",
    "updated_at": "2022-10-18T21:34:33.678862Z",
    "lead_time": 18.021
  },
  {
    "text": "Model Architecture BERT 's model architecture is a multi - layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al .",
    "id": 74,
    "label": [
      {
        "start": 19,
        "end": 23,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 73,
    "created_at": "2022-10-17T22:14:05.325900Z",
    "updated_at": "2022-10-18T21:34:19.417692Z",
    "lead_time": 85.709
  },
  {
    "text": "More recently , sentence or document encoders which produce contextual token representations have been pre - trained from unlabeled text and fine - tuned for a supervised downstream task ( Dai and Le , 2015;Howard and Ruder , 2018;Radford et al . , 2018 ) .",
    "id": 51,
    "annotator": 1,
    "annotation_id": 50,
    "created_at": "2022-10-17T22:06:18.373632Z",
    "updated_at": "2022-10-18T21:31:55.528763Z",
    "lead_time": 79.401
  },
  {
    "text": "( 2018 ) shows that the cloze task can be used to improve the robustness of text generation models .",
    "id": 48,
    "annotator": 1,
    "annotation_id": 47,
    "created_at": "2022-10-17T22:05:35.277426Z",
    "updated_at": "2022-10-18T21:31:01.981386Z",
    "lead_time": 14.081
  },
  {
    "text": "To train sentence representations , prior work has used objectives to rank candidate next sentences ( Jernite et al . , 2017;Logeswaran and Lee , 2018 ) , left - to - right generation of next sentence words given a representation of the previous sentence ( Kiros et al . , 2015 ) , or denoising autoencoder derived objectives ( Hill et al . , 2016 ) .",
    "id": 40,
    "label": [
      {
        "start": 285,
        "end": 306,
        "text": "denoising autoencoder",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 39,
    "created_at": "2022-10-17T21:54:35.748609Z",
    "updated_at": "2022-10-18T21:30:40.462294Z",
    "lead_time": 39.986000000000004
  },
  {
    "text": "The code and pre - trained models are available at https://github.com/ google - research / bert .",
    "id": 32,
    "annotator": 1,
    "annotation_id": 31,
    "created_at": "2022-10-17T21:53:51.535817Z",
    "updated_at": "2022-10-18T21:30:10.778975Z",
    "lead_time": 13.46
  },
  {
    "text": "In addition to the masked language model , we also use a \" next sentence prediction \" task that jointly pretrains text - pair representations .",
    "id": 24,
    "annotator": 1,
    "annotation_id": 332,
    "created_at": "2022-10-18T21:29:44.722193Z",
    "updated_at": "2022-10-18T21:29:44.722220Z",
    "lead_time": 0.464
  },
  {
    "text": "The contributions of our paper are as follows : • We demonstrate the importance of bidirectional pre - training for language representations .",
    "id": 25,
    "annotator": 1,
    "annotation_id": 24,
    "created_at": "2022-10-17T21:52:39.853307Z",
    "updated_at": "2022-10-18T21:29:43.181061Z",
    "lead_time": 4.065
  },
  {
    "text": "It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQuAD v1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQuAD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) .",
    "id": 8,
    "label": [
      {
        "start": 114,
        "end": 118,
        "text": "GLUE",
        "labels": [
          "MetricName"
        ]
      },
      {
        "start": 128,
        "end": 134,
        "text": "80.5 %",
        "labels": [
          "MetricValue"
        ]
      },
      {
        "start": 174,
        "end": 182,
        "text": "MultiNLI",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 235,
        "end": 245,
        "text": "SQuAD v1.1",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 246,
        "end": 264,
        "text": "question answering",
        "labels": [
          "TaskName"
        ]
      },
      {
        "start": 270,
        "end": 272,
        "text": "F1",
        "labels": [
          "MetricName"
        ]
      },
      {
        "start": 276,
        "end": 280,
        "text": "93.2",
        "labels": [
          "MetricValue"
        ]
      },
      {
        "start": 183,
        "end": 191,
        "text": "accuracy",
        "labels": [
          "MetricName"
        ]
      },
      {
        "start": 195,
        "end": 201,
        "text": "86.7 %",
        "labels": [
          "MetricValue"
        ]
      },
      {
        "start": 336,
        "end": 338,
        "text": "F1",
        "labels": [
          "MetricName"
        ]
      },
      {
        "start": 342,
        "end": 346,
        "text": "83.1",
        "labels": [
          "MetricValue"
        ]
      },
      {
        "start": 320,
        "end": 330,
        "text": "SQuAD v2.0",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 204,
        "end": 209,
        "text": "4.6 %",
        "labels": [
          "MetricValue"
        ]
      },
      {
        "start": 349,
        "end": 352,
        "text": "5.1",
        "labels": [
          "MetricValue"
        ]
      },
      {
        "start": 283,
        "end": 286,
        "text": "1.5",
        "labels": [
          "MetricValue"
        ]
      },
      {
        "start": 137,
        "end": 142,
        "text": "7.7 %",
        "labels": [
          "MetricValue"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 8,
    "created_at": "2022-10-17T21:44:19.751499Z",
    "updated_at": "2022-10-18T21:28:31.654674Z",
    "lead_time": 72.342
  },
  {
    "text": "BERT : Pre - training of Deep Bidirectional Transformers for Language Understanding .",
    "id": 3,
    "label": [
      {
        "start": 0,
        "end": 4,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 3,
    "created_at": "2022-10-17T21:43:50.628606Z",
    "updated_at": "2022-10-18T21:27:36.476579Z",
    "lead_time": 17.747
  },
  {
    "text": "BERT alleviates the previously mentioned unidirectionality constraint by using a \" masked language model \" ( MLM ) pre - training objective , inspired by the Cloze task ( Taylor , 1953 ) .",
    "id": 21,
    "label": [
      {
        "start": 0,
        "end": 4,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 20,
    "created_at": "2022-10-17T21:50:39.183239Z",
    "updated_at": "2022-10-18T21:27:18.492908Z",
    "lead_time": 491.145
  },
  {
    "text": "For the feature - based approach , we concatenate the last 4 layers of BERT as the features , which was shown to be the best approach in Section 5.3 . From the table it can be seen that fine - tuning is surprisingly robust to different masking strategies .",
    "id": 323,
    "label": [
      {
        "start": 71,
        "end": 75,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 331,
    "created_at": "2022-10-18T20:20:25.088278Z",
    "updated_at": "2022-10-18T20:20:25.088301Z",
    "lead_time": 3.295
  },
  {
    "text": "The right part of the paper represents the Dev set results .",
    "id": 322,
    "annotator": 1,
    "annotation_id": 330,
    "created_at": "2022-10-18T20:20:20.622387Z",
    "updated_at": "2022-10-18T20:20:20.622410Z",
    "lead_time": 1.046
  },
  {
    "text": "In the table , MASK means that we replace the target token with the [ MASK ] symbol for MLM ; SAME means that we keep the target token as is ; RND means that we replace the target token with another random token .",
    "id": 320,
    "annotator": 1,
    "annotation_id": 328,
    "created_at": "2022-10-18T20:20:14.661227Z",
    "updated_at": "2022-10-18T20:20:14.661252Z",
    "lead_time": 2.579
  },
  {
    "text": "The results are presented in Table 8 .",
    "id": 319,
    "annotator": 1,
    "annotation_id": 327,
    "created_at": "2022-10-18T20:20:11.292084Z",
    "updated_at": "2022-10-18T20:20:11.292126Z",
    "lead_time": 0.304
  },
  {
    "text": "For NER , we report both fine - tuning and feature - based approaches , as we expect the mismatch will be amplified for the feature - based approach as the model will not have the chance to adjust the representations .",
    "id": 318,
    "label": [
      {
        "start": 4,
        "end": 7,
        "text": "NER",
        "labels": [
          "TaskName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 326,
    "created_at": "2022-10-18T20:20:09.906465Z",
    "updated_at": "2022-10-18T20:20:09.906488Z",
    "lead_time": 7.28
  },
  {
    "text": "We report the Dev results for both MNLI and NER .",
    "id": 317,
    "label": [
      {
        "start": 35,
        "end": 39,
        "text": "MNLI",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 44,
        "end": 47,
        "text": "NER",
        "labels": [
          "TaskName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 325,
    "created_at": "2022-10-18T20:20:00.498002Z",
    "updated_at": "2022-10-18T20:20:00.498026Z",
    "lead_time": 7.08
  },
  {
    "text": "Note that the purpose of the masking strategies is to reduce the mismatch between pre - training and fine - tuning , as the [ MASK ] symbol never appears during the fine - tuning stage .",
    "id": 316,
    "annotator": 1,
    "annotation_id": 323,
    "created_at": "2022-10-18T20:19:35.838516Z",
    "updated_at": "2022-10-18T20:19:35.838538Z",
    "lead_time": 2.719
  },
  {
    "text": "The following is an ablation study to evaluate the effect of different masking strategies .",
    "id": 315,
    "annotator": 1,
    "annotation_id": 322,
    "created_at": "2022-10-18T20:19:32.024410Z",
    "updated_at": "2022-10-18T20:19:32.024433Z",
    "lead_time": 1.452
  },
  {
    "text": "In Section 3.1 , we mention that BERT uses a mixed strategy for masking the target tokens when pre - training with the masked language model ( MLM ) objective .",
    "id": 314,
    "label": [
      {
        "start": 33,
        "end": 37,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 321,
    "created_at": "2022-10-18T20:19:29.327017Z",
    "updated_at": "2022-10-18T20:19:29.327044Z",
    "lead_time": 10.005
  },
  {
    "text": "C.2 Ablation for Different Masking Procedures .",
    "id": 313,
    "annotator": 1,
    "annotation_id": 320,
    "created_at": "2022-10-18T20:19:17.847199Z",
    "updated_at": "2022-10-18T20:19:17.847300Z",
    "lead_time": 0.602
  },
  {
    "text": "C Additional Ablation Studies .",
    "id": 312,
    "annotator": 1,
    "annotation_id": 319,
    "created_at": "2022-10-18T20:19:16.323183Z",
    "updated_at": "2022-10-18T20:19:16.323205Z",
    "lead_time": 0.554
  },
  {
    "text": "For our GLUE submission , we always predicted the majority class .",
    "id": 311,
    "label": [
      {
        "start": 8,
        "end": 12,
        "text": "GLUE",
        "labels": [
          "DatasetName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 318,
    "created_at": "2022-10-18T20:19:14.597685Z",
    "updated_at": "2022-10-18T20:19:14.597708Z",
    "lead_time": 3.746
  },
  {
    "text": "14 WNLI Winograd NLI is a small natural language inference dataset ( Levesque et al . , 2011 ) .",
    "id": 308,
    "label": [
      {
        "start": 3,
        "end": 7,
        "text": "WNLI",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 8,
        "end": 20,
        "text": "Winograd NLI",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 32,
        "end": 58,
        "text": "natural language inference",
        "labels": [
          "TaskName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 316,
    "created_at": "2022-10-18T20:18:51.123842Z",
    "updated_at": "2022-10-18T20:18:51.123863Z",
    "lead_time": 13.661
  },
  {
    "text": "RTE Recognizing Textual Entailment is a binary entailment task similar to MNLI , but with much less training data ( Bentivogli et al . , 2009 ) .",
    "id": 307,
    "label": [
      {
        "start": 0,
        "end": 3,
        "text": "RTE",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 4,
        "end": 34,
        "text": "Recognizing Textual Entailment",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 74,
        "end": 78,
        "text": "MNLI",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 40,
        "end": 57,
        "text": "binary entailment",
        "labels": [
          "TaskName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 315,
    "created_at": "2022-10-18T20:18:35.881676Z",
    "updated_at": "2022-10-18T20:18:35.881703Z",
    "lead_time": 20.958
  },
  {
    "text": "MRPC Microsoft Research Paraphrase Corpus consists of sentence pairs automatically extracted from online news sources , with human annotations for whether the sentences in the pair are semantically equivalent ( Dolan and Brockett , 2005 ) .",
    "id": 306,
    "label": [
      {
        "start": 0,
        "end": 4,
        "text": "MRPC",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 5,
        "end": 41,
        "text": "Microsoft Research Paraphrase Corpus",
        "labels": [
          "DatasetName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 314,
    "created_at": "2022-10-18T20:18:13.195060Z",
    "updated_at": "2022-10-18T20:18:13.195101Z",
    "lead_time": 9.432
  },
  {
    "text": "They were annotated with a score from 1 to 5 denoting how similar the two sentences are in terms of semantic meaning .",
    "id": 305,
    "annotator": 1,
    "annotation_id": 313,
    "created_at": "2022-10-18T20:17:59.319882Z",
    "updated_at": "2022-10-18T20:17:59.319906Z",
    "lead_time": 1.805
  },
  {
    "text": "The Semantic Textual Similarity Benchmark is a collection of sentence pairs drawn from news headlines and other sources ( Cer et al . , 2017 ) .",
    "id": 304,
    "label": [
      {
        "start": 4,
        "end": 41,
        "text": "Semantic Textual Similarity Benchmark",
        "labels": [
          "DatasetName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 312,
    "created_at": "2022-10-18T20:17:56.495686Z",
    "updated_at": "2022-10-18T20:17:56.495707Z",
    "lead_time": 6.037
  },
  {
    "text": "STS - B.",
    "id": 303,
    "annotator": 1,
    "annotation_id": 311,
    "created_at": "2022-10-18T20:17:49.627813Z",
    "updated_at": "2022-10-18T20:17:49.627834Z",
    "lead_time": 0.128
  },
  {
    "text": "with human annotations of their sentiment ( Socher et al . , 2013 ) .",
    "id": 301,
    "annotator": 1,
    "annotation_id": 309,
    "created_at": "2022-10-18T20:17:17.881556Z",
    "updated_at": "2022-10-18T20:17:17.881581Z",
    "lead_time": 0.235
  },
  {
    "text": "E M ' C T 1 T [ SEP ] ...",
    "id": 300,
    "annotator": 1,
    "annotation_id": 308,
    "created_at": "2022-10-18T20:17:16.279257Z",
    "updated_at": "2022-10-18T20:17:16.279281Z",
    "lead_time": 0.26
  },
  {
    "text": "E N E 1 ' ...",
    "id": 299,
    "annotator": 1,
    "annotation_id": 307,
    "created_at": "2022-10-18T20:17:14.925524Z",
    "updated_at": "2022-10-18T20:17:14.925551Z",
    "lead_time": 1.491
  },
  {
    "text": "E [ CLS ] E 1 E [ SEP ] Class Label ...",
    "id": 298,
    "annotator": 1,
    "annotation_id": 306,
    "created_at": "2022-10-18T20:17:03.846212Z",
    "updated_at": "2022-10-18T20:17:03.846235Z",
    "lead_time": 0.144
  },
  {
    "text": "...",
    "id": 297,
    "annotator": 1,
    "annotation_id": 305,
    "created_at": "2022-10-18T20:17:02.577266Z",
    "updated_at": "2022-10-18T20:17:02.577288Z",
    "lead_time": 2.026
  },
  {
    "text": "[ CLS ] E [ CLS ] E 1 E 2 E N C T 1 T 2 T N Single Sentence B - PER O O ...",
    "id": 296,
    "annotator": 1,
    "annotation_id": 304,
    "created_at": "2022-10-18T20:16:59.416637Z",
    "updated_at": "2022-10-18T20:16:59.416661Z",
    "lead_time": 0.189
  },
  {
    "text": "...",
    "id": 294,
    "annotator": 1,
    "annotation_id": 302,
    "created_at": "2022-10-18T20:16:53.483368Z",
    "updated_at": "2022-10-18T20:16:53.483389Z",
    "lead_time": 1.914
  },
  {
    "text": "T M ' [ CLS ] Tok 1 [ SEP ] ...",
    "id": 291,
    "annotator": 1,
    "annotation_id": 300,
    "created_at": "2022-10-18T20:16:43.987565Z",
    "updated_at": "2022-10-18T20:16:43.987588Z",
    "lead_time": 0.123
  },
  {
    "text": "T N T 1 ' ...",
    "id": 290,
    "annotator": 1,
    "annotation_id": 299,
    "created_at": "2022-10-18T20:16:43.005626Z",
    "updated_at": "2022-10-18T20:16:43.005652Z",
    "lead_time": 0.121
  },
  {
    "text": "E M ' C T 1 T [ SEP ] ...",
    "id": 289,
    "annotator": 1,
    "annotation_id": 298,
    "created_at": "2022-10-18T20:16:41.914972Z",
    "updated_at": "2022-10-18T20:16:41.914996Z",
    "lead_time": 0.166
  },
  {
    "text": "E N E 1 ' ...",
    "id": 288,
    "annotator": 1,
    "annotation_id": 297,
    "created_at": "2022-10-18T20:16:40.848302Z",
    "updated_at": "2022-10-18T20:16:40.848320Z",
    "lead_time": 0.484
  },
  {
    "text": "QNLI Question Natural Language Inference is a version of the Stanford Question Answering Dataset ( Rajpurkar et al . , 2016 ) which has been converted to a binary classification task ( Wang et al . , 2018a ...",
    "id": 287,
    "label": [
      {
        "start": 0,
        "end": 4,
        "text": "QNLI",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 5,
        "end": 40,
        "text": "Question Natural Language Inference",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 61,
        "end": 96,
        "text": "Stanford Question Answering Dataset",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 156,
        "end": 177,
        "text": "binary classification",
        "labels": [
          "TaskName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 296,
    "created_at": "2022-10-18T20:16:39.463148Z",
    "updated_at": "2022-10-18T20:16:39.463172Z",
    "lead_time": 35.131
  },
  {
    "text": "Given a pair of sentences , the goal is to predict whether the second sentence is an entailment , contradiction , or neutral with respect to the first one .",
    "id": 285,
    "annotator": 1,
    "annotation_id": 294,
    "created_at": "2022-10-18T20:15:24.349941Z",
    "updated_at": "2022-10-18T20:15:37.134818Z",
    "lead_time": 2.7510000000000003
  },
  {
    "text": "The GLUE benchmark includes the following datasets , the descriptions of which were originally summarized in Wang et al .",
    "id": 283,
    "label": [
      {
        "start": 4,
        "end": 8,
        "text": "GLUE",
        "labels": [
          "DatasetName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 292,
    "created_at": "2022-10-18T20:15:09.478552Z",
    "updated_at": "2022-10-18T20:15:09.478576Z",
    "lead_time": 9.625
  },
  {
    "text": "Our task - specific models are formed by incorporating BERT with one additional output layer , so a minimal number of parameters need to be learned from scratch .",
    "id": 282,
    "label": [
      {
        "start": 55,
        "end": 59,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 291,
    "created_at": "2022-10-18T20:14:58.794884Z",
    "updated_at": "2022-10-18T20:14:58.794909Z",
    "lead_time": 5.142
  },
  {
    "text": "The illustration of fine - tuning BERT on different tasks can be seen in Figure 4 .",
    "id": 281,
    "label": [
      {
        "start": 34,
        "end": 38,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 290,
    "created_at": "2022-10-18T20:14:50.081170Z",
    "updated_at": "2022-10-18T20:14:50.081191Z",
    "lead_time": 3.798
  },
  {
    "text": "A.5 Illustrations of Fine - tuning on Different Tasks .",
    "id": 280,
    "annotator": 1,
    "annotation_id": 289,
    "created_at": "2022-10-18T20:14:45.323989Z",
    "updated_at": "2022-10-18T20:14:45.324013Z",
    "lead_time": 0.477
  },
  {
    "text": "To isolate the effect of these differences , we perform ablation experiments in Section 5.1 which demonstrate that the majority of the improvements are in fact coming from the two pre - training tasks and the bidirectionality they enable .",
    "id": 279,
    "annotator": 1,
    "annotation_id": 288,
    "created_at": "2022-10-18T20:14:43.737494Z",
    "updated_at": "2022-10-18T20:14:43.737523Z",
    "lead_time": 1.593
  },
  {
    "text": "• GPT used the same learning rate of 5e-5 for all fine - tuning experiments ; BERT chooses a task - specific fine - tuning learning rate which performs the best on the development set .",
    "id": 278,
    "label": [
      {
        "start": 2,
        "end": 5,
        "text": "GPT",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 20,
        "end": 33,
        "text": "learning rate",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 37,
        "end": 41,
        "text": "5e-5",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 78,
        "end": 82,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 287,
    "created_at": "2022-10-18T20:14:40.062816Z",
    "updated_at": "2022-10-18T20:14:40.062841Z",
    "lead_time": 29.824
  },
  {
    "text": "• GPT uses a sentence separator ( [ SEP ] ) and classifier token ( [ CLS ] ) which are only introduced at fine - tuning time ; BERT learns [ SEP ] , [ CLS ] and sentence A / B embeddings during pre - training .",
    "id": 276,
    "label": [
      {
        "start": 2,
        "end": 5,
        "text": "GPT",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 127,
        "end": 131,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 285,
    "created_at": "2022-10-18T20:13:25.590521Z",
    "updated_at": "2022-10-18T20:13:25.590545Z",
    "lead_time": 8.651
  },
  {
    "text": "The core argument of this work is that the bi - directionality and the two pretraining tasks presented in Section 3.1 account for the majority of the empirical improvements , but we do note that there are several other differences between how BERT and GPT were trained : • GPT is trained on the BooksCorpus ( 800 M words ) ; BERT is trained on the BooksCorpus ( 800 M words ) and Wikipedia ( 2,500 M words ) .",
    "id": 275,
    "label": [
      {
        "start": 243,
        "end": 247,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 252,
        "end": 255,
        "text": "GPT",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 273,
        "end": 276,
        "text": "GPT",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 295,
        "end": 306,
        "text": "BooksCorpus",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 348,
        "end": 359,
        "text": "BooksCorpus",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 325,
        "end": 329,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 380,
        "end": 389,
        "text": "Wikipedia",
        "labels": [
          "DatasetName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 284,
    "created_at": "2022-10-18T20:13:15.916604Z",
    "updated_at": "2022-10-18T20:13:15.916625Z",
    "lead_time": 35.926
  },
  {
    "text": "In fact , many of the design decisions in BERT were intentionally made to make it as close to GPT as possible so that the two methods could be minimally compared .",
    "id": 274,
    "label": [
      {
        "start": 42,
        "end": 46,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 94,
        "end": 97,
        "text": "GPT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 283,
    "created_at": "2022-10-18T20:12:34.275619Z",
    "updated_at": "2022-10-18T20:12:38.594714Z",
    "lead_time": 8.7
  },
  {
    "text": "The most comparable existing pre - training method to BERT is OpenAI GPT , which trains a left - to - right Transformer LM on a large text corpus .",
    "id": 273,
    "label": [
      {
        "start": 54,
        "end": 58,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 62,
        "end": 72,
        "text": "OpenAI GPT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 282,
    "created_at": "2022-10-18T20:12:17.800387Z",
    "updated_at": "2022-10-18T20:12:25.806170Z",
    "lead_time": 15.297
  },
  {
    "text": "Note that in addition to the architecture differences , BERT and OpenAI GPT are finetuning approaches , while ELMo is a feature - based approach .",
    "id": 272,
    "label": [
      {
        "start": 56,
        "end": 60,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 65,
        "end": 75,
        "text": "OpenAI GPT",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 110,
        "end": 114,
        "text": "ELMo",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 281,
    "created_at": "2022-10-18T20:12:01.521128Z",
    "updated_at": "2022-10-18T20:12:09.115538Z",
    "lead_time": 14.026
  },
  {
    "text": "The comparisons between the model architectures are shown visually in Figure 3 .",
    "id": 271,
    "annotator": 1,
    "annotation_id": 280,
    "created_at": "2022-10-18T20:11:53.811037Z",
    "updated_at": "2022-10-18T20:11:53.811061Z",
    "lead_time": 2.314
  },
  {
    "text": "A.4 Comparison of BERT , ELMo , and OpenAI GPT Here we studies the differences in recent popular representation learning models including ELMo , OpenAI GPT and BERT .",
    "id": 270,
    "label": [
      {
        "start": 18,
        "end": 22,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 25,
        "end": 29,
        "text": "ELMo",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 36,
        "end": 46,
        "text": "OpenAI GPT",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 138,
        "end": 142,
        "text": "ELMo",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 145,
        "end": 155,
        "text": "OpenAI GPT",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 160,
        "end": 164,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 279,
    "created_at": "2022-10-18T20:11:49.564460Z",
    "updated_at": "2022-10-18T20:11:49.564481Z",
    "lead_time": 29.452
  },
  {
    "text": "Fine - tuning is typically very fast , so it is reasonable to simply run an exhaustive search over the above parameters and choose the model that performs best on the development set .",
    "id": 269,
    "annotator": 1,
    "annotation_id": 278,
    "created_at": "2022-10-18T20:11:18.924969Z",
    "updated_at": "2022-10-18T20:11:18.924994Z",
    "lead_time": 1.03
  },
  {
    "text": "The dropout probability was always kept at 0.1 . The optimal hyperparameter values are task - specific , but we found the following range of possible values to work well across all tasks : • Batch size : 16 , 32 We also observed that large data sets ( e.g. , 100k+ labeled training examples ) were far less sensitive to hyperparameter choice than small data sets .",
    "id": 268,
    "label": [
      {
        "start": 4,
        "end": 23,
        "text": "dropout probability",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 43,
        "end": 46,
        "text": "0.1",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 191,
        "end": 201,
        "text": "Batch size",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 204,
        "end": 206,
        "text": "16",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 209,
        "end": 211,
        "text": "32",
        "labels": [
          "HyperparameterValue"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 277,
    "created_at": "2022-10-18T20:11:16.852649Z",
    "updated_at": "2022-10-18T20:11:16.852670Z",
    "lead_time": 25.003
  },
  {
    "text": "For fine - tuning , most model hyperparameters are the same as in pre - training , with the exception of the batch size , learning rate , and number of training epochs .",
    "id": 267,
    "label": [
      {
        "start": 109,
        "end": 119,
        "text": "batch size",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 122,
        "end": 135,
        "text": "learning rate",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 142,
        "end": 167,
        "text": "number of training epochs",
        "labels": [
          "HyperparameterName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 276,
    "created_at": "2022-10-18T20:10:39.853372Z",
    "updated_at": "2022-10-18T20:10:50.778960Z",
    "lead_time": 11.461
  },
  {
    "text": "A.3 Fine - tuning Procedure .",
    "id": 266,
    "annotator": 1,
    "annotation_id": 275,
    "created_at": "2022-10-18T20:10:38.233254Z",
    "updated_at": "2022-10-18T20:10:38.233281Z",
    "lead_time": 0.375
  },
  {
    "text": "Then , we train the rest 10 % of the steps of sequence of 512 to learn the positional embeddings .",
    "id": 265,
    "annotator": 1,
    "annotation_id": 274,
    "created_at": "2022-10-18T20:10:36.605474Z",
    "updated_at": "2022-10-18T20:10:36.605501Z",
    "lead_time": 6.546
  },
  {
    "text": "Longer sequences are disproportionately expensive because attention is quadratic to the sequence length .",
    "id": 263,
    "annotator": 1,
    "annotation_id": 273,
    "created_at": "2022-10-18T20:10:20.920752Z",
    "updated_at": "2022-10-18T20:10:20.920783Z",
    "lead_time": 0.455
  },
  {
    "text": "Each pretraining took 4 days to complete .",
    "id": 262,
    "annotator": 1,
    "annotation_id": 272,
    "created_at": "2022-10-18T20:10:19.335536Z",
    "updated_at": "2022-10-18T20:10:19.335562Z",
    "lead_time": 2.413
  },
  {
    "text": "13 Training of BERT LARGE was performed on 16 Cloud TPUs ( 64 TPU chips total ) .",
    "id": 261,
    "label": [
      {
        "start": 15,
        "end": 25,
        "text": "BERT LARGE",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 271,
    "created_at": "2022-10-18T20:10:15.179052Z",
    "updated_at": "2022-10-18T20:10:15.179075Z",
    "lead_time": 3.184
  },
  {
    "text": "Training of BERT BASE was performed on 4 Cloud TPUs in Pod configuration ( 16 TPU chips total ) .",
    "id": 260,
    "label": [
      {
        "start": 12,
        "end": 21,
        "text": "BERT BASE",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 270,
    "created_at": "2022-10-18T20:10:10.989254Z",
    "updated_at": "2022-10-18T20:10:10.989279Z",
    "lead_time": 3.704
  },
  {
    "text": "E 1 E 2 E N ...",
    "id": 259,
    "annotator": 1,
    "annotation_id": 269,
    "created_at": "2022-10-18T20:10:06.411051Z",
    "updated_at": "2022-10-18T20:10:06.411083Z",
    "lead_time": 0.206
  },
  {
    "text": "T 1 T 2 T N ...",
    "id": 258,
    "annotator": 1,
    "annotation_id": 268,
    "created_at": "2022-10-18T20:10:04.979821Z",
    "updated_at": "2022-10-18T20:10:04.979845Z",
    "lead_time": 0.392
  },
  {
    "text": "E 1 E 2 E N ...",
    "id": 257,
    "annotator": 1,
    "annotation_id": 267,
    "created_at": "2022-10-18T20:10:01.810275Z",
    "updated_at": "2022-10-18T20:10:01.810301Z",
    "lead_time": 0.117
  },
  {
    "text": "T 1 T 2 T N ...",
    "id": 256,
    "annotator": 1,
    "annotation_id": 266,
    "created_at": "2022-10-18T20:10:00.974155Z",
    "updated_at": "2022-10-18T20:10:00.974182Z",
    "lead_time": 0.2
  },
  {
    "text": "E N ...",
    "id": 255,
    "annotator": 1,
    "annotation_id": 265,
    "created_at": "2022-10-18T20:09:59.988965Z",
    "updated_at": "2022-10-18T20:09:59.989037Z",
    "lead_time": 0.55
  },
  {
    "text": "The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood .",
    "id": 254,
    "annotator": 1,
    "annotation_id": 264,
    "created_at": "2022-10-18T20:09:58.656720Z",
    "updated_at": "2022-10-18T20:09:58.656748Z",
    "lead_time": 0.848
  },
  {
    "text": "We use a gelu activation ( Hendrycks and Gimpel , 2016 ) rather than the standard relu , following OpenAI GPT .",
    "id": 253,
    "label": [
      {
        "start": 99,
        "end": 109,
        "text": "OpenAI GPT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 263,
    "created_at": "2022-10-18T20:09:56.761033Z",
    "updated_at": "2022-10-18T20:09:56.761065Z",
    "lead_time": 6.946
  },
  {
    "text": "We use a dropout probability of 0.1 on all layers .",
    "id": 252,
    "label": [
      {
        "start": 9,
        "end": 28,
        "text": "dropout probability",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 32,
        "end": 35,
        "text": "0.1",
        "labels": [
          "HyperparameterValue"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 262,
    "created_at": "2022-10-18T20:09:48.817968Z",
    "updated_at": "2022-10-18T20:09:48.817992Z",
    "lead_time": 9.616
  },
  {
    "text": "We use Adam with learning rate of 1e-4 , β 1 = 0.9 , β 2 = 0.999 , L2 weight decay of 0.01 , learning rate warmup over the first 10,000 steps , and linear decay of the learning rate .",
    "id": 251,
    "label": [
      {
        "start": 17,
        "end": 30,
        "text": "learning rate",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 34,
        "end": 38,
        "text": "1e-4",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 41,
        "end": 44,
        "text": "β 1",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 47,
        "end": 50,
        "text": "0.9",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 53,
        "end": 56,
        "text": "β 2",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 59,
        "end": 64,
        "text": "0.999",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 67,
        "end": 82,
        "text": "L2 weight decay",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 86,
        "end": 90,
        "text": "0.01",
        "labels": [
          "HyperparameterValue"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 261,
    "created_at": "2022-10-18T20:09:37.869717Z",
    "updated_at": "2022-10-18T20:09:37.869741Z",
    "lead_time": 41.088
  },
  {
    "text": "We train with batch size of 256 sequences ( 256 sequences * 512 tokens = 128,000 tokens / batch ) for 1,000,000 steps , which is approximately 40 epochs over the 3.3 billion word corpus .",
    "id": 250,
    "label": [
      {
        "start": 14,
        "end": 24,
        "text": "batch size",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 28,
        "end": 31,
        "text": "256",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 112,
        "end": 117,
        "text": "steps",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 102,
        "end": 111,
        "text": "1,000,000",
        "labels": [
          "HyperparameterValue"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 260,
    "created_at": "2022-10-18T20:08:55.632798Z",
    "updated_at": "2022-10-18T20:08:55.632823Z",
    "lead_time": 25.333
  },
  {
    "text": "The LM masking is applied after WordPiece tokenization with a uniform masking rate of 15 % , and no special consideration given to partial word pieces .",
    "id": 249,
    "annotator": 1,
    "annotation_id": 259,
    "created_at": "2022-10-18T20:08:21.178930Z",
    "updated_at": "2022-10-18T20:08:21.178956Z",
    "lead_time": 1.22
  },
  {
    "text": "They are sampled such that the combined length is ≤ 512 tokens .",
    "id": 248,
    "annotator": 1,
    "annotation_id": 258,
    "created_at": "2022-10-18T20:08:19.338664Z",
    "updated_at": "2022-10-18T20:08:19.338692Z",
    "lead_time": 3.776
  },
  {
    "text": "50 % of the time B is the actual next sentence that follows A and 50 % of the time it is a random sentence , which is done for the \" next sentence prediction \" task .",
    "id": 247,
    "label": [
      {
        "start": 132,
        "end": 157,
        "text": " next sentence prediction",
        "labels": [
          "TaskName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 257,
    "created_at": "2022-10-18T20:08:06.494862Z",
    "updated_at": "2022-10-18T20:08:14.549273Z",
    "lead_time": 9.229
  },
  {
    "text": "The first sentence receives the A embedding and the second receives the B embedding .",
    "id": 246,
    "annotator": 1,
    "annotation_id": 256,
    "created_at": "2022-10-18T20:08:02.069438Z",
    "updated_at": "2022-10-18T20:08:02.069467Z",
    "lead_time": 0.536
  },
  {
    "text": "To generate each training input sequence , we sample two spans of text from the corpus , which we refer to as \" sentences \" even though they are typically much longer than single sentences ( but can be shorter also ) .",
    "id": 245,
    "annotator": 1,
    "annotation_id": 255,
    "created_at": "2022-10-18T20:08:00.661119Z",
    "updated_at": "2022-10-18T20:08:00.661147Z",
    "lead_time": 1.787
  },
  {
    "text": "Our major contribution is further generalizing these findings to deep bidirectional architectures , allowing the same pre - trained model to successfully tackle a broad set of NLP tasks .",
    "id": 244,
    "annotator": 1,
    "annotation_id": 254,
    "created_at": "2022-10-18T20:07:57.530216Z",
    "updated_at": "2022-10-18T20:07:57.530237Z",
    "lead_time": 1.821
  },
  {
    "text": "In particular , these results enable even low - resource tasks to benefit from deep unidirectional architectures .",
    "id": 243,
    "annotator": 1,
    "annotation_id": 253,
    "created_at": "2022-10-18T20:07:53.344256Z",
    "updated_at": "2022-10-18T20:07:53.344280Z",
    "lead_time": 3.088
  },
  {
    "text": "Recent empirical improvements due to transfer learning with language models have demonstrated that rich , unsupervised pre - training is an integral part of many language understanding systems .",
    "id": 242,
    "annotator": 1,
    "annotation_id": 252,
    "created_at": "2022-10-18T20:07:49.077716Z",
    "updated_at": "2022-10-18T20:07:49.077743Z",
    "lead_time": 1.196
  },
  {
    "text": "Conclusion .",
    "id": 241,
    "annotator": 1,
    "annotation_id": 248,
    "created_at": "2022-10-18T20:07:14.218387Z",
    "updated_at": "2022-10-18T20:07:46.864930Z",
    "lead_time": 0.511
  },
  {
    "text": "This demonstrates that BERT is effective for both finetuning and feature - based approaches .",
    "id": 240,
    "label": [
      {
        "start": 23,
        "end": 27,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 251,
    "created_at": "2022-10-18T20:07:45.698336Z",
    "updated_at": "2022-10-18T20:07:45.698362Z",
    "lead_time": 4.013
  },
  {
    "text": "The best performing method concatenates the token representations from the top four hidden layers of the pre - trained Transformer , which is only 0.3 F1 behind fine - tuning the entire model .",
    "id": 239,
    "label": [
      {
        "start": 119,
        "end": 130,
        "text": "Transformer",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 151,
        "end": 153,
        "text": "F1",
        "labels": [
          "MetricName"
        ]
      },
      {
        "start": 147,
        "end": 150,
        "text": "0.3",
        "labels": [
          "MetricValue"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 250,
    "created_at": "2022-10-18T20:07:40.180829Z",
    "updated_at": "2022-10-18T20:07:40.180851Z",
    "lead_time": 20.17
  },
  {
    "text": "BERT LARGE performs competitively with state - of - the - art methods .",
    "id": 238,
    "label": [
      {
        "start": 0,
        "end": 10,
        "text": "BERT LARGE",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 249,
    "created_at": "2022-10-18T20:07:19.060194Z",
    "updated_at": "2022-10-18T20:07:19.060216Z",
    "lead_time": 3.556
  },
  {
    "text": "Results are presented in Table 7 .",
    "id": 237,
    "annotator": 1,
    "annotation_id": 247,
    "created_at": "2022-10-18T20:07:10.688956Z",
    "updated_at": "2022-10-18T20:07:10.688976Z",
    "lead_time": 0.734
  },
  {
    "text": "These contextual embeddings are used as input to a randomly initialized two - layer 768 - dimensional BiLSTM before the classification layer .",
    "id": 236,
    "label": [
      {
        "start": 102,
        "end": 108,
        "text": "BiLSTM",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 246,
    "created_at": "2022-10-18T20:07:07.995026Z",
    "updated_at": "2022-10-18T20:07:07.995047Z",
    "lead_time": 10.263
  },
  {
    "text": "To ablate the fine - tuning approach , we apply the feature - based approach by extracting the activations from one or more layers without fine - tuning any parameters of BERT .",
    "id": 235,
    "label": [
      {
        "start": 171,
        "end": 175,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 245,
    "created_at": "2022-10-18T20:06:56.728643Z",
    "updated_at": "2022-10-18T20:06:56.728670Z",
    "lead_time": 2.612
  },
  {
    "text": "We use the representation of the first sub - token as the input to the token - level classifier over the NER label set .",
    "id": 234,
    "label": [
      {
        "start": 105,
        "end": 108,
        "text": "NER",
        "labels": [
          "TaskName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 243,
    "created_at": "2022-10-18T20:06:36.058396Z",
    "updated_at": "2022-10-18T20:06:39.663409Z",
    "lead_time": 3.9800000000000004
  },
  {
    "text": "Following standard practice , we formulate this as a tagging task but do not use a CRF layer in the output .",
    "id": 233,
    "annotator": 1,
    "annotation_id": 242,
    "created_at": "2022-10-18T20:06:34.667665Z",
    "updated_at": "2022-10-18T20:06:34.667692Z",
    "lead_time": 1.788
  },
  {
    "text": "In the input to BERT , we use a case - preserving WordPiece model , and we include the maximal document context provided by the data .",
    "id": 232,
    "label": [
      {
        "start": 16,
        "end": 20,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 241,
    "created_at": "2022-10-18T20:06:30.062721Z",
    "updated_at": "2022-10-18T20:06:30.062753Z",
    "lead_time": 2.822
  },
  {
    "text": "In this section , we compare the two approaches by applying BERT to the CoNLL-2003 Named Entity Recognition ( NER ) task ( Tjong Kim Sang and De Meulder , 2003 ) .",
    "id": 231,
    "label": [
      {
        "start": 60,
        "end": 64,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 83,
        "end": 107,
        "text": "Named Entity Recognition",
        "labels": [
          "TaskName"
        ]
      },
      {
        "start": 110,
        "end": 113,
        "text": "NER",
        "labels": [
          "TaskName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 240,
    "created_at": "2022-10-18T20:06:11.482192Z",
    "updated_at": "2022-10-18T20:06:24.386333Z",
    "lead_time": 17.988
  },
  {
    "text": "Second , there are major computational benefits to pre - compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation .",
    "id": 230,
    "annotator": 1,
    "annotation_id": 239,
    "created_at": "2022-10-18T20:06:05.102419Z",
    "updated_at": "2022-10-18T20:06:05.102444Z",
    "lead_time": 2.714
  },
  {
    "text": "First , not all tasks can be easily represented by a Transformer encoder architecture , and therefore require a task - specific model architecture to be added .",
    "id": 229,
    "label": [
      {
        "start": 53,
        "end": 64,
        "text": "Transformer",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 238,
    "created_at": "2022-10-18T20:06:01.259226Z",
    "updated_at": "2022-10-18T20:06:01.259252Z",
    "lead_time": 6.255
  },
  {
    "text": "However , the feature - based approach , where fixed features are extracted from the pretrained model , has certain advantages .",
    "id": 228,
    "annotator": 1,
    "annotation_id": 237,
    "created_at": "2022-10-18T20:05:53.448257Z",
    "updated_at": "2022-10-18T20:05:53.448278Z",
    "lead_time": 2.297
  },
  {
    "text": "All of the BERT results presented so far have used the fine - tuning approach , where a simple classification layer is added to the pre - trained model , and all parameters are jointly fine - tuned on a downstream task .",
    "id": 227,
    "label": [
      {
        "start": 11,
        "end": 15,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 236,
    "created_at": "2022-10-18T20:05:48.674337Z",
    "updated_at": "2022-10-18T20:05:48.674361Z",
    "lead_time": 5.993
  },
  {
    "text": "However , as expected , using only the MASK strategy was problematic when applying the featurebased approach to NER .",
    "id": 324,
    "label": [
      {
        "start": 112,
        "end": 115,
        "text": "NER",
        "labels": [
          "TaskName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 235,
    "created_at": "2022-10-18T20:05:29.681738Z",
    "updated_at": "2022-10-18T20:05:29.681763Z",
    "lead_time": 5.747
  },
  {
    "text": "Interestingly , using only the RND strategy performs much worse than our strategy as well .",
    "id": 325,
    "annotator": 1,
    "annotation_id": 234,
    "created_at": "2022-10-18T20:05:23.050369Z",
    "updated_at": "2022-10-18T20:05:23.050453Z",
    "lead_time": 0.848
  },
  {
    "text": "Appendix for \" BERT : Pre - training of Deep Bidirectional Transformers for Language Understanding \" We organize the appendix into three sections : • Additional implementation details for BERT are presented in Appendix A ; .",
    "id": 326,
    "label": [
      {
        "start": 15,
        "end": 19,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 45,
        "end": 71,
        "text": "Bidirectional Transformers",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 188,
        "end": 192,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 233,
    "created_at": "2022-10-18T20:05:09.294111Z",
    "updated_at": "2022-10-18T20:05:21.358800Z",
    "lead_time": 15.164000000000001
  },
  {
    "text": "A Additional Details for BERT .",
    "id": 327,
    "label": [
      {
        "start": 25,
        "end": 29,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 232,
    "created_at": "2022-10-18T20:05:04.082797Z",
    "updated_at": "2022-10-18T20:05:04.082821Z",
    "lead_time": 3.338
  },
  {
    "text": "A.1 Illustration of the Pre - training Tasks We provide examples of the pre - training tasks in the following .",
    "id": 328,
    "annotator": 1,
    "annotation_id": 231,
    "created_at": "2022-10-18T20:04:59.362612Z",
    "updated_at": "2022-10-18T20:04:59.362642Z",
    "lead_time": 2.272
  },
  {
    "text": "Masked LM and the Masking Procedure Assuming the unlabeled sentence is my dog is hairy , and during the random masking procedure we chose the 4 - th token ( which corresponding to hairy ) , our masking procedure can be further illustrated by • 10 % of the time : Replace the word with a random word , e.g. , my dog is hairy → my dog is apple • 10 % of the time : Keep the word unchanged , e.g. , my dog is hairy → my dog is hairy .",
    "id": 329,
    "annotator": 1,
    "annotation_id": 230,
    "created_at": "2022-10-18T20:04:55.213681Z",
    "updated_at": "2022-10-18T20:04:55.213705Z",
    "lead_time": 2.742
  },
  {
    "text": "The purpose of this is to bias the representation towards the actual observed word .",
    "id": 330,
    "annotator": 1,
    "annotation_id": 229,
    "created_at": "2022-10-18T20:04:51.420183Z",
    "updated_at": "2022-10-18T20:04:51.420204Z",
    "lead_time": 1.072
  },
  {
    "text": "Compared to standard langauge model training , the masked LM only make predictions on 15 % of tokens in each batch , which suggests that more pre - training steps may be required for the model .",
    "id": 334,
    "annotator": 1,
    "annotation_id": 227,
    "created_at": "2022-10-18T20:04:42.503841Z",
    "updated_at": "2022-10-18T20:04:42.503866Z",
    "lead_time": 2.042
  },
  {
    "text": "Additionally , because random replacement only occurs for 1.5 % of all tokens ( i.e. , 10 % of 15 % ) , this does not seem to harm the model 's language understanding capability .",
    "id": 332,
    "annotator": 1,
    "annotation_id": 226,
    "created_at": "2022-10-18T20:04:38.914840Z",
    "updated_at": "2022-10-18T20:04:38.914864Z",
    "lead_time": 2.491
  },
  {
    "text": "In Section C.2 , we evaluate the impact this procedure .",
    "id": 333,
    "annotator": 1,
    "annotation_id": 225,
    "created_at": "2022-10-18T20:04:35.350260Z",
    "updated_at": "2022-10-18T20:04:35.350291Z",
    "lead_time": 1.153
  },
  {
    "text": "Feature - based Approach with BERT .",
    "id": 226,
    "label": [
      {
        "start": 30,
        "end": 34,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 224,
    "created_at": "2022-10-18T20:04:19.156977Z",
    "updated_at": "2022-10-18T20:04:19.157004Z",
    "lead_time": 5.24
  },
  {
    "text": "Both of these prior works used a featurebased approach -we hypothesize that when the model is fine - tuned directly on the downstream tasks and uses only a very small number of randomly initialized additional parameters , the taskspecific models can benefit from the larger , more expressive pre - trained representations even when downstream task data is very small .",
    "id": 225,
    "annotator": 1,
    "annotation_id": 223,
    "created_at": "2022-10-18T20:04:12.181815Z",
    "updated_at": "2022-10-18T20:04:12.181839Z",
    "lead_time": 3.431
  },
  {
    "text": "( 2016 ) mentioned in passing that increasing hidden dimension size from 200 to 600 helped , but increasing further to 1,000 did not bring further improvements .",
    "id": 224,
    "label": [
      {
        "start": 46,
        "end": 67,
        "text": "hidden dimension size",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 73,
        "end": 76,
        "text": "200",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 80,
        "end": 83,
        "text": "600",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 119,
        "end": 124,
        "text": "1,000",
        "labels": [
          "HyperparameterValue"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 222,
    "created_at": "2022-10-18T20:03:42.688218Z",
    "updated_at": "2022-10-18T20:04:07.625716Z",
    "lead_time": 26.767
  },
  {
    "text": "( 2018b ) presented mixed results on the downstream task impact of increasing the pre - trained bi - LM size from two to four layers and Melamud et al .",
    "id": 223,
    "annotator": 1,
    "annotation_id": 221,
    "created_at": "2022-10-18T20:03:39.267396Z",
    "updated_at": "2022-10-18T20:03:39.267427Z",
    "lead_time": 4.492
  },
  {
    "text": "Peters et al .",
    "id": 222,
    "annotator": 1,
    "annotation_id": 220,
    "created_at": "2022-10-18T20:03:33.961146Z",
    "updated_at": "2022-10-18T20:03:33.961169Z",
    "lead_time": 0.254
  },
  {
    "text": "However , we believe that this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks , provided that the model has been sufficiently pre - trained .",
    "id": 221,
    "annotator": 1,
    "annotation_id": 219,
    "created_at": "2022-10-18T20:03:32.718911Z",
    "updated_at": "2022-10-18T20:03:32.718934Z",
    "lead_time": 6.572
  },
  {
    "text": "It has long been known that increasing the model size will lead to continual improvements on large - scale tasks such as machine translation and language modeling , which is demonstrated by the LM perplexity of held - out training data shown in Table 6 .",
    "id": 220,
    "label": [
      {
        "start": 121,
        "end": 140,
        "text": "machine translation",
        "labels": [
          "TaskName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 218,
    "created_at": "2022-10-18T20:03:24.201384Z",
    "updated_at": "2022-10-18T20:03:24.201407Z",
    "lead_time": 11.747
  },
  {
    "text": "By contrast , BERT BASE contains 110 M parameters and BERT LARGE contains 340 M parameters .",
    "id": 219,
    "label": [
      {
        "start": 14,
        "end": 23,
        "text": "BERT BASE",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 54,
        "end": 64,
        "text": "BERT LARGE",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 217,
    "created_at": "2022-10-18T20:03:11.065768Z",
    "updated_at": "2022-10-18T20:03:11.065791Z",
    "lead_time": 12.989
  },
  {
    "text": "( 2017 ) is ( L=6 , H=1024 , A=16 ) with 100 M parameters for the encoder , and the largest Transformer we have found in the literature is ( L=64 , H=512 , A=2 ) with 235 M parameters ( Al - Rfou et al . , 2018 ) .",
    "id": 218,
    "label": [
      {
        "start": 14,
        "end": 15,
        "text": "L",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 20,
        "end": 21,
        "text": "H",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 29,
        "end": 30,
        "text": "A",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 141,
        "end": 142,
        "text": "L",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 148,
        "end": 149,
        "text": "H",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 156,
        "end": 157,
        "text": "A",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 16,
        "end": 17,
        "text": "6",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 22,
        "end": 26,
        "text": "1024",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 31,
        "end": 33,
        "text": "16",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 143,
        "end": 145,
        "text": "64",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 150,
        "end": 153,
        "text": "512",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 158,
        "end": 159,
        "text": "2",
        "labels": [
          "HyperparameterValue"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 216,
    "created_at": "2022-10-18T20:02:56.378764Z",
    "updated_at": "2022-10-18T20:02:56.378859Z",
    "lead_time": 47.252
  },
  {
    "text": "For example , the largest Transformer explored in Vaswani et al .",
    "id": 217,
    "label": [
      {
        "start": 26,
        "end": 37,
        "text": "Transformer",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 215,
    "created_at": "2022-10-18T20:02:04.363142Z",
    "updated_at": "2022-10-18T20:02:08.026865Z",
    "lead_time": 10.029
  },
  {
    "text": "It is also perhaps surprising that we are able to achieve such significant improvements on top of models which are already quite large relative to the existing literature .",
    "id": 216,
    "annotator": 1,
    "annotation_id": 214,
    "created_at": "2022-10-18T20:01:56.648162Z",
    "updated_at": "2022-10-18T20:01:56.648188Z",
    "lead_time": 1.095
  },
  {
    "text": "We can see that larger models lead to a strict accuracy improvement across all four datasets , even for MRPC which only has 3,600 labeled training examples , and is substantially different from the pre - training tasks .",
    "id": 215,
    "label": [
      {
        "start": 104,
        "end": 108,
        "text": "MRPC",
        "labels": [
          "DatasetName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 213,
    "created_at": "2022-10-18T20:01:53.523652Z",
    "updated_at": "2022-10-18T20:01:53.523676Z",
    "lead_time": 5.697
  },
  {
    "text": "In this table , we report the average Dev Set accuracy from 5 random restarts of fine - tuning .",
    "id": 214,
    "label": [
      {
        "start": 46,
        "end": 54,
        "text": "accuracy",
        "labels": [
          "MetricName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 212,
    "created_at": "2022-10-18T20:01:46.909021Z",
    "updated_at": "2022-10-18T20:01:46.909041Z",
    "lead_time": 12.398
  },
  {
    "text": "Results on selected GLUE tasks are shown in Table 6 .",
    "id": 213,
    "label": [
      {
        "start": 20,
        "end": 24,
        "text": "GLUE",
        "labels": [
          "DatasetName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 211,
    "created_at": "2022-10-18T20:01:31.718577Z",
    "updated_at": "2022-10-18T20:01:31.718602Z",
    "lead_time": 5.188
  },
  {
    "text": "In this section , we explore the effect of model size on fine - tuning task accuracy .",
    "id": 211,
    "annotator": 1,
    "annotation_id": 210,
    "created_at": "2022-10-18T20:01:15.269275Z",
    "updated_at": "2022-10-18T20:01:15.269297Z",
    "lead_time": 5.693
  },
  {
    "text": "Effect of Model Size .",
    "id": 210,
    "annotator": 1,
    "annotation_id": 209,
    "created_at": "2022-10-18T20:01:08.675327Z",
    "updated_at": "2022-10-18T20:01:08.675361Z",
    "lead_time": 0.787
  },
  {
    "text": "We recognize that it would also be possible to train separate LTR and RTL models and represent each token as the concatenation of the two models , as ELMo does .",
    "id": 208,
    "label": [
      {
        "start": 150,
        "end": 154,
        "text": "ELMo",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 208,
    "created_at": "2022-10-18T20:01:05.376484Z",
    "updated_at": "2022-10-18T20:01:05.376508Z",
    "lead_time": 4.697
  },
  {
    "text": "However : ( a ) this is twice as expensive as a single bidirectional model ; ( b ) this is non - intuitive for tasks like QA , since the RTL model would not be able to condition the answer on the question ; ( c ) this it is strictly less powerful than a deep bidirectional model , since it can use both left and right context at every layer .",
    "id": 209,
    "label": [
      {
        "start": 122,
        "end": 124,
        "text": "QA",
        "labels": [
          "TaskName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 207,
    "created_at": "2022-10-18T20:00:59.368363Z",
    "updated_at": "2022-10-18T20:00:59.368384Z",
    "lead_time": 10.42
  },
  {
    "text": "The BiLSTM hurts performance on the GLUE tasks .",
    "id": 207,
    "label": [
      {
        "start": 4,
        "end": 10,
        "text": "BiLSTM",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 36,
        "end": 40,
        "text": "GLUE",
        "labels": [
          "DatasetName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 206,
    "created_at": "2022-10-18T20:00:22.510709Z",
    "updated_at": "2022-10-18T20:00:22.510728Z",
    "lead_time": 8.655
  },
  {
    "text": "This does significantly improve results on SQuAD , but the results are still far worse than those of the pretrained bidirectional models .",
    "id": 206,
    "label": [
      {
        "start": 43,
        "end": 48,
        "text": "SQuAD",
        "labels": [
          "DatasetName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 205,
    "created_at": "2022-10-18T20:00:12.660127Z",
    "updated_at": "2022-10-18T20:00:12.660150Z",
    "lead_time": 5.026
  },
  {
    "text": "In order to make a good faith attempt at strengthening the LTR system , we added a randomly initialized BiLSTM on top .",
    "id": 205,
    "label": [
      {
        "start": 104,
        "end": 110,
        "text": "BiLSTM",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 204,
    "created_at": "2022-10-18T20:00:01.427450Z",
    "updated_at": "2022-10-18T20:00:06.074387Z",
    "lead_time": 10.487
  },
  {
    "text": "For SQuAD it is intuitively clear that a LTR model will perform poorly at token predictions , since the token - level hidden states have no rightside context .",
    "id": 204,
    "label": [
      {
        "start": 4,
        "end": 9,
        "text": "SQuAD",
        "labels": [
          "DatasetName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 203,
    "created_at": "2022-10-18T19:59:54.170227Z",
    "updated_at": "2022-10-18T19:59:54.170256Z",
    "lead_time": 6.324
  },
  {
    "text": "The LTR model performs worse than the MLM model on all tasks , with large drops on MRPC and SQuAD .",
    "id": 203,
    "label": [
      {
        "start": 83,
        "end": 87,
        "text": "MRPC",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 92,
        "end": 97,
        "text": "SQuAD",
        "labels": [
          "DatasetName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 202,
    "created_at": "2022-10-18T19:59:46.470069Z",
    "updated_at": "2022-10-18T19:59:46.470093Z",
    "lead_time": 17.134
  },
  {
    "text": "In Table 5 , we show that removing NSP hurts performance significantly on QNLI , MNLI , and SQuAD 1.1 . Next , we evaluate the impact of training bidirectional representations by comparing \" No NSP \" to \" LTR & No NSP \" .",
    "id": 202,
    "label": [
      {
        "start": 35,
        "end": 38,
        "text": "NSP",
        "labels": [
          "TaskName"
        ]
      },
      {
        "start": 74,
        "end": 78,
        "text": "QNLI",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 81,
        "end": 85,
        "text": "MNLI",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 92,
        "end": 101,
        "text": "SQuAD 1.1",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 194,
        "end": 197,
        "text": "NSP",
        "labels": [
          "TaskName"
        ]
      },
      {
        "start": 214,
        "end": 217,
        "text": "NSP",
        "labels": [
          "TaskName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 201,
    "created_at": "2022-10-18T19:59:27.654335Z",
    "updated_at": "2022-10-18T19:59:27.654358Z",
    "lead_time": 25.539
  },
  {
    "text": "We first examine the impact brought by the NSP task .",
    "id": 201,
    "label": [
      {
        "start": 43,
        "end": 46,
        "text": "NSP",
        "labels": [
          "TaskName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 200,
    "created_at": "2022-10-18T19:59:00.877772Z",
    "updated_at": "2022-10-18T19:59:00.877797Z",
    "lead_time": 6.426
  },
  {
    "text": "This is directly comparable to OpenAI GPT , but using our larger training dataset , our input representation , and our fine - tuning scheme .",
    "id": 200,
    "label": [
      {
        "start": 31,
        "end": 41,
        "text": "OpenAI GPT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 199,
    "created_at": "2022-10-18T19:58:52.608774Z",
    "updated_at": "2022-10-18T19:58:52.608801Z",
    "lead_time": 3.413
  },
  {
    "text": "Additionally , this model was pre - trained without the NSP task .",
    "id": 199,
    "label": [
      {
        "start": 56,
        "end": 59,
        "text": "NSP",
        "labels": [
          "TaskName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 198,
    "created_at": "2022-10-18T19:58:47.972274Z",
    "updated_at": "2022-10-18T19:58:47.972303Z",
    "lead_time": 6.689
  },
  {
    "text": "The left - only constraint was also applied at fine - tuning , because removing it introduced a pre - train / fine - tune mismatch that degraded downstream performance .",
    "id": 198,
    "annotator": 1,
    "annotation_id": 197,
    "created_at": "2022-10-18T19:58:40.327889Z",
    "updated_at": "2022-10-18T19:58:40.327911Z",
    "lead_time": 3.137
  },
  {
    "text": "A left - context - only model which is trained using a standard Left - to - Right ( LTR ) LM , rather than an MLM .",
    "id": 197,
    "annotator": 1,
    "annotation_id": 196,
    "created_at": "2022-10-18T19:58:35.989708Z",
    "updated_at": "2022-10-18T19:58:35.989734Z",
    "lead_time": 3.96
  },
  {
    "text": "LTR & No NSP : .",
    "id": 196,
    "label": [
      {
        "start": 9,
        "end": 12,
        "text": "NSP",
        "labels": [
          "TaskName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 195,
    "created_at": "2022-10-18T19:58:16.214945Z",
    "updated_at": "2022-10-18T19:58:29.391928Z",
    "lead_time": 13.552
  },
  {
    "text": "We demonstrate the importance of the deep bidirectionality of BERT by evaluating two pretraining objectives using exactly the same pretraining data , fine - tuning scheme , and hyperparameters as BERT BASE : No NSP : A bidirectional model which is trained using the \" masked LM \" ( MLM ) but without the \" next sentence prediction \" ( NSP ) task .",
    "id": 195,
    "label": [
      {
        "start": 62,
        "end": 66,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 196,
        "end": 205,
        "text": "BERT BASE",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 306,
        "end": 330,
        "text": "next sentence prediction",
        "labels": [
          "TaskName"
        ]
      },
      {
        "start": 335,
        "end": 338,
        "text": "NSP",
        "labels": [
          "TaskName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 194,
    "created_at": "2022-10-18T19:57:56.419473Z",
    "updated_at": "2022-10-18T19:58:10.890244Z",
    "lead_time": 18.785
  },
  {
    "text": "Effect of Pre - training Tasks .",
    "id": 194,
    "annotator": 1,
    "annotation_id": 193,
    "created_at": "2022-10-18T19:57:50.913982Z",
    "updated_at": "2022-10-18T19:57:50.914009Z",
    "lead_time": 0.186
  },
  {
    "text": "Additional ablation studies can be found in Appendix C.",
    "id": 193,
    "annotator": 1,
    "annotation_id": 192,
    "created_at": "2022-10-18T19:57:49.911249Z",
    "updated_at": "2022-10-18T19:57:49.911277Z",
    "lead_time": 0.254
  },
  {
    "text": "In this section , we perform ablation experiments over a number of facets of BERT in order to better understand their relative importance .",
    "id": 192,
    "label": [
      {
        "start": 77,
        "end": 81,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 191,
    "created_at": "2022-10-18T19:57:48.237155Z",
    "updated_at": "2022-10-18T19:57:48.237178Z",
    "lead_time": 9.936
  },
  {
    "text": "Ablation Studies .",
    "id": 191,
    "annotator": 1,
    "annotation_id": 190,
    "created_at": "2022-10-18T19:57:37.440700Z",
    "updated_at": "2022-10-18T19:57:37.440726Z",
    "lead_time": 0.426
  },
  {
    "text": "BERT LARGE outperforms the authors ' baseline ESIM+ELMo system by +27.1 % and OpenAI GPT by 8.3 % .",
    "id": 190,
    "label": [
      {
        "start": 0,
        "end": 10,
        "text": "BERT LARGE",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 51,
        "end": 55,
        "text": "ELMo",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 46,
        "end": 50,
        "text": "ESIM",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 78,
        "end": 88,
        "text": "OpenAI GPT",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 66,
        "end": 73,
        "text": "+27.1 %",
        "labels": [
          "MetricValue"
        ]
      },
      {
        "start": 92,
        "end": 97,
        "text": "8.3 %",
        "labels": [
          "MetricValue"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 189,
    "created_at": "2022-10-18T19:57:21.334492Z",
    "updated_at": "2022-10-18T19:57:34.824803Z",
    "lead_time": 32.147999999999996
  },
  {
    "text": "Results are presented in Table 4 .",
    "id": 189,
    "annotator": 1,
    "annotation_id": 188,
    "created_at": "2022-10-18T19:57:01.524539Z",
    "updated_at": "2022-10-18T19:57:01.524560Z",
    "lead_time": 0.347
  },
  {
    "text": "We fine - tune the model for 3 epochs with a learning rate of 2e-5 and a batch size of 16 .",
    "id": 188,
    "label": [
      {
        "start": 45,
        "end": 58,
        "text": "learning rate",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 73,
        "end": 83,
        "text": "batch size",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 62,
        "end": 66,
        "text": "2e-5",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 87,
        "end": 89,
        "text": "16",
        "labels": [
          "HyperparameterValue"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 187,
    "created_at": "2022-10-18T19:57:00.188393Z",
    "updated_at": "2022-10-18T19:57:00.188417Z",
    "lead_time": 18.481
  },
  {
    "text": "The only task - specific parameters introduced is a vector whose dot product with the [ CLS ] token representation C denotes a score for each choice which is normalized with a softmax layer .",
    "id": 187,
    "annotator": 1,
    "annotation_id": 186,
    "created_at": "2022-10-18T19:56:39.816146Z",
    "updated_at": "2022-10-18T19:56:39.816174Z",
    "lead_time": 3.152
  },
  {
    "text": "When fine - tuning on the SWAG dataset , we construct four input sequences , each containing the concatenation of the given sentence ( sentence A ) and a possible continuation ( sentence B ) .",
    "id": 186,
    "label": [
      {
        "start": 26,
        "end": 30,
        "text": "SWAG",
        "labels": [
          "DatasetName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 185,
    "created_at": "2022-10-18T19:56:35.465633Z",
    "updated_at": "2022-10-18T19:56:35.465677Z",
    "lead_time": 10.346
  },
  {
    "text": "Given a sentence , the task is to choose the most plausible continuation among four choices .",
    "id": 185,
    "annotator": 1,
    "annotation_id": 184,
    "created_at": "2022-10-18T19:56:23.871078Z",
    "updated_at": "2022-10-18T19:56:23.871108Z",
    "lead_time": 1.686
  },
  {
    "text": "The Situations With Adversarial Generations ( SWAG ) dataset contains 113k sentence - pair completion examples that evaluate grounded commonsense inference ( Zellers et al . , 2018 ) .",
    "id": 184,
    "label": [
      {
        "start": 4,
        "end": 43,
        "text": "Situations With Adversarial Generations",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 46,
        "end": 50,
        "text": "SWAG",
        "labels": [
          "DatasetName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 183,
    "created_at": "2022-10-18T19:56:21.089612Z",
    "updated_at": "2022-10-18T19:56:21.089638Z",
    "lead_time": 12.787
  },
  {
    "text": "SWAG .",
    "id": 183,
    "label": [
      {
        "start": 0,
        "end": 4,
        "text": "SWAG",
        "labels": [
          "DatasetName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 182,
    "created_at": "2022-10-18T19:56:05.349664Z",
    "updated_at": "2022-10-18T19:56:05.349694Z",
    "lead_time": 37.762
  },
  {
    "text": "We observe a +5.1 F1 improvement over the previous best system .",
    "id": 182,
    "label": [
      {
        "start": 18,
        "end": 20,
        "text": "F1",
        "labels": [
          "MetricName"
        ]
      },
      {
        "start": 13,
        "end": 17,
        "text": "+5.1",
        "labels": [
          "MetricValue"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 181,
    "created_at": "2022-10-18T19:55:19.902634Z",
    "updated_at": "2022-10-18T19:55:25.051795Z",
    "lead_time": 14.73
  },
  {
    "text": "The results compared to prior leaderboard entries and top published work ( Sun et al . , 2018;Wang et al . , 2018b ) are shown in Table 3 , excluding systems that use BERT as one of their components .",
    "id": 181,
    "label": [
      {
        "start": 167,
        "end": 171,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 180,
    "created_at": "2022-10-18T19:55:07.679637Z",
    "updated_at": "2022-10-18T19:55:07.679661Z",
    "lead_time": 5.195
  },
  {
    "text": "System .",
    "id": 180,
    "annotator": 1,
    "annotation_id": 179,
    "created_at": "2022-10-18T19:55:01.631889Z",
    "updated_at": "2022-10-18T19:55:01.631916Z",
    "lead_time": 0.203
  },
  {
    "text": "We fine - tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48 .",
    "id": 179,
    "label": [
      {
        "start": 36,
        "end": 49,
        "text": "learning rate",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 53,
        "end": 57,
        "text": "5e-5",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 78,
        "end": 80,
        "text": "48",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 64,
        "end": 74,
        "text": "batch size",
        "labels": [
          "HyperparameterName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 178,
    "created_at": "2022-10-18T19:55:00.120772Z",
    "updated_at": "2022-10-18T19:55:00.120797Z",
    "lead_time": 28.157
  },
  {
    "text": "We did not use TriviaQA data for this model .",
    "id": 178,
    "label": [
      {
        "start": 15,
        "end": 23,
        "text": "TriviaQA",
        "labels": [
          "DatasetName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 177,
    "created_at": "2022-10-18T19:54:30.803846Z",
    "updated_at": "2022-10-18T19:54:30.803872Z",
    "lead_time": 5.94
  },
  {
    "text": "We predict a non - null answer when ŝ i , j > s null + τ , where the threshold τ is selected on the dev set to maximize F1 .",
    "id": 177,
    "label": [
      {
        "start": 120,
        "end": 122,
        "text": "F1",
        "labels": [
          "MetricName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 176,
    "created_at": "2022-10-18T19:54:17.144265Z",
    "updated_at": "2022-10-18T19:54:21.614512Z",
    "lead_time": 5.636
  },
  {
    "text": "ŝ i , j = max j≥i S•T i + E•T j .",
    "id": 176,
    "annotator": 1,
    "annotation_id": 175,
    "created_at": "2022-10-18T19:54:14.946140Z",
    "updated_at": "2022-10-18T19:54:14.946164Z",
    "lead_time": 0.437
  },
  {
    "text": "For prediction , we compare the score of the no - answer span : s null = S•C + E•C to the score of the best non - null span 12 The TriviaQA data we used consists of paragraphs from TriviaQA - Wiki formed of the first 400 tokens in documents , that contain at least one of the provided possible answers .",
    "id": 175,
    "label": [
      {
        "start": 181,
        "end": 196,
        "text": "TriviaQA - Wiki",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 131,
        "end": 139,
        "text": "TriviaQA",
        "labels": [
          "DatasetName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 174,
    "created_at": "2022-10-18T19:53:57.241659Z",
    "updated_at": "2022-10-18T19:54:07.054857Z",
    "lead_time": 16.637999999999998
  },
  {
    "text": "The probability space for the start and end answer span positions is extended to include the position of the [ CLS ] token .",
    "id": 174,
    "annotator": 1,
    "annotation_id": 173,
    "created_at": "2022-10-18T19:53:48.243809Z",
    "updated_at": "2022-10-18T19:53:48.243833Z",
    "lead_time": 1.48
  },
  {
    "text": "We treat questions that do not have an answer as having an answer span with start and end at the [ CLS ] token .",
    "id": 173,
    "annotator": 1,
    "annotation_id": 172,
    "created_at": "2022-10-18T19:53:45.562634Z",
    "updated_at": "2022-10-18T19:53:45.562660Z",
    "lead_time": 2.106
  },
  {
    "text": "We use a simple approach to extend the SQuAD v1.1 BERT model for this task .",
    "id": 172,
    "label": [
      {
        "start": 50,
        "end": 54,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 39,
        "end": 49,
        "text": "SQuAD v1.1",
        "labels": [
          "DatasetName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 171,
    "created_at": "2022-10-18T19:53:41.046489Z",
    "updated_at": "2022-10-18T19:53:41.046514Z",
    "lead_time": 13.58
  },
  {
    "text": "12 4.3 SQuAD v2.0 The SQuAD 2.0 task extends the SQuAD 1.1 problem definition by allowing for the possibility that no short answer exists in the provided paragraph , making the problem more realistic .",
    "id": 171,
    "label": [
      {
        "start": 7,
        "end": 17,
        "text": "SQuAD v2.0",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 22,
        "end": 31,
        "text": "SQuAD 2.0",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 49,
        "end": 58,
        "text": "SQuAD 1.1",
        "labels": [
          "DatasetName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 170,
    "created_at": "2022-10-18T19:53:24.820601Z",
    "updated_at": "2022-10-18T19:53:24.820621Z",
    "lead_time": 32.835
  },
  {
    "text": "tuning data , we only lose 0.1 - 0.4 F1 , still outperforming all existing systems by a wide margin .",
    "id": 170,
    "label": [
      {
        "start": 37,
        "end": 39,
        "text": "F1",
        "labels": [
          "MetricName"
        ]
      },
      {
        "start": 27,
        "end": 30,
        "text": "0.1",
        "labels": [
          "MetricValue"
        ]
      },
      {
        "start": 33,
        "end": 36,
        "text": "0.4",
        "labels": [
          "MetricValue"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 169,
    "created_at": "2022-10-18T19:52:50.077620Z",
    "updated_at": "2022-10-18T19:52:50.077664Z",
    "lead_time": 18.085
  },
  {
    "text": "In fact , our single BERT model outperforms the top ensemble system in terms of F1 score .",
    "id": 169,
    "label": [
      {
        "start": 21,
        "end": 25,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 80,
        "end": 82,
        "text": "F1",
        "labels": [
          "MetricName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 168,
    "created_at": "2022-10-18T19:52:30.579454Z",
    "updated_at": "2022-10-18T19:52:30.579485Z",
    "lead_time": 9.751
  },
  {
    "text": "Our best performing system outperforms the top leaderboard system by +1.5 F1 in ensembling and +1.3 F1 as a single system .",
    "id": 168,
    "label": [
      {
        "start": 74,
        "end": 76,
        "text": "F1",
        "labels": [
          "MetricName"
        ]
      },
      {
        "start": 100,
        "end": 102,
        "text": "F1",
        "labels": [
          "MetricName"
        ]
      },
      {
        "start": 69,
        "end": 73,
        "text": "+1.5",
        "labels": [
          "MetricValue"
        ]
      },
      {
        "start": 95,
        "end": 99,
        "text": "+1.3",
        "labels": [
          "MetricValue"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 167,
    "created_at": "2022-10-18T19:52:19.476815Z",
    "updated_at": "2022-10-18T19:52:19.476851Z",
    "lead_time": 20.256
  },
  {
    "text": "We therefore use modest data augmentation in our system by first fine - tuning on TriviaQA ( Joshi et al . , 2017 ) befor fine - tuning on SQuAD .",
    "id": 167,
    "label": [
      {
        "start": 82,
        "end": 90,
        "text": "TriviaQA",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 139,
        "end": 144,
        "text": "SQuAD",
        "labels": [
          "DatasetName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 166,
    "created_at": "2022-10-18T19:51:57.910144Z",
    "updated_at": "2022-10-18T19:51:57.910166Z",
    "lead_time": 13.067
  },
  {
    "text": "The top results from the SQuAD leaderboard do not have up - to - date public system descriptions available , 11 and are allowed to use any public data when training their systems .",
    "id": 166,
    "label": [
      {
        "start": 25,
        "end": 30,
        "text": "SQuAD",
        "labels": [
          "DatasetName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 165,
    "created_at": "2022-10-18T19:51:42.959499Z",
    "updated_at": "2022-10-18T19:51:42.959521Z",
    "lead_time": 5.896
  },
  {
    "text": "Table 2 shows top leaderboard entries as well as results from top published systems ( Seo et al . , 2017;Clark and Gardner , 2018;Peters et al . , 2018a;Hu et al . , 2018 ) .",
    "id": 165,
    "annotator": 1,
    "annotation_id": 164,
    "created_at": "2022-10-18T19:51:34.424058Z",
    "updated_at": "2022-10-18T19:51:34.424085Z",
    "lead_time": 2.856
  },
  {
    "text": "We fine - tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32 .",
    "id": 164,
    "label": [
      {
        "start": 35,
        "end": 48,
        "text": "learning rate",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 52,
        "end": 56,
        "text": "5e-5",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 63,
        "end": 73,
        "text": "batch size",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 77,
        "end": 79,
        "text": "32",
        "labels": [
          "HyperparameterValue"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 163,
    "created_at": "2022-10-18T19:51:12.808868Z",
    "updated_at": "2022-10-18T19:51:28.911547Z",
    "lead_time": 16.337
  },
  {
    "text": "The training objective is the sum of the log - likelihoods of the correct start and end positions .",
    "id": 163,
    "annotator": 1,
    "annotation_id": 162,
    "created_at": "2022-10-18T19:51:11.272092Z",
    "updated_at": "2022-10-18T19:51:11.272121Z",
    "lead_time": 1.449
  },
  {
    "text": "The score of a candidate span from position i to position j is defined as S•T i + E•T j , and the maximum scoring span where j ≥ i is used as a prediction .",
    "id": 162,
    "annotator": 1,
    "annotation_id": 161,
    "created_at": "2022-10-18T19:51:07.918650Z",
    "updated_at": "2022-10-18T19:51:07.918677Z",
    "lead_time": 1.078
  },
  {
    "text": "The analogous formula is used for the end of the answer span .",
    "id": 161,
    "annotator": 1,
    "annotation_id": 160,
    "created_at": "2022-10-18T19:51:05.598675Z",
    "updated_at": "2022-10-18T19:51:05.598697Z",
    "lead_time": 1.733
  },
  {
    "text": "The probability of word i being the start of the answer span is computed as a dot product between T i and S followed by a softmax over all of the words in the paragraph : P i = e S•T i j e S•T j .",
    "id": 160,
    "annotator": 1,
    "annotation_id": 159,
    "created_at": "2022-10-18T19:50:42.286892Z",
    "updated_at": "2022-10-18T19:50:42.286917Z",
    "lead_time": 4.962
  },
  {
    "text": "We only introduce a start vector S ∈ R H and an end vector E ∈ R H during fine - tuning .",
    "id": 159,
    "annotator": 1,
    "annotation_id": 158,
    "created_at": "2022-10-18T19:50:36.001440Z",
    "updated_at": "2022-10-18T19:50:36.001463Z",
    "lead_time": 4.416
  },
  {
    "text": "As shown in Figure 1 , in the question answering task , we represent the input question and passage as a single packed sequence , with the question using the A embedding and the passage using the B embedding .",
    "id": 158,
    "label": [
      {
        "start": 30,
        "end": 48,
        "text": "question answering",
        "labels": [
          "TaskName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 157,
    "created_at": "2022-10-18T19:50:29.298252Z",
    "updated_at": "2022-10-18T19:50:29.298278Z",
    "lead_time": 10.614
  },
  {
    "text": "10 https://gluebenchmark.com/leaderboard Wikipedia containing the answer , the task is to predict the answer text span in the passage .",
    "id": 157,
    "annotator": 1,
    "annotation_id": 156,
    "created_at": "2022-10-18T19:50:17.239049Z",
    "updated_at": "2022-10-18T19:50:17.239072Z",
    "lead_time": 4.984
  },
  {
    "text": "Given a question and a passage from 9 The GLUE data set distribution does not include the Test labels , and we only made a single GLUE evaluation server submission for each of BERTBASE and BERTLARGE .",
    "id": 156,
    "label": [
      {
        "start": 42,
        "end": 46,
        "text": "GLUE",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 130,
        "end": 134,
        "text": "GLUE",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 176,
        "end": 184,
        "text": "BERTBASE",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 189,
        "end": 198,
        "text": "BERTLARGE",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 155,
    "created_at": "2022-10-18T19:50:09.990262Z",
    "updated_at": "2022-10-18T19:50:09.990286Z",
    "lead_time": 22.77
  },
  {
    "text": "The effect of model size is explored more thoroughly in Section 5.2 . SQuAD v1.1 . The Stanford Question Answering Dataset ( SQuAD v1.1 ) is a collection of 100k crowdsourced question / answer pairs ( Rajpurkar et al . , 2016 ) .",
    "id": 155,
    "label": [
      {
        "start": 70,
        "end": 80,
        "text": "SQuAD v1.1",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 87,
        "end": 122,
        "text": "Stanford Question Answering Dataset",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 125,
        "end": 135,
        "text": "SQuAD v1.1",
        "labels": [
          "DatasetName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 154,
    "created_at": "2022-10-18T19:49:45.714404Z",
    "updated_at": "2022-10-18T19:49:45.714433Z",
    "lead_time": 23.09
  },
  {
    "text": "We find that BERT LARGE significantly outperforms BERT BASE across all tasks , especially those with very little training data .",
    "id": 154,
    "label": [
      {
        "start": 13,
        "end": 23,
        "text": "BERT LARGE",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 50,
        "end": 59,
        "text": "BERT BASE",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 153,
    "created_at": "2022-10-18T19:49:20.502566Z",
    "updated_at": "2022-10-18T19:49:20.502589Z",
    "lead_time": 16.361
  },
  {
    "text": "For the largest and most widely reported GLUE task , MNLI , BERT obtains a 4.6 % absolute accuracy improvement .",
    "id": 152,
    "label": [
      {
        "start": 41,
        "end": 45,
        "text": "GLUE",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 53,
        "end": 57,
        "text": "MNLI",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 60,
        "end": 64,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 75,
        "end": 80,
        "text": "4.6 %",
        "labels": [
          "MetricValue"
        ]
      },
      {
        "start": 90,
        "end": 98,
        "text": "accuracy",
        "labels": [
          "MetricName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 152,
    "created_at": "2022-10-18T19:49:02.859685Z",
    "updated_at": "2022-10-18T19:49:02.859708Z",
    "lead_time": 11.637
  },
  {
    "text": "On the official GLUE leaderboard 10 , BERT LARGE obtains a score of 80.5 , compared to OpenAI GPT , which obtains 72.8 as of the date of writing .",
    "id": 153,
    "label": [
      {
        "start": 16,
        "end": 20,
        "text": "GLUE",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 38,
        "end": 48,
        "text": "BERT LARGE",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 87,
        "end": 97,
        "text": "OpenAI GPT",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 59,
        "end": 64,
        "text": "score",
        "labels": [
          "MetricName"
        ]
      },
      {
        "start": 68,
        "end": 72,
        "text": "80.5",
        "labels": [
          "MetricValue"
        ]
      },
      {
        "start": 114,
        "end": 118,
        "text": "72.8",
        "labels": [
          "MetricValue"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 151,
    "created_at": "2022-10-18T19:48:50.005170Z",
    "updated_at": "2022-10-18T19:48:50.005196Z",
    "lead_time": 99.119
  },
  {
    "text": "Note that BERT BASE and OpenAI GPT are nearly identical in terms of model architecture apart from the attention masking .",
    "id": 151,
    "label": [
      {
        "start": 10,
        "end": 19,
        "text": "BERT BASE",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 24,
        "end": 34,
        "text": "OpenAI GPT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 150,
    "created_at": "2022-10-17T22:43:18.998285Z",
    "updated_at": "2022-10-17T22:43:18.998314Z",
    "lead_time": 10.95
  },
  {
    "text": "Both BERT BASE and BERT LARGE outperform all systems on all tasks by a substantial margin , obtaining 4.5 % and 7.0 % respective average accuracy improvement over the prior state of the art .",
    "id": 150,
    "label": [
      {
        "start": 5,
        "end": 14,
        "text": "BERT BASE",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 19,
        "end": 29,
        "text": "BERT LARGE",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 137,
        "end": 145,
        "text": "accuracy",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 102,
        "end": 107,
        "text": "4.5 %",
        "labels": [
          "MetricValue"
        ]
      },
      {
        "start": 112,
        "end": 117,
        "text": "7.0 %",
        "labels": [
          "MetricValue"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 149,
    "created_at": "2022-10-17T22:43:04.983051Z",
    "updated_at": "2022-10-17T22:43:04.983074Z",
    "lead_time": 26.522
  },
  {
    "text": "The General Language Understanding Evaluation ( GLUE ) benchmark ( Wang et al . , 2018a ) is a collection of diverse natural language understanding tasks .",
    "id": 141,
    "label": [
      {
        "start": 4,
        "end": 45,
        "text": "General Language Understanding Evaluation",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 48,
        "end": 52,
        "text": "GLUE",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 117,
        "end": 147,
        "text": "natural language understanding",
        "labels": [
          "TaskName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 140,
    "created_at": "2022-10-17T22:29:34.871752Z",
    "updated_at": "2022-10-17T22:42:34.417632Z",
    "lead_time": 618.824
  },
  {
    "text": "9 Results are presented in Table 1 .",
    "id": 149,
    "annotator": 1,
    "annotation_id": 148,
    "created_at": "2022-10-17T22:31:08.868537Z",
    "updated_at": "2022-10-17T22:31:08.868559Z",
    "lead_time": 0.519
  },
  {
    "text": "With random restarts , we use the same pre - trained checkpoint but perform different fine - tuning data shuffling and classifier layer initialization .",
    "id": 148,
    "annotator": 1,
    "annotation_id": 147,
    "created_at": "2022-10-17T22:31:05.729676Z",
    "updated_at": "2022-10-17T22:31:05.729698Z",
    "lead_time": 1.001
  },
  {
    "text": "Additionally , for BERT LARGE we found that finetuning was sometimes unstable on small datasets , so we ran several random restarts and selected the best model on the Dev set .",
    "id": 147,
    "label": [
      {
        "start": 19,
        "end": 29,
        "text": "BERT LARGE",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 146,
    "created_at": "2022-10-17T22:31:03.605599Z",
    "updated_at": "2022-10-17T22:31:03.605622Z",
    "lead_time": 8.645
  },
  {
    "text": "For each task , we selected the best fine - tuning learning rate ( among 5e-5 , 4e-5 , 3e-5 , and 2e-5 ) on the Dev set .",
    "id": 146,
    "label": [
      {
        "start": 51,
        "end": 64,
        "text": "learning rate",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 73,
        "end": 77,
        "text": "5e-5",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 80,
        "end": 84,
        "text": "4e-5",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 87,
        "end": 91,
        "text": "3e-5",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 98,
        "end": 102,
        "text": "2e-5",
        "labels": [
          "HyperparameterValue"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 145,
    "created_at": "2022-10-17T22:30:53.429541Z",
    "updated_at": "2022-10-17T22:30:53.429565Z",
    "lead_time": 21.914
  },
  {
    "text": "We use a batch size of 32 and fine - tune for 3 epochs over the data for all GLUE tasks .",
    "id": 145,
    "label": [
      {
        "start": 9,
        "end": 19,
        "text": "batch size",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 23,
        "end": 25,
        "text": "32",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 77,
        "end": 81,
        "text": "GLUE",
        "labels": [
          "DatasetName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 144,
    "created_at": "2022-10-17T22:30:30.236070Z",
    "updated_at": "2022-10-17T22:30:30.236093Z",
    "lead_time": 18.409
  },
  {
    "text": "We compute a standard classification loss with C and W , i.e. , log(softmax(CW T ) ) .",
    "id": 144,
    "annotator": 1,
    "annotation_id": 143,
    "created_at": "2022-10-17T22:30:10.765051Z",
    "updated_at": "2022-10-17T22:30:10.765078Z",
    "lead_time": 0.834
  },
  {
    "text": "The only new parameters introduced during fine - tuning are classification layer weights W ∈ R K×H , where K is the number of labels .",
    "id": 143,
    "annotator": 1,
    "annotation_id": 142,
    "created_at": "2022-10-17T22:29:59.942382Z",
    "updated_at": "2022-10-17T22:29:59.942405Z",
    "lead_time": 1.115
  },
  {
    "text": "Detailed descriptions of GLUE datasets are included in Appendix B.1 . To fine - tune on GLUE , we represent the input sequence ( for single sentence or sentence pairs ) as described in Section 3 , and use the final hidden vector C ∈ R H corresponding to the first input token ( [ CLS ] ) as the aggregate representation .",
    "id": 142,
    "label": [
      {
        "start": 25,
        "end": 29,
        "text": "GLUE",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 88,
        "end": 92,
        "text": "GLUE",
        "labels": [
          "DatasetName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 141,
    "created_at": "2022-10-17T22:29:57.761186Z",
    "updated_at": "2022-10-17T22:29:57.761212Z",
    "lead_time": 21.874
  },
  {
    "text": "GLUE .",
    "id": 140,
    "label": [
      {
        "start": 0,
        "end": 4,
        "text": "GLUE",
        "labels": [
          "DatasetName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 139,
    "created_at": "2022-10-17T22:29:10.102189Z",
    "updated_at": "2022-10-17T22:29:10.102212Z",
    "lead_time": 19.023
  },
  {
    "text": "In this section , we present BERT fine - tuning results on 11 NLP tasks .",
    "id": 139,
    "label": [
      {
        "start": 29,
        "end": 33,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 138,
    "created_at": "2022-10-17T22:28:49.952405Z",
    "updated_at": "2022-10-17T22:28:49.952429Z",
    "lead_time": 5.088
  },
  {
    "text": "More details can be found in Appendix A.5 . Experiments .",
    "id": 138,
    "annotator": 1,
    "annotation_id": 137,
    "created_at": "2022-10-17T22:28:42.891285Z",
    "updated_at": "2022-10-17T22:28:42.891313Z",
    "lead_time": 1.121
  },
  {
    "text": "7 We describe the task - specific details in the corresponding subsections of Section 4 .",
    "id": 137,
    "annotator": 1,
    "annotation_id": 136,
    "created_at": "2022-10-17T22:28:40.594742Z",
    "updated_at": "2022-10-17T22:28:40.594767Z",
    "lead_time": 1.189
  },
  {
    "text": "All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU , or a few hours on a GPU , starting from the exact same pre - trained model .",
    "id": 136,
    "annotator": 1,
    "annotation_id": 135,
    "created_at": "2022-10-17T22:28:34.299952Z",
    "updated_at": "2022-10-17T22:28:34.299974Z",
    "lead_time": 6.767
  },
  {
    "text": "Compared to pre - training , fine - tuning is relatively inexpensive .",
    "id": 135,
    "annotator": 1,
    "annotation_id": 134,
    "created_at": "2022-10-17T22:28:26.444070Z",
    "updated_at": "2022-10-17T22:28:26.444094Z",
    "lead_time": 1.247
  },
  {
    "text": "At the output , the token representations are fed into an output layer for tokenlevel tasks , such as sequence tagging or question answering , and the [ CLS ] representation is fed into an output layer for classification , such as entailment or sentiment analysis .",
    "id": 134,
    "label": [
      {
        "start": 122,
        "end": 140,
        "text": "question answering",
        "labels": [
          "TaskName"
        ]
      },
      {
        "start": 231,
        "end": 241,
        "text": "entailment",
        "labels": [
          "TaskName"
        ]
      },
      {
        "start": 245,
        "end": 263,
        "text": "sentiment analysis",
        "labels": [
          "TaskName"
        ]
      },
      {
        "start": 102,
        "end": 118,
        "text": "sequence tagging",
        "labels": [
          "TaskName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 133,
    "created_at": "2022-10-17T22:28:17.972134Z",
    "updated_at": "2022-10-17T22:28:23.890138Z",
    "lead_time": 26.752
  },
  {
    "text": "For each task , we simply plug in the taskspecific inputs and outputs into BERT and finetune all the parameters end - to - end .",
    "id": 132,
    "label": [
      {
        "start": 75,
        "end": 79,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 132,
    "created_at": "2022-10-17T22:27:54.944228Z",
    "updated_at": "2022-10-17T22:27:54.944253Z",
    "lead_time": 7.255
  },
  {
    "text": "At the input , sentence A and sentence B from pre - training are analogous to ( 1 ) sentence pairs in paraphrasing , ( 2 ) hypothesis - premise pairs in entailment , ( 3 ) question - passage pairs in question answering , and ( 4 ) a degenerate text-∅ pair in text classification or sequence tagging .",
    "id": 133,
    "label": [
      {
        "start": 153,
        "end": 163,
        "text": "entailment",
        "labels": [
          "TaskName"
        ]
      },
      {
        "start": 200,
        "end": 218,
        "text": "question answering",
        "labels": [
          "TaskName"
        ]
      },
      {
        "start": 259,
        "end": 278,
        "text": "text classification",
        "labels": [
          "TaskName"
        ]
      },
      {
        "start": 282,
        "end": 298,
        "text": "sequence tagging",
        "labels": [
          "TaskName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 131,
    "created_at": "2022-10-17T22:27:33.796350Z",
    "updated_at": "2022-10-17T22:27:46.208664Z",
    "lead_time": 45.197
  },
  {
    "text": "BERT instead uses the self - attention mechanism to unify these two stages , as encoding a concatenated text pair with self - attention effectively includes bidirectional cross attention between two sentences .",
    "id": 131,
    "label": [
      {
        "start": 0,
        "end": 4,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 130,
    "created_at": "2022-10-17T22:26:50.120117Z",
    "updated_at": "2022-10-17T22:26:50.120142Z",
    "lead_time": 4.003
  },
  {
    "text": "( 2017 ) .",
    "id": 130,
    "annotator": 1,
    "annotation_id": 129,
    "created_at": "2022-10-17T22:26:45.333723Z",
    "updated_at": "2022-10-17T22:26:45.333746Z",
    "lead_time": 0.544
  },
  {
    "text": "( 2016 ) ; Seo et al .",
    "id": 129,
    "annotator": 1,
    "annotation_id": 128,
    "created_at": "2022-10-17T22:26:38.957167Z",
    "updated_at": "2022-10-17T22:26:38.957194Z",
    "lead_time": 0.212
  },
  {
    "text": "For applications involving text pairs , a common pattern is to independently encode text pairs before applying bidirectional cross attention , such as Parikh et al .",
    "id": 128,
    "annotator": 1,
    "annotation_id": 127,
    "created_at": "2022-10-17T22:26:37.865075Z",
    "updated_at": "2022-10-17T22:26:37.865101Z",
    "lead_time": 1.855
  },
  {
    "text": "Fine - tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream taskswhether they involve single text or text pairs - by swapping out the appropriate inputs and outputs .",
    "id": 127,
    "label": [
      {
        "start": 74,
        "end": 85,
        "text": "Transformer",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 93,
        "end": 97,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 126,
    "created_at": "2022-10-17T22:26:35.057241Z",
    "updated_at": "2022-10-17T22:26:35.057264Z",
    "lead_time": 15.292
  },
  {
    "text": "Fine - tuning BERT .",
    "id": 126,
    "annotator": 1,
    "annotation_id": 125,
    "created_at": "2022-10-17T22:26:17.945534Z",
    "updated_at": "2022-10-17T22:26:17.945561Z",
    "lead_time": 0.805
  },
  {
    "text": "It is critical to use a document - level corpus rather than a shuffled sentence - level corpus such as the Billion Word Benchmark ( Chelba et al . , 2013 ) in order to extract long contiguous sequences .",
    "id": 125,
    "label": [
      {
        "start": 107,
        "end": 129,
        "text": "Billion Word Benchmark",
        "labels": [
          "DatasetName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 124,
    "created_at": "2022-10-17T22:26:04.445505Z",
    "updated_at": "2022-10-17T22:26:04.445529Z",
    "lead_time": 7.814
  },
  {
    "text": "For Wikipedia we extract only the text passages and ignore lists , tables , and headers .",
    "id": 124,
    "annotator": 1,
    "annotation_id": 123,
    "created_at": "2022-10-17T22:25:55.655965Z",
    "updated_at": "2022-10-17T22:25:55.655987Z",
    "lead_time": 2.995
  },
  {
    "text": "For the pre - training corpus we use the BooksCorpus ( 800 M words ) ( Zhu et al . , 2015 ) and English Wikipedia ( 2,500 M words ) .",
    "id": 123,
    "label": [
      {
        "start": 41,
        "end": 52,
        "text": "BooksCorpus",
        "labels": [
          "DatasetName"
        ]
      },
      {
        "start": 96,
        "end": 113,
        "text": "English Wikipedia",
        "labels": [
          "DatasetName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 122,
    "created_at": "2022-10-17T22:25:26.743947Z",
    "updated_at": "2022-10-17T22:25:49.965445Z",
    "lead_time": 26.935000000000002
  },
  {
    "text": "The pre - training procedure largely follows the existing literature on language model pre - training .",
    "id": 122,
    "annotator": 1,
    "annotation_id": 121,
    "created_at": "2022-10-17T22:25:18.545496Z",
    "updated_at": "2022-10-17T22:25:18.545524Z",
    "lead_time": 2.182
  },
  {
    "text": "Pre - training data .",
    "id": 121,
    "annotator": 1,
    "annotation_id": 120,
    "created_at": "2022-10-17T22:25:15.262082Z",
    "updated_at": "2022-10-17T22:25:15.262109Z",
    "lead_time": 0.409
  },
  {
    "text": "However , in prior work , only sentence embeddings are transferred to down - stream tasks , where BERT transfers all parameters to initialize end - task model parameters .",
    "id": 120,
    "label": [
      {
        "start": 98,
        "end": 102,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 119,
    "created_at": "2022-10-17T22:25:13.856857Z",
    "updated_at": "2022-10-17T22:25:13.856881Z",
    "lead_time": 8.843
  },
  {
    "text": "( 2017 ) and Logeswaran and Lee ( 2018 ) .",
    "id": 119,
    "annotator": 1,
    "annotation_id": 118,
    "created_at": "2022-10-17T22:25:04.524943Z",
    "updated_at": "2022-10-17T22:25:04.524966Z",
    "lead_time": 1.196
  },
  {
    "text": "5 Despite its simplicity , we demonstrate in Section 5.1 that pre - training towards this task is very beneficial to both QA and NLI .",
    "id": 117,
    "label": [
      {
        "start": 122,
        "end": 124,
        "text": "QA",
        "labels": [
          "TaskName"
        ]
      },
      {
        "start": 129,
        "end": 132,
        "text": "NLI",
        "labels": [
          "TaskName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 116,
    "created_at": "2022-10-17T22:24:51.234009Z",
    "updated_at": "2022-10-17T22:24:51.234032Z",
    "lead_time": 8.436
  },
  {
    "text": "Specifically , when choosing the sentences A and B for each pretraining example , 50 % of the time B is the actual next sentence that follows A ( labeled as IsNext ) , and 50 % of the time it is a random sentence from the corpus ( labeled as NotNext ) .",
    "id": 115,
    "annotator": 1,
    "annotation_id": 114,
    "created_at": "2022-10-17T22:24:24.415930Z",
    "updated_at": "2022-10-17T22:24:24.415957Z",
    "lead_time": 4.022
  },
  {
    "text": "In order to train a model that understands sentence relationships , we pre - train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus .",
    "id": 114,
    "annotator": 1,
    "annotation_id": 113,
    "created_at": "2022-10-17T22:24:19.363329Z",
    "updated_at": "2022-10-17T22:24:19.363359Z",
    "lead_time": 5.947
  },
  {
    "text": "Then , T i will be used to predict the original token with cross entropy loss .",
    "id": 112,
    "annotator": 1,
    "annotation_id": 111,
    "created_at": "2022-10-17T22:23:30.382011Z",
    "updated_at": "2022-10-17T22:23:30.382038Z",
    "lead_time": 8.353
  },
  {
    "text": "In all of our experiments , we mask 15 % of all WordPiece tokens in each sequence at random .",
    "id": 106,
    "annotator": 1,
    "annotation_id": 110,
    "created_at": "2022-10-17T22:23:18.462904Z",
    "updated_at": "2022-10-17T22:23:18.462941Z",
    "lead_time": 1.238
  },
  {
    "text": "If the i - th token is chosen , we replace the i - th token with ( 1 ) the [ MASK ] token 80 % of the time ( 2 ) a random token 10 % of the time ( 3 ) the unchanged i - th token 10 % of the time .",
    "id": 111,
    "annotator": 1,
    "annotation_id": 109,
    "created_at": "2022-10-17T22:22:57.289692Z",
    "updated_at": "2022-10-17T22:22:57.289718Z",
    "lead_time": 5.481
  },
  {
    "text": "The training data generator chooses 15 % of the token positions at random for prediction .",
    "id": 110,
    "annotator": 1,
    "annotation_id": 108,
    "created_at": "2022-10-17T22:22:50.601867Z",
    "updated_at": "2022-10-17T22:22:50.601895Z",
    "lead_time": 27.431
  },
  {
    "text": "To mitigate this , we do not always replace \" masked \" words with the actual [ MASK ] token .",
    "id": 109,
    "annotator": 1,
    "annotation_id": 107,
    "created_at": "2022-10-17T22:22:21.499249Z",
    "updated_at": "2022-10-17T22:22:21.499277Z",
    "lead_time": 2.02
  },
  {
    "text": "Although this allows us to obtain a bidirectional pre - trained model , a downside is that we are creating a mismatch between pre - training and fine - tuning , since the [ MASK ] token does not appear during fine - tuning .",
    "id": 108,
    "annotator": 1,
    "annotation_id": 106,
    "created_at": "2022-10-17T22:22:16.457509Z",
    "updated_at": "2022-10-17T22:22:16.457539Z",
    "lead_time": 2.351
  },
  {
    "text": "In contrast to denoising auto - encoders ( Vincent et al . , 2008 ) , we only predict the masked words rather than reconstructing the entire input .",
    "id": 107,
    "label": [
      {
        "start": 15,
        "end": 40,
        "text": "denoising auto - encoders",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 105,
    "created_at": "2022-10-17T22:21:39.886821Z",
    "updated_at": "2022-10-17T22:22:12.995982Z",
    "lead_time": 40.738
  },
  {
    "text": "In this case , the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary , as in a standard LM .",
    "id": 105,
    "annotator": 1,
    "annotation_id": 103,
    "created_at": "2022-10-17T22:21:12.417751Z",
    "updated_at": "2022-10-17T22:21:12.417774Z",
    "lead_time": 1.607
  },
  {
    "text": "In order to train a deep bidirectional representation , we simply mask some percentage of the input tokens at random , and then predict those masked tokens .",
    "id": 103,
    "annotator": 1,
    "annotation_id": 101,
    "created_at": "2022-10-17T22:20:59.712139Z",
    "updated_at": "2022-10-17T22:20:59.712166Z",
    "lead_time": 4.083
  },
  {
    "text": "former is often referred to as a \" Transformer encoder \" while the left - context - only version is referred to as a \" Transformer decoder \" since it can be used for text generation .",
    "id": 102,
    "annotator": 1,
    "annotation_id": 100,
    "created_at": "2022-10-17T22:20:54.657477Z",
    "updated_at": "2022-10-17T22:20:54.657503Z",
    "lead_time": 14.995
  },
  {
    "text": "Unfortunately , standard conditional language models can only be trained left - to - right or right - to - left , since bidirectional conditioning would allow each word to indirectly \" see itself \" , and the model could trivially predict the target word in a multi - layered context .",
    "id": 101,
    "annotator": 1,
    "annotation_id": 99,
    "created_at": "2022-10-17T22:20:17.075485Z",
    "updated_at": "2022-10-17T22:20:17.075511Z",
    "lead_time": 11.299
  },
  {
    "text": "This step is presented in the left part of Figure 1 .",
    "id": 99,
    "annotator": 1,
    "annotation_id": 98,
    "created_at": "2022-10-17T22:19:45.414458Z",
    "updated_at": "2022-10-17T22:19:45.414485Z",
    "lead_time": 1.317
  },
  {
    "text": "Instead , we pre - train BERT using two unsupervised tasks , described in this section .",
    "id": 98,
    "label": [
      {
        "start": 25,
        "end": 29,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 97,
    "created_at": "2022-10-17T22:19:42.510278Z",
    "updated_at": "2022-10-17T22:19:42.510301Z",
    "lead_time": 3.101
  },
  {
    "text": "( 2018 ) , we do not use traditional left - to - right or right - to - left language models to pre - train BERT .",
    "id": 97,
    "label": [
      {
        "start": 107,
        "end": 111,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 96,
    "created_at": "2022-10-17T22:19:32.413379Z",
    "updated_at": "2022-10-17T22:19:38.469147Z",
    "lead_time": 6.938000000000001
  },
  {
    "text": "( 2018a ) and Radford et al .",
    "id": 96,
    "annotator": 1,
    "annotation_id": 95,
    "created_at": "2022-10-17T22:19:29.995436Z",
    "updated_at": "2022-10-17T22:19:29.995466Z",
    "lead_time": 0.398
  },
  {
    "text": "Unlike Peters et al .",
    "id": 95,
    "annotator": 1,
    "annotation_id": 94,
    "created_at": "2022-10-17T22:19:28.440022Z",
    "updated_at": "2022-10-17T22:19:28.440045Z",
    "lead_time": 0.888
  },
  {
    "text": "Pre - training BERT .",
    "id": 94,
    "label": [
      {
        "start": 15,
        "end": 19,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 93,
    "created_at": "2022-10-17T22:19:26.456267Z",
    "updated_at": "2022-10-17T22:19:26.456331Z",
    "lead_time": 4.638
  },
  {
    "text": "A visualization of this construction can be seen in Figure 2 .",
    "id": 93,
    "annotator": 1,
    "annotation_id": 92,
    "created_at": "2022-10-17T22:19:20.576742Z",
    "updated_at": "2022-10-17T22:19:20.576769Z",
    "lead_time": 0.636
  },
  {
    "text": "For a given token , its input representation is constructed by summing the corresponding token , segment , and position embeddings .",
    "id": 92,
    "annotator": 1,
    "annotation_id": 91,
    "created_at": "2022-10-17T22:19:18.344326Z",
    "updated_at": "2022-10-17T22:19:18.344351Z",
    "lead_time": 1.72
  },
  {
    "text": "As shown in Figure 1 , we denote input embedding as E , the final hidden vector of the special [ CLS ] token as C ∈ R H , and the final hidden vector for the i th input token as T i ∈ R H .",
    "id": 91,
    "annotator": 1,
    "annotation_id": 90,
    "created_at": "2022-10-17T22:19:15.681688Z",
    "updated_at": "2022-10-17T22:19:15.681715Z",
    "lead_time": 2.121
  },
  {
    "text": "Second , we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B.",
    "id": 90,
    "annotator": 1,
    "annotation_id": 89,
    "created_at": "2022-10-17T22:19:12.303915Z",
    "updated_at": "2022-10-17T22:19:12.303935Z",
    "lead_time": 0.776
  },
  {
    "text": "First , we separate them with a special token ( [ SEP ] ) .",
    "id": 89,
    "annotator": 1,
    "annotation_id": 88,
    "created_at": "2022-10-17T22:19:10.571141Z",
    "updated_at": "2022-10-17T22:19:10.571170Z",
    "lead_time": 0.522
  },
  {
    "text": "We differentiate the sentences in two ways .",
    "id": 88,
    "annotator": 1,
    "annotation_id": 87,
    "created_at": "2022-10-17T22:19:08.907920Z",
    "updated_at": "2022-10-17T22:19:08.907946Z",
    "lead_time": 2.866
  },
  {
    "text": "Sentence pairs are packed together into a single sequence .",
    "id": 87,
    "annotator": 1,
    "annotation_id": 86,
    "created_at": "2022-10-17T22:19:04.972386Z",
    "updated_at": "2022-10-17T22:19:04.972410Z",
    "lead_time": 1.223
  },
  {
    "text": "The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks .",
    "id": 86,
    "annotator": 1,
    "annotation_id": 85,
    "created_at": "2022-10-17T22:19:01.682499Z",
    "updated_at": "2022-10-17T22:19:01.682525Z",
    "lead_time": 2.701
  },
  {
    "text": "The first token of every sequence is always a special classification token ( [ CLS ] ) .",
    "id": 85,
    "annotator": 1,
    "annotation_id": 84,
    "created_at": "2022-10-17T22:18:57.747128Z",
    "updated_at": "2022-10-17T22:18:57.747154Z",
    "lead_time": 2.348
  },
  {
    "text": "We use WordPiece embeddings ( Wu et al . , 2016 ) with a 30,000 token vocabulary .",
    "id": 84,
    "annotator": 1,
    "annotation_id": 83,
    "created_at": "2022-10-17T22:18:53.586802Z",
    "updated_at": "2022-10-17T22:18:53.586828Z",
    "lead_time": 4.788
  },
  {
    "text": "A \" sequence \" refers to the input token sequence to BERT , which may be a single sentence or two sentences packed together .",
    "id": 83,
    "label": [
      {
        "start": 53,
        "end": 57,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 82,
    "created_at": "2022-10-17T22:18:47.823623Z",
    "updated_at": "2022-10-17T22:18:47.823668Z",
    "lead_time": 9.857
  },
  {
    "text": "Throughout this work , a \" sentence \" can be an arbitrary span of contiguous text , rather than an actual linguistic sentence .",
    "id": 82,
    "annotator": 1,
    "annotation_id": 81,
    "created_at": "2022-10-17T22:18:36.665371Z",
    "updated_at": "2022-10-17T22:18:36.665400Z",
    "lead_time": 3.981
  },
  {
    "text": "4 Input / Output Representations To make BERT handle a variety of down - stream tasks , our input representation is able to unambiguously represent both a single sentence and a pair of sentences ( e.g. , Question , Answer ) in one token sequence .",
    "id": 81,
    "label": [
      {
        "start": 41,
        "end": 45,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 80,
    "created_at": "2022-10-17T22:18:30.718105Z",
    "updated_at": "2022-10-17T22:18:30.718126Z",
    "lead_time": 8.819
  },
  {
    "text": "Critically , however , the BERT Transformer uses bidirectional self - attention , while the GPT Transformer uses constrained self - attention where every token can only attend to context to its left .",
    "id": 80,
    "label": [
      {
        "start": 27,
        "end": 31,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 92,
        "end": 95,
        "text": "GPT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 79,
    "created_at": "2022-10-17T22:18:20.969566Z",
    "updated_at": "2022-10-17T22:18:20.969590Z",
    "lead_time": 18.322
  },
  {
    "text": "BERT BASE was chosen to have the same model size as OpenAI GPT for comparison purposes .",
    "id": 79,
    "label": [
      {
        "start": 0,
        "end": 9,
        "text": "BERT BASE",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 52,
        "end": 62,
        "text": "OpenAI GPT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 78,
    "created_at": "2022-10-17T22:18:01.588340Z",
    "updated_at": "2022-10-17T22:18:01.588362Z",
    "lead_time": 14.776
  },
  {
    "text": "3 We primarily report results on two model sizes : BERT BASE ( L=12 , H=768 , A=12 , Total Param - eters=110 M ) and BERT LARGE ( L=24 , H=1024 , A=16 , Total Parameters=340 M ) .",
    "id": 78,
    "label": [
      {
        "start": 51,
        "end": 60,
        "text": "BERT BASE",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 63,
        "end": 64,
        "text": "L",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 65,
        "end": 67,
        "text": "12",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 70,
        "end": 71,
        "text": "H",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 72,
        "end": 75,
        "text": "768",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 105,
        "end": 110,
        "text": "110 M",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 132,
        "end": 134,
        "text": "24",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 139,
        "end": 143,
        "text": "1024",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 137,
        "end": 138,
        "text": "H",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 130,
        "end": 131,
        "text": "L",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 117,
        "end": 127,
        "text": "BERT LARGE",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 80,
        "end": 82,
        "text": "12",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 146,
        "end": 147,
        "text": "A",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 148,
        "end": 150,
        "text": "16",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 153,
        "end": 169,
        "text": "Total Parameters",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 170,
        "end": 175,
        "text": "340 M",
        "labels": [
          "HyperparameterValue"
        ]
      },
      {
        "start": 85,
        "end": 104,
        "text": "Total Param - eters",
        "labels": [
          "HyperparameterName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 77,
    "created_at": "2022-10-17T22:17:44.752311Z",
    "updated_at": "2022-10-17T22:17:44.752333Z",
    "lead_time": 96.46
  },
  {
    "text": "( 2017 ) as well as excellent guides such as \" The Annotated Transformer . \" 2 In this work , we denote the number of layers ( i.e. , Transformer blocks ) as L , the hidden size as H , and the number of self - attention heads as A.",
    "id": 77,
    "label": [
      {
        "start": 108,
        "end": 124,
        "text": "number of layers",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 166,
        "end": 177,
        "text": "hidden size",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 193,
        "end": 225,
        "text": "number of self - attention heads",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 158,
        "end": 159,
        "text": "L",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 181,
        "end": 182,
        "text": "H",
        "labels": [
          "HyperparameterName"
        ]
      },
      {
        "start": 229,
        "end": 230,
        "text": "A",
        "labels": [
          "HyperparameterName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 76,
    "created_at": "2022-10-17T22:14:59.036728Z",
    "updated_at": "2022-10-17T22:16:03.819166Z",
    "lead_time": 85.221
  },
  {
    "text": "( 2017 ) and released in the tensor2tensor library .",
    "id": 75,
    "annotator": 1,
    "annotation_id": 74,
    "created_at": "2022-10-17T22:14:09.733988Z",
    "updated_at": "2022-10-17T22:14:25.013452Z",
    "lead_time": 11.478
  },
  {
    "text": "There is mini - mal difference between the pre - trained architecture and the final downstream architecture .",
    "id": 73,
    "annotator": 1,
    "annotation_id": 72,
    "created_at": "2022-10-17T22:12:58.234784Z",
    "updated_at": "2022-10-17T22:12:58.234813Z",
    "lead_time": 2.181
  },
  {
    "text": "The question - answering example in Figure 1 will serve as a running example for this section .",
    "id": 71,
    "label": [
      {
        "start": 4,
        "end": 24,
        "text": "question - answering",
        "labels": [
          "TaskName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 71,
    "created_at": "2022-10-17T22:12:54.400234Z",
    "updated_at": "2022-10-17T22:12:54.400260Z",
    "lead_time": 6.417
  },
  {
    "text": "A distinctive feature of BERT is its unified architecture across different tasks .",
    "id": 72,
    "label": [
      {
        "start": 25,
        "end": 29,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 70,
    "created_at": "2022-10-17T22:12:47.307117Z",
    "updated_at": "2022-10-17T22:12:47.307143Z",
    "lead_time": 6.596
  },
  {
    "text": "Each downstream task has separate fine - tuned models , even though they are initialized with the same pre - trained parameters .",
    "id": 70,
    "annotator": 1,
    "annotation_id": 69,
    "created_at": "2022-10-17T22:12:21.487011Z",
    "updated_at": "2022-10-17T22:12:21.487035Z",
    "lead_time": 1.836
  },
  {
    "text": "For finetuning , the BERT model is first initialized with the pre - trained parameters , and all of the parameters are fine - tuned using labeled data from the downstream tasks .",
    "id": 69,
    "label": [
      {
        "start": 21,
        "end": 25,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 68,
    "created_at": "2022-10-17T22:12:16.892859Z",
    "updated_at": "2022-10-17T22:12:16.893045Z",
    "lead_time": 7.358
  },
  {
    "text": "During pre - training , the model is trained on unlabeled data over different pre - training tasks .",
    "id": 68,
    "annotator": 1,
    "annotation_id": 67,
    "created_at": "2022-10-17T22:12:08.399455Z",
    "updated_at": "2022-10-17T22:12:08.399482Z",
    "lead_time": 1.691
  },
  {
    "text": "There are two steps in our framework : pre - training and fine - tuning .",
    "id": 67,
    "annotator": 1,
    "annotation_id": 66,
    "created_at": "2022-10-17T22:12:01.521984Z",
    "updated_at": "2022-10-17T22:12:01.522005Z",
    "lead_time": 1.572
  },
  {
    "text": "We introduce BERT and its detailed implementation in this section .",
    "id": 66,
    "label": [
      {
        "start": 13,
        "end": 17,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 65,
    "created_at": "2022-10-17T22:11:58.360135Z",
    "updated_at": "2022-10-17T22:11:58.360162Z",
    "lead_time": 6.913
  },
  {
    "text": "Computer vision research has also demonstrated the importance of transfer learning from large pre - trained models , where an effective recipe is to fine - tune models pre - trained with Ima - geNet ( Deng et al . , 2009;Yosinski et al . , 2014 ) .",
    "id": 64,
    "label": [
      {
        "start": 187,
        "end": 198,
        "text": "Ima - geNet",
        "labels": [
          "DatasetName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 64,
    "created_at": "2022-10-17T22:11:08.977938Z",
    "updated_at": "2022-10-17T22:11:48.804461Z",
    "lead_time": 13.168000000000001
  },
  {
    "text": "BERT .",
    "id": 65,
    "label": [
      {
        "start": 0,
        "end": 4,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 63,
    "created_at": "2022-10-17T22:11:06.471744Z",
    "updated_at": "2022-10-17T22:11:06.471769Z",
    "lead_time": 6.233
  },
  {
    "text": "There has also been work showing effective transfer from supervised tasks with large datasets , such as natural language inference ( Conneau et al . , 2017 ) and machine translation ( McCann et al . , 2017 ) .",
    "id": 63,
    "label": [
      {
        "start": 104,
        "end": 130,
        "text": "natural language inference",
        "labels": [
          "TaskName"
        ]
      },
      {
        "start": 162,
        "end": 181,
        "text": "machine translation",
        "labels": [
          "TaskName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 62,
    "created_at": "2022-10-17T22:10:50.157526Z",
    "updated_at": "2022-10-17T22:10:50.157551Z",
    "lead_time": 15.193
  },
  {
    "text": "Transfer Learning from Supervised Data .",
    "id": 62,
    "annotator": 1,
    "annotation_id": 61,
    "created_at": "2022-10-17T22:10:34.021167Z",
    "updated_at": "2022-10-17T22:10:34.021196Z",
    "lead_time": 6.075
  },
  {
    "text": "ing and auto - encoder objectives have been used for pre - training such models ( Howard and Ruder , 2018;Radford et al . , 2018;Dai and Le , 2015 ) .",
    "id": 61,
    "label": [
      {
        "start": 8,
        "end": 22,
        "text": "auto - encoder",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 60,
    "created_at": "2022-10-17T22:10:24.734221Z",
    "updated_at": "2022-10-17T22:10:24.734248Z",
    "lead_time": 11.766
  },
  {
    "text": "E M ' C T 1 T [ SEP ] ...",
    "id": 60,
    "annotator": 1,
    "annotation_id": 59,
    "created_at": "2022-10-17T22:10:11.305608Z",
    "updated_at": "2022-10-17T22:10:11.305638Z",
    "lead_time": 0.126
  },
  {
    "text": "E N E 1 ' ...",
    "id": 59,
    "annotator": 1,
    "annotation_id": 58,
    "created_at": "2022-10-17T22:10:10.360592Z",
    "updated_at": "2022-10-17T22:10:10.360617Z",
    "lead_time": 0.235
  },
  {
    "text": "...",
    "id": 58,
    "annotator": 1,
    "annotation_id": 57,
    "created_at": "2022-10-17T22:10:08.472852Z",
    "updated_at": "2022-10-17T22:10:08.472877Z",
    "lead_time": 0.473
  },
  {
    "text": "...",
    "id": 57,
    "annotator": 1,
    "annotation_id": 56,
    "created_at": "2022-10-17T22:10:05.317713Z",
    "updated_at": "2022-10-17T22:10:05.317742Z",
    "lead_time": 0.194
  },
  {
    "text": "E M ' C T 1 T [ SEP ] ...",
    "id": 56,
    "annotator": 1,
    "annotation_id": 55,
    "created_at": "2022-10-17T22:10:03.930905Z",
    "updated_at": "2022-10-17T22:10:03.930929Z",
    "lead_time": 0.318
  },
  {
    "text": "E N E 1 ' ...",
    "id": 55,
    "annotator": 1,
    "annotation_id": 54,
    "created_at": "2022-10-17T22:10:01.718118Z",
    "updated_at": "2022-10-17T22:10:01.718138Z",
    "lead_time": 3.241
  },
  {
    "text": "Left - to - right language model - BERT BERT E [ CLS ] E 1 E [ SEP ] ...",
    "id": 54,
    "label": [
      {
        "start": 35,
        "end": 39,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 40,
        "end": 44,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 53,
    "created_at": "2022-10-17T22:09:52.470029Z",
    "updated_at": "2022-10-17T22:09:52.470058Z",
    "lead_time": 9.755
  },
  {
    "text": "The advantage of these approaches is that few parameters need to be learned from scratch .",
    "id": 52,
    "annotator": 1,
    "annotation_id": 52,
    "created_at": "2022-10-17T22:09:12.682846Z",
    "updated_at": "2022-10-17T22:09:12.682873Z",
    "lead_time": 1.601
  },
  {
    "text": "At least partly due to this advantage , OpenAI GPT ( Radford et al . , 2018 ) achieved previously state - of - the - art results on many sentencelevel tasks from the GLUE benchmark ( Wang et al . , 2018a ) .",
    "id": 53,
    "label": [
      {
        "start": 40,
        "end": 50,
        "text": "OpenAI GPT",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 166,
        "end": 170,
        "text": "GLUE",
        "labels": [
          "DatasetName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 51,
    "created_at": "2022-10-17T22:08:28.708417Z",
    "updated_at": "2022-10-17T22:08:28.708440Z",
    "lead_time": 126.817
  },
  {
    "text": "As with the feature - based approaches , the first works in this direction only pre - trained word embedding parameters from unlabeled text ( Collobert and Weston , 2008 ) .",
    "id": 50,
    "annotator": 1,
    "annotation_id": 49,
    "created_at": "2022-10-17T22:05:43.241802Z",
    "updated_at": "2022-10-17T22:05:43.241828Z",
    "lead_time": 2.746
  },
  {
    "text": "Unsupervised Fine - tuning Approaches .",
    "id": 49,
    "annotator": 1,
    "annotation_id": 48,
    "created_at": "2022-10-17T22:05:38.191361Z",
    "updated_at": "2022-10-17T22:05:38.191387Z",
    "lead_time": 1.677
  },
  {
    "text": "Fedus et al .",
    "id": 47,
    "annotator": 1,
    "annotation_id": 46,
    "created_at": "2022-10-17T22:05:23.502587Z",
    "updated_at": "2022-10-17T22:05:23.502612Z",
    "lead_time": 7.935
  },
  {
    "text": "( 2016 ) proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs . Similar to ELMo , their model is feature - based and not deeply bidirectional .",
    "id": 46,
    "label": [
      {
        "start": 152,
        "end": 156,
        "text": "ELMo",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 133,
        "end": 138,
        "text": "LSTMs",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 45,
    "created_at": "2022-10-17T22:05:13.950211Z",
    "updated_at": "2022-10-17T22:05:13.950240Z",
    "lead_time": 6.3
  },
  {
    "text": "When integrating contextual word embeddings with existing task - specific architectures , ELMo advances the state of the art for several major NLP benchmarks ( Peters et al . , 2018a ) including question answering ( Rajpurkar et al . , 2016 ) , sentiment analysis ( Socher et al . , 2013 ) , and named entity recognition ( Tjong Kim Sang and De Meulder , 2003 ) .",
    "id": 44,
    "label": [
      {
        "start": 90,
        "end": 94,
        "text": "ELMo",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 195,
        "end": 213,
        "text": "question answering",
        "labels": [
          "TaskName"
        ]
      },
      {
        "start": 296,
        "end": 320,
        "text": "named entity recognition",
        "labels": [
          "TaskName"
        ]
      },
      {
        "start": 245,
        "end": 263,
        "text": "sentiment analysis",
        "labels": [
          "TaskName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 43,
    "created_at": "2022-10-17T21:55:19.388098Z",
    "updated_at": "2022-10-17T22:04:20.271062Z",
    "lead_time": 36.985
  },
  {
    "text": "( 2018 ) , which uses unidirectional language models for pre - training , BERT uses masked language models to enable pretrained deep bidirectional representations .",
    "id": 27,
    "label": [
      {
        "start": 74,
        "end": 78,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 26,
    "created_at": "2022-10-17T21:53:09.152492Z",
    "updated_at": "2022-10-17T22:03:21.054739Z",
    "lead_time": 12.187999999999999
  },
  {
    "text": "For example , in OpenAI GPT , the authors use a left - toright architecture , where every token can only attend to previous tokens in the self - attention layers of the Transformer ( Vaswani et al . , 2017 ) .",
    "id": 18,
    "label": [
      {
        "start": 17,
        "end": 27,
        "text": "OpenAI GPT",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 169,
        "end": 180,
        "text": "Transformer",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 19,
    "created_at": "2022-10-17T21:50:12.062554Z",
    "updated_at": "2022-10-17T22:01:50.226435Z",
    "lead_time": 10.642
  },
  {
    "text": "We introduce a new language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers .",
    "id": 4,
    "label": [
      {
        "start": 56,
        "end": 60,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 80,
        "end": 135,
        "text": "Bidirectional Encoder Representations from Transformers",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 4,
    "created_at": "2022-10-17T21:43:53.049976Z",
    "updated_at": "2022-10-17T22:00:54.315900Z",
    "lead_time": 12.717
  },
  {
    "text": "Melamud et al .",
    "id": 45,
    "annotator": 1,
    "annotation_id": 44,
    "created_at": "2022-10-17T21:55:25.997735Z",
    "updated_at": "2022-10-17T21:55:25.997763Z",
    "lead_time": 1.298
  },
  {
    "text": "The contextual representation of each token is the concatenation of the left - to - right and right - to - left representations .",
    "id": 43,
    "annotator": 1,
    "annotation_id": 42,
    "created_at": "2022-10-17T21:54:47.602510Z",
    "updated_at": "2022-10-17T21:54:47.602534Z",
    "lead_time": 0.566
  },
  {
    "text": "They extract context - sensitive features from a left - to - right and a right - to - left language model .",
    "id": 42,
    "annotator": 1,
    "annotation_id": 41,
    "created_at": "2022-10-17T21:54:45.483882Z",
    "updated_at": "2022-10-17T21:54:45.483906Z",
    "lead_time": 0.997
  },
  {
    "text": "ELMo and its predecessor ( Peters et al . , 2017(Peters et al . , , 2018a ) ) generalize traditional word embedding research along a different dimension .",
    "id": 41,
    "label": [
      {
        "start": 0,
        "end": 4,
        "text": "ELMo",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 40,
    "created_at": "2022-10-17T21:54:43.308723Z",
    "updated_at": "2022-10-17T21:54:43.308745Z",
    "lead_time": 6.49
  },
  {
    "text": "These approaches have been generalized to coarser granularities , such as sentence embeddings ( Kiros et al . , 2015;Logeswaran and Lee , 2018 ) or paragraph embeddings ( Le and Mikolov , 2014 ) .",
    "id": 39,
    "annotator": 1,
    "annotation_id": 38,
    "created_at": "2022-10-17T21:54:17.155237Z",
    "updated_at": "2022-10-17T21:54:17.155263Z",
    "lead_time": 1.978
  },
  {
    "text": "To pretrain word embedding vectors , left - to - right language modeling objectives have been used ( Mnih and Hinton , 2009 ) , as well as objectives to discriminate correct from incorrect words in left and right context ( Mikolov et al . , 2013 ) .",
    "id": 38,
    "annotator": 1,
    "annotation_id": 37,
    "created_at": "2022-10-17T21:54:13.334296Z",
    "updated_at": "2022-10-17T21:54:13.334327Z",
    "lead_time": 3.685
  },
  {
    "text": "Pre - trained word embeddings are an integral part of modern NLP systems , offering significant improvements over embeddings learned from scratch ( Turian et al . , 2010 ) .",
    "id": 37,
    "annotator": 1,
    "annotation_id": 36,
    "created_at": "2022-10-17T21:54:08.623921Z",
    "updated_at": "2022-10-17T21:54:08.623954Z",
    "lead_time": 2.965
  },
  {
    "text": "Learning widely applicable representations of words has been an active area of research for decades , including non - neural ( Brown et al . , 1992;Ando and Zhang , 2005;Blitzer et al . , 2006 ) and neural ( Mikolov et al . , 2013;Pennington et al . , 2014 ) methods .",
    "id": 36,
    "annotator": 1,
    "annotation_id": 35,
    "created_at": "2022-10-17T21:54:03.310866Z",
    "updated_at": "2022-10-17T21:54:03.310892Z",
    "lead_time": 2.762
  },
  {
    "text": "Unsupervised Feature - based Approaches .",
    "id": 35,
    "annotator": 1,
    "annotation_id": 34,
    "created_at": "2022-10-17T21:53:59.104694Z",
    "updated_at": "2022-10-17T21:53:59.104715Z",
    "lead_time": 1.332
  },
  {
    "text": "There is a long history of pre - training general language representations , and we briefly review the most widely - used approaches in this section .",
    "id": 34,
    "annotator": 1,
    "annotation_id": 33,
    "created_at": "2022-10-17T21:53:56.785231Z",
    "updated_at": "2022-10-17T21:53:56.785253Z",
    "lead_time": 2.331
  },
  {
    "text": "Related Work .",
    "id": 33,
    "annotator": 1,
    "annotation_id": 32,
    "created_at": "2022-10-17T21:53:52.941189Z",
    "updated_at": "2022-10-17T21:53:52.941215Z",
    "lead_time": 0.185
  },
  {
    "text": "• BERT advances the state of the art for eleven NLP tasks .",
    "id": 31,
    "label": [
      {
        "start": 2,
        "end": 6,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 30,
    "created_at": "2022-10-17T21:53:41.115241Z",
    "updated_at": "2022-10-17T21:53:41.115265Z",
    "lead_time": 7.997
  },
  {
    "text": "BERT is the first finetuning based representation model that achieves state - of - the - art performance on a large suite of sentence - level and token - level tasks , outperforming many task - specific architectures .",
    "id": 30,
    "label": [
      {
        "start": 0,
        "end": 4,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 29,
    "created_at": "2022-10-17T21:53:28.158621Z",
    "updated_at": "2022-10-17T21:53:28.158648Z",
    "lead_time": 7.366
  },
  {
    "text": "( 2018a ) , which uses a shallow concatenation of independently trained left - to - right and right - to - left LMs . • We show that pre - trained representations reduce the need for many heavily - engineered taskspecific architectures .",
    "id": 29,
    "annotator": 1,
    "annotation_id": 28,
    "created_at": "2022-10-17T21:53:19.352123Z",
    "updated_at": "2022-10-17T21:53:19.352148Z",
    "lead_time": 3.676
  },
  {
    "text": "This is also in contrast to Peters et al .",
    "id": 28,
    "annotator": 1,
    "annotation_id": 27,
    "created_at": "2022-10-17T21:53:14.113959Z",
    "updated_at": "2022-10-17T21:53:14.113986Z",
    "lead_time": 1.949
  },
  {
    "text": "Unlike Radford et al .",
    "id": 26,
    "annotator": 1,
    "annotation_id": 25,
    "created_at": "2022-10-17T21:52:41.854741Z",
    "updated_at": "2022-10-17T21:52:41.854764Z",
    "lead_time": 0.721
  },
  {
    "text": "Unlike left - toright language model pre - training , the MLM objective enables the representation to fuse the left and the right context , which allows us to pretrain a deep bidirectional Transformer .",
    "id": 23,
    "annotator": 1,
    "annotation_id": 23,
    "created_at": "2022-10-17T21:50:51.886316Z",
    "updated_at": "2022-10-17T21:50:51.886342Z",
    "lead_time": 2.136
  },
  {
    "text": "The masked language model randomly masks some of the tokens from the input , and the objective is to predict the original vocabulary i d of the masked word based only on its context .",
    "id": 22,
    "annotator": 1,
    "annotation_id": 22,
    "created_at": "2022-10-17T21:50:48.694926Z",
    "updated_at": "2022-10-17T21:50:48.694954Z",
    "lead_time": 7.052
  },
  {
    "text": "In this paper , we improve the fine - tuning based approaches by proposing BERT : Bidirectional Encoder Representations from Transformers .",
    "id": 20,
    "label": [
      {
        "start": 75,
        "end": 79,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 82,
        "end": 137,
        "text": "Bidirectional Encoder Representations from Transformers",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 21,
    "created_at": "2022-10-17T21:50:40.187178Z",
    "updated_at": "2022-10-17T21:50:40.187205Z",
    "lead_time": 0.181
  },
  {
    "text": "Such restrictions are sub - optimal for sentence - level tasks , and could be very harmful when applying finetuning based approaches to token - level tasks such as question answering , where it is crucial to incorporate context from both directions .",
    "id": 19,
    "label": [
      {
        "start": 164,
        "end": 182,
        "text": "question answering",
        "labels": [
          "TaskName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 18,
    "created_at": "2022-10-17T21:50:09.058402Z",
    "updated_at": "2022-10-17T21:50:09.058426Z",
    "lead_time": 14.442
  },
  {
    "text": "The major limitation is that standard language models are unidirectional , and this limits the choice of architectures that can be used during pre - training .",
    "id": 17,
    "annotator": 1,
    "annotation_id": 17,
    "created_at": "2022-10-17T21:49:40.054543Z",
    "updated_at": "2022-10-17T21:49:40.054575Z",
    "lead_time": 4.571
  },
  {
    "text": "We argue that current techniques restrict the power of the pre - trained representations , especially for the fine - tuning approaches .",
    "id": 16,
    "annotator": 1,
    "annotation_id": 16,
    "created_at": "2022-10-17T21:49:34.092641Z",
    "updated_at": "2022-10-17T21:49:34.092667Z",
    "lead_time": 1.952
  },
  {
    "text": "The two approaches share the same objective function during pre - training , where they use unidirectional language models to learn general language representations .",
    "id": 15,
    "annotator": 1,
    "annotation_id": 15,
    "created_at": "2022-10-17T21:49:31.012303Z",
    "updated_at": "2022-10-17T21:49:31.012328Z",
    "lead_time": 8.743
  },
  {
    "text": "The fine - tuning approach , such as the Generative Pre - trained Transformer ( OpenAI GPT ) ( Radford et al . , 2018 ) , introduces minimal task - specific parameters , and is trained on the downstream tasks by simply fine - tuning all pretrained parameters .",
    "id": 14,
    "label": [
      {
        "start": 80,
        "end": 90,
        "text": "OpenAI GPT",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 41,
        "end": 77,
        "text": "Generative Pre - trained Transformer",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 14,
    "created_at": "2022-10-17T21:49:20.065292Z",
    "updated_at": "2022-10-17T21:49:20.065321Z",
    "lead_time": 0.927
  },
  {
    "text": "The feature - based approach , such as ELMo ( Peters et al . , 2018a ) , uses task - specific architectures that include the pre - trained representations as additional features .",
    "id": 13,
    "label": [
      {
        "start": 39,
        "end": 43,
        "text": "ELMo",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 13,
    "created_at": "2022-10-17T21:46:10.878794Z",
    "updated_at": "2022-10-17T21:46:10.878822Z",
    "lead_time": 43.058
  },
  {
    "text": "There are two existing strategies for applying pre - trained language representations to downstream tasks : feature - based and fine - tuning .",
    "id": 12,
    "annotator": 1,
    "annotation_id": 12,
    "created_at": "2022-10-17T21:45:26.885337Z",
    "updated_at": "2022-10-17T21:45:26.885366Z",
    "lead_time": 10.756
  },
  {
    "text": "These include sentence - level tasks such as natural language inference ( Bowman et al . , 2015;Williams et al . , 2018 ) and paraphrasing ( Dolan and Brockett , 2005 ) , which aim to predict the relationships between sentences by analyzing them holistically , as well as token - level tasks such as named entity recognition and question answering , where models are required to produce fine - grained output at the token level ( Tjong Kim Sang and De Meulder , 2003;Rajpurkar et al . , 2016 ) .",
    "id": 11,
    "label": [
      {
        "start": 45,
        "end": 71,
        "text": "natural language inference",
        "labels": [
          "TaskName"
        ]
      },
      {
        "start": 300,
        "end": 324,
        "text": "named entity recognition",
        "labels": [
          "TaskName"
        ]
      },
      {
        "start": 329,
        "end": 347,
        "text": "question answering",
        "labels": [
          "TaskName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 11,
    "created_at": "2022-10-17T21:45:14.998672Z",
    "updated_at": "2022-10-17T21:45:14.998701Z",
    "lead_time": 31.7
  },
  {
    "text": "Language model pre - training has been shown to be effective for improving many natural language processing tasks ( Dai and Le , 2015 ; Peters et al . , 2018a;Radford et al . , 2018;Howard and Ruder , 2018 ) .",
    "id": 10,
    "annotator": 1,
    "annotation_id": 10,
    "created_at": "2022-10-17T21:44:29.738376Z",
    "updated_at": "2022-10-17T21:44:34.973725Z",
    "lead_time": 5.649
  },
  {
    "text": "Introduction .",
    "id": 9,
    "annotator": 1,
    "annotation_id": 9,
    "created_at": "2022-10-17T21:44:24.136946Z",
    "updated_at": "2022-10-17T21:44:24.136971Z",
    "lead_time": 1.22
  },
  {
    "text": "BERT is conceptually simple and empirically powerful .",
    "id": 7,
    "label": [
      {
        "start": 0,
        "end": 4,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 7,
    "created_at": "2022-10-17T21:44:17.480654Z",
    "updated_at": "2022-10-17T21:44:17.480679Z",
    "lead_time": 1.844
  },
  {
    "text": "As a result , the pre - trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models for a wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications .",
    "id": 6,
    "label": [
      {
        "start": 32,
        "end": 36,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      },
      {
        "start": 174,
        "end": 192,
        "text": "question answering",
        "labels": [
          "TaskName"
        ]
      },
      {
        "start": 197,
        "end": 215,
        "text": "language inference",
        "labels": [
          "TaskName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 6,
    "created_at": "2022-10-17T21:44:11.435477Z",
    "updated_at": "2022-10-17T21:44:11.435501Z",
    "lead_time": 4.637
  },
  {
    "text": "Unlike recent language representation models ( Peters et al . , 2018a;Radford et al . , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers .",
    "id": 5,
    "label": [
      {
        "start": 97,
        "end": 101,
        "text": "BERT",
        "labels": [
          "MethodName"
        ]
      }
    ],
    "annotator": 1,
    "annotation_id": 5,
    "created_at": "2022-10-17T21:43:55.131908Z",
    "updated_at": "2022-10-17T21:43:55.131934Z",
    "lead_time": 1.022
  }
]